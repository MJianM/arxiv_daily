## Updated on 2026.02.13
> Usage instructions: [here](./docs/README.md#usage)

<details>
  <summary>Table of Contents</summary>
  <ol>
    <li><a href=#manipulation>Manipulation</a></li>
  </ol>
</details>

## Manipulation

|Publish Date|Title|Chinese Summary|Authors|PDF|Code|
|---|---|---|---|---|---|
|**2026-02-12**|**FAIL: Flow Matching Adversarial Imitation Learning for Image Generation**|现有流匹配模型的训练后对齐与模仿学习等价，但监督微调无法有效纠正策略漂移，偏好优化成本高昂。为此，本文提出了流匹配对抗模仿学习（FAIL），通过对抗训练最小化策略与专家之间的差异，无需显式奖励或配对比较。研究者们开发了两种算法：FAIL-PD利用可微ODE求解器提供低方差梯度，而FAIL-PG则是一个黑盒替代方案。实验结果表明，仅使用1.3万个演示数据，FAIL在FLUX模型微调后，在提示跟随和美学基准上达到了有竞争力的性能，并能有效泛化到离散图像和视频生成任务，同时作为正则化器缓解奖励欺骗问题。|Weidi Xie Team|[2602.12155](http://arxiv.org/abs/2602.12155)|null|
|**2026-02-12**|**GigaBrain-0.5M*: a VLA That Learns From World Model-Based Reinforcement Learning**|当前视觉-语言-动作（VLA）模型在直接预测多步动作时，受限于场景理解和未来预判能力。针对此问题，本文提出GigaBrain-0.5M*，该VLA模型通过基于世界模型的强化学习进行训练，利用了视频世界模型强大的时空推理和准确预测能力。该模型构建在已在超过10,000小时机器人操作数据上预训练的GigaBrain-0.5之上，并整合了RAMP（通过世界模型条件策略的强化学习）以实现鲁棒的跨任务适应性。实验结果显示，RAMP在RECAP基线上实现了约30%的显著性能提升，尤其在洗衣折叠、箱子打包和浓缩咖啡制作等复杂任务上，并展示了可靠的长周期执行能力。|Zheng Zhu Team|[2602.12099](http://arxiv.org/abs/2602.12099)|**[link](https://gigabrain05m.github.io/)**|
|**2026-02-12**|**Affordance-Graphed Task Worlds: Self-Evolving Task Generation for Scalable Embodied Learning**|在现实世界中训练机器人策略成本高昂且难以扩展，而现有生成式模拟方法难以生成逻辑连贯的长周期任务，且开环执行无法应对动态物理不确定性。为解决这些挑战，本文提出了Affordance-Graphed Task Worlds (AGT-World) 框架，该框架能根据真实世界观察自主构建交互式模拟环境及相应的机器人任务策略。AGT-World将任务空间形式化为结构化图，实现复杂目标的精确分层分解，并引入了带有混合反馈的自进化机制（结合VLM推理和几何验证）来自主优化策略。广泛实验表明，该方法在成功率和泛化能力上显著优于现有方法，实现了提案、执行和修正的自改进循环，从而促进可扩展的机器人学习。|Changshui Zhang Team|[2602.12065](http://arxiv.org/abs/2602.12065)|null|
|**2026-02-12**|**HoloBrain-0 Technical Report**|为弥合基础模型研究与可靠的真实世界机器人部署之间的差距，本文提出了HoloBrain-0，这是一个全面的视觉-语言-动作（VLA）框架。其核心是一个新颖的VLA架构，显式整合了机器人本体先验信息（如多视角相机参数和运动学描述URDF），以增强3D空间推理并支持多样化的机器人本体。通过“预训练-后训练”范式，HoloBrain-0在RoboTwin 2.0、LIBERO和GenieSim等仿真基准上，以及具有挑战性的长周期真实世界操作任务中取得了最先进的成果。值得一提的是，其0.2B参数的高效变体能与大得多的基线模型媲美，并支持低延迟的设备部署。为加速研究和实际应用，研究者们全面开源了HoloBrain生态系统。|Zhizhong Su Team|[2602.12062](http://arxiv.org/abs/2602.12062)|null|
|**2026-02-12**|**When would Vision-Proprioception Policies Fail in Robotic Manipulation?**|本体信息对机器人精确伺服控制至关重要，但现有研究发现，在机器人运动转换阶段，视觉-本体策略中的视觉模态作用有限，策略倾向于更快地减少损失的本体信号，从而抑制了视觉模态的学习。为解决此问题，本文提出了基于阶段引导的梯度调整（GAP）算法。该算法利用本体信息估计轨迹中每个时间步属于运动转换阶段的概率，并据此自适应地调整本体梯度的幅度，以实现视觉-本体策略内的动态协作。广泛实验证明，GAP在仿真和真实世界、单臂和双臂设置以及与多种模型兼容性方面均适用，并能有效生成鲁棒且可泛化的视觉-本体策略。|Di Hu Team|[2602.12032](http://arxiv.org/abs/2602.12032)|null|
|**2026-02-12**|**Accelerating Robotic Reinforcement Learning with Agent Guidance**|强化学习在机器人泛化操作技能学习中潜力巨大，但其在现实世界的应用受限于严重的样本效率问题。现有的人机协作（HIL）方法通过人类纠正加速训练，却面临可扩展性瓶颈和高方差。针对此，本文提出了Agent-guided Policy Search (AGPS) 框架，通过引入多模态智能体取代人类监督者，实现训练流程的自动化。AGPS的核心思想是将智能体视为语义世界模型，注入内在价值先验来构造物理探索，并通过可执行工具提供精确的校正路点和空间约束以剪枝探索。实验结果表明，AGPS在精度插入和可变形物体操作等任务上均优于HIL方法，显著提高了样本效率，为无需人力的可扩展机器人学习铺平了道路。|Yaodong Yang Team|[2602.11978](http://arxiv.org/abs/2602.11978)|null|
|**2026-02-12**|**Robot-DIFT: Distilling Diffusion Features for Geometrically Consistent Visuomotor Control**|本文提出，通用机器人操作中的关键瓶颈可能在于当前视觉骨干网络与闭环控制物理需求之间的结构不匹配，因为现有视觉编码器虽然优化语义不变性，却忽视了操作所需的几何敏感性。生成式扩散模型虽能编码几何依赖，但直接部署存在随机不稳定性、推理延迟和表示漂移。为弥合这一差距，研究者们提出了Robot-DIFT框架，通过流形蒸馏将几何信息源与推理过程解耦。该方法将冻结的扩散教师模型蒸馏到确定性的空间-语义特征金字塔网络（S2-FPN），从而保留了生成模型的丰富几何先验，同时确保了时间稳定性、实时执行和抗漂移鲁棒性。实验证明，Robot-DIFT在几何一致性和控制性能上均优于领先的判别性基线模型。|Georgia Chalvatzaki Team|[2602.11934](http://arxiv.org/abs/2602.11934)|null|
|**2026-02-12**|**JEPA-VLA: Video Predictive Embedding is Needed for VLA Models**|尽管基于预训练视觉-语言模型（VLMs）的VLA模型在机器人操作方面取得了进展，但仍存在样本效率低和泛化能力有限的问题，这与被忽视的预训练视觉表示不足有关。研究发现，现有VLA中常用的视觉表示在捕获关键任务相关环境信息和诱导有效策略先验方面均不足。相比之下，在视频上预训练的预测嵌入，特别是V-JEPA 2，能够灵活地舍弃不可预测的环境因素并编码任务相关的时态动态，从而有效弥补现有视觉表示的不足。基于这些观察，本文提出了JEPA-VLA，一种简单而有效的方法，将预测嵌入自适应地集成到现有VLA中。实验结果表明，JEPA-VLA在LIBERO、LIBERO-plus、RoboTwin2.0和真实机器人任务等一系列基准上带来了显著的性能提升。|Mingsheng Long Team|[2602.11832](http://arxiv.org/abs/2602.11832)|null|
|**2026-02-12**|**Clutt3R-Seg: Sparse-view 3D Instance Segmentation for Language-grounded Grasping in Cluttered Scenes**|在杂乱环境中，遮挡、有限视角和噪声掩码会严重影响3D实例分割性能，从而阻碍语言引导机器人抓取。为解决这些挑战，本文提出了Clutt3R-Seg，一个用于杂乱场景中语言引导抓取的零样本鲁棒3D实例分割流水线。其核心思想是引入语义线索的分层实例树，通过跨视图分组和条件替换，利用噪声掩码作为信息线索，抑制过分割和欠分割，从而产生视图一致的掩码和鲁棒的3D实例。为处理多阶段任务中的场景变化，该方法还引入了一致性感知更新机制。Clutt3R-Seg在合成和真实世界数据集上，以及真实机器人上的评估均表明，它在杂乱和稀疏视图场景中持续优于最先进的基线，即使在重度杂乱序列中也表现出色。|Ayoung Kim Team|[2602.11660](http://arxiv.org/abs/2602.11660)|null|
|**2026-02-12**|**ViTaS: Visual Tactile Soft Fusion Contrastive Learning for Visuomotor Learning**|触觉信息在机器人操作中日益受到关注，但现有方法主要关注视觉和触觉特征的直接拼接对齐，导致在遮挡场景中效果不佳，且忽视了两种模态固有的互补性。为解决此问题，本文提出了ViTaS框架，一个结合视觉和触觉信息来引导智能体行为的简单而有效的方法。研究者们引入了Soft Fusion对比学习（一种改进的对比学习方法）和CVAE模块，以充分利用视觉-触觉表示中的对齐和互补性。实验结果表明，ViTaS在12个仿真和3个真实世界环境中均验证了其有效性，并显著优于现有基线方法。|Huazhe Xu Team|[2602.11643](http://arxiv.org/abs/2602.11643)|null|
|**2026-02-12**|**EasyMimic: A Low-Cost Framework for Robot Imitation Learning from Human Videos**|针对机器人模仿学习中真实世界数据收集成本高昂的问题，尤其对低成本家用机器人而言，本研究提出了EasyMimic框架。该方法通过标准RGB相机捕获人类视频演示，提取3D手部轨迹，并利用动作对齐模块将其映射到机器人控制空间。为弥合人机领域差距，引入了手部视觉增强策略，并通过协同训练在处理过的人类数据和少量机器人数据上快速适应新任务。实验表明，EasyMimic在LeRobot平台上实现了高效的操作任务学习，显著降低了对昂贵机器人数据的依赖。|Qin Jin Team|[2602.11464](http://arxiv.org/abs/2602.11464)|null|
|**2026-02-12**|**Scaling World Model for Hierarchical Manipulation Policies**|针对视觉-语言-动作（VLA）模型在分布外（OOD）场景中泛化能力不足的挑战，本研究提出了分层视觉-语言-动作框架VISTA。该框架利用大规模预训练世界模型作为高级规划器，进行鲁棒且可泛化的视觉子目标任务分解，并将任务划分为带有目标图像的子任务序列。低级VLA策略则依据文本和视觉指导生成动作。实验结果表明，与原始文本目标相比，世界模型合成的目标图像为低级策略提供了更具视觉和物理基础的细节，使VLA在OOD场景中的性能显著提升，从14%增至69%，明显优于现有基线。|Xinghang Li Team|[2602.10983](http://arxiv.org/abs/2602.10983)|null|
|**2026-02-11**|**Human Preference Modeling Using Visual Motion Prediction Improves Robot Skill Learning from Egocentric Human Video**|针对传统从以自我为中心的人类视频中学习机器人行为时，奖励函数构建和跨领域迁移的局限性，本研究提出了一种通过建模人类偏好来学习机器人行为的方法。该方法通过学习预测后续图像中跟踪点的运动来定义奖励函数，其依据是机器人行为中预测与观察到的物体运动的一致性。随后，利用改进的Soft Actor Critic算法，并结合少量机器人演示，在真实机器人上优化策略。实验结果表明，该奖励模型学习到的策略在模拟和真实机器人的多项任务中均达到或超越了现有方法。|Christopher G. Atkeson Team|[2602.11393](http://arxiv.org/abs/2602.11393)|null|
|**2026-02-11**|**MolmoSpaces: A Large-Scale Open Ecosystem for Robot Navigation and Manipulation**|鉴于机器人大规模部署对环境多样性和鲁棒性的高要求，而现有基准测试缺乏足够的多样性且物理评估成本高昂，本研究推出了MolmoSpaces生态系统。该系统包含超过23万个多样化的室内环境和13万个丰富标注的物体，兼容多种模拟器，支持静态/移动操作、导航等具身任务。MolmoSpaces-Bench基准套件包含8个任务，用于评估机器人在复杂场景中的交互能力。实验结果显示，该基准测试具有良好的模拟到真实相关性，并成功验证了新型零样本策略的性能提升。|Ranjay Krishna Team|[2602.11337](http://arxiv.org/abs/2602.11337)|null|
|**2026-02-11**|**YOR: Your Own Mobile Manipulator for Generalizable Robotics**|针对低成本移动操作机器人平台的最优形态问题，本研究提出并设计了YOR，一个开源、低成本的移动操纵器。YOR集成了全向基座、伸缩式垂直升降机构和两个带抓手的机械臂，实现了全身移动和操作能力。其设计强调模块化、易于组装和经济性，物料清单成本低于10,000美元。实验证明，YOR能够完成需要协调全身控制、双手操作和自主导航的复杂任务，以远低于现有平台的成本提供了具有竞争力的移动操作功能。|Zichen Jeff Cui Team|[2602.11150](http://arxiv.org/abs/2602.11150)|null|
|**2026-02-11**|**ABot-M0: VLA Foundation Model for Robotic Manipulation with Action Manifold Learning**|针对跨多样硬件构建通用具身智能体时数据碎片化、表示不一致等挑战，本研究提出了ABot-M0框架。该框架通过系统的数据整理流程构建了包含600多万条轨迹的UniACT-dataset，并联合优化模型架构和训练策略，实现异构数据的统一高效表示。基于“动作流形假设”，引入动作流形学习（AML）直接预测干净连续的动作序列，提升效率和稳定性。此外，通过双流机制整合VLM语义和多视图输入实现模块化感知。实验证实了各组件的独立效益和累加效果，提升了跨平台泛化能力。|Mu Xu Team|[2602.11236](http://arxiv.org/abs/2602.11236)|**[link](https://amap-cvlab.github.io/ABot-Manipulation/)**|
|**2026-02-11**|**OSIL: Learning Offline Safe Imitation Policies with Safety Inferred from Non-preferred Trajectories**|针对离线安全模仿学习（IL）中缺乏每时间步安全成本或奖励信息的问题，同时考虑到在线学习的风险和安全成本指定的难度，本研究提出了一种新型离线安全IL算法OSIL。该算法通过从“非偏好轨迹”中推断安全性，将安全策略学习重构为约束马尔可夫决策过程（CMDP），并通过学习一个估计非偏好行为可能性的成本模型来解决。实验结果表明，OSIL能够完全从离线演示中学习到更安全且不影响奖励性能的策略，优于多种基线方法。|Balaraman Ravindran Team|[2602.11018](http://arxiv.org/abs/2602.11018)|null|
|**2026-02-11**|**Towards Learning a Generalizable 3D Scene Representation from 2D Observations**|针对现有3D占用预测方法多在相机坐标系中操作，难以直接应用于机器人操纵的问题，本研究提出了一种可泛化的神经辐射场方法。该模型能够从以机器人自我为中心的观察中预测3D工作空间占用，并在全局工作空间框架中构建表示。它能集成灵活的源视图，并无需场景特定微调即可泛化到未见物体排列。在类人机器人上的实验表明，模型在40个真实场景上训练后，实现了26毫米的重建误差（包括遮挡区域），验证了其超越传统立体视觉方法，推断完整3D占用信息的能力。|Stefan Wermter Team|[2602.10943](http://arxiv.org/abs/2602.10943)|null|
|**2026-02-11**|**Semi-Supervised Cross-Domain Imitation Learning**|鉴于跨域模仿学习（CDIL）在专家数据收集成本高昂时的价值，但现有监督或无监督方法存在依赖显式对齐或稳定性差的问题，本研究引入了半监督CDIL（SS-CDIL）设置。本研究提出首个具有理论依据的SS-CDIL算法，该方法仅使用少量目标专家演示和未标记的不完善轨迹。为解决领域差异，引入了新颖的跨域损失函数学习域间状态-动作映射，并设计了自适应权重函数来平衡源域和目标域知识。实验证明，该方法在最小监督下实现了稳定且数据高效的策略学习，性能优于基线。|Ping-Chun Hsieh Team|[2602.10793](http://arxiv.org/abs/2602.10793)|null|
|**2026-02-11**|**Say, Dream, and Act: Learning Video World Models for Instruction-Driven Robot Manipulation**|针对机器人操作中缺乏对环境演化预测能力导致的错误和低效，以及现有视觉-语言模型（VLM）和世界模型在预测未来状态或生成空间一致帧上的局限性，本研究提出了一个用于快速、预测性视频条件动作的框架。该方法首先选择并调整鲁棒的视频生成模型以确保可靠预测，然后采用对抗蒸馏进行快速视频生成，最后训练一个动作模型，利用生成的视频和真实观察纠正空间错误。实验表明，该方法生成的视频预测在时间连贯性和空间准确性上表现出色，显著提升了具身一致性、空间指代能力和任务完成率。|Yanwei Fu Team|[2602.10717](http://arxiv.org/abs/2602.10717)|null|

<p align=right>(<a href=#updated-on-20260213>back to top</a>)</p>
