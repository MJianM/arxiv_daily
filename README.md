## Updated on 2026.02.22

## Categories

- [Manipulation](#manipulation)
- [World Model](#world-model)
- [VLM](#vlm)
- [VLA](#vla)
- [Humanoid](#humanoid)

## Manipulation

|Publish Date|Title|Chinese Summary|Authors|PDF|Code|
|---|---|---|---|---|---|
|**2026-02-19**|**RoboGene: Boosting VLA Pre-training via Diversity-Driven Agentic Framework for Real-World Task Generation**|鉴于通用机器人操作面临真实世界交互数据稀缺的瓶颈，以及现有任务生成方法的可扩展性差和物理不可行性问题，本研究引入了RoboGene框架。该框架旨在自动化生成多样化、物理上合理的操作任务，并支持单臂、双臂和移动机器人。RoboGene整合了多样性驱动采样、自反思机制以强制物理约束以及人机交互细化三大核心组件。通过大规模定量分析和真实世界实验，结果表明RoboGene在任务质量、可行性和多样性方面显著优于现有基础模型，且经RoboGene预训练的视觉语言动作（VLA）模型展现出更高的成功率和卓越的泛化能力。|Jian Tang Team|[2602.16444](http://arxiv.org/abs/2602.16444)|null|
|**2026-02-19**|**IRIS: Learning-Driven Task-Specific Cinema Robot Arm for Visuomotor Motion Control**|针对传统机器人摄像系统成本高昂、操作复杂的问题，本研究提出了IRIS（智能机器人成像系统），一种专为自主、学习驱动电影级运动控制设计的6自由度机械臂。该系统结合轻量级3D打印硬件设计与基于Transformer的动作块视觉运动模仿学习框架，能够直接从人类演示中学习对象感知且感知平滑的摄像机轨迹，无需显式几何编程。实验结果表明，IRIS平台成本低廉、负载能力强、重复精度高，并在真实世界中展现了准确的轨迹跟踪、可靠的自主执行以及对多样化电影级运动的泛化能力。|Ali Bereyhi Team|[2602.17537](http://arxiv.org/abs/2602.17537)|null|
|**2026-02-19**|**Benchmarking the Effects of Object Pose Estimation and Reconstruction on Robotic Grasping Success**|鉴于3D重建质量对下游机器人抓取任务性能影响的评估不足，本研究引入了一个大规模、基于物理的基准来评估6D姿态估计器和3D网格模型在抓取中的功能效率。通过在不同重建模型上生成抓取姿态并在真实物体上执行，该研究量化了姿态误差、抓取鲁棒性和3D重建几何误差的综合影响。实验结果显示，重建缺陷显著减少抓取姿态候选数，但在姿态估计准确时对抓取性能影响甚微；同时揭示了抓取成功率与姿态误差主要受空间误差影响，且简单平移误差能为对称物体抓取姿态成功提供洞察。|Torsten Sattler Team|[2602.17101](http://arxiv.org/abs/2602.17101)|null|
|**2026-02-18**|**BPP: Long-Context Robot Imitation Learning by Focusing on Key History Frames**|针对机器人任务中历史观察条件（history conditioning）的挑战，即现有策略依赖当前观察且容易产生虚假关联，本研究提出了“Big Picture Policies (BPP)”方法。该方法利用视觉-语言模型检测的最小化有意义关键帧来构建历史信息，从而将多样化的轨迹投射到紧凑、任务相关的事件集上，有效减少了训练和部署之间的分布偏移。在四项真实的复杂操作任务和三项模拟任务上的评估结果显示，BPP的成功率比最佳对比方法高出70%。|Aviral Kumar Team|[2602.15010](http://arxiv.org/abs/2602.15010)|null|
|**2026-02-18**|**Learning Humanoid End-Effector Control for Open-Vocabulary Visual Loco-Manipulation**|为提升人形机器人在野外对任意物体的视觉运动操作能力，解决现有模仿学习方法因数据限制导致的泛化性不足问题，本研究提出了HERO范式。该范式结合了大型视觉模型的强大泛化和开放词汇理解能力与仿真训练带来的高控制性能，通过设计一种精确的残差感知末端执行器跟踪策略，将跟踪误差降低了3.2倍。HERO将经典机器人学与机器学习相结合，通过逆运动学、学习型神经正向模型、目标调整和重新规划，实现了在多样化真实环境中对各种日常物体进行可靠操作。|Saurabh Gupta Team|[2602.16705](http://arxiv.org/abs/2602.16705)|**[link](https://hero-humanoid.github.io/)**|
|**2026-02-18**|**VIGOR: Visual Goal-In-Context Inference for Unified Humanoid Fall Safety**|针对人形机器人在复杂环境中跌倒恢复的挑战，现有方法常将跌倒安全分解或依赖在平面地形训练的策略而缺乏泛化性，本研究提出了一种统一的跌倒安全方法。该方法基于人类自然跌倒和恢复姿态的高度受限特性，并通过对齐可从平面地形迁移至复杂地形，同时强调快速全身反应需整合感知-运动表征。通过稀疏人类演示在仿真和真实地形上训练教师模型，并蒸馏为仅依赖自我中心深度和本体感受的学生模型。实验证明，该方法在仿真和真实Unitree G1人形机器人上，无需真实世界微调，在多样化的非平面环境中实现了鲁棒、零样本的跌倒安全。|Stella X. Yu Team|[2602.16511](http://arxiv.org/abs/2602.16511)|null|
|**2026-02-18**|**SparTa: Sparse Graphical Task Models from a Handful of Demonstrations**|为解决机器人从演示中高效学习长周期操作任务的挑战，本研究关注于推断机器人应实现的任务目标，而非具体执行方式。提出了一种利用图形化对象关系表示场景状态的演示分割和池化方法，以提取操作图序列并估计任务阶段中对象状态的分布。该方法还通过预训练视觉特征进行对象匹配，以增强从多个演示中学习的鲁棒性。在广泛实验中，该方法展示了高演示分割准确性及从多演示中学习最小任务模型的效用，并在仿真和真实机器人部署中实现了跨环境的可靠执行。|Abhinav Valada Team|[2602.16911](http://arxiv.org/abs/2602.16911)|null|
|**2026-02-18**|**MALLVI: a multi agent framework for integrated generalized robotics manipulation**|针对大型语言模型（LLMs）在机器人任务规划中开环操作且缺乏鲁棒环境反馈的脆弱性问题，本研究提出了MALLVi，一个多智能体大型语言和视觉框架，旨在实现闭环反馈驱动的机器人操作。MALLVi协调分解器、定位器、思考者和反思者等专业智能体，根据自然语言指令和图像生成原子动作，并利用视觉语言模型评估反馈以进行迭代决策和错误恢复。仿真和真实世界实验证明，这种迭代闭环多智能体协调显著提高了零样本操作任务的泛化性和成功率。|Babak Khalaj Team|[2602.16898](http://arxiv.org/abs/2602.16898)|null|
|**2026-02-17**|**Learning to Retrieve Navigable Candidates for Efficient Vision-and-Language Navigation**|针对大型语言模型（LLMs）在视觉-语言导航（VLN）中决策效率低下的问题，本研究提出了一种检索增强框架，旨在提高LLM-based VLN的效率和稳定性，且无需对底层模型进行修改或微调。该方法引入了双层检索机制：在情节层面，指令级嵌入检索器提供任务特定先验；在步骤层面，模仿学习的候选检索器在LLM推理前修剪不相关方向。实验结果显示，在R2R基准测试上，该方法显著提升了成功率、Oracle成功率和SPL，且两层检索模块各自为全局指导和步进决策效率贡献互补效益。|Lina Yao Team|[2602.15724](http://arxiv.org/abs/2602.15724)|null|
|**2026-02-17**|**Selective Perception for Robot: Task-Aware Attention in Multimodal VLA**|针对机器人视觉-语言-动作（VLA）模型中静态多视角融合带来的计算开销和背景噪声问题，本研究受人类主动感知启发，提出了一种动态信息融合框架以提高效率和鲁棒性。该框架引入轻量级自适应路由架构，实时分析文本提示和腕部摄像头数据，预测各视角与任务的相关性，并有条件地衰减低信息效用视图的计算。同时，利用视觉-语言模型建立了自动化标注流程以高效获取训练数据。真实世界实验结果表明，该方法在推理效率和控制性能上均显著优于现有VLA模型，验证了动态信息融合在资源受限、实时机器人控制环境中的有效性。|Soo-Chul Lim Team|[2602.15543](http://arxiv.org/abs/2602.15543)|null|
|**2026-02-17**|**Feasibility-aware Imitation Learning from Observation with Multimodal Feedback**|针对手持演示模仿学习中，演示数据缺乏机器人动作且演示运动可能对机器人不可行的问题，本研究提出了可行性感知行为克隆（FABCO）方法。FABCO将从观察中行为克隆与可行性估计相结合，通过学习机器人动力学模型来评估演示运动的可复现性。估计出的可行性用于多模态反馈，以引导演示者生成更可行的运动，并用于可行性感知策略学习，以减少不可行演示对策略学习的影响。在两项任务中对15名参与者进行的实验表明，FABCO将模仿学习性能提高了3.2倍以上。|Takamitsu Matsubara Team|[2602.15351](http://arxiv.org/abs/2602.15351)|null|
|**2026-02-16**|**PhyScensis: Physics-Augmented LLM Agents for Complex Physical Scene Arrangement**|鉴于自动生成交互式3D环境对于机器人仿真数据收集的重要性，但现有工作常忽略物体间的物理关系，本研究提出了PhyScensis框架，以解决复杂物理场景生成中物体密度高、支持关系复杂以及需同时建模空间和物理属性的挑战。该框架是一个基于LLM代理并结合物理引擎的系统，通过LLM代理迭代提出空间和物理谓词，由配备物理引擎的求解器实现，并利用反馈机制进行配置优化。实验结果表明，PhyScensis在场景复杂性、视觉质量和物理准确性方面均优于现有方法，为机器人操作提供了统一的复杂物理场景布局生成方案。|Chuang Gan Team|[2602.14968](http://arxiv.org/abs/2602.14968)|null|
|**2026-02-16**|**Affordance Transfer Across Object Instances via Semantically Anchored Functional Map**|针对传统示教学习（LfD）收集物理示教数据耗时且难以扩展，以及从人类视频学习在几何差异大的物体间泛化互动行为的难题，本研究提出了“语义锚定功能图”（SemFM）框架。该方法从图像重建粗糙网格，识别语义对应功能区域，选择互斥的语义锚点，并通过功能图在表面传播这些约束以实现密集的语义一致对应。这使得演示的交互区域能够以轻量级和可解释的方式跨越几何多样性物体进行转移。实验结果表明，该方法在合成物体类别和真实机器人操作任务中实现了准确的示能（affordance）迁移，且计算成本适中，适用于实际的机器人感知-行动流程。|Weiming Zhi Team|[2602.14874](http://arxiv.org/abs/2602.14874)|null|
|**2026-02-16**|**DriveFine: Refining-Augmented Masked Diffusion VLA for Precise and Robust Driving**|鉴于自动驾驶领域视觉-语言-行动（VLA）模型中的生成式规划器（如扩散模型和基于Token的模型）存在的缺点，如模态对齐困难、训练效率低、泛化能力有限及累积因果误差等问题，本研究提出了DriveFine，一个结合灵活解码和自校正能力的掩码扩散VLA模型。该模型引入了即插即用的block-MoE（专家混合）模块，将精炼专家与生成专家无缝结合，并通过推理时的显式专家选择和训练时的梯度阻断实现专家解耦。此外，设计了一种混合强化学习策略来促进精炼专家的有效探索并保持训练稳定性。在NAVSIM v1、v2和Navhard基准上的大量实验证明，DriveFine表现出强大的有效性和鲁棒性。|Yan Wang Team|[2602.14577](http://arxiv.org/abs/2602.14577)|null|
|**2026-02-16**|**RoboSolver: A Multi-Agent Large Language Model Framework for Solving Robotic Arm Problems**|为整合大型语言模型（LLMs）和视觉-语言模型（VLMs）与计算工具的能力以自动化解决机器人操作臂问题，本研究提出了一种智能多智能体框架。该框架能够接受文本和视觉输入，并根据用户查询自动执行正逆运动学计算、关键点速度和加速度计算、机器人3D仿真生成，最终在仿真环境中执行运动控制。通过与GPT-4o、DeepSeek-V3.2和Claude-Sonnet-4.5等模型及Gemini 2.5 Pro VLM结合的基准测试，结果显示该框架显著提高了各种任务的准确性，例如与GPT-4o结合在文本描述下的正运动学计算准确率达到0.97，在视觉输入下达到0.93，且在多类机器人任务中也表现出0.97的准确率，远超单一模型的表现。|Alireza Taheri Team|[2602.14438](http://arxiv.org/abs/2602.14438)|null|
|**2026-02-16**|**A Soft Wrist with Anisotropic and Selectable Stiffness for Robust Robot Learning in Contact-rich Manipulation**|针对非结构化环境中接触密集型操作任务中现有软末端执行器变形范围有限、缺乏定向刚度控制或驱动系统复杂等问题，本研究引入了一种新型柔性腕部机构CLAW（Compliant Leaf-spring Anisotropic soft Wrist）。该机制通过两个正交的板簧和带锁定机构的旋转关节，实现了大范围的六自由度变形、可调的三种模式各向异性刚度，同时保持了轻量化和低成本。在模仿学习的实验评估中，CLAW在插栓任务中的成功率达到76%，优于Fin Ray夹持器（43%）和刚性夹持器（36%），展示了其在处理精密装配和精细物体操作等接触密集型场景中的潜力，有望提高机器人学习的鲁棒性。|Masashi Hamaya Team|[2602.14434](http://arxiv.org/abs/2602.14434)|null|
|**2026-02-16**|**AdaptManip: Learning Adaptive Whole-Body Object Lifting and Delivery with Online Recurrent State Estimation**|针对人形机器人集成导航、物体抓取和递送（运动操作）任务中，现有模仿学习方法依赖人类示教且对干扰脆弱的问题，本研究提出了AdaptManip框架。该框架通过强化学习训练鲁棒的运动操作策略，无需人类示教数据，并由三个耦合组件构成：实时跟踪被操纵物体的循环对象状态估计器、用于稳定运动和操作控制的全身基础策略以及基于激光雷达的机器人全局位姿估计器。所有组件均在仿真中训练并零样本部署到真实硬件。实验结果表明，AdaptManip在适应性和整体成功率上显著优于包括模仿学习在内的基线方法，且精确的对象状态估计即使在遮挡下也能提升操作性能，并在真实世界中成功展示了自主导航、物体抓取和递送能力。|Sehoon Ha Team|[2602.14363](http://arxiv.org/abs/2602.14363)|**[link](https://morganbyrd03.github.io/adaptmanip/)**|
|**2026-02-15**|**GRAIL: Goal Recognition Alignment through Imitation Learning**|鉴于从智能体行为中理解其目标对于AI系统与人类意图对齐至关重要，但现有目标识别方法常依赖最优策略表示而与实际（可能次优）行为不符，本研究提出了通过模仿学习实现目标识别对齐（GRAIL）的方法。GRAIL利用模仿学习和逆强化学习，直接从（可能次优的）示教轨迹中为每个候选目标学习一个目标导向策略。通过对观察到的部分轨迹进行单次前向传播，GRAIL在保留经典目标识别的单次推理能力的同时，能够捕获次优和系统性偏差行为。实验结果显示，GRAIL在系统性偏差最优行为下将F1分数提高超过0.5，在次优行为下提高约0.1-0.3，在带噪声的最优轨迹下提高高达0.4，同时在完全最优设置下保持竞争力。|Reuth Mirsky Team|[2602.14252](http://arxiv.org/abs/2602.14252)|null|
|**2026-02-15**|**Learning Part-Aware Dense 3D Feature Field for Generalizable Articulated Object Manipulation**|鉴于关节物体操作在多样化物体间的泛化挑战，以及现有2D基础特征在提升至3D空间时面临运行时间长、多视角不一致及空间分辨率低等问题，本研究提出了Part-Aware 3D Feature Field (PA3FF)。PA3FF是一种新型的密集3D特征，通过对比学习利用大规模标注数据集中的3D部件提议进行训练，能从点云输入中预测连续的3D特征场，其中点特征的距离反映功能部件的接近程度。在此基础上，研究引入了Part-Aware Diffusion Policy (PADP) 模仿学习框架，以提升样本效率和泛化能力。实验证明，PA3FF在模拟和真实世界的操作任务中始终优于多种2D和3D表征，并能支持对应学习和分割等多种下游任务。|Hao Dong Team|[2602.14193](http://arxiv.org/abs/2602.14193)|**[link](https://pa3ff.github.io)**|
|**2026-02-15**|**RoboAug: One Annotation to Hundreds of Scenes via Region-Contrastive Data Augmentation for Robotic Manipulation**|为解决机器人学习在多样化、未知场景中泛化能力差的问题，同时避免对大规模预训练和完美上游物体检测的依赖，本研究提出了RoboAug，一个生成式数据增强框架。该方法仅需单张图像的边界框标注，即可利用预训练的生成模型进行精确的语义数据增强，并整合即插即用的区域对比损失，使模型聚焦于任务相关区域。在UR-5e、AgileX和Tien Kung 2.0三款机器人上进行的超过3.5万次真实世界实验表明，RoboAug显著优于现有数据增强基线方法。在包含多样化背景、干扰物和光照条件的未知场景中，该方法将UR-5e的成功率从0.09提升至0.47，AgileX从0.16提升至0.60，Tien Kung 2.0从0.19提升至0.67，凸显了其在真实世界操作任务中的卓越泛化能力和有效性。|Jian Tang Team|[2602.14032](http://arxiv.org/abs/2602.14032)|null|
|**2026-02-15**|**WoVR: World Models as Reliable Simulators for Post-Training VLA Policies with RL**|为克服强化学习在机器人VLA模型中因大量真实世界交互需求而难以直接部署，以及世界模型模拟器在长周期想象轨迹中易产生幻觉和误差累积的问题，本研究提出了WoVR框架。WoVR通过可控的动作条件视频世界模型提高轨迹稳定性，利用关键帧初始化轨迹减少有效误差深度，并通过世界模型-策略协同演化保持对齐。实验结果显示，WoVR在LIBERO基准和真实机器人操作任务中实现了稳定的长周期想象轨迹和有效的策略优化，平均成功率在LIBERO上提升了29.3个百分点，在真实机器人上提升了30.0个百分点，证明了在有效控制幻觉的情况下，学习到的世界模型可作为实用的强化学习模拟器。|Dongbin Zhao Team|[2602.13977](http://arxiv.org/abs/2602.13977)|null|
|**2026-02-14**|**Enabling Option Learning in Sparse Rewards with Hindsight Experience Replay**|针对分层强化学习框架（如MOC）在稀疏奖励多目标环境中表现不佳，特别是在对象操作任务中难以发现与对象的交互策略问题，本研究首先提出了MOC-HER，将回溯经验回放（HER）集成到MOC中。在此基础上，为更有效处理对象操作任务，进一步引入了Dual Objectives Hindsight Experience Replay (2HER)，通过同时生成对象最终状态目标和智能体效应器位置目标，奖励智能体与对象的交互和任务完成。实验结果表明，MOC-2HER在机器人操作环境中的成功率高达90%，远高于MOC和MOC-HER的不足11%，验证了双目标重标记策略的有效性。|Gabriel de Oliveira Ramos Team|[2602.13865](http://arxiv.org/abs/2602.13865)|null|
|**2026-02-14**|**Mean Flow Policy with Instantaneous Velocity Constraint for One-step Action Generation**|针对流基策略在强化学习中表达能力与计算负担的权衡问题，本研究提出了一种新的生成式策略函数——平均速度策略（MVP）。MVP通过建模平均速度场实现最快的一步动作生成，并引入瞬时速度约束（IVC）以确保高表达能力。理论上证明IVC作为关键边界条件可提高学习精度和策略表达力。实验结果表明，MVP在Robomimic和OGBench的多个机器人操作任务中取得了最先进的成功率，并在训练和推理速度上显著优于现有流基策略基线。|Shengbo Eben Li Team|[2602.13810](http://arxiv.org/abs/2602.13810)|null|
|**2026-02-14**|**Gaussian Sequences with Multi-Scale Dynamics for 4D Reconstruction from Monocular Casual Videos**|为解决从单目日常视频中进行四维（4D）动态场景重建的ill-posed问题，本研究基于真实世界动态的多尺度规律性，设计了多尺度动态机制以分解复杂运动场。在此基础上，提出了具有多尺度动态的高斯序列，通过多级运动组合构建动态3D高斯表示，显著减轻了重建歧义并促进物理合理性。同时，结合视觉基础模型的多模态先验提供补充监督，进一步约束解空间并提高重建保真度。实验证明，该方法在动态新视角合成任务中，在基准和真实世界操作数据集上均显著优于现有方法，实现了从单目视频中准确且全局一致的4D重建。|Lei Sun Team|[2602.13806](http://arxiv.org/abs/2602.13806)|null|
|**2026-02-14**|**MOTIF: Learning Action Motifs for Few-shot Cross-Embodiment Transfer**|尽管视觉-语言-动作（VLA）模型推动了通用机器人学习，但由于运动学异构性以及收集足够真实世界示范数据进行微调的高成本，跨具身（cross-embodiment）迁移仍然充满挑战。现有跨具身策略通常依赖共享-私有架构，其私有参数容量有限且缺乏明确的适应机制。为解决这些局限性，本文提出了MOTIF框架，旨在实现高效的小样本跨具身迁移，它将具身无关的时空模式（称为动作基序）与异构动作数据解耦。具体而言，MOTIF首先通过带有进度感知对齐和具身对抗约束的矢量量化学习统一的基序，以确保时间和跨具身一致性。然后，设计一个轻量级预测器从实时输入预测这些基序，并将其与机器人特定状态融合，以指导流匹配策略在新的具身上生成动作。模拟和真实世界环境的评估均验证了MOTIF的优越性，在小样本迁移场景中显著优于强基线，模拟中提升6.5%，真实世界中提升43.7%。|Heng Tao Shen Team|[2602.13764](http://arxiv.org/abs/2602.13764)|null|
|**2026-02-14**|**HybridFlow: A Two-Step Generative Policy for Robotic Manipulation**|现有的机器人操作策略受推理延迟限制，缺乏足够的实时环境交互能力。尽管流匹配等更快的生成方法正逐步取代扩散方法，但其精度仍难以满足机器人操作的严格要求。本文关注MeanFlow作为流匹配的单步变体虽然快速但在动作生成精度上的不足。为平衡推理速度和生成质量，本文提出了HybridFlow，这是一种具有2-NFE（函数评估次数）的三阶段方法，包括MeanFlow模式下的全局跳转、用于分布对齐的ReNoise以及ReFlow模式下的局部细化。该方法利用MeanFlow单步生成的快速优势，同时以最少的生成步骤确保动作精度。真实世界实验表明，HybridFlow在成功率上比16步扩散策略高出15-25%，并将推理时间从152毫秒缩短到19毫秒（8倍加速，约52赫兹）；在未见颜色OOD抓取和可变形物体折叠任务上分别达到了70.0%和66.3%的成功率。这些结果表明HybridFlow是一种实用的低延迟方法，能增强机器人操作策略的真实世界交互能力。|Yide Liu Team|[2602.13718](http://arxiv.org/abs/2602.13718)|null|
|**2026-02-14**|**Symmetry-Aware Fusion of Vision and Tactile Sensing via Bilateral Force Priors for Robotic Manipulation**|机器人插入任务需要精密的、富接触的交互，仅凭视觉难以解决。尽管触觉反馈具有直观价值，但现有研究表明，朴素的视觉-触觉融合往往未能持续提供改进。为解决此问题，本文提出了一种用于视觉-触觉融合的跨模态Transformer（CMT），它通过结构化的自注意力与交叉注意力机制整合腕部摄像头观测和触觉信号。为稳定触觉嵌入，本文进一步引入了物理信息正则化，鼓励双边力平衡，反映了人类运动控制的原理。在TacSL基准上的实验表明，带有对称正则化的CMT实现了96.59%的插入成功率，超越了朴素和门控融合基线，并与“腕部+接触力”的优越配置（96.09%）非常接近。这些结果突出表明：触觉感知对于精确对齐不可或缺，以及经过物理信息正则化强化的原则性多模态融合，能够充分发挥视觉和触觉的互补优势，在现实感知条件下接近最优性能。|Tao Yu Team|[2602.13689](http://arxiv.org/abs/2602.13689)|null|
|**2026-02-14**|**Hierarchical Audio-Visual-Proprioceptive Fusion for Precise Robotic Manipulation**|现有机器人操作方法主要依赖视觉和本体感受，在部分可观测的真实世界环境中难以推断接触相关的交互状态。而声学线索能自然编码丰富的接触动态，但在多模态融合中却未被充分利用，且大多数融合方法错误地假设模态作用均一。为实现基于声学信息的精确机器人操作，本文提出一种分层表示融合框架，逐步整合音频、视觉和本体感受。该方法首先将视觉和本体感受表示条件化于声学线索，然后明确建模高阶跨模态交互以捕捉模态间的互补依赖。融合后的表示被扩散策略用于直接从多模态观测生成连续机器人动作。在真实世界机器人操作任务（如倒液体和开柜门）上的广泛实验表明，该方法持续优于现有最先进的多模态融合框架，尤其是在声学线索提供视觉无法轻易获得的任务相关信息时。此外，通过互信息分析解释了音频线索在机器人操作中的作用。|Peng Liu Team|[2602.13640](http://arxiv.org/abs/2602.13640)|null|
|**2026-02-13**|**Imitating What Works: Simulation-Filtered Modular Policy Learning from Human Videos**|通过观看人类视频学习操作技能，有望为机器人学习提供大规模数据的新来源。然而，人类视频在学习抓取后动作方面提供了强信号，但在学习抓取行为方面用处较小，特别是对于没有类似人手的机器人而言，任意稳定的抓取通常不兼容任务。为解决这一挑战，本文提出了Perceive-Simulate-Imitate (PSI) 框架，用于使用经过模拟中成对抓取-轨迹过滤处理的人类视频运动数据训练模块化操作策略。这一模拟步骤通过抓取适用性标签扩展了轨迹数据，从而能够监督学习面向任务的抓取能力。真实世界实验表明，该框架可以无需任何机器人数据高效学习精确操作技能，与简单使用抓取生成器相比，性能显著更鲁棒。|Wei-Chiu Ma Team|[2602.13197](http://arxiv.org/abs/2602.13197)|null|
|**2026-02-13**|**UniManip: General-Purpose Zero-Shot Robotic Manipulation with Agentic Operational Graph**|针对现有机器人操作方法在零样本泛化方面的不足，即端到端VLA模型缺乏精度而传统规划器语义刚性问题，本文提出了UniManip框架。该框架基于双层Agentic Operational Graph (AOG)，通过高层Agentic层进行任务编排和低层Scene层表示动态状态，实现语义推理与物理接地的统一，并以动态智能体循环方式主动实例化场景图、规划无碰撞轨迹并自主恢复失败。实验结果表明，UniManip在未见对象和任务上展现出鲁棒的零样本能力，成功率显著高于现有VLA和分层基线，且支持从固定基座到移动操作的零样本迁移。|Ziwei Wang Team|[2602.13086](http://arxiv.org/abs/2602.13086)|**[link](https://henryhcliu.github.io/unimanip)**|
|**2026-02-13**|**How Swarms Differ: Challenges in Collective Behaviour Comparison**|针对群体行为分析中数值特征集通常缺乏通用性且难以定量衡量行为相似性的问题，本研究深入探讨了特征集对集体行为的影响。我们从现有群体机器人学工作中筛选出特征集和相似性度量，并评估了它们在特定行为背景外的鲁棒性。研究发现，特征集和相似性度量的相互作用决定了区分相似行为群体的有效性，并提出了一种基于自组织图的方法来识别特征空间中行为难以区分的区域。|Jonas Kuckling Team|[2602.13016](http://arxiv.org/abs/2602.13016)|null|
|**2026-02-13**|**SafeFlowMPC: Predictive and Safe Trajectory Planning for Robot Manipulators with Learning-based Policies**|面对机器人日益融入日常生活对灵活性和实时反应能力的需求，以及学习方法缺乏安全保证和优化方法泛化能力不足的挑战，本文提出了SafeFlowMPC框架。该框架结合了流匹配与在线优化，旨在融合学习和优化方法的优势，并通过次优模型预测控制公式，实时确保操作安全性。在KUKA 7自由度机械臂上的真实世界实验（包括抓取和人机交接任务）中，SafeFlowMPC展现了强大的性能。|Andreas Kugi Team|[2602.12794](http://arxiv.org/abs/2602.12794)|null|
|**2026-02-13**|**Scaling Single Human Demonstrations for Imitation Learning using Generative Foundational Models**|鉴于模仿学习中收集机器人演示数据的困难以及人类演示到机器人转移的挑战，本文提出了Real2Gen框架，仅通过单个人类演示来训练操作策略。Real2Gen从人类演示中提取关键信息并传输到模拟环境，利用可编程专家智能体生成无限量的训练数据来学习流匹配策略。实验结果表明，Real2Gen平均成功率提高了26.6%，并且由于训练数据的丰富性，训练出的策略具有更强的泛化能力，实现了纯模拟训练策略的零样本真实世界部署。|Abhinav Valada Team|[2602.12734](http://arxiv.org/abs/2602.12734)|null|
|**2026-02-13**|**$\mathcal{X}$-KD: General Experiential Knowledge Distillation for Large Language Models**|针对大语言模型知识蒸馏（KD）中，现有方法常忽视教师模型原始学习环境的问题，本文提出了Experiential Knowledge Distillation ($\mathcal{X}$-KD) 框架。受经验学习理论和逆强化学习启发，$\mathcal{X}$-KD采用Approximated Variational Reward Imitation Learning (AVRIL) 框架，联合建模教师的原始奖励函数并执行策略蒸馏，使学生模型能在教师的原始学习环境中学习。实验证明，$\mathcal{X}$ -KD在抽象摘要、机器翻译和算术推理任务上均优于基线方法，并实现了更好的性能-多样性权衡和数据效率。|Yuyu Yuan Team|[2602.12674](http://arxiv.org/abs/2602.12674)|null|
|**2026-02-13**|**PMG: Parameterized Motion Generator for Human-like Locomotion Control**|为解决人形机器人运动中，现有全身体参考引导方法对高级命令接口适应性差、对数据和校准敏感等实际挑战，本文提出了Parameterized Motion Generator (PMG)。PMG是一种基于人类运动结构分析的实时运动生成器，通过紧凑的参数化运动数据和高维控制命令合成参考轨迹，并结合模仿学习流水线和仿真到现实电机参数识别模块。实验证明，该集成系统能生成自然、类人运动，精确响应高维控制输入（如VR远程操作），并实现高效、可验证的仿真到现实迁移。|Houde Liu Team|[2602.12656](http://arxiv.org/abs/2602.12656)|null|
|**2026-02-13**|**Real-to-Sim for Highly Cluttered Environments via Physics-Consistent Inter-Object Reasoning**|为解决从单视图观测重建物理有效3D场景时，现有方法常忽略物理约束导致无效状态，进而影响下游模拟可靠性的问题，本文提出了一种新颖的物理约束Real-to-Sim管道。该管道能够从单视图RGB-D数据重建物理一致的3D场景，其核心是一个可微分优化管道，通过接触图建模空间依赖，并利用可微分刚体模拟联合优化物体姿态和物理属性。实验结果表明，重建场景具有高物理保真度，能忠实复现真实世界接触动力学，从而实现稳定可靠的接触密集型操作。|Jun Ma Team|[2602.12633](http://arxiv.org/abs/2602.12633)|**[link](https://physics-constrained-real2sim.github.io)**|
|**2026-02-13**|**FlowHOI: Flow-based Semantics-Grounded Generation of Hand-Object Interactions for Dexterous Robot Manipulation**|近期视觉-语言-动作（VLA）模型能够生成看似合理的末端执行器运动，但在长程、富接触的任务中常常失败，因为缺乏对手-物体交互（HOI）结构的显式表示。为解决此问题，本文提出了FlowHOI，一个两阶段流匹配框架，它能根据第一人称观测、语言指令和3D高斯飞溅（3DGS）场景重建，生成语义接地、时间连贯的HOI序列，包括手部姿态、物体姿态和手-物体接触状态。该框架将以几何为中心的抓取与以语义为中心的操作解耦，后者通过紧凑的3D场景令牌进行条件化，并采用运动-文本对齐损失来语义化生成的交互。为解决高保真HOI监督数据稀缺的问题，本文引入了一个重建流水线，从大规模第一人称视频中恢复对齐的手-物体轨迹和网格，为鲁棒生成提供了HOI先验。FlowHOI在GRAB和HOT3D基准上实现了最高的动作识别精度和比最强扩散基线高1.7倍的物理模拟成功率，同时推理速度提升了40倍。此外，通过将生成的HOI表示重定向到真实机器人执行流程，本文在四个灵巧操作任务上验证了真实机器人执行的可行性。|Xingxing Zuo Team|[2602.13444](http://arxiv.org/abs/2602.13444)|**[link](https://huajian-zeng.github.io/projects/flowhoi/)**|
|**2026-02-12**|**FAIL: Flow Matching Adversarial Imitation Learning for Image Generation**|鉴于流匹配模型后训练与模仿学习的数学等同性，以及监督微调无法纠正策略漂移而偏好优化成本高昂的问题，本文提出了Flow Matching Adversarial Imitation Learning (FAIL) 框架。该框架通过对抗训练最小化策略与专家之间的散度，无需明确奖励或成对比较，并推导出了FAIL-PD和FAIL-PG两种算法。实验证明，FAIL在仅使用少量演示数据的情况下，能在提示遵循和美学基准上取得竞争性性能，并能有效泛化至离散图像和视频生成，同时作为鲁棒正则化器减轻奖励欺骗。|Weidi Xie Team|[2602.12155](http://arxiv.org/abs/2602.12155)|null|
|**2026-02-12**|**GigaBrain-0.5M*: a VLA That Learns From World Model-Based Reinforcement Learning**|针对传统VLA模型在场景理解和未来预测上的局限性，本研究提出了GigaBrain-0.5M*，一个基于世界模型强化学习的VLA模型。该模型在预训练的GigaBrain-0.5基础上，通过RAMP（Reinforcement leArning via world Model-conditioned Policy）进一步整合了世界模型强化学习，以实现鲁棒的跨任务适应性。实验结果表明，RAMP在洗衣折叠、箱子包装和意式浓缩咖啡制作等复杂任务中，相较于RECAP基线性能提升了约30%，并且在实际部署中展示了可靠的长期执行能力，能够无故障完成复杂操作任务。|Zheng Zhu Team|[2602.12099](http://arxiv.org/abs/2602.12099)|**[link](https://gigabrain05m.github.io/)**|
|**2026-02-12**|**Affordance-Graphed Task Worlds: Self-Evolving Task Generation for Scalable Embodied Learning**|鉴于在真实世界中训练机器人策略成本高昂且难以扩展，而现有生成模拟方法难以生成逻辑连贯的长时任务并处理动态物理不确定性，本研究提出了Affordance-Graphed Task Worlds (AGT-World) 框架。该框架能够根据真实世界观测自主构建交互式模拟环境和相应的机器人任务策略，通过将任务空间形式化为结构化图实现复杂目标的精确分层分解，并引入结合视觉-语言模型推理和几何验证的混合反馈自进化机制来完善策略。广泛实验证明，AGT-World在成功率和泛化能力上显著优于现有方法，实现了提议、执行和修正的自改进循环，以支持可扩展的机器人学习。|Changshui Zhang Team|[2602.12065](http://arxiv.org/abs/2602.12065)|null|
|**2026-02-12**|**HoloBrain-0 Technical Report**|为了弥合基础模型研究与可靠的真实世界机器人部署之间的差距，本研究引入了HoloBrain-0，一个全面的视觉-语言-动作 (VLA) 框架。该框架的核心是一个新颖的VLA架构，明确融入了机器人本体先验信息（如多视图相机参数和运动学描述），以增强3D空间推理并支持多样化的本体。通过“预训练-后训练”范式进行验证，该系统在RoboTwin 2.0、LIBERO和GenieSim等模拟基准测试中取得了最先进的成果，并在长时程真实世界操作任务中表现出色。值得注意的是，其高效的0.2B参数变体能与大得多的基线媲美，并支持低延迟的设备部署。为加速研究和实际应用，HoloBrain生态系统已完全开源。|Zhizhong Su Team|[2602.12062](http://arxiv.org/abs/2602.12062)|null|
|**2026-02-12**|**When would Vision-Proprioception Policies Fail in Robotic Manipulation?**|针对视觉-本体感受策略在复杂任务中泛化能力不一致的问题，本研究发现，在机器人运动转换的子阶段，视觉模态的作用有限，策略倾向于更简洁的本体感受信号，抑制了视觉学习。为此，我们提出了梯度调整与阶段引导 (GAP) 算法，通过利用本体感受估计运动转换阶段的概率，并据此自适应地调节本体感受梯度的幅度，从而实现视觉与本体感受的动态协作。综合实验表明，GAP算法在模拟和真实世界环境、单臂和双臂设置以及不同模型类型中均适用，并能形成鲁棒且可泛化的视觉-本体感受策略。|Di Hu Team|[2602.12032](http://arxiv.org/abs/2602.12032)|null|
|**2026-02-12**|**Accelerating Robotic Reinforcement Learning with Agent Guidance**|强化学习在机器人操作中面临严重的样本效率问题，而现有人机协作 (HIL) 方法虽能加速训练，但受限于可扩展性、操作员疲劳和不一致的人类专业知识。为解决此问题，本研究提出了Agent-guided Policy Search (AGPS) 框架，通过多模态智能体取代人工监督者，实现训练流程自动化。其核心思想是将智能体视为语义世界模型，注入内在价值先验来结构化物理探索，并利用可执行工具通过纠正性路点和空间约束提供精确指导。实验证明，AGPS在样本效率方面优于HIL方法，从而实现了无劳动力的可扩展机器人学习路径。|Yaodong Yang Team|[2602.11978](http://arxiv.org/abs/2602.11978)|null|
|**2026-02-12**|**Robot-DIFT: Distilling Diffusion Features for Geometrically Consistent Visuomotor Control**|本研究认为机器人操作泛化性的瓶颈在于当前视觉骨干网络与闭环控制物理需求之间的结构性不匹配，尤其在于现有模型缺乏精细的几何敏感性。鉴于生成扩散模型内在地编码了几何依赖性，但其随机性和延迟阻碍了直接应用，我们提出了Robot-DIFT框架。该框架通过流形蒸馏 (Manifold Distillation) 将冻结的扩散教师模型蒸馏到确定性空间-语义特征金字塔网络 (S2-FPN) 中，从而在保持生成模型丰富几何先验的同时，确保了时间稳定性、实时执行和抗漂移鲁棒性。在DROID数据集上的预训练结果表明，Robot-DIFT在几何一致性和控制性能上均优于领先的判别式基线，验证了视觉学习方式对机器人行为能力的关键影响。|Georgia Chalvatzaki Team|[2602.11934](http://arxiv.org/abs/2602.11934)|null|
|**2026-02-12**|**JEPA-VLA: Video Predictive Embedding is Needed for VLA Models**|现有VLA模型在机器人操作中仍面临样本效率低和泛化能力有限的问题，本研究认为这与预训练视觉表示在环境理解和策略先验方面知识不足有关。通过深入分析，我们发现现有VLA中常用的视觉表示未能有效捕获关键任务相关信息及诱导有效策略先验，而通过视频预训练的预测嵌入，特别是V-JEPA 2，能够灵活地处理不可预测因素并编码任务相关的时间动态。基于此，我们提出了JEPA-VLA，一种将预测嵌入自适应集成到现有VLA的简单有效方法。实验证明，JEPA-VLA在LIBERO、LIBERO-plus、RoboTwin2.0和真实机器人任务等基准测试中均取得了显著的性能提升。|Mingsheng Long Team|[2602.11832](http://arxiv.org/abs/2602.11832)|null|
|**2026-02-12**|**Clutt3R-Seg: Sparse-view 3D Instance Segmentation for Language-grounded Grasping in Cluttered Scenes**|针对杂乱环境中3D实例分割在遮挡、有限视角和噪声掩码下的挑战，本研究提出了Clutt3R-Seg，一种用于语言引导抓取的零样本鲁棒3D实例分割流水线。该方法的核心思想是引入语义线索的分层实例树，通过跨视图分组和条件替换，将噪声掩码作为信息线索，从而抑制过分割和欠分割，产生视图一致的掩码和鲁棒的3D实例。为应对多阶段任务中的场景变化，该方法还引入了一致性感知更新机制。在合成和真实世界数据集及真实机器人上的验证表明，Clutt3R-Seg在杂乱和稀疏视图场景中持续优于现有最先进基线，尤其在重度杂乱序列中表现出超过2.2倍的性能提升。|Ayoung Kim Team|[2602.11660](http://arxiv.org/abs/2602.11660)|null|
|**2026-02-12**|**ViTaS: Visual Tactile Soft Fusion Contrastive Learning for Visuomotor Learning**|针对现有机器人操作中视觉与触觉信息融合方法在遮挡场景下效果不佳、未能充分利用两种模态互补性且集成机制多为直接拼接的问题，本研究提出了ViTaS框架。该框架旨在结合视觉和触觉信息指导智能体行为，并引入了软融合对比学习（Soft Fusion Contrastive Learning）以及一个CVAE模块，以更好地利用视觉-触觉表示中的对齐和互补性。在12个模拟环境和3个真实世界环境中的实验验证表明，ViTaS显著优于现有基线，证明了其在利用多模态信息方面的有效性。|Huazhe Xu Team|[2602.11643](http://arxiv.org/abs/2602.11643)|null|
|**2026-02-12**|**EasyMimic: A Low-Cost Framework for Robot Imitation Learning from Human Videos**|鉴于大规模真实世界数据采集成本高昂阻碍了机器人模仿学习，尤其对低成本家用机器人而言，本研究提出了EasyMimic框架。该框架是一个低成本、可复制的解决方案，使机器人能通过标准RGB相机捕获的人类视频演示快速学习操作策略。其方法首先从视频中提取3D手部轨迹，并利用动作对齐模块将其映射到低成本机器人的夹持器控制空间；为弥合人机领域差距，引入了简单的手部视觉增强策略，并通过协同训练方法在处理过的人类数据和少量机器人数据上微调模型。实验证明，EasyMimic在LeRobot平台上在多种操作任务中取得了高性能，显著减少了对昂贵机器人数据采集的依赖，为智能机器人进入家庭提供了实用途径。|Qin Jin Team|[2602.11464](http://arxiv.org/abs/2602.11464)|null|
|**2026-02-12**|**Scaling World Model for Hierarchical Manipulation Policies**|视觉-语言-动作（VLA）模型在通用机器人操作中虽有前景，但在有限真实机器人数据下，其在分布外（OOD）场景中的泛化能力仍脆弱。本研究提出VISTA，一个分层视觉-语言-动作框架，利用大规模预训练世界模型的泛化能力实现鲁棒且可泛化的视觉子目标任务分解。该框架将世界模型作为高层规划器来生成带有目标图像的子任务序列，VLA作为低层执行器遵循指导生成动作。实验结果表明，与原始文本目标相比，合成的目标图像提供了更具视觉和物理细节的指导，使低层策略能泛化到新物体和场景，将VLA在OOD场景中的性能从14%提升至69%。|Xinghang Li Team|[2602.10983](http://arxiv.org/abs/2602.10983)|null|
|**2026-02-12**|**Self-Refining Vision Language Model for Robotic Failure Detection and Reasoning**|针对机器人故障推理中，真实世界故障的复杂性及丰富推理标签获取成本高昂的问题，本文提出了ARMOR框架。该框架将故障检测和推理建模为一个多任务自细化过程，模型通过迭代预测检测结果和自然语言推理，并从大规模稀疏二元标签和少量丰富推理标注的异构监督中学习。实验结果表明，ARMOR在故障检测率上比现有方法提升高达30%，在LLM模糊匹配分数测量的推理能力上提升高达100%，展现了对异构监督和开放式推理的鲁棒性。|Yesh Dattatreya Team|[2602.12405](http://arxiv.org/abs/2602.12405)|null|
|**2026-02-11**|**Human Preference Modeling Using Visual Motion Prediction Improves Robot Skill Learning from Egocentric Human Video**|当前从第一视角人类视频中学习机器人的方法，在度量视觉状态长期价值时存在假设限制，且在跨实体和环境时需要迁移学习到的价值函数。本研究提出一种新方法，通过预测后续图像中跟踪点的运动来建模人类偏好，并将奖励函数定义为机器人行为中预测与观察到的物体运动的一致性。随后，利用改进的Soft Actor Critic (SAC) 算法（通过10次机器人演示初始化）来估计此奖励的价值函数，并在机器人上优化策略。结果显示，该方法在真实机器人上有效，并且在模拟和真实机器人上的多项任务中，其学习到的策略与现有工作相当或超越。|Christopher G. Atkeson Team|[2602.11393](http://arxiv.org/abs/2602.11393)|null|
|**2026-02-11**|**MolmoSpaces: A Large-Scale Open Ecosystem for Robot Navigation and Manipulation**|机器人大规模部署要求对日常情境具备鲁棒性，但现有基准测试缺乏足够多样化的场景。本研究引入了MolmoSpaces，一个支持机器人策略大规模基准测试的开放生态系统，包含超过23万个多样化室内环境和13万个丰富注释的物体资产，且与模拟器无关。同时设计了MolmoSpaces-Bench基准套件。实验证明，MolmoSpaces-Bench具有强大的模拟到真实关联性，验证了新型零样本策略的优越性，并揭示了对提示措辞、初始关节位置和相机遮挡的关键敏感性，为机器人学习研究提供了可扩展的基础。|Ranjay Krishna Team|[2602.11337](http://arxiv.org/abs/2602.11337)|null|
|**2026-02-11**|**YOR: Your Own Mobile Manipulator for Generalizable Robotics**|随着机器人学习的发展，低成本机器人平台日益增多，但移动操作的最佳外形仍然是待解问题。本研究推出YOR，一个开源、低成本的移动操纵器，集成了全向基座、伸缩式垂直升降器和带抓手的双臂，以实现全身移动和操作。其设计强调模块化、易于组装和经济性（物料清单成本低于1万美元）。YOR在需要协调全身控制、双臂操作和自主导航的任务中展现出其能力，以远低于现有平台的成本提供了具有竞争力的移动操作研究功能。|Zichen Jeff Cui Team|[2602.11150](http://arxiv.org/abs/2602.11150)|null|
|**2026-02-11**|**ABot-M0: VLA Foundation Model for Robotic Manipulation with Action Manifold Learning**|在异构硬件上构建通用具身智能体是机器人学的一大挑战，面临数据碎片化、表示不一致和训练目标不匹配等问题。本研究提出了ABot-M0框架，它通过建立系统的数据整理管道，同时优化模型架构和训练策略，将异构原始数据转换为统一、高效的表示。通过构建大规模UniACT数据集，统一预训练提高了跨平台和任务的知识迁移和泛化能力。为提升动作预测效率和稳定性，引入了动作流形学习（AML）。此外，框架通过双流机制实现模块化感知，整合VLM语义与几何先验和多视角输入。实验结果表明各组件独立且具有累加效益。|Mu Xu Team|[2602.11236](http://arxiv.org/abs/2602.11236)|**[link](https://amap-cvlab.github.io/ABot-Manipulation/)**|
|**2026-02-11**|**OSIL: Learning Offline Safe Imitation Policies with Safety Inferred from Non-preferred Trajectories**|离线安全模仿学习面临在缺乏每时间步安全成本或奖励信息的演示数据中学习安全和奖励最大化策略的难题。本研究针对此问题提出了一种新颖的离线安全模仿学习算法OSIL，它通过非首选轨迹推断安全性。OSIL将安全策略学习表述为约束马尔可夫决策过程（CMDP），并通过推导奖励最大化目标的下界并学习一个估计非首选行为可能性的成本模型来重新构建CMDP问题，从而无需明确的安全成本和奖励标注。实验证明，OSIL能学习到更安全的策略，满足成本约束而不降低奖励性能，优于现有基线方法。|Balaraman Ravindran Team|[2602.11018](http://arxiv.org/abs/2602.11018)|null|
|**2026-02-11**|**Towards Learning a Generalizable 3D Scene Representation from 2D Observations**|现有方法在相机坐标系中构建表示，限制了其在机器人操作中的直接应用。本研究引入了一种可泛化的神经辐射场方法，用于根据以机器人自我为中心的观察来预测3D工作空间占用。该模型在全局工作空间框架中构建占用表示，使其直接适用于机器人操作，并能集成灵活的源视图，无需场景特定微调即可泛化到未见物体布局。在人形机器人上的实验证明，该模型在40个真实场景上训练后，实现了26毫米的重建误差（包括遮挡区域），验证了其推断完整3D占用（超越传统立体视觉方法）的能力。|Stefan Wermter Team|[2602.10943](http://arxiv.org/abs/2602.10943)|null|
|**2026-02-11**|**Semi-Supervised Cross-Domain Imitation Learning**|跨域模仿学习（CDIL）通过领域知识迁移加速策略学习，但在专家数据收集成本高昂时尤为重要。现有CDIL方法或依赖监督（不稳定），或无监督（不稳健）。本研究提出了半监督CDIL（SS-CDIL）设置及其首个具有理论依据的算法。该方法仅使用少量目标专家演示和未标记的不完善轨迹等离线数据。为解决域差异，提出一种新颖的跨域损失函数来学习域间状态-动作映射，并设计自适应权重函数以平衡源域和目标域知识。实验表明，该方法在MuJoCo和Robosuite上始终优于基线，实现了最小监督下稳定且数据高效的策略学习。|Ping-Chun Hsieh Team|[2602.10793](http://arxiv.org/abs/2602.10793)|null|
|**2026-02-11**|**Say, Dream, and Act: Learning Video World Models for Instruction-Driven Robot Manipulation**|机器人操作通常缺乏预测环境响应动作的能力，导致错误和低效，且现有视觉-语言模型（VLM）和世界模型在预测未来状态或生成空间一致帧方面存在局限。本研究提出一种用于快速且可预测的视频条件动作框架。该方法首先选择并适应一个鲁棒的视频生成模型以确保可靠的未来预测，然后应用对抗蒸馏进行快速、少步骤的视频生成，最后训练一个动作模型，利用生成的视频和真实观察来纠正空间误差。大量实验表明，该方法生成的时间连贯、空间准确的视频预测直接支持精确操作，在具身一致性、空间指代能力和任务完成度方面显著优于现有基线。|Yanwei Fu Team|[2602.10717](http://arxiv.org/abs/2602.10717)|null|

<p align=right>(<a href=#updated-on-20260222>back to top</a>)</p>

## World Model

|Publish Date|Title|Chinese Summary|Authors|PDF|Code|
|---|---|---|---|---|---|
|**2026-02-19**|**Reverso: Efficient Time Series Foundation Models for Zero-shot Forecasting**|当前零样本时间序列预测的基础模型，尤其是基于Transformer的大型模型，虽然性能优异，但存在参数量巨大、效率低下且使用成本高昂的问题。为此，本研究提出了一种学习高效时间序列基础模型的简洁方法，无需大型Transformer。通过采用结合长卷积和线性RNN层（如DeltaNet）的混合模型Reverso家族，并辅以数据增强和推理策略，该方法在保持相似性能的同时，模型规模缩小了数百倍，显著提升了性能-效率的帕累托前沿。|Yoon Kim Team|[2602.17634](http://arxiv.org/abs/2602.17634)|null|
|**2026-02-19**|**AI Gamestore: Scalable, Open-Ended Evaluation of Machine General Intelligence with Human Games**|在评估AI的通用智能时，传统基准测试的范围狭窄且易饱和。本研究提出通过评估AI系统玩“所有可想象的人类游戏”的能力来衡量其通用智能，并为此引入了“人类游戏的多重宇宙”概念。为实现此愿景，作者开发了AI GameStore平台，利用大型语言模型和人机协作从现有数字游戏平台合成新的、具有代表性的人类游戏。概念验证表明，即使是领先的视觉-语言模型在这些游戏中表现仍远低于人类水平，尤其在世界模型学习、记忆和规划方面存在不足，该平台为推动AI通用智能发展提供了实用途径。|Joshua B. Tenenbaum Team|[2602.17594](http://arxiv.org/abs/2602.17594)|null|
|**2026-02-19**|**Systematic Evaluation of Single-Cell Foundation Model Interpretability Reveals Attention Captures Co-Expression Rather Than Unique Regulatory Signal**|单细胞基础模型在机制可解释性方面缺乏系统评估框架。本研究提出了一个包含多项分析、统计测试及多种细胞和扰动模式的综合评估框架，并将其应用于scGPT和Geneformer模型。研究发现，尽管注意力模式编码了结构化的生物学信息（早期层捕获蛋白质-蛋白质相互作用，晚期层捕获转录调控），但这种结构对扰动预测并未带来额外价值，简单的基因层面基线表现更优。为解决注意力机制的尺度失效问题，引入的细胞状态分层可解释性（CSSI）方法将基因调控网络（GRN）恢复能力提高了1.85倍，并为该领域建立了可重用的质量控制标准。|Ihor Kendiukhov Team|[2602.17532](http://arxiv.org/abs/2602.17532)|null|
|**2026-02-19**|**Dataless Weight Disentanglement in Task Arithmetic via Kronecker-Factored Approximate Curvature**|任务算术为基础模型提供了模块化、可扩展的适应方式，但多个任务向量的组合可能导致跨任务干扰和表示漂移。现有正则化方法通常需要外部任务数据，这与模块化和数据可用性限制相冲突。本研究提出一种无数据方法，将正则化表示漂移问题建模为曲率矩阵近似问题，并采用Kronecker分解近似曲率（K-FAC）技术。该方法在任务添加和否定方面取得了SOTA结果，具有与任务数量无关的恒定复杂度，并提高了对任务向量重缩放的鲁棒性，从而无需进行额外调优。|Simone Calderara Team|[2602.17385](http://arxiv.org/abs/2602.17385)|null|
|**2026-02-19**|**Computer-Using World Model**|在复杂的软件环境中，智能体需推理操作后果，但真实执行不支持反事实探索，使得大规模试错学习和规划变得不切实际。本研究引入了计算机使用世界模型（CUWM），一个桌面软件的世界模型，它能预测给定当前状态和候选动作的下一个UI状态。CUWM采用两阶段分解UI动态：首先预测相关状态变化的文本描述，再将其视觉化以合成截图。在真实Microsoft Office应用上的离线UI转换数据上训练，并通过强化学习进行优化。结果显示，CUWM在测试时通过动作搜索提高了决策质量和执行鲁棒性，增强了Office任务的性能。|Dongmei Zhang Team|[2602.17365](http://arxiv.org/abs/2602.17365)|null|
|**2026-02-19**|**FRAPPE: Infusing World Modeling into Generalist Policies via Multiple Future Representation Alignment**|使视觉-语言-动作（VLA）模型具备世界建模能力对提升机器人推理和泛化至关重要，但现有方法存在像素级重建过度强调和推理时误差累积问题。本研究提出“通过并行渐进扩展实现未来表示对齐（FRAPPE）”方法，采用两阶段微调策略：首先学习预测未来观测的潜在表示，随后并行扩展计算量并与多个视觉基础模型对齐表示。FRAPPE显著提高了微调效率并减少了对带动作标注数据的依赖。实验证明，FRAPPE在RoboTwin基准和真实任务中超越了现有SOTA，并在长时序和未见场景中表现出强大的泛化能力。|Donglin Wang Team|[2602.17259](http://arxiv.org/abs/2602.17259)|**[link](https://h-zhao1997.github.io/frappe)**|
|**2026-02-19**|**Structured Prototype-Guided Adaptation for EEG Foundation Models**|脑电图（EEG）基础模型在全微调下性能优异，但在真实临床场景中受试者级监督有限时泛化能力差，这源于有限监督与模型参数空间的高度可塑性之间的结构性不匹配。本研究提出SCOPE框架，一个结构化置信度感知原型引导的EFM微调适配方法。SCOPE通过构建可靠外部监督（学习几何正则化任务先验、构建类级原型并生成置信度感知伪标签）和引入轻量级ProAdapter来适应冻结的EFM。实验结果显示，SCOPE在标签受限的跨受试者设置下，在多种EEG任务和基础模型上均实现了强大的性能和效率。|Mengling Feng Team|[2602.17251](http://arxiv.org/abs/2602.17251)|null|
|**2026-02-19**|**Decoding the Human Factor: High Fidelity Behavioral Prediction for Strategic Foresight**|在高风险环境中预测人类决策是AI的挑战，大型语言模型（LLMs）在生成个体特异性行为时常遇困难，提示词方法易受身份漂移影响。本研究引入大型行为模型（LBM），一个通过对高维特质配置文件进行条件化处理的行为基础模型，实现高保真度个体战略选择预测。LBM从瞬态提示转向行为嵌入，并在关联稳定倾向、动机状态和情境约束的专有数据集上训练。评估显示，LBM微调显著提升了行为预测能力，且其性能随特质维度增加而持续提升，克服了提示词方法的复杂性上限，为行为模拟提供了可扩展方案。|Shula Grinapol Team|[2602.17222](http://arxiv.org/abs/2602.17222)|null|
|**2026-02-19**|**Continual learning and refinement of causal models through dynamic predicate invention**|智能体在复杂环境中高效导航需内化世界逻辑，但传统世界建模方法通常存在样本效率低、缺乏透明度和可扩展性差的问题。本研究提出一种框架，通过将连续模型学习与修复整合到智能体决策循环中，在线构建符号因果世界模型。该框架利用元解释学习和谓词发明来发现语义上有意义且可重用的抽象，从而构建解耦的、高质量概念的层次结构。结果表明，其提升推理方法可扩展到复杂关系动力学领域，解决了命题方法组合爆炸的问题，并实现了比PPO神经网络基线高出数个数量级的样本效率。|Peter Flach Team|[2602.17217](http://arxiv.org/abs/2602.17217)|null|
|**2026-02-19**|**JEPA-DNA: Grounding Genomic Foundation Models through Joint-Embedding Predictive Architectures**|基因组基础模型（GFMs）通常依赖掩码语言建模（MLM）或下一词元预测（NTP），这些方法擅长捕获局部基因组语法，但缺乏全局生物学功能上下文。本研究提出JEPA-DNA，一个结合联合嵌入预测架构（JEPA）与传统生成目标的新型预训练框架。JEPA-DNA通过耦合词元级恢复与潜在空间预测目标（监督CLS词元）引入潜在接地，促使模型预测掩码基因组片段的高级功能嵌入。评估结果显示，JEPA-DNA在监督和零样本任务中均持续优于仅生成基线，为构建理解基因组字母及其底层功能逻辑的基础模型提供了可扩展途径。|Yoli Shavit Team|[2602.17162](http://arxiv.org/abs/2602.17162)|null|
|**2026-02-19**|**AudioChat: Unified Audio Storytelling, Editing, and Understanding with Transfusion Forcing**|现有音频基础模型在处理包含多说话者和背景音效的复杂多源声学场景（即“音频故事”）时面临挑战。为解决此问题，本研究提出了AudioChat框架，通过LLM驱动的工具调用代理模拟用户与系统交互生成训练数据，并引入了“Audio Transfusion Forcing”训练目标，使模型能够通过结构化思维链分解高级指令并进行多轮交互式音频理解与生成。为评估模型性能，研究开发了三项新的任务性能衡量指标。|Zeyu Jin Team|[2602.17097](http://arxiv.org/abs/2602.17097)|null|
|**2026-02-18**|**Parameter-free representations outperform single-cell foundation models on downstream benchmarks**|单细胞RNA测序（scRNA-seq）数据展现出可重现的统计结构，促使了基于Transformer的大规模基础模型（如TranscriptFormer）发展，用于下游任务。本研究探讨了是否无需计算密集型深度学习表示也能实现类似性能。通过采用依赖精心归一化和线性方法的简单、可解释的流水线，研究在多个单细胞基础模型常用基准测试中达到了SOTA或接近SOTA的性能，甚至在涉及训练数据中未出现的新细胞类型和生物体的域外任务中超越了基础模型。研究结果强调了严格基准测试的重要性，并表明细胞身份的生物学特性可通过单细胞基因表达数据的简单线性表示来捕获。|Pankaj Mehta Team|[2602.16696](http://arxiv.org/abs/2602.16696)|null|
|**2026-02-18**|**Are Object-Centric Representations Better At Compositional Generalization?**|组合泛化能力是人类认知的基本要素，也是机器学习的重大挑战，而以对象为中心的（OC）表示被认为能支持此能力，但视觉丰富环境中的系统性证据有限。本研究引入了跨三个受控视觉世界的视觉问答基准，以评估带有或不带OC偏置的视觉编码器在泛化到未见过的对象属性组合时的表现。研究通过严谨控制训练数据多样性、样本量和计算等因素，对DINOv2和SigLIP2及其OC版本进行了比较。结果表明，在更复杂的组合泛化任务中，OC方法表现更优；原始密集表示仅在更简单的任务中超越OC且需要更多计算；OC模型在样本效率上更高，在数据集大小、训练数据多样性或下游计算受限时提供了更强的组合泛化能力。|Andrea Dittadi Team|[2602.16689](http://arxiv.org/abs/2602.16689)|null|
|**2026-02-18**|**Scaling Open Discrete Audio Foundation Models with Interleaved Semantic, Acoustic, and Text Tokens**|当前音频语言模型多以文本为先，限制了通用音频建模。本文提出并系统研究了原生音频基础模型，通过大规模的下一令牌预测，联合建模语义内容、声学细节和文本，以支持通用音频生成和跨模态能力。研究者系统探索了数据源、文本混合比例等设计选择，并首次对离散音频模型进行了缩放定律研究。基于这些发现，他们训练了SODA模型套件，并在保留说话者声音的语音-语音翻译等任务中展示了其作为灵活骨干网络的有效性。|Diyi Yang Team|[2602.16687](http://arxiv.org/abs/2602.16687)|null|
|**2026-02-18**|**Retrieval-Augmented Foundation Models for Matched Molecular Pair Transformations to Recapitulate Medicinal Chemistry Intuition**|针对药物化学中类似物设计缺乏高效、可控的分子编辑方法，现有机器学习方法存在局限性的问题，本文提出了一种基于大规模匹配分子对转换（MMPTs）的变异到变异（variable-to-variable）类似物生成基础模型。该模型通过提示机制实现用户对转换模式的精确控制，并引入MMPT-RAG框架利用外部参考类似物进行上下文指导。实验证明，该方法在通用化学语料库和专利数据集上显著提升了生成多样性、新颖性和可控性，能在实际发现场景中恢复出真实的类似物结构。|Liang Zhao Team|[2602.16684](http://arxiv.org/abs/2602.16684)|null|
|**2026-02-18**|**Learning Situated Awareness in the Real World**|多模态基础模型（MFMs）现有基准多侧重环境中心空间关系，忽视了以观察者为中心的、需推理智能体视角和动作的情境感知。为填补此空白，研究者引入了SAW-Bench，一个基于Ray-Ban Meta智能眼镜录制真实世界视频的新型基准，用于评估自我中心情境感知。该基准包含786个视频和2071个问答对，通过六项感知任务探测模型的观察者中心理解。评估显示，即使是最佳MFM，与人类仍有显著差距，模型常未能推断出连贯的相机几何导致空间推理错误，突显了超越被动观察、理解以观察者为中心的物理动态的重要性。|Xin Eric Wang Team|[2602.16682](http://arxiv.org/abs/2602.16682)|null|
|**2026-02-18**|**VETime: Vision Enhanced Zero-Shot Time Series Anomaly Detection**|时间序列异常检测（TSAD）需要识别点异常和上下文异常，但现有基础模型在精细点定位和全局上下文理解之间存在权衡。为解决这一困境，本文提出了VETime，首个通过细粒度视觉-时间对齐和动态融合统一时间与视觉模态的TSAD框架。VETime引入可逆图像转换和补丁级时间对齐模块，以建立共享的视觉-时间轴并保留细节。此外，它设计了异常窗口对比学习和任务自适应多模态融合机制。大量实验表明，VETime在零样本场景中显著优于先进模型，实现了更高的定位精度和更低的计算开销。|Chen Zhang Team|[2602.16681](http://arxiv.org/abs/2602.16681)|null|
|**2026-02-18**|**Learning to unfold cloth: Scaling up world models to deformable object manipulation**|机器人布料操作因其复杂的物理特性而极具挑战性，需要通用策略以适应不同布料特性。本文提出一种改进的DreamerV2强化学习架构应用于空中布料操作，通过引入表面法线作为输入，并优化回放缓冲区及数据增强程序，增强了机器人使用的世界模型来应对物理复杂性。在仿真和物理机器人的零样本部署实验中，该方法成功实现了多种布料的空中展开，证明了所提出架构在泛化性能上的显著优势。|Subramanian Ramamoorthy Team|[2602.16675](http://arxiv.org/abs/2602.16675)|null|
|**2026-02-18**|**A Systematic Evaluation of Sample-Level Tokenization Strategies for MEG Foundation Models**|神经影像数据的基础模型需将连续神经时间序列数据“令牌化”，但不同令牌化策略的影响尚不明确。本文对应用于脑磁图（MEG）数据的基于Transformer的大型神经影像模型（LNMs）的样本级令牌化策略进行了系统评估。通过比较可学习（引入基于自编码器的新方法）和不可学习令牌器在信号重建保真度、基础建模性能及下游任务上的表现，研究发现在多个MEG数据集上，两者均实现了高重建精度和大致相当的性能，表明简单的固定样本级令牌化策略足以支持神经基础模型的开发。|Mark W. Woolrich Team|[2602.16626](http://arxiv.org/abs/2602.16626)|null|
|**2026-02-18**|**Why Thinking Hurts? Diagnosing and Rectifying the Reasoning Shift in Foundation Recommender Models**|将思维链（CoT）推理整合到语义ID推荐基础模型中，常导致推荐性能下降，究其原因在于“通用子空间”中冗长推理的文本惯性，使得模型忽视关键语义ID。为解决此问题，本文提出一个训练无关的“推理时子空间对齐”框架。该方法通过压缩推理链和应用偏差减去的对比解码，有效缓解了无根据的文本漂移。实验证明，此框架能有效校准推理过程，使基础模型在利用推理能力的同时，不牺牲基于ID的推荐准确性。|Enhong Chen Team|[2602.16587](http://arxiv.org/abs/2602.16587)|null|
|**2026-02-18**|**Arc2Morph: Identity-Preserving Facial Morphing with Arc2Face**|人脸融合攻击对电子身份文档中的人脸识别系统构成严峻威胁，尤其是在护照注册过程中缺乏实时监督采集时。本文提出一种基于身份条件化人脸基础模型Arc2Face的新型人脸融合技术，能够从紧凑的身份表示合成逼真的人脸图像。通过在多个大规模数据集上与现有先进方法进行比较，实验结果表明，所提出的深度学习方法在融合攻击潜力方面达到了与传统上最具挑战性的基于地标技术相当的水平，证实了其在融合生成过程中有效保留和管理身份信息的能力。|Davide Maltoni Team|[2602.16569](http://arxiv.org/abs/2602.16569)|null|
|**2026-02-18**|**MMA: Multimodal Memory Agent**|长时程多模态智能体在依赖外部记忆时，常因检索到过时、低可信或冲突信息而产生过度自信的错误。针对此问题，本文提出了多模态记忆智能体（MMA），该智能体结合来源可信度、时间衰减和冲突感知网络共识，动态评估每个记忆项的可靠性，并利用此信号加权证据或在支持不足时弃权。同时，引入了MMA-Bench基准来研究信念动态。实验结果表明，在FEVER数据集上，MMA在保持基线准确率的同时，将方差降低了35.2%；在LoCoMo上，提高了操作准确性并减少了错误答案；在MMA-Bench上，视觉模式下MMA的准确率显著优于基线，并揭示了RAG智能体中潜在的“视觉安慰剂效应”。|Hao Tang Team|[2602.16493](http://arxiv.org/abs/2602.16493)|null|
|**2026-02-18**|**RoboGene: Boosting VLA Pre-training via Diversity-Driven Agentic Framework for Real-World Task Generation**|通用机器人操作受限于稀缺且成本高昂的真实世界交互数据，且现有任务策划方法不可扩展或易产生不可行指令。为解决这一问题，本文提出了RoboGene框架，旨在自动化生成多样化且物理上可行的单臂、双臂和移动机器人操作任务。RoboGene包含多样性驱动采样、自反思机制以强制物理约束以及人机协作精炼等核心组件。实验结果表明，RoboGene在定量分析和大规模真实世界实验中显著优于现有基础模型，且使用RoboGene预训练的VLA模型展现出更高的成功率和泛化能力，强调了高质量任务生成的重要性。|Jian Tang Team|[2602.16444](http://arxiv.org/abs/2602.16444)|null|
|**2026-02-18**|**Automated Histopathology Report Generation via Pyramidal Feature Extraction and the UNI Foundation Model**|从千兆像素级的组织病理学全玻片图像（WSIs）生成精确的诊断文本面临巨大挑战。本文提出了一个分层视觉语言框架，将一个冻结的病理学基础模型与Transformer解码器相结合进行报告生成。该方法通过多分辨率金字塔补丁选择和背景伪影去除技术处理WSI，并利用UNI Vision Transformer提取特征，随后由Transformer解码器生成文本，并使用BioGPT进行分词。为提高可靠性，该框架还引入了一个基于检索的验证步骤，通过比较生成报告与参考语料库来修正报告内容。|Serkan Sokmen Team|[2602.16422](http://arxiv.org/abs/2602.16422)|null|
|**2026-02-18**|**CADEvolve: Creating Realistic CAD via Program Evolution**|计算机辅助设计（CAD）的AI自动化受限于缺乏复杂操作和设计意图的数据集，导致现有方法难以生成工业级程序。为此，本文提出了CADEvolve，一个基于进化的管道和数据集，它从简单基元开始，通过VLM引导的编辑和验证，逐步生成工业级复杂性的CAD程序。该方法生成了8k个复杂零件作为可执行的CadQuery参数化生成器，并经过处理和增强后，形成了一个包含130万个脚本的统一数据集。实验结果表明，在CADEvolve上微调的VLM在DeepCAD、Fusion 360和MCB基准测试的Image2CAD任务上均达到了最先进的性能。|Dmitrii Zhemchuzhnikov Team|[2602.16317](http://arxiv.org/abs/2602.16317)|null|
|**2026-02-18**|**AFFMAE: Scalable and Efficient Vision Pretraining for Desktop Graphics Cards**|自监督预训练虽提高了计算机视觉效率，但高分辨率训练仍需服务器级基础设施，限制了基础模型的开发。传统MAE与分层架构结合时面临密集网格和掩码感知设计挑战。本文提出了AFFMAE，一个基于自适应、非网格token合并的掩码友好型分层预训练框架，通过丢弃被掩码的token并仅对可见token执行动态合并，消除了密集网格假设并保持了分层可扩展性。实验结果表明，在相同参数量下，AFFMAE在高分辨率电子显微镜分割任务上匹配了ViT-MAE的性能，同时将FLOPs减少高达7倍，内存使用减半，并在单个RTX 5090上实现了更快的训练。|Behzad Najafian Team|[2602.16249](http://arxiv.org/abs/2602.16249)|null|
|**2026-02-18**|**EasyControlEdge: A Foundation-Model Fine-Tuning for Edge Detection**|在实际边缘检测中，清晰度和数据效率至关重要，但用有限数据生成清晰边缘图极具挑战，且图像生成基础模型在边缘检测领域的潜力尚未充分挖掘。本文提出了EasyControlEdge，旨在将图像生成基础模型适应于边缘检测任务，以实现高清晰度和数据高效性。该方法通过引入边缘导向目标和高效像素空间损失来专门化基础模型，并在推理时利用基于无条件动力学的引导，实现通过引导尺度控制边缘密度。实验结果显示，EasyControlEdge在多个基准测试中持续优于现有方法，特别是在无后处理清晰度评估和有限训练数据条件下表现突出。|Tadahiro Taniguchi Team|[2602.16238](http://arxiv.org/abs/2602.16238)|null|
|**2026-02-18**|**Factored Latent Action World Models**|从无动作视频中学习潜在动作是构建可控世界模型的强大范式，但现有方法通常依赖单一动力学模型控制整个场景，难以应对多实体复杂环境。为解决此问题，本文提出了因子化潜在动作模型（FLAM），该框架将场景分解为独立因子，每个因子推断其自身潜在动作并预测下一时刻状态。这种因子化结构能够更准确地建模复杂的多实体动力学。实验结果表明，FLAM在模拟和真实世界多实体数据集上的预测准确性和表示质量均优于现有方法，改善了视频生成质量，并促进了下游策略学习。|Peter Stone Team|[2602.16229](http://arxiv.org/abs/2602.16229)|null|
|**2026-02-18**|**World Model Failure Classification and Anomaly Detection for Autonomous Inspection**|自主检查机器人可降低工业现场监测的成本与风险，但遮挡、视角受限等问题使准确读数充满挑战。本文提出了一个混合框架，结合监督式故障分类与异常检测，将检查任务分类为成功、已知故障或异常情况。该方法以带有压缩视频输入的世界模型为骨干，并通过共形预测阈值确定的两个决策函数在人类观察者之前进行分类。在办公室和工业现场仪表检查上的实验结果显示，该框架在区分成功、故障和OOD情况方面准确率超过90%，且分类发生时间早于人类观察者，展现了其在自主检查中实现鲁棒、预见性故障检测的潜力。|Shayegan Omidshafiei Team|[2602.16182](http://arxiv.org/abs/2602.16182)|null|
|**2026-02-18**|**BrainRVQ: A High-Fidelity EEG Foundation Model via Dual-Domain Residual Quantization and Hierarchical Autoregression**|由于低信噪比和复杂的时频谱非平稳性，开发脑电图（EEG）基础模型并重建精细信息仍面临挑战。为此，本研究提出了BrainRVQ，一个通用的EEG基础模型，在大规模临床EEG数据上预训练。该模型采用双域残差向量量化（DD-RVQ）分词器将时域波形和频谱模式解耦为分层离散代码，并通过分层自回归预训练目标和“重要性引导的课程掩码策略”从粗到细地重建这些代码。在8个下游数据集上的广泛实验表明，BrainRVQ持续优于现有先进基线，验证了其学习鲁棒和可泛化神经表示的有效性。|Luca Mainardi Team|[2602.16951](http://arxiv.org/abs/2602.16951)|null|
|**2026-02-18**|**How should AI knowledge be governed? Epistemic authority, structural transparency, and the case for open cognitive graphs**|教育AI系统在实际应用中拥有认知权威，但缺乏类似人类教育者的制度化问责机制，这构成了结构性治理难题。本研究将教育AI重新定义为公共教育认知基础设施，并提出了“开放认知图谱（OCG）”作为技术接口，通过显式表示概念、前提、错误认知和支架，使AI行为的认知逻辑可被审查和修改。在此基础上，研究引入了“主干-分支”治理模型来组织认知权威，并通过社区治理的教育基础模型案例研究，展示了如何通过制度化流程整合分布式专业知识，从而实现教育AI与民主问责制的对齐。|Yi Hua Team|[2602.16949](http://arxiv.org/abs/2602.16949)|null|
|**2026-02-18**|**BEMEval-Doc2Schema: Benchmarking Large Language Models for Structured Data Extraction in Building Energy Modeling**|大语言模型（LLM）等基础模型为自动化建筑能耗建模（BEM）提供了新机遇，但缺乏公开数据集和标准化评估指标限制了系统性评估。本研究提出了BEMEval基准框架，其中BEMEval-Doc2Schema专注于从建筑文档中提取结构化数据，并引入“键值重叠率（KVOR）”指标以量化模型输出与真实图式的一致性。利用该框架，在三个数据集上评估了GPT-5和Gemini 2.5，结果显示Gemini 2.5持续优于GPT-5，且少样本提示均能提高准确性，同时简单图式表现更佳。该研究为AI辅助BEM工作流奠定了首个社区驱动的评估基准。|Liang Zhang Team|[2602.16926](http://arxiv.org/abs/2602.16926)|null|
|**2026-02-18**|**Beyond the Flag: A Framework for Integrating Cybersecurity Competitions into K-12 Education for Cognitive Apprenticeship and Ethical Skill Development**|夺旗（CTF）竞赛是弥补网络安全人才缺口的有效教学工具，但在K-12教育中面临教师准备不足和公平性等实施障碍。本研究提出了“网络安全伦理认知学徒（ECAC）”框架，该模型整合了认知学徒理论与嵌入式伦理发展，分为五个阶段，旨在提供“低门槛，高上限”的学习路径，以扩大包括少数族裔和女性在内的不同学生群体的参与。通过将教育者重新定义为“首席学习者”，ECAC还提供了解决教师专业知识差距的可持续方案，旨在将CTF转变为培养更专业、更具伦理素养和更多样化网络安全人才的综合学习体验。|Nam Son Nguyen Team|[2602.16921](http://arxiv.org/abs/2602.16921)|null|
|**2026-02-18**|**StereoAdapter-2: Globally Structure-Consistent Underwater Stereo Depth Estimation**|水下立体深度估计受光衰减、散射和折射导致的严重域偏移影响，现有GRU迭代精修方法在处理大视差和无纹理区域时性能受限。本研究提出了StereoAdapter-2，将传统ConvGRU更新器替换为基于选择性状态空间模型的新型ConvSS2D算子，该算子通过四向扫描策略与极线几何对齐，从而在一个更新步骤内以线性计算复杂度实现高效长距离空间传播。此外，研究构建了大规模合成水下立体数据集UW-StereoDepth-80K。结合动态LoRA适应，该框架在TartanAir-UW和SQUID基准测试中实现了17%和7.2%的性能提升，并通过真实机器人平台验证了其鲁棒性。|Hao Tang Team|[2602.16915](http://arxiv.org/abs/2602.16915)|null|
|**2026-02-18**|**SparTa: Sparse Graphical Task Models from a Handful of Demonstrations**|机器人学习中高效学习长时程操作任务是一项核心挑战，现有方法多关注学习动作而非任务目标。本研究提出通过一系列图形化的对象关系来表示不断变化的场景状态，并引入一种演示分割和池化方法，以提取操作图并估计跨任务阶段的对象状态分布。与以往仅捕获局部交互的基于图的方法不同，本方法能捕获从控制开始到操作结束的完整对象交互，并通过使用预训练视觉特征进行对象匹配以提高学习的鲁棒性。实验证明了方法在演示分割准确性上的有效性，以及学习到任务模型支持在仿真和真实机器人上跨环境可靠执行的能力。|Abhinav Valada Team|[2602.16911](http://arxiv.org/abs/2602.16911)|null|
|**2026-02-18**|**Position: Why a Dynamical Systems Perspective is Needed to Advance Time Series Modeling**|时间序列（TS）建模已发展至TS基础模型，但其进展尚不完全明朗，领域需要动力系统（DS）视角以实现进一步突破。本研究强调了动力系统重建（DSR）的潜力，该方法旨在从数据中推断底层动力系统模型，从而不仅能实现短期预测，还能预测系统的长期统计行为。DS理论还能提供领域无关的见解，揭示TS生成机制、性能上限、未知区域的泛化能力及潜在控制策略。本研究回顾了DS理论和DSR的核心概念，并提出了将这些见解转化为TS建模的具体建议，以期实现更优预测和更低的计算与内存开销。|Lukas Eisenmann Team|[2602.16864](http://arxiv.org/abs/2602.16864)|null|
|**2026-02-17**|**MedProbCLIP: Probabilistic Adaptation of Vision-Language Foundation Model for Reliable Radiograph-Report Retrieval**|视觉-语言基础模型在多模态理解方面潜力巨大，但其确定性嵌入难以满足高风险生物医学应用对可靠性的要求。本文提出了MedProbCLIP，一个用于胸部X射线和放射学报告表示学习及双向检索的概率视觉-语言学习框架。MedProbCLIP通过概率对比目标将图像和文本表示建模为高斯嵌入，显式捕获不确定性和多对多对应关系，并利用变分信息瓶颈减轻过度自信预测。实验结果表明，在MIMIC-CXR数据集上，MedProbCLIP在检索和零样本分类方面均优于现有基线，并展现出卓越的校准性、风险-覆盖行为、选择性检索可靠性以及对临床相关损坏的鲁棒性，提升了放射学图像-文本检索系统的可信度和安全性。|Gongbo Liang Team|[2602.16019](http://arxiv.org/abs/2602.16019)|null|
|**2026-02-17**|**Neural Scaling Laws for Boosted Jet Tagging**|大型语言模型（LLMs）的成功表明计算规模扩展是性能提升的关键，然而高能物理（HEP）领域最先进模型的训练计算量远低于工业界。针对该背景，本文研究了使用公共JetClass数据集进行增压射流分类的神经网络扩展定律。研究推导了计算最优的扩展定律，并识别出一个可通过增加计算持续接近的有效性能极限。研究结果表明，增加计算能可靠地推动性能接近渐近极限，且更具表达能力的低级特征可以提高性能极限并在固定数据集大小下改善结果，同时量化了数据重复对有效数据集大小的增益影响。|Lukas Heinrich Team|[2602.15781](http://arxiv.org/abs/2602.15781)|null|
|**2026-02-16**|**EditCtrl: Disentangled Local and Global Control for Real-Time Generative Video Editing**|背景：高保真生成式视频编辑依赖预训练视频基础模型，但其计算成本高昂，即使是局部编辑也需处理整个视频上下文。方法：本文提出EditCtrl，一个高效的视频修复控制框架。它引入了新颖的局部视频上下文模块，仅对掩码标记进行操作，使计算成本与编辑区域大小成正比。同时，一个轻量级的时间全局上下文嵌入器确保视频整体上下文的一致性。结果：EditCtrl比现有先进方法计算效率高10倍，并提升了编辑质量。该方法还支持多区域文本提示编辑和自回归内容传播。|Caleb Leak Team|[2602.15031](http://arxiv.org/abs/2602.15031)|**[link](https://yehonathanlitman.github.io/edit_ctrl)**|
|**2026-02-16**|**Generalization from Low- to Moderate-Resolution Spectra with Neural Networks for Stellar Parameter Estimation: A Case Study with DESI**|背景：在恒星光谱分析中，跨巡天泛化能力（特别是从低分辨率到中分辨率光谱的迁移）是一个关键挑战。方法：本文研究了使用预训练多层感知机（MLPs）解决此问题，以LAMOST低分辨率光谱到DESI中分辨率光谱的迁移为例。作者在LAMOST低分辨率光谱或其嵌入上预训练MLPs，并在DESI光谱上进行微调，比较了直接在光谱上训练的MLPs与基于Transformer模型嵌入训练的MLPs，并评估了不同的微调策略。结果：预训练在LAMOST低分辨率光谱上的MLPs表现出色，即便不微调也能获得良好性能，适度微调可进一步提升。研究表明，简单预训练MLPs能提供有竞争力的跨巡天泛化能力，但光谱基础模型在跨巡天恒星参数估计中的作用仍需深入探索。|Viska Wei Team|[2602.15021](http://arxiv.org/abs/2602.15021)|null|
|**2026-02-16**|**Cold-Start Personalization via Training-Free Priors from Structured World Models**|背景：冷启动个性化（即在无用户历史数据时推断用户偏好）是一个挑战，因为用户只关心少数偏好维度，且关键维度因人而异。现有强化学习方法在多轮交互中难以有效利用偏好数据的分因子结构。方法：本文提出Pep（Preference Elicitation with Priors）框架，将冷启动偏好获取分解为离线结构学习和在线贝叶斯推理。Pep离线从完整用户档案中学习偏好相关性的结构化世界模型，然后在线进行免训练的贝叶斯推理，以选择信息丰富的提问并预测完整的偏好档案。结果：Pep在生成响应与用户偏好的一致性方面达到80.8%，远高于强化学习的68.5%，且交互次数减少3-5倍。它仅用约1万参数就实现此效果，而强化学习需80亿参数，突显了利用偏好数据分因子结构的重要性。|Asli Celikyilmaz Team|[2602.15012](http://arxiv.org/abs/2602.15012)|null|
|**2026-02-16**|**PDE foundation models are skillful AI weather emulators for the Martian atmosphere**|背景：为火星大气层构建熟练的预测天气模拟器，面临训练数据和计算资源不足的挑战。方法：本文展示了如何将预训练在多源偏微分方程数值解上的AI基础模型（Poseidon PDE基础模型）适配并微调，以构建火星大气的预测天气模拟器。研究扩展了Poseidon模型从二维到三维的方法，同时保留了预训练信息，并探讨了在稀疏初始条件下的模型性能。结果：通过预训练与模型扩展的结合，模型在独立验证年份的性能提升了34.4%。这表明偏微分方程基础模型不仅能近似其他偏微分方程的解，还能作为解决实际世界复杂交互问题的锚定模型，尤其是在训练数据或计算预算有限的情况下。|Juan Bernabe-Moreno Team|[2602.15004](http://arxiv.org/abs/2602.15004)|null|
|**2026-02-16**|**Use What You Know: Causal Foundation Models with Partial Graphs**|背景：传统的因果量估计依赖于为特定假设定制的估计器。新兴的因果基础模型（CFMs）提供统一方法，但目前无法融入领域知识，导致预测次优。方法：本文提出将因果信息（如完整因果图或部分祖先信息）条件化到CFMs中的方法。研究系统评估了不同的条件化策略，发现将可学习偏差注入注意力机制是利用完整和部分因果信息最有效的方法。结果：通过条件化，通用CFM的性能可以与针对特定因果结构训练的专用模型相媲美。这一方法克服了构建一体化因果基础模型的核心障碍，使其能够以数据驱动的方式回答因果查询，同时有效利用任何程度的领域专业知识。|Bernhard Schölkopf Team|[2602.14972](http://arxiv.org/abs/2602.14972)|null|
|**2026-02-16**|**Model Context Protocol (MCP) Tool Descriptions Are Smelly! Towards Improving AI Agent Efficiency with Augmented MCP Tool Descriptions**|背景：基于基础模型（FM）的智能体通过工具描述与外部系统交互，但这些自然语言描述中的缺陷可能误导FM，其普遍性和影响尚不明确。方法：本文对103个MCP服务器上的856个工具进行了大规模实证研究。通过识别工具描述的六个组成部分，开发了评分标准并据此形式化了“工具描述异味”。利用FM-based扫描器进行操作化评估，并增强描述以评估其对智能体性能的影响。结果：97.1%的工具描述至少含有一种“异味”，其中56%未能清晰阐明目的。尽管增强所有组件的描述可使任务成功率中位数提升5.85个百分点，但执行步骤也增加了67.46%，并导致性能下降，揭示了性能与成本之间的权衡。|Ahmed E. Hassan Team|[2602.14878](http://arxiv.org/abs/2602.14878)|null|
|**2026-02-16**|**World Models for Policy Refinement in StarCraft II**|背景：尽管大型语言模型（LLMs）展现出强大的推理能力，但现有基于LLM的《星际争霸II》（SC2）智能体主要关注策略优化，缺乏可学习的、动作条件化的转移模型来辅助决策。方法：本文提出StarWM，这是首个在部分可观测环境下预测SC2未来观测的世界模型。为学习SC2的混合动态，作者引入了一种将观测分解为五个语义模块的结构化文本表示，并构建了首个用于SC2动态预测的指令微调数据集SC2-Dynamics-50k。StarWM被集成到“生成-模拟-细化”决策循环中，形成StarWM-Agent。结果：StarWM在资源预测准确性等指标上比零样本基线提升近60%。在线评估中，StarWM-Agent对不同难度级别（Hard、Harder、VeryHard）的胜率分别提升了30%、15%和30%，同时改善了宏观管理稳定性和战术风险评估。|Bo Xu Team|[2602.14857](http://arxiv.org/abs/2602.14857)|null|
|**2026-02-16**|**SAILS: Segment Anything with Incrementally Learned Semantics for Task-Invariant and Training-Free Continual Learning**|背景：持续学习在类增量语义分割（CISS）中面临重复训练、高计算成本和灾难性遗忘的限制，制约了其实际应用。方法：本文提出SAILS（Segment Anything with Incrementally Learned Semantics），一个免训练的CISS框架，它将CISS解耦为两个阶段：首先利用Segment Anything Model (SAM)进行零样本区域提取，然后通过固定特征空间中的原型进行语义关联。SAILS通过选择性类内聚类，为每个类生成多个原型以更好地建模类内变异性。结果：SAILS无需增量训练，但在标准CISS数据集上通常超越了现有的基于训练的方法，特别是在遗忘问题严重的长期和挑战性任务序列中。SAILS完全消除了遗忘，保持了任务不变的一致性能，并展现出正向反向迁移。|René Schuster Team|[2602.14767](http://arxiv.org/abs/2602.14767)|null|
|**2026-02-16**|**Depth Completion as Parameter-Efficient Test-Time Adaptation**|背景：现有深度补全方法通常通过训练任务特定编码器来利用辅助输入，但易过拟合且泛化性差。而3D基础模型可提供更强的几何先验。方法：本文提出CAPA，一个参数高效的测试时优化框架，用于利用稀疏几何线索对预训练3D基础模型进行深度补全。CAPA冻结基础模型骨干，仅通过参数高效微调（如LoRA或VPT）更新少量参数，并利用推理时稀疏观测直接计算梯度进行指导。对于视频，CAPA引入序列级参数共享以利用时间相关性并强制多帧一致性。结果：CAPA与任何基于ViT的基础模型兼容，并在室内外数据集的各种条件下取得了最先进的结果。它有效地将基础模型的几何先验与场景特定测量相结合，修正了畸变。|Shengyu Huang Team|[2602.14751](http://arxiv.org/abs/2602.14751)|null|
|**2026-02-16**|**WebWorld: A Large-Scale World Model for Web Agent Training**|背景：网页智能体需要大量轨迹以实现泛化，但真实世界训练受网络延迟、速率限制和安全风险制约。现有模拟器局限于封闭环境。方法：本文推出WebWorld系列，首个大规模训练的开放网络模拟器。它利用可扩展数据管道在超过100万次开放网络交互中进行训练，支持推理、多格式数据和超过30步的长周期模拟。结果：WebWorld在WebWorld-Bench上实现了与Gemini-3-Pro相当的模拟性能。在WebWorld合成轨迹上训练的Qwen3-14B在WebArena上性能提升9.2%，达到与GPT-4o相当的水平。WebWorld作为世界模型，在推理时搜索方面超越了GPT-5。此外，它还展现出跨领域泛化能力，为构建世界模型提供了可复现的方法。|Zuozhu Liu Team|[2602.14721](http://arxiv.org/abs/2602.14721)|null|
|**2026-02-16**|**Arbor: A Framework for Reliable Navigation of Critical Conversation Flows**|针对大型语言模型在医疗分诊等高风险领域难以遵循结构化工作流程的问题，以及单一提示方法在长提示下易导致指令依从性下降的挑战，本文提出了Arbor框架。该框架将决策树导航分解为节点级任务，通过基于DAG的编排机制动态检索和评估转换，并将响应生成解耦。实验结果表明，与单一提示基线相比，Arbor在真实临床分诊对话中将平均轮次准确率提高了29.4个百分点，同时显著降低了延迟和成本，证明了架构分解能有效提升模型性能并降低对模型固有能力的依赖。|Luís Ungaro Team|[2602.14643](http://arxiv.org/abs/2602.14643)|null|
|**2026-02-16**|**Tabular Foundation Models Can Learn Association Rules**|传统的关联规则挖掘（ARM）方法存在规则爆炸和可扩展性差的问题，而现有神经方法在低数据量下性能不佳，但表格基础模型（TFMs）为解决这些局限性提供了基础。为此，本文提出了一个模型无关的关联规则学习框架，能够利用TFMs从任何条件概率模型中提取关联规则，并实例化了TabProbe。实验结果显示，TabProbe利用TFMs作为条件概率估计器，无需频繁项集挖掘即可生成简洁、高质量的关联规则，在低数据设置下仍保持强大的预测性能和鲁棒性。|Victoria Degeler Team|[2602.14622](http://arxiv.org/abs/2602.14622)|null|
|**2026-02-16**|**MATEO: A Multimodal Benchmark for Temporal Reasoning and Planning in LVLMs**|AI智能体在实现复杂目标时需要规划，但现有研究对基础模型时间执行顺序（TEO）的理解有限，多局限于线性近似或纯文本输入。为解决此问题，本文引入了MATEO（MultimodAl Temporal Execution Order）基准，旨在评估和提升大型视觉语言模型（LVLMs）的时间推理能力。通过收集高质量多模态食谱语料库及相应的TEO图注释，作者使用MATEO评估了六个最先进的LVLM，考察了不同模型规模、语言上下文、多模态输入结构和微调策略对时间推理能力的影响。|Giuseppe Riccardi Team|[2602.14589](http://arxiv.org/abs/2602.14589)|null|
|**2026-02-16**|**MedVAR: Towards Scalable and Efficient Medical Image Generation via Next-scale Autoregressive Prediction**|医学图像生成在数据增强和隐私保护中至关重要，但现有方法在架构效率、多器官数据和原则性评估方面存在不足。为此，本文提出了MedVAR，首个基于自回归的医学基础模型，采用“下一尺度预测”范式，实现快速可扩展的医学图像合成。MedVAR以粗到精的方式生成图像，并构建了一个包含约44万张CT和MRI图像的协调数据集。综合实验表明，MedVAR在图像保真度、多样性和可扩展性方面均达到最先进水平，为未来的医学生成基础模型提供了 promising 的架构方向。|Yueming Jin Team|[2602.14512](http://arxiv.org/abs/2602.14512)|null|
|**2026-02-16**|**Covariance-Aware Transformers for Quadratic Programming and Decision Making**|针对Transformer在涉及协方差矩阵的决策问题中的应用潜力，本文首先证明线性注意力机制可通过模拟梯度下降求解无约束二次规划（QP），并扩展至求解L1惩罚/约束QP。在此基础上，本文提出了Time2Decide，一种通过显式输入协方差矩阵来增强时间序列基础模型（TSFM）的通用方法。实验结果表明，Time2Decide在经典的投资组合优化问题上，其性能普遍优于基础TSFM模型，并在特定条件下甚至超越了传统的“预测-优化”流程，证明Transformer通过显式利用二阶统计量能有效解决复杂决策问题。|Samet Oymak Team|[2602.14506](http://arxiv.org/abs/2602.14506)|null|
|**2026-02-16**|**A Soft Wrist with Anisotropic and Selectable Stiffness for Robust Robot Learning in Contact-rich Manipulation**|在非结构化环境中，机器人进行接触密集型操作任务时，现有软末端执行器因形变范围有限、缺乏定向刚度控制或系统复杂而面临挑战。本文介绍了一种名为CLAW（Compliant Leaf-spring Anisotropic soft Wrist）的新型软腕机构，它通过简单的板簧和锁定旋转关节设计，实现了大范围6自由度形变和可调的各向异性刚度，同时保持轻量和低成本。在模仿学习实验中，CLAW在插销任务中实现了76%的成功率，显著优于其他夹具，并在处理高精度装配和精细物体操作等接触密集型场景中表现出强大潜力，预示其能增强机器人学习的鲁棒性。|Masashi Hamaya Team|[2602.14434](http://arxiv.org/abs/2602.14434)|null|
|**2026-02-15**|**WIMLE: Uncertainty-Aware World Models with IMLE for Sample-Efficient Continuous Control**|基于模型的强化学习（MBRL）常因模型误差累积、世界模型处理多模态动力学不佳及预测过度自信而表现受限。本文提出了WIMLE，一种将隐式最大似然估计（IMLE）扩展到MBRL框架的方法，旨在学习随机、多模态的世界模型，并利用集成和潜在采样估计预测不确定性。WIMLE在训练中根据预测置信度加权合成转换，以稳定学习。在40个连续控制任务上的实验结果表明，WIMLE实现了卓越的样本效率和有竞争力的渐近性能，尤其在挑战性任务上显著提升了样本效率，凸显了IMLE基多模态和不确定性感知加权对稳定MBRL的价值。|Ke Li Team|[2602.14351](http://arxiv.org/abs/2602.14351)|**[link](https://openreview.net/forum?id=mzLOnTb3WH)**|
|**2026-02-15**|**Multi-Agent Debate: A Unified Agentic Framework for Tabular Anomaly Detection**|表格异常检测常依赖单一或静态集成检测器，但异构模型在分布漂移、缺失数据和稀有异常下常出现分歧。本文提出了MAD（Multi-Agent Debating）框架，将这种分歧作为核心信号，通过数学协调层解决。框架中每个代理是一个ML检测器，提供异常分数、置信度和证据，并由LLM评论员增强。协调器将消息转换为损失并更新代理影响力，生成最终异常分数和可审计的辩论轨迹。实验表明，MAD在各种表格异常基准测试上提高了鲁棒性，并提供了更清晰的模型分歧追踪。|Sheng Li Team|[2602.14251](http://arxiv.org/abs/2602.14251)|null|
|**2026-02-15**|**Towards Spatial Transcriptomics-driven Pathology Foundation Models**|空间转录组学（ST）能够提供超越组织学评估的分子景观，多模态基础模型也显示了形态分子耦合提升组织学表征的潜力。为整合局部分子信息到病理视觉编码器，本文提出了Spatial Expression-Aligned Learning (SEAL) 框架，作为一种参数高效的视觉-组学自监督微调方法，可应用于现有病理学基础模型。SEAL通过在涵盖14个器官的70多万个配对基因表达点-组织区域示例上进行训练，在38项幻灯片级和15项补丁级下游任务上，持续优于纯视觉和ST预测基线，并展示了强大的域泛化能力和基因到图像检索等跨模态能力，为病理学基础模型的ST引导微调提供了通用且实用的框架。|Faisal Mahmood Team|[2602.14177](http://arxiv.org/abs/2602.14177)|null|
|**2026-02-15**|**ARport: An Augmented Reality System for Markerless Image-Guided Port Placement in Robotic Surgery**|精确的端口放置是机器人辅助手术的关键步骤，但术前规划与术中执行之间存在差距。本文提出了ARport，一个增强现实（AR）系统，旨在自动将预规划的套管布局映射到患者体表，提供直观的术中空间指导。ARport在光学透视头戴式显示器（OST-HMD）上实现，无需外部传感器或标记，通过基础模型提取患者体表并进行无标记配准，实现术前解剖模型与患者体表的对齐，从而现场可视化套管布局。全尺寸人体模型实验表明，ARport能够准确叠加预规划的套管位置，实现虚拟规划与真实解剖之间的一致空间对应，为临床工作流程的无缝集成提供了高效且极简的解决方案。|Qi Dou Team|[2602.14153](http://arxiv.org/abs/2602.14153)|null|
|**2026-02-13**|**Imitating What Works: Simulation-Filtered Modular Policy Learning from Human Videos**|针对机器人通过观察人类视频学习抓取操作技能时，传统方法难以有效学习与任务兼容的抓取行为的问题，本研究提出了Perceive-Simulate-Imitate (PSI) 框架。该框架利用仿真中的抓取轨迹过滤技术，对人类视频数据进行处理，并生成带有抓取适用性标签的扩展轨迹数据，从而实现面向任务的抓取能力监督学习。真实世界实验表明，PSI无需任何机器人数据即可高效学习精确的操纵技能，并且相比简单使用抓取生成器的方法，性能显著提升，鲁棒性更强。|Wei-Chiu Ma Team|[2602.13197](http://arxiv.org/abs/2602.13197)|null|
|**2026-02-13**|**Order Matters in Retrosynthesis: Structure-aware Generation via Reaction-Center-Guided Discrete Flow Matching**|为解决现有免模板逆合成方法学习效率低和半模板方法泛化受限的问题，本研究提出了一种结构感知的免模板框架，核心在于利用原子排序信息。该方法将反应中心原子置于序列头部，通过位置归纳偏差编码化学反应的两阶段特性，并采用RetroDiT骨干网络与离散流匹配相结合。实验结果表明，该方法在USPTO-50k和USPTO-Full数据集上取得了SOTA性能，且在预测反应中心下，性能超越了使用更多数据训练的基础模型，并验证了结构先验的重要性。|Tianshu Yu Team|[2602.13136](http://arxiv.org/abs/2602.13136)|null|
|**2026-02-13**|**A Calibrated Memorization Index (MI) for Detecting Training Data Leakage in Generative MRI Models**|鉴于图像生成模型可能复制训练数据，尤其在医学图像生成中引发隐私问题，本研究提出了一种校准的逐样本度量方法来检测训练数据的记忆化和重复。该方法利用MRI基础模型提取图像特征，聚合多层白化最近邻相似度，并映射为有界的“过拟合/新颖性指数”（ONI）和“记忆化指数”（MI）分数。在多个MRI数据集上的实验结果表明，该度量能稳健检测重复数据，并提供一致的度量值，在样本级别实现了近乎完美的重复项检测。|Ibrahim Habli Team|[2602.13066](http://arxiv.org/abs/2602.13066)|null|
|**2026-02-13**|**INHerit-SG: Incremental Hierarchical Semantic Scene Graphs with RAG-Style Retrieval**|针对现有语义场景图在机器人导航中难以支持可解释的人类意图推理的问题，本研究提出了INHerit-SG框架。该框架将地图定义为RAG-ready的知识库，通过引入自然语言描述作为语义锚点对齐人类意图，并采用异步双进程架构和分层结构解耦几何分割与语义推理，通过事件触发机制保持地图长期一致性。实验在新建数据集和真实世界环境中进行，结果表明INHerit-SG在复杂查询上达到了最先进性能，并提高了检索成功率和可靠性，展现了其在下游导航任务中的可扩展性。|Yang Gao Team|[2602.12971](http://arxiv.org/abs/2602.12971)|null|
|**2026-02-13**|**Information-theoretic analysis of world models in optimal reward maximizers**|为量化最优行为对世界内部表示的需求，本研究考虑了一个具有n个状态和m个动作的受控马尔可夫过程，并假设转移动态存在均匀先验。研究证明，观察一个对任何非恒定奖励函数最优的确定性策略，可以精确地传达n log m比特关于环境的信息。具体来说，环境与最优策略之间的互信息为n log m比特。这些发现为实现最优性所需的“隐式世界模型”提供了精确的信息理论下限，适用于多种奖励最大化目标。|Alex Altair Team|[2602.12963](http://arxiv.org/abs/2602.12963)|null|
|**2026-02-13**|**Scaling Single Human Demonstrations for Imitation Learning using Generative Foundational Models**|鉴于机器人模仿学习中收集演示数据耗时且不易从人类演示直接迁移，本研究提出Real2Gen框架，旨在从单个“人类”演示中训练机器人操纵策略。该方法从人类演示中提取必要信息并传输至仿真环境，在仿真中利用可编程专家智能体生成无限数据来训练流匹配策略。实验结果显示，Real2Gen在三个真实世界任务上成功率平均提升26.6%，并且由于训练数据丰富多样，训练策略的泛化能力显著提高，纯仿真训练的策略还能零样本部署到真实世界。|Abhinav Valada Team|[2602.12734](http://arxiv.org/abs/2602.12734)|null|
|**2026-02-13**|**MedXIAOHE: A Comprehensive Recipe for Building Medical MLLMs**|为提升真实世界临床应用中的通用医学理解和推理能力，本研究提出了医疗视觉-语言基础模型MedXIAOHE。该模型采用实体感知持续预训练框架，组织异构医学语料以拓宽知识覆盖并减少长尾问题；通过强化学习和工具增强的代理训练，整合多样化医学推理模式以支持带可验证决策轨迹的多步骤诊断推理；并融合用户偏好规则、证据推理和低幻觉长文本报告生成，提高真实世界使用的可靠性。MedXIAOHE在多项医学基准测试中取得了最先进的性能，并超越了领先的闭源多模态系统。|Zhixiong Yang Team|[2602.12705](http://arxiv.org/abs/2602.12705)|null|
|**2026-02-13**|**RelBench v2: A Large-Scale Benchmark and Repository for Relational Data**|为推动关系深度学习（RDL）的发展，并应对日益增长的模型规模需求，本研究引入了RelBench v2，一个大规模、真实的关系数据库基准扩展。RelBench v2新增了四个大型数据集和“自动完成任务”，旨在直接推理关系表中缺失的属性值，并整合了外部基准和评估框架以实现统一的关系-时间评估。实验结果表明，RDL模型在自动完成、预测和推荐任务中始终优于单表基线，突出了显式建模关系结构的重要性。|Jure Leskovec Team|[2602.12606](http://arxiv.org/abs/2602.12606)|**[link](https://relbench.stanford.edu)**|
|**2026-02-13**|**The Constant Eye: Benchmarking and Bridging Appearance Robustness in Autonomous Driving**|针对自动驾驶算法在OOD条件下易受外观变化影响，且难以区分外观与结构场景变化导致规划器失效的问题，本研究建立了Navdream，一个高保真鲁棒性基准。该基准利用生成式像素对齐风格迁移，隔离外观变化对驾驶性能的影响。为弥合这一差距，研究提出了一种通用感知接口，利用冻结的视觉基础模型（DINOv3）提取外观不变特征作为规划器的稳定接口。实验表明，现有规划算法在OOD外观下性能显著下降，而该即插即用解决方案在各种规划范式中实现了卓越的零样本泛化，在极端外观变化下仍保持一致性能。|Yiyi Liao Team|[2602.12563](http://arxiv.org/abs/2602.12563)|null|
|**2026-02-13**|**Self-Supervised JEPA-based World Models for LiDAR Occupancy Completion and Forecasting**|鉴于自动驾驶需要世界模型来支持长期规划，且模型学习需具备自监督的可扩展性，本研究提出AD-LiST-JEPA，一个基于联合嵌入预测架构（JEPA）的自监督世界模型。该模型旨在利用JEPA框架从激光雷达数据预测未来时空演变。通过下游基于激光雷达的占用完成和预测（OCF）任务评估学习到的表示质量，概念验证实验表明，经过JEPA世界模型学习预训练后的编码器在OCF性能上有所提升，证明了该方法在感知和预测联合任务中的潜力。|Anna Choromanska Team|[2602.12540](http://arxiv.org/abs/2602.12540)|null|
|**2026-02-13**|**Multi-Agent Model-Based Reinforcement Learning with Joint State-Action Learned Embeddings**|在部分可观察和高度动态环境中，多智能体协调学习面临表示学习和数据效率挑战。为此，本文提出了一种新颖的基于模型的强化学习框架，该框架将联合状态-动作表示学习与想象式展开相结合。作者设计了一个使用变分自编码器训练的世界模型，并利用学习到的状态-动作嵌入（SALE）进行增强，将其注入到预测未来展开的想象模块和估计联合动作值函数的联合智能体网络中。在星际争霸II微管理、多智能体MuJoCo和基于级别的觅食挑战等基准测试中，该方法在有限真实环境交互下，通过将想象轨迹与基于SALE的动作值相结合，显著优于基线算法，验证了其在多智能体模型范式中学习联合状态-动作嵌入的有效性。|David Meger Team|[2602.12520](http://arxiv.org/abs/2602.12520)|null|
|**2026-02-12**|**The Observer Effect in World Models: Invasive Adaptation Corrupts Latent Physics**|判断神经网络模型是内化了物理定律还是仅利用统计捷径，尤其是在分布外（OOD）变化下，仍是一个难题。传统的适应性评估方法（如微调或高容量探针）可能改变被测量的表示，从而混淆自监督学习（SSL）期间的真实学习内容。为解决此问题，本文提出了一种非侵入性评估协议PhyIP，该协议基于线性表示假设，通过测试物理量能否从冻结表示中线性解码来评估。在流体动力学和轨道力学任务中，实验发现当SSL错误率较低时，潜在结构变得线性可访问，PhyIP在OOD测试中成功恢复了内能和牛顿反平方标度（ρ>0.90）。相比之下，基于适应性的评估可能使这种结构崩溃（ρ≈0.05）。这表明低容量探针能更准确地评估物理世界模型，而适应性评估可能掩盖潜在结构。|Barbara Hammer Team|[2602.12218](http://arxiv.org/abs/2602.12218)|null|
|**2026-02-12**|**LDA-1B: Scaling Latent Dynamics Action Model via Universal Embodied Data Ingestion**|当前机器人基础模型多依赖大规模行为克隆，忽视了异构具身数据中可迁移的动力学知识，而现有统一世界模型（UWM）因粗糙数据使用和碎片化数据集难以扩展。为此，本文提出了LDA-1B，一个通过通用具身数据摄取进行扩展的机器人基础模型，它联合学习动力学、策略和视觉预测，并为不同质量数据分配不同角色。为支持大规模训练，作者构建并标准化了EI-30k数据集（超过3万小时的人类和机器人轨迹）。通过在结构化的DINO潜在空间中进行预测，实现了异构数据的可扩展动力学学习，避免了冗余的像素空间外观建模，并采用多模态扩散Transformer处理异步视觉和动作流，实现了10亿参数规模的稳定训练。实验结果表明，LDA-1B在接触密集型、灵巧型和长程任务上分别比现有方法（如π0.5）提高了21%、48%和23%，并能通过利用30%通常有害且被丢弃的低质量轨迹，实现数据高效微调，性能提升10%。|He Wang Team|[2602.12215](http://arxiv.org/abs/2602.12215)|**[link](https://pku-epic.github.io/LDA)**|
|**2026-02-12**|**DreamID-Omni: Unified Framework for Controllable Human-Centric Audio-Video Generation**|尽管基础模型在音视频生成方面取得进展，但以人物为中心的多任务（如参考音视频生成、视频编辑、音频驱动动画）仍被孤立处理，且难以在单一框架内实现对多角色身份和音色的精确解耦控制。本文提出了DreamID-Omni，一个统一的可控人物中心音视频生成框架。作者设计了一个对称条件扩散Transformer，通过对称条件注入方案整合异构条件信号。为解决多人物场景中普遍存在的身份-音色绑定失败和说话人混淆问题，提出了双层解耦策略：在信号层面采用同步RoPE确保严格的注意力空间绑定，在语义层面采用结构化字幕建立显式属性-主题映射。此外，还设计了多任务渐进训练方案，利用弱约束生成先验来正则化强约束任务。大量实验证明，DreamID-Omni在视频、音频和音视频一致性方面均达到了全面的最先进性能，甚至超越了领先的商业模型。|Xiangwang Hou Team|[2602.12160](http://arxiv.org/abs/2602.12160)|**[link](https://guoxu1233.github.io/DreamID-Omni/)**|
|**2026-02-12**|**It's TIME: Towards the Next Generation of Time Series Forecasting Benchmarks**|时间序列基础模型（TSFMs）正在革新预测领域，但现有基准在数据构成、数据完整性、任务制定和分析视角上存在局限。为解决这些问题，本文提出了TIME，一个新一代任务中心基准，包含50个全新数据集和98个预测任务，专为严格的零样本TSFM评估而设计，避免数据泄露。通过整合大型语言模型和人类专业知识，建立了严格的人机协作基准构建流程，确保高数据完整性，并根据真实操作需求和变量可预测性重新定义任务。此外，作者提出了一种新颖的模式级评估视角，超越了基于静态元标签的传统数据集级评估，通过利用结构化时间序列特征表征内在时间属性，为模型能力提供了更具普适性的见解。对12个代表性TSFMs进行评估，并建立了一个多粒度排行榜，以促进深入分析和可视化检查。|Chenghao Liu Team|[2602.12147](http://arxiv.org/abs/2602.12147)|null|
|**2026-02-12**|**Commencing-Student Enrolment Forecasting Under Data Sparsity with Time Series Foundation Models**|许多大学面临日益增长的财政压力，亟需准确预测新生入学人数，然而高等教育入学预测通常数据稀疏，年度序列短且受报告变化和体制转变影响。流行的经典方法因短样本导致参数估计和模型选择不稳定，以及结构性中断导致外推能力下降而不可靠。近期，TSFMs在泄漏受限的协变量构建下，为年度、数据稀疏的机构预测提供了强大的零样本先验。本文在零样本设置下，对多种TSFM家族进行了基准测试，并测试了一组紧凑、防泄漏的协变量集。作者引入了“机构运营状况指数”（IOCI），这是一个从时间戳文件证据中提取的可转移的0-100区间状态协变量，并结合了具有稳定特征工程的Google Trends需求代理。使用严格对齐的回溯测试，结果表明，在没有机构特定训练的情况下，条件化TSFMs的表现与经典基准相当，具体表现差异因群体和模型而异。|Surangika Ranathunga Team|[2602.12120](http://arxiv.org/abs/2602.12120)|null|
|**2026-02-12**|**The Pensieve Paradigm: Stateful Language Models Mastering Their Own Context**|当前的大型语言模型（LLMs）虽有强大的数据库和检索系统，但缺乏主动管理自身状态和记忆的机制，被动接受手动提供的上下文，受限于固定窗口大小。本研究引入了StateLM，一种新型基础模型，它具备内部推理循环来管理自身状态。该模型配备了一系列记忆工具（如上下文剪枝、文档索引、笔记）并被训练主动管理这些工具，从而摆脱固定窗口的架构限制。实验结果表明，StateLM在所有模型规模的长文档问答任务中持续优于标准LLMs，在聊天记忆任务中绝对准确率提升10%至20%，在深度研究任务BrowseComp-Plus上更是达到52%的准确率，远超标准LLM的5%左右。这使得LLMs从被动预测器转变为状态感知代理，实现了状态化和可管理的推理过程。|Yan Wang Team|[2602.12108](http://arxiv.org/abs/2602.12108)|null|
|**2026-02-12**|**GigaBrain-0.5M*: a VLA That Learns From World Model-Based Reinforcement Learning**|直接从当前观察预测多步动作块的视觉-语言-动作（VLA）模型，因场景理解和未来预测能力受限而存在不足。相比之下，预训练于网络规模视频语料库的视频世界模型具有强大的时空推理和准确的未来预测能力，为增强VLA学习提供了基础。本研究提出了GigaBrain-0.5M*，一个通过世界模型驱动强化学习训练的VLA模型，其基于在超过1万小时机器人操作数据上预训练的GigaBrain-0.5。GigaBrain-0.5M*通过RAMP（通过世界模型条件策略进行强化学习）整合强化学习，以实现鲁棒的跨任务适应。实验结果表明，RAMP相对于RECAP基线取得了显著性能提升，在“叠衣服”、“装箱”和“制作浓缩咖啡”等挑战性任务上提升了约30%，并且在真实世界部署中展示了可靠的长时序复杂操作能力。|Zheng Zhu Team|[2602.12099](http://arxiv.org/abs/2602.12099)|**[link](https://gigabrain05m.github.io/)**|
|**2026-02-12**|**Affordance-Graphed Task Worlds: Self-Evolving Task Generation for Scalable Embodied Learning**|在现实世界中直接训练机器人策略成本高昂且难以扩展，而生成式仿真虽能合成大规模数据，但现有方法常难以生成逻辑连贯的长时序任务，且因开环执行难以应对动态物理不确定性。本研究提出了Affordance-Graphed Task Worlds (AGT-World)，一个统一框架，能够根据现实世界观察自主构建交互式仿真环境及相应的机器人任务策略。AGT-World通过将任务空间形式化为结构化图，实现复杂目标的精确分层分解为原子原语，并引入结合视觉-语言模型推理和几何验证的混合反馈自进化机制来自主优化策略。大量实验证明，AGT-World在成功率和泛化能力上显著优于现有方法，实现了提议、执行和纠错的自改进循环，从而促进可扩展的机器人学习。|Changshui Zhang Team|[2602.12065](http://arxiv.org/abs/2602.12065)|null|
|**2026-02-12**|**VLAW: Iterative Co-Improvement of Vision-Language-Action Policy and World Model**|视觉-语言-动作（VLA）模型的性能和可靠性可通过迭代在线交互提升，但现实世界策略回放数据收集昂贵。虽然学习到的模拟器（动作条件视频生成模型）有望生成额外回放数据，但现有世界模型缺乏物理保真度，难以精确建模接触密集物体操纵中的关键物理细节。本研究提出一种简单的迭代改进算法，利用真实世界回放数据提升世界模型的保真度，然后该世界模型再用于生成补充合成数据以改进VLA模型。在真实机器人实验中，该方法显著提升了最先进VLA模型在多个下游任务上的性能，相比基础策略，成功率绝对提升39.2%，相比使用生成合成回放数据进行训练，提升了11.6%。|Chelsea Finn Team|[2602.12063](http://arxiv.org/abs/2602.12063)|null|
|**2026-02-12**|**HoloBrain-0 Technical Report**|基础模型研究与可靠的真实世界机器人部署之间存在差距。为弥合此差距，本研究引入了HoloBrain-0，一个全面的视觉-语言-动作（VLA）框架。其核心是一个新颖的VLA架构，明确融入了机器人具身先验（如多视角相机参数和运动学描述），以增强3D空间推理并支持多样化的具身形态。该设计通过可扩展的“预训练-后训练”范式进行验证，在RoboTwin 2.0、LIBERO和GenieSim等仿真基准测试中取得了最先进的结果，并在具有挑战性的长时序真实世界操作任务中表现出色。值得一提的是，其高效的0.2B参数变体能与更大的基线模型相媲美，支持低延迟的设备端部署。为加速研究，HoloBrain生态系统已完全开源，包括预训练VLA基础模型、后训练检查点和全栈VLA基础设施RoboOrchard。|Zhizhong Su Team|[2602.12062](http://arxiv.org/abs/2602.12062)|null|
|**2026-02-12**|**FedGRPO: Privately Optimizing Foundation Models with Group-Relative Rewards from Domain Client**|针对联邦基础模型(FedFMs)中现有知识迁移方法存在的训练成本高、通信开销大及隐私风险问题，本研究将其重构为强化学习式评估过程。提出了FedGRPO框架，通过构建轻量级置信图进行能力专家选择，并利用"组相对"概念将问题与解决方案打包为候选策略，仅聚合客户端返回的标量奖励信号，而非直接交换数据或模型更新。实验结果表明，FedGRPO在保证隐私和降低通信开销的同时，实现了优于传统FedFMs基线的下游任务精度和通信效率。|Yuxing Han Team|[2602.12014](http://arxiv.org/abs/2602.12014)|null|
|**2026-02-12**|**Accelerating Robotic Reinforcement Learning with Agent Guidance**|强化学习(RL)在机器人操作技能学习中面临样本效率低下，而现有的人在环(HIL)方法受限于可扩展性与人类监督的低效性。本研究提出了Agent-guided Policy Search (AGPS)框架，用多模态智能体取代人类监督者，将智能体视为语义世界模型，注入内在价值先验来指导物理探索。通过可执行工具，智能体提供精确的纠正航点和空间约束来修剪探索。实验结果表明，AGPS在精细插入和变形物体操作任务中，样本效率优于HIL方法，从而实现了自动化监督，为可扩展的机器人学习提供了新途径。|Yaodong Yang Team|[2602.11978](http://arxiv.org/abs/2602.11978)|null|
|**2026-02-12**|**Where Bits Matter in World Model Planning: A Paired Mixed-Bit Study for Efficient Spatial Reasoning**|为实现高效空间推理，本研究探讨了低比特规划行为在有限精度预算下，是否主要由总比特位宽决定，或由比特位在模型模块间的分配方式决定。通过在DINO-WM上进行混合位评估，观察到8比特和6比特设置接近FP16，3比特设置性能崩溃，而4比特设置对位宽分配敏感。在4比特过渡区，保留编码器精度可提升规划性能。这些发现表明，模块感知、预算感知的量化策略对于高效空间推理具有重要研究意义。|Vaishak Menon Team|[2602.11882](http://arxiv.org/abs/2602.11882)|null|
|**2026-02-12**|**PuYun-LDM: A Latent Diffusion Model for High-Resolution Ensemble Weather Forecasts**|潜在扩散模型(LDMs)在 <=0.25° 的高分辨率集合天气预报中存在扩散性受限问题，且现有频率方法因缺乏对变量间频谱异质性的考虑而导致正则化强度不均。本研究提出了PuYun-LDM模型，结合3D掩码自编码器(3D-MAE)编码天气状态演化特征作为扩散模型的额外条件，并采用变量感知掩码频率建模(VA-MFM)策略自适应选择阈值。实验证明，PuYun-LDM增强了潜在扩散性，在短时效预报中表现优于ENS，长时效预报中与ENS相当，并能在短时间内高效生成全球预报。|Bin Wang Team|[2602.11807](http://arxiv.org/abs/2602.11807)|null|
|**2026-02-12**|**HAIC: Humanoid Agile Object Interaction Control via Dynamics-Aware World Model**|类人机器人在处理非结构化环境中复杂全身任务时，现有方法难以应对具有独立动力学和非完整约束的欠驱动物体，且缺乏外部状态估计。本研究提出了HAIC统一框架，通过仅利用本体感受历史数据估计高阶物体状态的动力学预测器，并将预测投影至静态几何先验形成空间接地动态占用图，使策略能在盲区推断碰撞边界和接触可供性。通过非对称微调，世界模型持续适应学生策略。实验表明，HAIC在敏捷任务和多物体长时序任务中均取得了高成功率，有效应对惯性扰动并预测多物体动力学。|Renjing Xu Team|[2602.11758](http://arxiv.org/abs/2602.11758)|**[link](https://haic-humanoid.github.io/)**|
|**2026-02-12**|**ABot-N0: Technical Report on the VLA Foundation Model for Versatile Embodied Navigation**|具身导航领域长期受限于任务特定的架构。本研究引入了ABot-N0，一个统一的视觉-语言-动作(VLA)基础模型，旨在实现点目标、物体目标、指令跟随、POI目标和人物跟随五项核心导航任务的“大一统”。模型采用分层“大脑-行动”架构，将基于大型语言模型的认知大脑与基于流匹配的动作专家结合。为支持大规模学习，研究团队开发了ABot-N0数据引擎， curating 16.9M专家轨迹和5.0M推理样本。ABot-N0在7个基准测试中取得了新的SOTA性能，并整合规划器与分层拓扑记忆，实现动态真实世界环境中的鲁棒长时序任务。|Mu Xu Team|[2602.11598](http://arxiv.org/abs/2602.11598)|**[link](https://amap-cvlab.github.io/ABot-Navigation/ABot-N0/)**|
|**2026-02-12**|**Brain4FMs: A Benchmark of Foundation Models for Electrical Brain Signal**|脑基础模型(BFMs)在神经科学领域快速发展，但缺乏统一的方法理解和标准化评估框架。本研究旨在填补这一空白，通过自监督学习(SSL)分类法组织BFMs，并从数据集角度总结常见下游任务，整理临床和以人为中心神经技术应用的代表性公共数据集。在此基础上，推出了Brain4FMs开放评估平台，集成了15个代表性BFM和18个公共数据集。该平台支持标准化比较和分析预训练数据、SSL策略和架构对泛化和下游性能的影响，以指导更准确、可迁移的BFMs开发。|Yang Yang Team|[2602.11558](http://arxiv.org/abs/2602.11558)|null|
|**2026-02-12**|**TS-Memory: Plug-and-Play Memory for Time Series Foundation Models**|时间序列基础模型(TSFMs)在分布偏移下的下游领域适应面临挑战，现有参数化适应易导致灾难性遗忘，而非参数化检索则引入高推理延迟。本研究提出了参数化记忆蒸馏方法并实现为TS-Memory，一个轻量级记忆适配器。TS-Memory分两阶段训练：首先构建离线、无信息泄露的kNN教师模型合成置信度感知的量化目标；其次通过置信度门控监督将检索诱导的分布校正蒸馏到记忆适配器中。实验表明，TS-Memory在多种TSFMs和基准测试上持续改进点预测和概率预测性能，且效率与冻结骨干网络相当，实现了无检索部署。|Yuxuan Liang Team|[2602.11550](http://arxiv.org/abs/2602.11550)|null|
|**2026-02-12**|**Budget-Constrained Agentic Large Language Models: Intention-Based Planning for Costly Tool Use**|在预算约束下，大型语言模型调用外部工具解决多步骤任务的工具增强型智能体面临巨大状态-动作空间、高结果方差和过高探索成本导致直接规划难以处理的问题。本研究提出了INTENT，一个推理时规划框架，利用意图感知的层次世界模型来预测未来的工具使用和风险校准成本，并在线指导决策。实验证明，INTENT在成本增强型StableToolBench上严格遵循硬预算可行性，显著提高了任务成功率，并在工具价格和预算动态变化下表现出鲁棒性。|Qi Qi Team|[2602.11541](http://arxiv.org/abs/2602.11541)|null|
|**2026-02-12**|**Vascular anatomy-aware self-supervised pre-training for X-ray angiogram analysis**|X射线血管造影图像分析受限于带标注数据稀缺，大规模自监督学习(SSL)在该领域潜力未被充分挖掘。本研究引入了VasoMIM框架，通过解剖引导的掩码策略强制模型学习血管语义，并利用解剖一致性损失保留血管结构一致性，从而增强学习表征的判别力。同时，策展了迄今最大的X射线血管造影预训练数据集XA-170K。VasoMIM在多个下游任务中展现出卓越的可迁移性和领先的性能，凸显了其作为X射线血管造影分析基础模型的巨大潜力。|Zeng-Guang Hou Team|[2602.11536](http://arxiv.org/abs/2602.11536)|null|
|**2026-02-12**|**Semantic-aware Adversarial Fine-tuning for CLIP**|当前研究表明，通过对抗性微调CLIP图像编码器可增强其零样本分类的对抗鲁棒性，但生成对抗样本（AEs）时仅依赖图像与单一手动模板的余弦相似度，不足以衡量图文对的语义相似性，导致微调后的模型鲁棒性不足。为解决此问题，本文提出了一种语义集成攻击方法，通过最小化原始图像与一组精炼文本描述（由基础模型生成并去除了幻觉）之间的平均相似度来生成语义感知的AEs。在此基础上，作者提出了语义感知对抗微调（SAFT）框架。实验结果表明，SAFT在16个数据集上的零样本对抗鲁棒性方面显著优于现有方法，实现了实质性提升。|Feng Liu Team|[2602.12461](http://arxiv.org/abs/2602.12461)|null|
|**2026-02-12**|**Stabilizing Native Low-Rank LLM Pretraining**|基础模型日益增长的参数量带来了巨大的计算和内存挑战，而低秩分解是降低成本的潜在途径，但从头开始仅使用低秩权重训练模型且性能匹配全秩模型仍缺乏稳定的方法。本文研究表明，无需先验方法的“全秩”辅助指导，大型语言模型（LLMs）可以从头开始仅使用低秩分解权重训练所有非嵌入矩阵。作者发现权重矩阵更新中谱范数（最大奇异值）的失控增长是导致原生低秩训练不稳定和损失尖峰的主要因素，并提出Spectron方法：通过正交化进行谱重归一化，根据因子当前的谱范数动态限制所得权重更新。实验证明，Spectron实现了稳定、端到端的低秩训练，开销可忽略不计，并为原生低秩Transformer建立了计算最优的缩放定律，展示了可预测的幂律行为和相对于全秩模型改进的推理效率。|Eugene Belilovsky Team|[2602.12429](http://arxiv.org/abs/2602.12429)|null|
|**2026-02-12**|**Policy4OOD: A Knowledge-Guided World Model for Policy Intervention Simulation against the Opioid Overdose Crisis**|阿片类药物危机是美国严重的公共卫生问题，但由于政策互动复杂且系统动态，评估干预措施极具挑战。本文提出Policy4OOD，一个知识引导的时空世界模型，旨在整合预测、反事实推理和优化三种关键能力来有效评估阿片类政策。该模型通过策略知识图谱、州级空间依赖性及社会经济时间序列的联合编码，构建一个策略条件化的Transformer来预测阿片类药物相关结果。训练完成后，世界模型可作为模拟器，通过前向传播进行预测，通过替换历史策略编码进行反事实分析，并通过蒙特卡洛树搜索进行策略优化。实验结果表明，空间依赖性和结构化策略知识显著提高了预测准确性，验证了该模型在数据驱动的公共卫生决策支持中的潜力。|Yanfang Ye Team|[2602.12373](http://arxiv.org/abs/2602.12373)|null|
|**2026-02-12**|**Free Lunch in Medical Image Foundation Model Pre-training via Randomized Synthesis and Disentanglement**|医学图像基础模型（MIFMs）在临床任务中展现巨大潜力，但其发展受限于大规模标注数据集的稀缺、异质性和高成本。本文提出RaSD（Randomized Synthesis and Disentanglement），一个可扩展的框架，可完全利用合成数据预训练MIFMs。RaSD通过随机高斯分布模拟解剖结构和外观变异，使模型接触足够的多尺度结构和外观扰动，从而迫使其依赖不变和任务相关的解剖线索而非数据集特有纹理，实现鲁棒和可迁移的表示学习。在120万3D体和960万2D图像上进行预训练后，RaSD模型在6种成像模态、48个数据集和56个下游任务中，持续优于从零开始训练的模型，在17个任务上取得了最佳性能，并在大多数其他任务上与使用大型真实数据集预训练的模型表现相当。这些结果证明了仅合成数据即可驱动鲁棒表示学习的能力，为医学AI领域带来了范式转变。|Hao Chen Team|[2602.12317](http://arxiv.org/abs/2602.12317)|null|

<p align=right>(<a href=#updated-on-20260222>back to top</a>)</p>

## VLM

|Publish Date|Title|Chinese Summary|Authors|PDF|Code|
|---|---|---|---|---|---|
|**2026-02-19**|**Pushing the Frontier of Black-Box LVLM Attacks via Fine-Grained Detail Targeting**|针对黑盒对抗性攻击大型视觉-语言模型（LVLMs）时存在的梯度缺失和复杂多模态边界问题，现有M-Attack方法通过局部裁剪匹配表现良好，但易引入高方差梯度，导致优化不稳定。研究人员将局部匹配重构为非对称期望，并对M-Attack进行梯度去噪升级，提出了M-Attack-V2。该方法引入了多裁剪对齐（MCA）以降低源端梯度方差，辅助目标对齐（ATA）以提供更平滑的目标流形，并利用补丁动量和补丁大小集成（PE+）强化可迁移方向。实验结果表明，M-Attack-V2显著提升了LVLMs上的黑盒攻击成功率，例如将Claude-4.0的成功率从8%提高到30%，并超越了现有黑盒LVLM攻击方法。|Zhiqiang Shen Team|[2602.17645](http://arxiv.org/abs/2602.17645)|**[link](https://github.com/vila-lab/M-Attack-V2)**|
|**2026-02-19**|**Catastrophic Forgetting Resilient One-Shot Incremental Federated Learning**|面对现代大数据系统生成的大规模、异构、隐私敏感的增量数据流，传统联邦学习（FL）因假设静态数据流和多轮通信，难以在通信受限下处理增量学习并面临灾难性遗忘。本文提出了One-Shot Incremental Federated Learning (OSI-FL)，首次在FL框架下同时解决通信开销和灾难性遗忘。OSI-FL通过客户端VLM提取类别特定嵌入并单轮通信至服务器，服务器利用预训练扩散模型合成新数据进行训练。为应对增量任务和灾难性遗忘，引入选择性样本保留（SSR）机制，根据样本损失识别并保留最具信息量的样本，以确保代表性样本参与后续训练。实验证明，OSI-FL在类增量和域增量场景下，在三个基准数据集上均优于包括传统和一次性FL方法在内的基线。|Monowar Bhuyan Team|[2602.17625](http://arxiv.org/abs/2602.17625)|null|
|**2026-02-19**|**AI Gamestore: Scalable, Open-Ended Evaluation of Machine General Intelligence with Human Games**|当前AI基准测试在评估机器通用智能方面面临局限，它们通常只考核狭窄能力且易饱和。为此，本文提出通过评估AI系统玩和学习玩“所有可想象的人类游戏”的能力，来衡量其类人通用智能，并引入了“人类游戏多重宇宙”这一评估空间。为实现此愿景，研究者开发了AI GameStore，一个利用LLMs和人工循环来合成新人类游戏的可扩展平台，它能自动从现有数字游戏平台获取并改编游戏环境。作为概念验证，平台生成了100款游戏并评估了七个前沿视觉-语言模型（VLMs），结果显示最佳模型在多数游戏中仅达到人类平均得分的不到10%，尤其在需要世界模型学习、记忆和规划的游戏中表现不足。|Joshua B. Tenenbaum Team|[2602.17594](http://arxiv.org/abs/2602.17594)|null|
|**2026-02-19**|**LATA: Laplacian-Assisted Transductive Adaptation for Conformal Uncertainty in Medical VLMs**|尽管医学视觉-语言模型（VLMs）在零样本识别方面表现强大，但在领域偏移下其可靠性依赖于经过校准的不确定性，而传统的Split Conformal Prediction (SCP) 方法在少样本、不平衡情况下常导致预测集过大和类别覆盖不平衡。本文提出了LATA（Laplacian-Assisted Transductive Adaptation），一种无需训练和标签的精炼方法，通过在图像-图像k-NN图上平滑零样本概率来提高效率和类别平衡，同时通过确定性变换保持SCP的有效性。LATA还引入了“失败感知”共形分数，提供实例级难度和标签合理性。实验证明，LATA在三个医学VLM和九个下游任务上，持续减少了预测集大小和类别覆盖不平衡，同时匹配或收紧了目标覆盖率，优于现有转导基线并显著减少了计算量。|Zongyuan Ge Team|[2602.17535](http://arxiv.org/abs/2602.17535)|null|
|**2026-02-19**|**Selective Training for Large Vision Language Models via Visual Information Gain**|大型视觉语言模型（LVLMs）常表现出语言偏见，即在缺乏视觉证据时仍生成答案。现有工作通过各种策略缓解此问题，但通常缺乏量化训练样本或token从图像中实际受益程度的指标。本文引入了视觉信息增益（VIG），一个基于困惑度的指标，用于衡量视觉输入对预测不确定性的降低程度，支持样本和token级别的细粒度分析。利用VIG，研究者提出了一种VIG引导的选择性训练方案，优先处理高VIG的样本和token。实验结果表明，该方法通过专注于视觉信息量大的样本和token，显著改善了视觉接地能力，缓解了语言偏见，并在大幅减少监督的情况下实现了优越的性能。|Sangheum Hwang Team|[2602.17186](http://arxiv.org/abs/2602.17186)|null|
|**2026-02-18**|**Saliency-Aware Multi-Route Thinking: Revisiting Vision-Language Reasoning**|视觉-语言模型（VLMs）在推理中整合视觉和文本模态时面临挑战，尤其是在推理过程中视觉输入稀缺和早期视觉接地错误累积的问题，且现有视觉接地指导粗糙且噪声大。本文提出了显著性感知原则（SAP）选择方法，它作用于高级推理原则而非token级轨迹，从而在噪声反馈下实现对离散生成的稳定控制，并允许后续推理步骤在需要时重新参考视觉证据。SAP还支持多路径推理，无需额外训练，具有模型无关和数据无关的优点。实验结果表明，SAP在可比的token生成预算下，尤其在减少物体幻觉方面取得了竞争性性能，并产生了更稳定的推理和更低的响应延迟，优于CoT风格的长序列推理。|Jundong Li Team|[2602.16702](http://arxiv.org/abs/2602.16702)|null|
|**2026-02-18**|**A Contrastive Learning Framework Empowered by Attention-based Feature Adaptation for Street-View Image Classification**|街景图像属性分类是图像分类的重要下游任务，但其计算成本高，且现有基于CLIP等预训练VLM的适应或微调方法，常因依赖全局图像嵌入而限制了对复杂街景中细粒度局部属性的捕获能力。为解决此问题，本文提出了CLIP-MHAdapter，一种轻量级CLIP适应范式的变体。CLIP-MHAdapter在patch tokens上追加了一个配备多头自注意力的瓶颈MLP，以有效建模patch间依赖关系。该模型仅包含约140万可训练参数，但在Global StreetScapes数据集的八个属性分类任务上实现了卓越或具有竞争力的精度，达到新的SOTA结果，同时维持了较低的计算成本。|James Haworth Team|[2602.16590](http://arxiv.org/abs/2602.16590)|null|
|**2026-02-18**|**DressWild: Feed-Forward Pose-Agnostic Garment Sewing Pattern Generation from In-the-Wild Images**|针对现有服装版型生成方法在处理多样姿态和视角时的局限性以及优化方法计算成本高昂的问题，本文提出了DressWild。该方法是一个新颖的前馈流水线，通过利用视觉-语言模型规范姿态变化并提取姿态感知的三维服装特征，然后通过Transformer编码器融合这些特征，预测可直接用于物理模拟和虚拟试穿的2D缝纫版型参数和3D服装。实验证明，DressWild能从单张野外图像中鲁棒地重建多样化的缝纫版型和3D服装，无需多视角输入或迭代优化，提供了一种高效且可扩展的服装模拟解决方案。|Chenfanfu Jiang Team|[2602.16502](http://arxiv.org/abs/2602.16502)|null|
|**2026-02-18**|**Visual Self-Refine: A Pixel-Guided Paradigm for Accurate Chart Parsing**|针对大型视觉-语言模型（LVLMs）在图表解析等视觉感知密集任务中因视觉稠密导致数据遗漏、错位和幻觉等错误，本文提出了Visual Self-Refine（VSR）新范式。VSR受人类阅读复杂图表时使用“视觉锚点”的启发，使模型能生成像素级定位输出，并将其可视化反馈给自己进行错误修正。作者在图表解析领域实例化VSR为ChartVSR，将其解析过程分解为迭代修正像素级定位的“精炼阶段”和利用验证定位解析结构化数据的“解码阶段”。此外，还构建了高挑战性基准ChartP-Bench。这项工作展示了VSR作为通用视觉反馈机制在提升视觉中心任务准确性方面的潜力。|Dahua Lin Team|[2602.16455](http://arxiv.org/abs/2602.16455)|null|
|**2026-02-18**|**Designing Production-Scale OCR for India: Multilingual and Domain-Specific Systems**|针对印度OCR系统在语言多样性、文档异构性和部署限制方面的挑战，本文通过Chitrapathak系列研究了两种多语言OCR训练策略。第一种是结合通用视觉编码器与多语言语言模型进行端到端训练；第二种是微调现有OCR模型，即使其未针对目标语言训练。实验结果表明，第二种策略在准确性与延迟权衡方面表现更优，Chitrapathak-2在泰卢固语上达到SOTA，并实现3-6倍加速。此外，还提出了Parichay系列模型，专门用于印度9种政府文档的关键字段提取，达到了89.8%的精确匹配率和更快的推理速度。这些系统为印度语境下的生产级OCR流水线提供了实践指导。|Shubham Agarwal Team|[2602.16430](http://arxiv.org/abs/2602.16430)|null|
|**2026-02-18**|**Peeking Ahead of the Field Study: Exploring VLM Personas as Support Tools for Embodied Studies in HCI**|鉴于实地研究成本高昂、耗时且易出错，本文受快速原型启发，旨在探索VLM角色能否模拟实地研究结果。研究背景是尽管大型语言模型（LLMs）具有类人推理和语言能力，但自动驾驶车辆（AV）与行人交互需空间感知、情感共情和行为生成。为探究VLM角色模仿人类反应的程度，研究进行了平行实验：一项20人参与的真实街道穿越任务，以及一项20个VLM角色参与的视频研究。结果显示，VLM角色能模仿人类的反应模式（如平均穿越时间），但缺乏行为变异性和深度，它们在形成性研究、实地研究准备和人类数据增强方面展现了潜力。|Takeo Igarashi Team|[2602.16157](http://arxiv.org/abs/2602.16157)|null|
|**2026-02-18**|**Evaluating Demographic Misrepresentation in Image-to-Image Portrait Editing**|尽管文本到图像（T2I）生成中的人口偏见已广泛研究，但指令引导的图像到图像（I2I）编辑中与人口统计相关的失败模式仍未被充分探索。本文形式化了两种失败模式：软擦除（编辑被削弱或忽略）和刻板印象替换（引入非请求的刻板印象属性）。研究引入了一个受控基准，通过生成和编辑基于种族、性别和年龄的人像，并使用视觉-语言模型评分和人工评估来探究行为。分析表明，身份保留失败普遍存在，具有人口统计学上的不均衡性，并受隐含社会先验（如职业驱动的性别推断）的影响。研究还发现，提示级身份约束可在不更新模型的情况下显著减少少数群体的群体变化。这些发现确立了身份保留作为I2I编辑中一个核心且人口统计学上不均衡的失败模式，并推动了人口统计学鲁棒编辑系统的发展。|Jean Oh Team|[2602.16149](http://arxiv.org/abs/2602.16149)|null|
|**2026-02-18**|**IRIS: Intent Resolution via Inference-time Saccades for Open-Ended VQA in Large Vision-Language Models**|针对开放式视觉问答（VQA）中大型视觉-语言模型（VLMs）在处理歧义时的挑战，本文引入了IRIS（Intent Resolution via Inference-time Saccades）——一种无需训练的方法，利用实时眼动追踪数据解决歧义。通过包含500个独特图像-问题对的综合用户研究，结果表明，参与者开始口头提问时最近的注视点对于大型VLM的消歧最为信息丰富。该方法使歧义问题的响应准确性提高了一倍以上（从35.2%到77.2%），同时保持了无歧义查询的性能。研究还在各种最先进的VLM上验证了其效果。这项工作发布了新的基准数据集、实时交互协议和评估套件，强调了眼动数据在增强VQA准确性方面的潜力。|Miguel P. Eckstein Team|[2602.16138](http://arxiv.org/abs/2602.16138)|null|
|**2026-02-18**|**OmniCT: Towards a Unified Slice-Volume LVLM for Comprehensive CT Analysis**|针对CT影像诊断中现有大型视觉-语言模型（LVLMs）在切片级和体积级理解上的碎片化问题，即切片驱动模型缺乏空间一致性而体积驱动模型粒度粗糙，本文提出了OmniCT，一个强大的统一切片-体积LVLM。该模型通过空间一致性增强（SCE）引入体积一致性，器官级语义增强（OSE）明确对齐解剖区域，并构建了最大规模的切片-体积CT数据集和混合基准MedEval-CT。实验证明，OmniCT在多样化的临床任务中显著优于现有方法，同时满足微观细节敏感性和宏观空间推理需求，为跨模态医学影像理解建立了新范式。|Beng Chin Ooi Team|[2602.16110](http://arxiv.org/abs/2602.16110)|null|
|**2026-02-18**|**Narrow fine-tuning erodes safety alignment in vision-language agents**|终身多模态智能体在后训练中适应新任务，却面临能力获取与安全对齐保持的冲突。研究发现，在狭域有害数据集上微调对齐的视觉-语言模型（VLMs）会诱发严重的紧急失调，并广泛泛化到不相关任务和模态。通过在Gemma3-4B上的实验，结果显示失调程度随LoRA秩单调递增，且多模态评估揭示的失调远高于文本专用评估，表明单模态安全基准可能低估了VLM的对齐退化。即使训练数据中仅有10%的有害数据也会导致显著对齐退化。为缓解失调，研究评估了良性窄域微调和基于激活的引导两种策略，结果表明两者均能显著减少失调，但未能完全消除已学习的有害行为，这凸显了对健壮持续学习框架的需求。|Shivam Raval Team|[2602.16931](http://arxiv.org/abs/2602.16931)|null|
|**2026-02-18**|**MALLVI: a multi agent framework for integrated generalized robotics manipulation**|现有基于LLM的机器人操作任务规划方法常依赖专用模型或微调，且以开环方式运行，缺乏鲁棒的环境反馈，导致在动态环境中脆弱。本文提出了MALLVi，一个多智能体大型语言与视觉框架，旨在实现闭环反馈驱动的机器人操作。MALLVi给定指令和环境图像后，生成原子动作；执行后，VLM评估环境反馈，并决定下一步。MALLVi通过协调Decomposer、Localizer、Thinker和Reflector等专门智能体管理感知、定位、推理和高级规划，其中Reflector支持有针对性的错误检测和恢复。模拟和真实世界实验表明，迭代闭环多智能体协调提高了零样本操作任务的泛化能力和成功率。|Babak Khalaj Team|[2602.16898](http://arxiv.org/abs/2602.16898)|null|
|**2026-02-18**|**DODO: Discrete OCR Diffusion Models**|光学字符识别（OCR）是信息数字化的基础任务，但现代视觉语言模型（VLM）主要依赖的自回归解码对于长文档效率低下且速度慢。这归因于每个token的顺序生成。研究指出，OCR任务高度确定性，理论上可通过扩散模型实现高效并行解码，但现有掩码扩散模型因结构不稳定性无法有效利用。为解决此问题，本文提出了DODO，首个利用块离散扩散（block discrete diffusion）的VLM。DODO通过将生成分解为块来缓解全局扩散的同步错误，从而释放OCR任务的并行处理潜力。实验结果表明，DODO在实现接近SOTA精度的同时，推理速度比自回归基线快3倍。|Niv Nayman Team|[2602.16872](http://arxiv.org/abs/2602.16872)|null|
|**2026-02-17**|**MedProbCLIP: Probabilistic Adaptation of Vision-Language Foundation Model for Reliable Radiograph-Report Retrieval**|鉴于视觉-语言基础模型在生物医学应用中因确定性嵌入而缺乏可靠性，本文提出了MedProbCLIP，一个用于胸部X射线和放射学报告表示学习及双向检索的概率视觉-语言学习框架。MedProbCLIP通过概率对比目标将图像和文本表示建模为高斯嵌入，从而明确捕捉不确定性和多对多对应关系，并通过变分信息瓶颈减轻过度自信预测。模型在训练中采用多视图X射线编码和多节报告编码以提供细粒度监督，但推理时仅需单张图像和报告。在MIMIC-CXR数据集上的评估显示，MedProbCLIP在检索和零样本分类方面均优于现有基线，并展现出卓越的校准性、可靠性和鲁棒性，突显了概率建模在提升放射学图像-文本检索系统可信度和安全性方面的价值。|Gongbo Liang Team|[2602.16019](http://arxiv.org/abs/2602.16019)|null|
|**2026-02-17**|**BTReport: A Framework for Brain Tumor Radiology Report Generation with Clinically Relevant Features**|鉴于神经肿瘤学领域因缺乏开放的配对图像-报告数据集而限制了放射学报告生成（RRG）的进展，本文引入了BTReport，一个开源的脑肿瘤RRG框架。该框架通过确定性提取成像特征来构建自然语言放射学报告。与依赖大型视觉-语言模型进行图像解释和报告撰写的方法不同，BTReport将RRG分解为确定性特征提取和报告生成两个步骤，仅使用大型语言模型进行句法结构化和叙述格式化，从而提高了报告的可解释性并减少了幻觉。研究表明，用于报告生成的特征能预测关键临床结果，且BTReport生成的报告与参考临床报告更紧密对齐。此外，本文还发布了伴随数据集BTReport-BraTS。|Mehmet Kurt Team|[2602.16006](http://arxiv.org/abs/2602.16006)|null|
|**2026-02-17**|**Can Vision-Language Models See Squares? Text-Recognition Mediates Spatial Reasoning Across Three Model Families**|针对视觉-语言模型（VLMs）在二进制网格中无法准确本地化缺乏文本标识的填充单元格这一基本限制，本研究通过实验进行了探究。研究生成了两种类型的15x15网格图像（文本符号和无网格线填充方块），并要求三个前沿VLM进行转录。实验结果显示，在文本符号条件下，VLM表现良好（高准确率和F1分数），但在填充方块条件下，所有模型的准确率和F1分数均大幅下降。这表明，VLM似乎拥有一个高保真的文本识别路径用于空间推理，其性能远优于其原生视觉路径。每个模型在处理非文本视觉元素时均表现出严重的空间定位退化，且各有不同的失败模式（如欠计数、过计数和模板幻觉）。|Yuval Levental Team|[2602.15950](http://arxiv.org/abs/2602.15950)|null|
|**2026-02-17**|**Visual Memory Injection Attacks for Multi-Turn Conversations**|鉴于大型生成式视觉-语言模型（LVLM）在长上下文多轮对话中安全性的研究不足，本论文提出了一种名为视觉记忆注入（VMI）的隐蔽攻击。该攻击通过操纵图像，使得LVLM在正常提示下行为正常，但在触发提示下输出预设的目标信息以操纵用户。实验证明，VMI攻击在多轮对话中依然有效，并在多个开源LVLM上得到验证，揭示了通过扰动图像进行大规模用户操纵的可行性，强调了提高LVLM对抗此类攻击的鲁棒性需求。|Matthias Hein Team|[2602.15927](http://arxiv.org/abs/2602.15927)|null|
|**2026-02-17**|**Concept-Enhanced Multimodal RAG: Towards Interpretable and Accurate Radiology Report Generation**|针对放射学报告生成（RRG）中视觉-语言模型（VLM）存在的解释性差和易产生“幻觉”的问题，本研究提出了概念增强多模态检索增强生成（CEMRAG）框架。该框架将视觉表示分解为可解释的临床概念，并将其与多模态RAG结合，通过丰富的上下文提示提升RRG的解释性和事实准确性。实验结果表明，CEMRAG在多个数据集和VLM架构下，在临床准确性和NLP指标上均优于现有基线，挑战了可解释性与性能之间的权衡假设，为医疗VLM提供了可信赖的AI辅助放射学途径。|Valerio Guarrasi Team|[2602.15650](http://arxiv.org/abs/2602.15650)|null|
|**2026-02-17**|**CARE Drive A Framework for Evaluating Reason-Responsiveness of Vision Language Models in Automated Driving**|针对自动驾驶中基础模型评估主要关注结果性能而忽视决策是否反映人类相关考量的问题，本研究提出了CARE Drive框架。该框架独立于模型，通过在受控上下文变化下比较基线和原因增强模型决策，评估人类原因对决策行为的因果影响。在一个骑车人超车场景中，结果显示明确的人类原因显著影响模型决策，提高了与专家推荐行为的一致性，证明了可以在不修改模型参数的情况下系统评估基础模型的原因响应性。|Arkady Zgonnikov Team|[2602.15645](http://arxiv.org/abs/2602.15645)|null|
|**2026-02-17**|**Req2Road: A GenAI Pipeline for SDV Test Artifact Generation and On-Vehicle Execution**|为解决软件定义汽车（SDV）功能测试中需求分散、规范复杂以及测试资产异构的挑战，本研究提出了一种自动化管道。该管道利用大型语言模型和视觉-语言模型提取信号和行为逻辑，自动生成Gherkin场景并转换为可运行的测试脚本，并通过VSS集成和检索增强生成（RAG）实现标准化。在儿童存在检测系统（CPDS）上的评估显示，89%的需求可转化为可执行场景，验证了该端到端需求-测试管道在仿真和在环测试环境中的可行性，但仍需人工审查。|Alois Knoll Team|[2602.15591](http://arxiv.org/abs/2602.15591)|null|
|**2026-02-17**|**VLM-DEWM: Dynamic External World Model for Verifiable and Resilient Vision-Language Planning in Manufacturing**|针对视觉-语言模型（VLM）在智能制造动态工作单元中面临的无状态操作和不透明推理挑战，本研究提出了VLM-DEWM认知架构。该架构通过一个持久且可查询的动态外部世界模型（DEWM），将VLM推理与世界状态管理解耦，并结构化每个VLM决策为可外部化的推理轨迹。实验结果表明，VLM-DEWM显著提高了状态跟踪准确性（从56%提升至93%）和恢复成功率（从低于5%提升至95%），并通过结构化内存减少了计算开销，为动态制造环境中的长周期机器人操作提供了可靠且弹性的解决方案。|Ning Ji Team|[2602.15549](http://arxiv.org/abs/2602.15549)|null|
|**2026-02-17**|**Selective Perception for Robot: Task-Aware Attention in Multimodal VLA**|针对机器人领域中视觉-语言-动作（VLA）模型静态融合多视角输入导致的计算冗余和噪声问题，本研究提出了一种动态信息融合框架。该框架引入轻量级自适应路由架构，实时分析文本提示和腕部摄像头观测，预测多摄像头视图的任务相关性，并有选择地向策略网络提供必要视觉特征。为路由器训练，本研究还开发了利用VLM的自动化标注管道。实验结果表明，该方法在真实世界机器人操纵场景中显著提高了推理效率和控制性能，验证了动态信息融合在资源受限实时机器人控制环境中的有效性。|Soo-Chul Lim Team|[2602.15543](http://arxiv.org/abs/2602.15543)|null|
|**2026-02-17**|**Semantic-Guided 3D Gaussian Splatting for Transient Object Removal**|为解决3D Gaussian Splatting (3DGS) 重建中瞬态物体导致的重影伪影问题，本研究提出了一种基于视觉-语言模型（VLM）的语义过滤框架。该方法通过计算渲染视图与干扰文本提示之间的CLIP相似度得分，并累积到每个高斯函数上，对超出校准阈值的高斯函数进行不透明度正则化和周期性修剪。实验结果表明，该方法在RobustNeRF基准测试中持续改善了重建质量，同时保持了最小内存开销和实时渲染性能，通过语义分类有效解决了视差模糊问题。|Priyesh Shukla Team|[2602.15516](http://arxiv.org/abs/2602.15516)|null|
|**2026-02-17**|**On the Out-of-Distribution Generalization of Reasoning in Multimodal LLMs for Simple Visual Planning Tasks**|本研究旨在深入探究大型语言模型和视觉-语言模型中思维链（CoT）推理方法的泛化能力，尤其是在简单规划任务上的表现。研究者提出了一个评估框架，对基于网格的导航任务中不同输入表示和CoT策略的模型进行了微调和系统评估。实验结果表明，CoT推理能提高分布内泛化能力，但对分布外（如更大地图）的泛化能力多数情况下仍非常有限；值得注意的是，结合多种文本格式的推理轨迹产生了最佳的分布外泛化效果，且纯文本模型表现优于基于图像输入的模型。|Francesco Croce Team|[2602.15460](http://arxiv.org/abs/2602.15460)|null|
|**2026-02-16**|**BPP: Long-Context Robot Imitation Learning by Focusing on Key History Frames**|在机器人任务中，传统策略因仅依赖当前观测而无法有效利用历史信息，导致在需要记忆的任务中泛化性差，尤其是在部署时容易受训练中虚假关联的影响。为解决此问题，本研究提出大图策略（BPP），通过视觉-语言模型识别并利用一组最小的关键帧作为历史观测条件。实验结果表明，BPP显著减少了训练与部署间的分布偏移，并在四项真实世界操作任务和三项仿真任务中，成功率比现有最佳方法提高了70%。|Aviral Kumar Team|[2602.15010](http://arxiv.org/abs/2602.15010)|null|
|**2026-02-16**|**ThermEval: A Structured Benchmark for Evaluation of Vision-Language Models on Thermal Imagery**|当前视觉语言模型（VLMs）在RGB图像上表现优异，但无法泛化至夜间监控、搜救等关键场景的热成像图像，且现有基准无法评估其对温度感知和推理能力。本研究引入ThermEval-B，一个包含5.5万个热视觉问答对的结构化基准，并整合了ThermEval-D数据集，首次提供带有语义身体部位标注的密集逐像素温度图。实验评估25个VLM后发现，模型在温度推理上普遍失败，在色图变换下性能下降，且容易依赖语言先验，提示或微调仅带来微弱提升，证实了热图像理解需专门评估。 |Nipun Batra Team|[2602.14989](http://arxiv.org/abs/2602.14989)|null|
|**2026-02-16**|**DM0: An Embodied-Native Vision-Language-Action Model towards Physical AI**|针对传统方法将互联网预训练模型适配到物理任务的局限性，本研究提出了DM0，一个具身原生（Embodied-Native）的视觉-语言-动作（VLA）框架，旨在为物理AI提供统一的具身操作和导航能力。该框架采用三阶段训练流水线：首先对VLM进行大规模统一预训练，整合网络文本、自动驾驶和具身交互日志数据，随后构建流匹配动作专家，并通过混合训练策略及具身空间支架策略实现高层推理与低层控制的协调。DM0在RoboChallenge基准测试中，于专业和通用设置下均取得了最先进的性能。|Tiancai Wang Team|[2602.14974](http://arxiv.org/abs/2602.14974)|**[link](https://github.com/Dexmal/dexbotic)**|
|**2026-02-16**|**MATEO: A Multimodal Benchmark for Temporal Reasoning and Planning in LVLMs**|AI智能体实现复杂目标需要精细规划，其中时间执行顺序（TEO）至关重要，但现有基础模型对TEO的理解研究不足，多限于线性近似或纯文本输入。为弥补这一空白，本研究引入MATEO基准，旨在评估和提升大型视觉语言模型（LVLMs）的时间推理能力。通过收集高质量多模态菜谱语料并利用众包构建TEO图谱，MATEO提供了丰富的标注。对六个主流LVLMs的评估揭示了语言上下文、多模态输入结构和微调策略对时间推理能力的关键影响，突显了当前LVLMs在该领域的局限性。|Giuseppe Riccardi Team|[2602.14589](http://arxiv.org/abs/2602.14589)|null|
|**2026-02-16**|**Replanning Human-Robot Collaborative Tasks with Vision-Language Models via Semantic and Physical Dual-Correction**|人机协作（HRC）在装配任务中面临人类指令模糊导致机器人行为不可靠的问题，现有基于VLM的方法虽能解释指令，但易产生幻觉推理和物理执行失败。为此，本研究提出了一个HRC框架，通过引入双重校正机制增强VLM的推理能力。该机制包含一个内部校正模型在执行前验证逻辑和可行性，以及一个外部校正模型通过事后反馈纠正物理失败。仿真和真实世界实验均表明，该框架显著提高了任务成功率，并能有效支持机器人根据人类指令进行交互式重规划。|Kensuke Harada Team|[2602.14551](http://arxiv.org/abs/2602.14551)|null|
|**2026-02-16**|**Error Patterns in Historical OCR: A Comparative Analysis of TrOCR and a Vision-Language Model**|18世纪印刷文本的OCR因降级打印、古体字和非标准化拼写而充满挑战，现有基于Transformer和VLM的OCR系统虽总体准确率高，但CER和WER等指标对学术使用可靠性洞察有限。本研究通过比较专用OCR Transformer（TrOCR）和通用VLM（Qwen）在历史英文文本线级识别上的表现，发现Qwen虽在总体准确率和鲁棒性方面占优，但存在选择性语言正则化和拼写规范化，可能改变历史原貌。TrOCR则能更好地保持拼写忠实度，但易出现级联错误。研究强调了架构归纳偏差对OCR错误结构的影响，以及在历史文献数字化中进行架构感知评估的必要性。|Mikko Tolonen Team|[2602.14524](http://arxiv.org/abs/2602.14524)|null|
|**2026-02-16**|**S2D: Selective Spectral Decay for Quantization-Friendly Conditioning of Neural Activations**|大规模Transformer模型中的激活异常值对模型量化构成严重挑战，导致量化精度显著下降，且随预训练规模的增加而加剧。本研究通过理论分析和实证观察，揭示了激活异常值与权重的主奇异值之间的直接联系。在此基础上，提出选择性谱衰减（S²D）方法，在微调阶段仅对最大奇异值对应的权重分量进行正则化。实验证明S²D显著减少了激活异常值，使得模型在W4A4量化下PTQ精度提升高达7%，结合QAT时提升4%，且泛化至下游任务和视觉-语言模型，提高了大规模模型的部署效率。|Deepak Gupta Team|[2602.14432](http://arxiv.org/abs/2602.14432)|null|
|**2026-02-16**|**Multi-Turn Adaptive Prompting Attack on Large Vision-Language Models**|针对多轮越狱攻击在大型视觉-语言模型（LVLMs）中因视觉输入过于恶意而易被防御的问题，本研究提出了MAPA（Multi-turn Adaptive Prompting Attack）方法。MAPA在每个回合中交替使用文本和视觉攻击动作以引发最恶意响应，并在跨回合中通过迭代式来回调整攻击轨迹，逐步放大响应的恶意性。这种双层设计使MAPA持续优于现有最先进方法，在对抗Llava-V1.6-Mistral-7B、Qwen2.5-VL-7B-Instruct、Llama-3.2-Vision-11B-Instruct和GPT-4o-mini等模型的最新基准测试中，攻击成功率提高了11-35%。|Yiliao Song Team|[2602.14399](http://arxiv.org/abs/2602.14399)|null|
|**2026-02-15**|**Moving Beyond Sparse Grounding with Complete Screen Parsing Supervision**|现代计算机使用智能体（CUA）需要对屏幕进行结构化感知才能可靠地理解指令和执行操作，但现有标注数据集稀疏且多样性不足，限制了其泛化能力，且实际部署需要高效率。本研究引入ScreenParse，一个用于完整屏幕解析的大规模数据集，包含771K个网页截图的密集标注。通过Webshot自动化流程及VLM辅助，ScreenParse提供了详尽的UI元素信息。基于此数据集，我们训练了紧凑型ScreenVLM，其在密集解析上显著优于更大规模的基础VLM，并在公共基准上展现了强大的迁移能力，证明了密集屏幕监督能为UI理解提供可迁移的结构先验。|Peter Staar Team|[2602.14276](http://arxiv.org/abs/2602.14276)|null|
|**2026-02-15**|**Dual-Signal Adaptive KV-Cache Optimization for Long-Form Video Understanding in Vision-Language Models**|视觉-语言模型（VLMs）在处理长视频时面临内存瓶颈，因KV缓存随序列长度线性增长，而现有驱逐策略先计算完整注意力矩阵再丢弃token，导致计算浪费。本研究提出Sali-Cache，一个先验优化框架，通过主动内存管理实现双信号自适应缓存。该方法结合光流分析的时间滤波器和显著性检测的空间滤波器，在注意力操作前智能管理内存。实验表明，Sali-Cache在LLaVA 1.6架构上实现了2.20倍的内存压缩比，同时保持100%的准确率，并在相同内存预算下能更长时间保留上下文特征，实现了在消费级硬件上高效处理长视频内容。|Priyesh Shukla Team|[2602.14236](http://arxiv.org/abs/2602.14236)|null|
|**2026-02-15**|**Index Light, Reason Deep: Deferred Visual Ingestion for Visual-Dense Document Question Answering**|当前多模态文档问答系统采用“供给侧摄取”策略，即在索引阶段全面生成视觉描述，导致成本高昂且不可靠。本文提出了“延迟视觉摄取（DVI）”框架，该框架采用“需求侧摄取”策略，仅在索引阶段进行轻量级元数据提取以实现页面定位，将视觉理解延迟到用户提问时，再将原始图像与特定问题发送给视觉语言模型进行分析。实验结果表明，DVI在零摄取视觉语言模型成本下，取得了与现有方法相当的整体准确率（46.7% vs 48.9%），在视觉必要查询上的有效率达50%，并实现了100%的页面定位率，有效将问答准确率问题转化为页面定位问题。|Tao Xu Team|[2602.14162](http://arxiv.org/abs/2602.14162)|null|
|**2026-02-15**|**Annotation-Efficient Vision-Language Model Adaptation to the Polish Language Using the LLaVA Framework**|现有视觉-语言模型（VLMs）多基于英语数据训练，限制了其在其他语言和文化背景下的应用。为解决此问题，本研究复现并调整了LLaVA-Next方法，通过全自动化流程翻译、过滤现有数据集并补充合成数据，构建了一套波兰语VLM。实验结果显示，该方法在波兰语改编的MMBench上相较于LLaVA-1.6-Vicuna-13B实现了9.5%的性能提升，并在生成性评估中，其生成的标题在语言正确性方面获得了更高评价，证明大规模自动化翻译结合轻量级过滤能有效为低资源语言引导高质量多模态模型。|Wojciech Kusa Team|[2602.14073](http://arxiv.org/abs/2602.14073)|null|
|**2026-02-15**|**MarsRetrieval: Benchmarking Vision-Language Models for Planetary-Scale Geospatial Retrieval on Mars**|行星科学中，现有深度学习基准多局限于监督视觉任务，不支持文本引导的地理空间发现。为此，本研究引入了MarsRetrieval，一个用于评估视觉-语言模型在火星地理空间发现能力的检索基准，包含图像-文本检索、地貌检索和全球地理定位等任务。研究提出统一的检索协议以评估多模态嵌入架构。实验结果表明MarsRetrieval极具挑战性，即使是强大的基础模型也难以捕捉领域特定的地貌区别，且领域特定微调对于行星环境中的可泛化地理空间发现至关重要。|Hongxin Wei Team|[2602.13961](http://arxiv.org/abs/2602.13961)|null|
|**2026-02-14**|**RMPL: Relation-aware Multi-task Progressive Learning with Stage-wise Training for Multimedia Event Extraction**|多媒体事件抽取（MEE）受标注数据缺乏限制，现有基准M2E2仅提供评估标注，导致直接监督训练困难，现有方法未能有效学习结构化事件表示。为解决这些局限，本文提出了RMPL（Relation-aware Multi-task Progressive Learning）框架，用于低资源条件下的MEE。RMPL通过阶段式训练，整合了来自单模态事件抽取和多媒体关系抽取的异构监督，先学习事件中心表示，再进行微调。实验结果表明，在M2E2基准上，结合多个视觉语言模型的RMPL在不同模态设置下均显示出持续的性能改进。|Yu Hong Team|[2602.13748](http://arxiv.org/abs/2602.13748)|null|
|**2026-02-14**|**Fine-tuned Vision Language Model for Localization of Parasitic Eggs in Microscopic Images**|土壤传播性蠕虫（STH）感染在全球范围广泛，但诊断专业知识有限，手动显微镜诊断耗时且易错。为实现自动化诊断，本文旨在利用经过微调的视觉语言模型（VLM），例如Microsoft Florence，来定位显微图像中的所有寄生虫卵。初步实验结果显示，该定位VLM的mIOU达到了0.94，优于其他目标检测方法，表明其有望成为自动化框架的核心组件，为智能寄生虫诊断提供可扩展的工程解决方案。|Nouar AlDahoul Team|[2602.13712](http://arxiv.org/abs/2602.13712)|null|
|**2026-02-14**|**LeafNet: A Large-Scale Dataset and Comprehensive Benchmark for Foundational Vision-Language Understanding of Plant Diseases**|基础模型和视觉-语言预训练在VLM领域取得显著进展，但在植物病理学等农业特定领域的应用受限于缺乏大规模、全面的多模态数据集和基准。为弥补此空白，本研究引入了LeafNet数据集和LeafBench视觉问答基准，涵盖97种病害的18.6万张图像和13,950个问答对。对12个先进VLM的基准测试揭示了其疾病理解能力的显著差异，二元分类准确率超90%，但细粒度识别低于65%。研究证实，多模态架构整合语言表示显著增强了诊断精度，凸显了LeafBench对VLM在植物病理学应用中方法学进展和评估的重要性。|Luyl-Da Quach Team|[2602.13662](http://arxiv.org/abs/2602.13662)|null|
|**2026-02-14**|**KorMedMCQA-V: A Multimodal Benchmark for Evaluating Vision-Language Models on the Korean Medical Licensing Examination**|当前视觉-语言模型（VLM）评估基准多为英语或通用领域，缺乏针对韩语医疗领域的多模态问答基准。为此，本研究引入了KorMedMCQA-V，一个韩语医学执照考试风格的多模态多项选择问答基准，包含1534个问题和2043张图像，涵盖多种临床模态，且约30%的问题需整合多图像证据。在统一零样本评估协议下，对50多个VLM的基准测试显示，最佳专有模型准确率达96.9%，最佳开源模型为83.7%，而最佳韩语专用模型仅为43.2%。研究还发现推理导向模型性能显著提升，医学领域专业化收益不一，所有模型在多图像问题上表现下降，且性能因成像模态而异。|Edward Choi Team|[2602.13650](http://arxiv.org/abs/2602.13650)|null|
|**2026-02-14**|**Towards Sparse Video Understanding and Reasoning**|现有视频问答（VQA）方法通常均匀采样视频帧，效率低下且未能有效捕捉关键信息，在多轮VQA中存在挑战。本研究提出了REVISE（Reasoning with Video Sparsity），一个多轮视频问答智能体，它选择少量信息帧，在多轮中维护摘要状态，并在有信心时提前停止。为微调开源模型，引入了EAGER（Evidence-Adjusted Gain for Efficient Reasoning）这一无标注奖励机制，包含置信度增益、摘要充分性和正确且提前停止三项。在多个VQA基准测试中，REVISE在提高准确率的同时，显著减少了帧数、轮次和提示token，展现了实用的稀疏视频推理能力。|Han Liu Team|[2602.13602](http://arxiv.org/abs/2602.13602)|null|
|**2026-02-14**|**AdaVBoost: Mitigating Hallucinations in LVLMs via Token-Level Adaptive Visual Attention Boosting**|大规模视觉-语言模型（LVLMs）存在幻觉问题，现有视觉注意力增强方法通过预定义缩放缓解，但固定缩放因子可能在不同生成步骤中表现出过弱或过强的局限性。为解决此问题，本文提出了AdaVBoost，一个token级别的自适应视觉注意力增强框架，旨在每个生成步骤动态确定注意力增强的程度。该框架引入视觉接地熵（VGE）来估计幻觉风险，并根据VGE对高风险token施加强视觉注意力增强，对低风险token施加弱增强。实验结果表明，AdaVBoost在多个LVLMs和幻觉基准测试中显著优于基线方法。|Tianyu Pang Team|[2602.13600](http://arxiv.org/abs/2602.13600)|null|
|**2026-02-14**|**OpAgent: Operator Agent for Web Navigation**|自主网络智能体在复杂且不稳定的真实网站环境中，面临现有SFT或离线RL方法因分布漂移而导致的性能局限。本文提出了一个强大的在线强化学习WebAgent，通过与非受限广域网站的直接迭代交互来优化策略。该方法包含分层多任务微调，建立了强大的VLM；开发了在线交互环境和RL管道，引入混合奖励机制缓解长期导航中的信用分配挑战；并提出了OpAgent模块化框架，整合规划器、接地器、反射器和总结器以实现错误恢复和自校正。实验结果显示，RL增强模型在WebArena上成功率达38.1%，而OpAgent框架进一步提升至71.6%，达到新的SOTA水平。|Peng Di Team|[2602.13559](http://arxiv.org/abs/2602.13559)|null|
|**2026-02-13**|**Steerable Vision-Language-Action Policies for Embodied Reasoning and Hierarchical Control**|预训练视觉语言模型(VLMs)拥有丰富的常识先验知识，但在机器人控制中有效落地仍面临挑战，现有分层方法中VLM对低层行为的引导受限于自然语言接口。为此，研究者提出了“可操控策略”(Steerable Policies)，通过在不同抽象级别（如子任务、动作、像素坐标）的丰富合成指令上训练视觉语言动作模型(VLA)，以提升低层可控性并释放VLM的预训练知识。实验结果表明，无论是通过学习型高层具身推理器还是即插即用VLM控制，Steerable Policies在真实的机器人操作实验中均优于现有基线，尤其在泛化和长任务方面表现出色。|Sergey Levine Team|[2602.13193](http://arxiv.org/abs/2602.13193)|null|
|**2026-02-13**|**Implicit-Scale 3D Reconstruction for Multi-Food Volume Estimation from Monocular Images**|现有膳食评估方法多依赖单图像分析或基于外观的推断，缺乏明确几何推理且对尺度模糊敏感，难以在真实用餐场景中准确估计食物份量。本研究提出了Implicit-Scale 3D Reconstruction from Monocular Multi-Food Images基准数据集，将食物份量估计重构为单目观测下的隐式尺度3D重建问题，通过餐盘、餐具等上下文线索而非显式度量来推断尺度，并着重于复杂的多食物场景。实验结果显示，几何重建方法相比强大的视觉语言基线，在准确性和鲁棒性上均有提升，最佳方法在体积估计上达到了0.21 MAPE，几何精度为5.7 L1 Chamfer Distance。|Jiangpeng He Team|[2602.13041](http://arxiv.org/abs/2602.13041)|**[link](https://www.kaggle.com/competitions/3d-reconstruction-from-monocular-multi-food-images/data)**|
|**2026-02-13**|**Training-Free Acceleration for Document Parsing Vision-Language Model with Hierarchical Speculative Decoding**|文档解析是多模态理解中的核心任务，但基于视觉语言模型(VLM)的端到端方法在处理长文档时，常因自回归生成长序列而导致显著的推理延迟。针对这一问题，本研究提出了一种免训练且高效的加速方法，借鉴推测解码的思想，使用轻量级文档解析流水线作为草稿模型预测未来批次token，并由更精确的VLM并行验证。此外，该方法还利用文档的布局结构将页面划分为独立区域进行并行解码。实验结果表明，该方法在通用OmniDocBench上为dots.ocr模型提供了2.42倍的无损加速，在长文档解析任务上加速高达4.89倍。|Lianwen Jin Team|[2602.12957](http://arxiv.org/abs/2602.12957)|null|
|**2026-02-13**|**HoRAMA: Holistic Reconstruction with Automated Material Assignment for Ray Tracing using NYURay**|下一代无线网络需要精确的特定站点确定性信道传播预测，而无线射线追踪(RT)依赖高精度3D环境模型和材料属性，手动建模耗时且传统视觉3D重建方法缺乏RT兼容性。为此，本研究提出了HoRAMA（Holistic Reconstruction with Automated Material Assignment）系统，该系统能利用智能手机捕获的RGB视频，结合MASt3R-SLAM的密集点云生成和视觉语言模型辅助的材料分配，自动生成RT兼容的3D模型。实验结果表明，HoRAMA的射线追踪预测在匹配多径分量功率预测方面与手动创建的3D模型基线表现相当（2.28 dB RMSE vs 2.18 dB），同时将3D重建时间从两个月缩短到16小时，显著提高了效率。|Theodore S. Rappaport Team|[2602.12942](http://arxiv.org/abs/2602.12942)|null|
|**2026-02-13**|**RoadscapesQA: A Multitask, Multimodal Dataset for Visual Question Answering on Indian Roads**|理解道路场景对自动驾驶至关重要，但现有数据集可能无法充分覆盖印度复杂多样的驾驶环境。本研究推出了Roadscapes，一个多任务多模态数据集，包含多达9,000张在印度不同驾驶环境中拍摄的图像，并附带手动验证的边界框。该数据集利用基于规则的启发式方法推断场景属性，并生成用于对象定位、推理和场景理解的问答对，涵盖了印度城市和乡村的多种昼夜场景。Roadscapes旨在推动非结构化环境中视觉场景理解的研究，并提供了使用视觉语言模型进行图像问答任务的初步基线。|Jyothikamalesh S Team|[2602.12877](http://arxiv.org/abs/2602.12877)|null|
|**2026-02-13**|**Thinking Like a Radiologist: A Dataset for Anatomy-Guided Interleaved Vision Language Reasoning in Chest X-ray Interpretation**|放射学诊断涉及视觉检查与语言推理的反复交织，但现有医学大型视觉语言模型(LVLMs)多依赖纯文本思维链推理，易产生幻觉，且现有伪视觉解决方案仍缺乏丰富的视觉细节。为此，本研究提出了MMRad-IVL-22K，这是首个专为胸部X射线解读中原生交织的视觉语言推理设计的大规模数据集，反映了放射科医生反复推理和视觉检查的工作流程。实验结果表明，多模态思维链引导的报告生成在临床准确性和报告质量方面显著优于纯文本思维链（RadGraph指标提高6%），证实高保真交织视觉语言证据是可靠医疗AI不可替代的组成部分。在MMRad-IVL-22K上微调的模型在推理一致性和报告质量方面也优于通用和医学专用LVLMs。|Wei Shen Team|[2602.12843](http://arxiv.org/abs/2602.12843)|null|
|**2026-02-13**|**X-SYS: A Reference Architecture for Interactive Explanation Systems**|可解释AI (XAI) 方法虽多，但将其部署为交互式系统仍面临挑战，因其需要兼顾算法与系统能力以维持解释可用性。本研究将可解释性视为一个信息系统问题，提出了X-SYS，一个交互式解释系统的参考架构。X-SYS围绕STAR（可伸缩性、可追溯性、响应性、适应性）四个质量属性，并指定了包含XUI服务、解释服务等五个组件的分解结构，将交互模式映射到系统能力以解耦用户界面和后端计算。通过SemanticLens系统实现X-SYS，展示了其如何通过契约化服务边界实现独立演进、通过离线/在线分离确保响应性以及通过持久状态管理支持可追溯性，为在操作约束下设计交互式解释系统提供了可重用蓝图和具体实例。|Sebastian Lapuschkin Team|[2602.12748](http://arxiv.org/abs/2602.12748)|null|
|**2026-02-13**|**SignScene: Visual Sign Grounding for Mapless Navigation**|导航标志能帮助人类在陌生环境中无地图导航，但机器人如何利用标志进行无地图导航是一个挑战，核心在于如何解释复杂多样的标志及其抽象语义内容，并将其与局部3D场景匹配。本研究将此形式化为“标志接地”(sign grounding)问题，即把标志上的语义指令映射到对应的场景元素和导航动作。研究者利用视觉语言模型(VLMs)的语义常识和推理能力，并提出了SignScene，一种以标志为中心的空-语义表示，旨在以利于VLM有效推理的形式呈现导航相关场景元素和标志信息。在包含九种环境类型、114个查询的数据集上，该方法实现了88%的接地准确率，显著优于基线，并成功使Spot机器人在真实世界中仅依赖标志进行无地图导航。|David Hsu Team|[2602.12686](http://arxiv.org/abs/2602.12686)|null|
|**2026-02-13**|**IndicFairFace: Balanced Indian Face Dataset for Auditing and Mitigating Geographical Bias in Vision-Language Models**|视觉语言模型(VLMs)常从训练数据中继承并放大社会偏见，印度群体尤其被错误代表，现有公平性数据集将印度视为单一类别，忽视了其内部地理多样性。为解决这一局限，本研究提出了IndicFairFace，一个包含14,400张图像的新颖且平衡的人脸数据集，旨在代表印度的地理多样性，图像伦理获取并在各邦和性别间均匀平衡。通过IndicFairFace，研究量化了基于CLIP的VLM中存在的印度国内地理偏见，并利用后验迭代零空间投影去偏方法成功减少了这种偏见。实验证明，该去偏方法对现有嵌入空间的影响很小，基准数据集上的检索准确率平均下降不到1.5%，确立了IndicFairFace作为研究印度背景下VLM地理偏见的第一个基准。|Jiechao Gao Team|[2602.12659](http://arxiv.org/abs/2602.12659)|null|
|**2026-02-13**|**PISHYAR: A Socially Intelligent Smart Cane for Indoor Social Navigation and Multimodal Human-Robot Interaction for Visually Impaired People**|现有智能助行设备多侧重物理导航，但缺乏社交智能和多模态人机交互能力。本研究提出了PISHYAR，一款结合社交感知导航和多模态人机交互的智能拐杖。该系统包含社交导航框架（集成RGB-D感知、对象检测、活动识别、路径规划和触觉反馈）和代理式多模态LLM-VLM交互框架（集成语音识别、VLM、LLM和文本转语音，并支持动态模式路由）。通过仿真、真实世界实验和用户研究，PISHYAR在避障和社会顺从性导航中表现可靠，整体准确率约80%，集体活动识别稳健。初步用户研究显示，视障用户对其交互框架的可用性、信任度和感知社交性给予高度评价，突显了PISHYAR作为多模态辅助移动设备在提供社交互动支持方面的潜力。|Alireza Taheri Team|[2602.12597](http://arxiv.org/abs/2602.12597)|null|
|**2026-02-13**|**On Robustness and Chain-of-Thought Consistency of RL-Finetuned VLMs**|在视觉语言模型（VLM）中，尽管强化学习（RL）微调能提升推理任务性能，但仍面临视觉基础薄弱、幻觉和过度依赖文本线索的问题。研究发现，简单的文本扰动（如误导性描述）会显著降低模型的鲁棒性和置信度，并揭示了RL微调中存在的准确性与忠实性之间的权衡。具体来说，微调提高了基准准确性，却可能损害推理链的可靠性和模型对上下文变化的鲁棒性。这些结果强调了仅凭准确性评估的局限性，并呼吁在训练和评估中同时关注正确性、鲁棒性以及视觉基础推理的忠实性。|Arnab Mondal Team|[2602.12506](http://arxiv.org/abs/2602.12506)|null|
|**2026-02-13**|**Layer-Specific Fine-Tuning for Improved Negation Handling in Medical Vision-Language Models**|由于视觉语言模型（VLM）在区分肯定和否定医疗陈述方面存在局限性，本研究引入了一个放射学诊断基准来系统评估VLM对极性的敏感性。为解决此问题，我们构建了一个包含结构化声明和属性级否定上下文临床否定数据集，并提出了一种名为否定感知选择性训练（NAST）的自适应方法。NAST利用因果追踪效应（CTE）根据各层对否定处理的因果贡献来调整梯度更新。实验结果表明，NAST在不损害通用视觉-语言对齐的情况下，显著提高了VLM对肯定和否定临床陈述的辨别能力，凸显了因果可解释性在安全关键医疗领域中进行有针对性模型适应的价值。|Rahmatollah Beheshti Team|[2602.12498](http://arxiv.org/abs/2602.12498)|null|
|**2026-02-12**|**Scaling Verification Can Be More Effective than Scaling Policy Learning for Vision-Language-Action Alignment**|针对视觉-语言-动作（VLA）模型在执行自然语言指令时存在的"意图-动作差距"问题，本研究探索了测试时验证方法。我们首先揭示了具身指令遵循的测试时标度定律，发现联合扩展复述指令和生成动作数量能更高效地恢复正确动作。为利用这些规律，我们提出了CoVer，一个对比验证器，并引入了“启动时计算”和分层验证推理管道。在部署时，该框架预先计算VLM生成的复述指令，生成动作候选项，然后使用验证器选择最优提示和动作块。实验结果表明，CoVer在SIMPLER基准上取得了显著提升（分布内22%，分布外13%），并在真实世界实验中进一步提升45%，在PolaRiS基准上任务进展提升14%，成功率提升9%。|Marco Pavone Team|[2602.12281](http://arxiv.org/abs/2602.12281)|null|
|**2026-02-12**|**ExStrucTiny: A Benchmark for Schema-Variable Structured Information Extraction from Document Images**|尽管通用视觉语言模型（VLM）在传统文档理解基准上表现良好，但它们在多样化文档类型和灵活模式下进行整体、细粒度结构化信息提取的能力仍未充分研究。现有数据集在实体本体、查询复杂度或文档类型上存在局限。为解决这些不足，本研究引入了ExStrucTiny，一个新的文档图像结构化信息提取（IE）基准数据集，它统一了关键实体提取、关系提取和视觉问答的特性。该数据集通过结合人工和合成并经过人工验证的样本构建，涵盖了更广泛的文档类型和提取场景。对开放和封闭式VLM在该基准上的分析揭示了模式适应、查询欠规范和答案定位等挑战，为未来改进通用模型在文档结构化信息提取方面的能力奠定了基础。|Manuela Veloso Team|[2602.12203](http://arxiv.org/abs/2602.12203)|null|
|**2026-02-12**|**3DGSNav: Enhancing Vision-Language Model Reasoning for Object Navigation via Active 3D Gaussian Splatting**|针对零样本对象导航（ZSON）中视觉语言模型（VLM）决策受低级感知准确性限制的问题，本研究提出了3DGSNav框架。3DGSNav利用3D高斯泼溅（3DGS）作为VLM的持久记忆来增强空间推理能力。通过主动感知，该框架逐步构建环境的3DGS表示，从而实现轨迹引导的、边界感知的第一人称视角的自由视点渲染。此外，研究设计了结构化视觉提示并结合思维链（CoT）提示来进一步提升VLM的推理能力。在导航过程中，实时对象检测器用于过滤潜在目标，而VLM驱动的主动视点切换则进行目标再验证，确保高效可靠的识别。在多个基准测试和真实世界四足机器人实验中，3DGSNav展现出鲁棒且具竞争力的性能。|Xinyi Yu Team|[2602.12159](http://arxiv.org/abs/2602.12159)|null|
|**2026-02-12**|**Affordance-Graphed Task Worlds: Self-Evolving Task Generation for Scalable Embodied Learning**|鉴于在真实世界中训练机器人策略成本高昂且现有生成模拟难以生成逻辑连贯的长周期任务并处理动态物理不确定性，本研究提出了Affordance-Graphed Task Worlds (AGT-World) 统一框架。该框架基于真实世界观测自主构建交互式模拟环境和机器人任务策略，通过将任务空间形式化为结构化图，实现复杂目标的精确分层分解，并引入了结合VLM推理和几何验证的自演化机制来优化策略。实验结果表明，该方法在成功率和泛化能力上显著优于现有方法，实现了提议、执行和修正的自我改进循环，以支持可扩展的机器人学习。|Changshui Zhang Team|[2602.12065](http://arxiv.org/abs/2602.12065)|null|
|**2026-02-12**|**Can Local Vision-Language Models improve Activity Recognition over Vision Transformers? -- Case Study on Newborn Resuscitation**|鉴于新生儿复苏活动准确记录的重要性及其在实践中的不足，以及现有方法在识别细粒度活动上的挑战，本研究探索了生成式AI（GenAI）方法，特别是结合大型语言模型（LLM）的局部视觉-语言模型（VLM），以改进新生儿复苏视频中的活动识别。研究将几种零样本VLM策略和微调VLM（包括LoRA）与分类头进行评估，并与监督式TimeSFormer基线进行比较。结果表明，小型VLM虽然容易产生幻觉，但经过LoRA微调后，F1分数可达0.91，显著超越TimeSFormer的0.70。|Øyvind Meinich-Bache Team|[2602.12002](http://arxiv.org/abs/2602.12002)|null|
|**2026-02-12**|**Benchmarking Vision-Language Models for French PDF-to-Markdown Conversion**|针对PDF到Markdown转换作为RAG管道关键步骤，以及现有基准多侧重英文或中文并可能过度惩罚无关格式差异的问题，本研究引入了一个专门针对法语文档、通过模型不一致采样挑选的挑战性新基准。该基准涵盖手写表格、复杂布局、密集表格和富含图形的页面，并采用单元测试式检查和类别特定归一化进行评估，以减少无关的呈现差异。实验结果显示，在15个模型中，最强的专有模型在手写和表格处理上表现出更高的鲁棒性，同时一些开源系统在标准打印布局上仍具竞争力。|Nicolas Mery Team|[2602.11960](http://arxiv.org/abs/2602.11960)|null|
|**2026-02-12**|**Are Two LLMs Better Than One? A Student-Teacher Dual-Head LLMs Architecture for Pharmaceutical Content Optimization**|针对大型语言模型（LLM）在制药等受监管领域内容创作中面临的科学准确性和合规性挑战，以及手动质量控制（QC）低效易错的问题，本研究引入了LRBTC，一个模块化的LLM和VLM驱动的QC架构。该架构涵盖语言、法规、品牌、技术和内容结构检查，并结合了师生双模型架构、人工干预（HITL）工作流和瀑布式规则过滤机制。实验结果表明，LRBTC在AIReg-Bench上取得了83.0%的F1和97.5%的召回率，相比Gemini 2.5 Pro，遗漏违规减少5倍；在CSpelling上平均准确率提高26.7%。误差分析揭示当前模型擅长检测拼写错误，但在复杂医学语法和标点错误识别方面仍有提升空间。|Anubhav Girdhar Team|[2602.11957](http://arxiv.org/abs/2602.11957)|null|
|**2026-02-12**|**LAMP: Implicit Language Map for Robot Navigation**|为解决现有视觉-语言模型（VLM）零样本导航方法在大环境中因显式存储语言向量而导致的内存需求过高和细粒度规划分辨率有限的问题，本研究提出了LAMP（Language Map）框架。该框架通过学习连续的、语言驱动的隐式神经语言场来编码语言特征，并结合稀疏图实现高效的粗略路径规划，进而通过梯度优化在学习场中细化目标姿态。该方法还采用贝叶斯框架建模嵌入不确定性以增强泛化能力，并通过图采样策略扩展到大环境。实验证明，LAMP在内存效率和细粒度目标到达精度方面均优于现有显式方法。|Sunwook Choi Team|[2602.11862](http://arxiv.org/abs/2602.11862)|**[link](https://lab-of-ai-and-robotics.github.io/LAMP/)**|
|**2026-02-12**|**JEPA-VLA: Video Predictive Embedding is Needed for VLA Models**|针对现有视觉-语言-动作（VLA）模型在机器人操作中存在的样本效率低和泛化能力有限的问题，本研究认为这与被忽视的预训练视觉表征在环境理解和策略先验方面知识不足有关。研究发现，视频上预训练的预测嵌入（特别是V-JEPA 2）能有效捕捉任务相关的时态动态并过滤不可预测的环境因素，从而弥补现有视觉表征的不足。基于此，论文提出了JEPA-VLA，一种将预测嵌入自适应集成到现有VLA中的简单而有效的方法。实验结果表明，JEPA-VLA在一系列基准和真实机器人任务中均取得了显著的性能提升。|Mingsheng Long Team|[2602.11832](http://arxiv.org/abs/2602.11832)|null|
|**2026-02-12**|**Revis: Sparse Latent Steering to Mitigate Object Hallucination in Large Vision-Language Models**|为解决大型视觉-语言模型（LVLMs）普遍存在的物体幻觉问题，该问题源于视觉特征和预训练文本表示在网络深层中的纠缠，本研究提出了REVIS，一个无需训练的框架。REVIS基于潜在空间几何原理，通过正交投影精确提取纯视觉信息向量，并采用校准策略，仅在视觉信息被抑制的精确深度进行稀疏干预，从而有效恢复视觉信息。经验评估结果显示，REVIS与最先进的基线相比，将目标幻觉率降低了约19%，同时保持了模型原有的通用推理能力。|Zhou Yang Team|[2602.11824](http://arxiv.org/abs/2602.11824)|null|
|**2026-02-12**|**Adaptive Debiasing Tsallis Entropy for Test-Time Adaptation**|现有主流的视觉-语言模型测试时自适应（TTA）方法依赖香农熵（SE）衡量预测不确定性，但由于预训练数据偏差，SE会产生有偏的不确定性估计。为解决此问题，研究发现并证明Tsallis熵（TE）通过引入非广延参数q，天然适合表征有偏分布。在此基础上，本文提出了自适应去偏Tsallis熵（ADTE），为每个类别定制类特定的q^l参数，该参数通过对持续传入的测试实例估计标签偏差进行归一化得到。实验结果表明，ADTE在ImageNet及其五种变体上超越了最先进的方法，并在10个跨域基准测试中取得了最高的平均性能，证实了其有效性。|Jianfeng Lu Team|[2602.11743](http://arxiv.org/abs/2602.11743)|null|
|**2026-02-12**|**Adapting Vision-Language Models for E-commerce Understanding at Scale**|电商产品理解需要文本、图像和结构化属性等多模态的强大理解能力。通用视觉-语言模型（VLMs）虽能提供泛化能力，但如何将其有效地适应电商数据中以属性为中心、多图像和噪声多的特性，同时不牺牲通用性能，是一个未被充分探索的问题。本文通过大规模实验研究，展示了对通用VLM进行有针对性的适配，可以在大幅提升电商任务性能的同时，保持其广泛的多模态能力。此外，本文还提出了一个新颖且全面的评估套件，涵盖了深度产品理解、严格指令遵循和动态属性提取等任务。|Shahram Khadivi Team|[2602.11733](http://arxiv.org/abs/2602.11733)|null|
|**2026-02-12**|**STVG-R1: Incentivizing Instance-Level Reasoning and Grounding in Videos via Reinforcement Learning**|在视觉-语言模型（VLMs）中，文本描述与视觉坐标之间的不匹配常导致幻觉问题，在时空视频定位（STVG）等密集预测任务中尤为突出。现有方法通常增强视觉-文本对齐或添加辅助解码器，但这会引入额外的可训练模块，导致高昂的标注和计算成本。本文提出了一种新颖的视觉提示范式，通过将每帧坐标预测重新表述为实例级识别问题，为每个对象分配唯一且时间一致的ID，并将其嵌入视频作为视觉提示。此外，研究引入了STVG-R1，这是首个用于STVG的强化学习框架，通过任务驱动奖励联合优化时间精度、空间一致性和结构化格式正则化。实验结果表明，STVG-R1在六个基准测试中表现出色，在HCSTVG-v2上m_IoU比基线Qwen2.5-VL-7B高出20.9%，达到SOTA，并展现出强大的零样本泛化能力。|Qing Li Team|[2602.11730](http://arxiv.org/abs/2602.11730)|null|
|**2026-02-12**|**ScalSelect: Scalable Training-Free Multimodal Data Selection for Efficient Visual Instruction Tuning**|大规模视觉指令微调（VIT）是提升视觉-语言模型（VLMs）在多模态任务中性能的关键范式，但其在大型数据集上的训练因数据冗余而计算昂贵且低效。现有的数据选择方法要么需要昂贵的训练或梯度计算，要么依赖代理模型或指令无关表示且复杂度高，限制了可扩展性。本文提出了ScalSelect，一种可扩展的无训练多模态数据选择方法，其复杂度与样本数量呈线性关系，无需外部模型或辅助数据集。ScalSelect通过提取指令令牌在目标VLM中最受关注的视觉特征来构建样本表示，然后识别其表示最能近似整个数据集主导子空间的样本，从而实现无需成对比较的可扩展重要性评分。广泛的实验表明，ScalSelect仅使用16%的数据即可达到全数据集训练97.5%以上的性能，在某些设置下甚至超越了全数据训练。|Kai Chen Team|[2602.11636](http://arxiv.org/abs/2602.11636)|**[link](https://github.com/ChangtiWu/ScalSelect}{ScalSelect})**|
|**2026-02-12**|**SkillRater: Untangling Capabilities in Multimodal Data**|传统数据筛选方法通常只给样本分配一个单一的质量分数，但这在模型需要学习多种不同能力时显得局限。本文认为质量应是多维的，每个维度对应模型需掌握的一种能力。为此，我们提出了SkillRater框架，将数据过滤分解为多个专业的评估器，每个评估器负责一种能力，并通过元学习在独立验证目标上进行训练。这些评估器的分数通过渐进式选择规则组合：在每个训练阶段，只要任一评估器将样本排名高于随时间收紧的阈值，该样本即被保留，以在早期保持多样性，后期聚焦高价值样本。在视觉语言模型上的验证表明，SkillRater在视觉理解、OCR和STEM推理能力上均显著优于未过滤基线，且学习到的评估器信号几乎正交，证实了多维度分解的有效性。|Akshat Shrivastava Team|[2602.11615](http://arxiv.org/abs/2602.11615)|null|
|**2026-02-12**|**Chatting with Images for Introspective Visual Thinking**|当前大型视觉-语言模型（LVLMs）通常依赖基于单次视觉编码的纯文本推理，这常导致细粒度视觉信息丢失，尤其是在处理跨区域或多图像的视觉语义或几何关系推理时。为解决这些挑战，本文提出了“与图像对话”这一新框架，将视觉操作重新定义为语言引导的特征调制。在该框架下，模型在富有表现力的语言提示指导下，动态地对多个图像区域进行联合再编码，从而实现语言推理与视觉状态更新之间更紧密的耦合。我们通过ViLaVT实例化了这一范式，它是一个配备动态视觉编码器的新型LVLM，并采用两阶段课程（监督微调和强化学习）进行训练。广泛的实验表明，ViLaVT在八个基准测试中取得了显著且一致的改进，尤其在复杂的多图像和基于视频的空间推理任务上表现突出。|Tieniu Tan Team|[2602.11073](http://arxiv.org/abs/2602.11073)|null|
|**2026-02-12**|**Self-Refining Vision Language Model for Robotic Failure Detection and Reasoning**|针对机器人系统故障推理面临的挑战，即真实世界故障的复杂性及丰富推理标签获取成本高昂，本研究提出了ARMOR模型。ARMOR将故障检测与自然语言推理视为一个多任务自完善过程，通过迭代预测和基于历史输出的条件推理进行训练。它利用大规模稀疏二元标签和少量丰富推理标注的异构监督，并通过离线和在线模仿学习进行优化。在推理阶段，ARMOR生成多条完善轨迹并利用自确定性指标选择最可靠的预测。实验结果显示，ARMOR在故障检测率上比现有方法提升了30%，在LLM模糊匹配分数衡量的推理能力上提升了100%，证明了其在异构监督下的鲁棒性及超越预定义故障模式的开放式推理能力。|Yesh Dattatreya Team|[2602.12405](http://arxiv.org/abs/2602.12405)|null|
|**2026-02-12**|**What does RL improve for Visual Reasoning? A Frankenstein-Style Analysis**|为了理解强化学习（RL）在视觉语言模型（VLM）视觉推理中相比监督微调（IN）的具体贡献，本研究提出了一种“弗兰肯斯坦式”分析框架。该框架通过因果探测定位功能、参数比较表征更新以及模型合并测试可迁移性。研究发现，RL主要通过在模型的中间到后期层诱导一致的推理时段转移来提升性能，并且这些中间到后期的优化对于RL的性能增益是可迁移和必要的。这些结果表明，RL对视觉推理的可靠贡献并非统一增强视觉感知，而是系统地细化了Transformer中间到后期层的计算，从而改善了视觉到推理的对齐和推理性能，揭示了仅通过基准评估来理解多模态推理改进的局限性。|Tianyi Zhou Team|[2602.12395](http://arxiv.org/abs/2602.12395)|null|
|**2026-02-12**|**Synthetic Image Detection with CLIP: Understanding and Assessing Predictive Cues**|鉴于现代生成模型能产生近乎真实的照片，合成图像检测（SID）的泛化能力及其在实际应用中的表现面临挑战，特别是在新生成模型面前。本研究旨在探究CLIP作为SID基础模型的有效性及其内在线索。为此，我们构建了SynthCLIC数据集以减少语义偏差，并利用可解释的线性分类器和文本概念模型分析CLIP特征。结果显示，CLIP检测器在GAN基准上表现优异，但在高质量扩散数据集SynthCLIC上性能略有下降，且跨不同生成器家族的泛化能力显著受限。研究发现，检测器主要依赖高层摄影属性而非明显的生成器伪影。这些发现强调了持续模型更新和更广泛训练暴露的必要性，并肯定了CLIP作为更通用、鲁棒SID方法的强大基础。|Michael Graber Team|[2602.12381](http://arxiv.org/abs/2602.12381)|null|
|**2026-02-12**|**ForeAct: Steering Your VLA with Efficient Visual Foresight Planning**|在开放世界环境中，视觉-语言-动作（VLA）模型将高级语言指令转换为具体可执行动作面临挑战。本研究提出了视觉预见规划器（ForeAct），一个通用高效的规划器，它通过想象未来观察和子任务描述来逐步指导VLA。ForeAct包含一个高效的预见图像生成模块，能从当前视觉输入和语言指令在0.33秒内预测高质量的未来观察，并结合一个视觉-语言模型生成子任务描述。最先进的VLA模型无需修改架构，只需通过增强视觉输入即可无缝集成ForeAct。预见生成器在超过100万个多任务、跨具身情景中进行预训练，学习了鲁棒的具身动力学。在包含11个真实世界任务的基准测试中，ForeAct取得了87.4%的平均成功率，比基线模型有显著提升。|Song Han Team|[2602.12322](http://arxiv.org/abs/2602.12322)|null|
|**2026-02-12**|**LatentAM: Real-Time, Large-Scale Latent Gaussian Attention Mapping via Online Dictionary Learning**|为解决开放词汇机器人感知中从流式RGB-D观测构建可扩展潜在特征地图的挑战，并克服传统VLM嵌入方法缺乏通用性和依赖预训练的问题，本研究提出了LatentAM框架。LatentAM是一种在线3D高斯泼溅（3DGS）映射框架，它采用模型无关且无需预训练的在线字典学习方法，实现了与不同VLM的即插即用集成。该方法将高斯基元与紧凑查询向量关联，通过带有可学习字典的注意力机制转换为近似VLM嵌入。字典在流式观测中高效初始化并在线优化，同时结合基于体素哈希的地图管理策略以实现大规模环境下的GPU内存有界使用。实验结果表明，LatentAM在特征重建保真度上显著优于现有方法，并在评估数据集上实现了接近实时的速度（12-35 FPS）。|Yulun Tian Team|[2602.12314](http://arxiv.org/abs/2602.12314)|null|
|**2026-02-11**|**Hierarchical Concept Embedding & Pursuit for Interpretable Image Classification**|可解释设计模型在计算机视觉中日益受到关注，因为它们能为其预测提供忠实的解释。在图像分类中，这类模型通常从图像中提取人类可解释的概念并用于分类。稀疏概念恢复方法利用视觉-语言模型的潜在空间将图像嵌入表示为概念嵌入的稀疏组合，但这类方法忽视了概念的层次结构，可能导致预测正确但解释与层次结构不一致。为解决此问题，本文提出了分层概念嵌入与追踪（HCEP）框架，它在潜在空间中诱导概念嵌入的层次结构，并利用分层稀疏编码来恢复图像中存在的概念。实验结果表明，HCEP在概念精度和召回率上均优于基线，同时保持了有竞争力的分类准确率，并且在样本数量有限时展现出卓越的分类和概念恢复性能，证明了将层次结构融入稀疏编码能产生更可靠和可解释的图像分类模型。|René Vidal Team|[2602.11448](http://arxiv.org/abs/2602.11448)|null|
|**2026-02-11**|**Beyond VLM-Based Rewards: Diffusion-Native Latent Reward Modeling**|扩散模型和流匹配模型的偏好优化需要既具有判别鲁棒性又计算高效的奖励函数。视觉-语言模型（VLMs）已成为主要的奖励提供者，利用其丰富的多模态先验来指导对齐。然而，它们的计算和内存成本很高，并且通过像素空间奖励优化潜在扩散生成器会引入域不匹配，使对齐复杂化。本文提出了DiNa-LRM，一种扩散原生的潜在奖励模型，它直接在噪声扩散状态上构建偏好学习。DiNa-LRM引入了一种噪声校准的Thurstone似然，其不确定性依赖于扩散噪声，并利用预训练的潜在扩散骨干网络与时间步条件奖励头。实验结果显示，DiNa-LRM在图像对齐基准测试中显著优于现有基于扩散的奖励基线，并以极低的计算成本实现了与最先进VLM相当的性能，显著改善了偏好优化动态，实现了更快、更资源高效的模型对齐。|Wenhan Luo Team|[2602.11146](http://arxiv.org/abs/2602.11146)|**[link](https://github.com/HKUST-C4G/diffusion-rm)**|
|**2026-02-11**|**Active Zero: Self-Evolving Vision-Language Models through Active Environment Exploration**|自博弈已使大型语言模型（LLMs）通过自生成挑战实现自主提升。然而，现有视觉-语言模型（VLMs）的自博弈方法依赖与静态图像集的被动交互，导致对初始数据集的强依赖和学习效率低下。为解决这些局限性，本文提出了Active-Zero框架，将模型学习从被动交互转向主动探索视觉环境。Active-Zero采用三个共同进化的智能体：一个Searcher根据模型能力前沿从开放世界存储库检索图像；一个Questioner合成校准的推理任务；一个Solver通过准确性奖励进行优化。这种闭环机制实现了自我脚手架的自动课程学习。在Qwen2.5-VL-7B-Instruct的12个基准测试中，Active-Zero在推理任务上平均准确率达到53.97%（提升5.7%），在通用理解上达到59.77%（提升3.9%），持续优于现有自博弈基线，表明主动探索是可扩展和自适应自进化视觉-语言系统的关键要素。|Tat-Seng Chua Team|[2602.11241](http://arxiv.org/abs/2602.11241)|null|
|**2026-02-11**|**Safe mobility support system using crowd mapping and avoidance route planning using VLM**|自主移动机器人在动态、尤其是拥挤环境中安全有效导航仍面临挑战。本文提出了一种新颖的框架，该框架集成了视觉-语言模型（VLM）和高斯过程回归（GPR），用于生成动态人群密度图（“抽象图”），以实现自主机器人导航。我们的方法利用VLM识别抽象环境概念（如人群密度）的能力，并通过GPR以概率方式表示这些概念。在大学校园进行的真实世界试验结果表明，机器人成功地生成了避开静态障碍物和动态人群的路径，从而增强了导航的安全性和适应性。|Koichi Ozaki Team|[2602.10910](http://arxiv.org/abs/2602.10910)|null|

<p align=right>(<a href=#updated-on-20260222>back to top</a>)</p>

## VLA

|Publish Date|Title|Chinese Summary|Authors|PDF|Code|
|---|---|---|---|---|---|
|**2026-02-19**|**When Vision Overrides Language: Evaluating and Mitigating Counterfactual Failures in VLAs**|当前视觉-语言-动作（VLA）模型在缺乏强场景监督时，易受数据集偏差影响产生“视觉捷径”，导致在遵循语言指令时出现反事实失败。为系统研究此问题，本研究引入LIBERO-CF基准。针对此问题，本文提出Counterfactual Action Guidance (CAG)方法，通过结合标准VLA策略和语言无关的视觉-动作模块，在动作选择时进行反事实比较，从而显式正则化VLA中的语言条件，减少对视觉捷径的依赖。实验结果表明，CAG在LIBERO-CF上显著提升了语言遵循准确率和任务成功率，特别是在未充分观察的任务上，并在真实世界评估中有效减少反事实失败，提升了平均任务成功率。|Mingyu Ding Team|[2602.17659](http://arxiv.org/abs/2602.17659)|**[link](https://vla-va.github.io/)**|
|**2026-02-19**|**Web Verbs: Typed Abstractions for Reliable Task Composition on the Agentic Web**|随着Web向智能代理环境演变，现有Web代理依赖低级操作（如点击、按键），效率低下且缺乏鲁棒性。本文提出Web Verbs概念，旨在为Web动作提供一个语义层，通过Web级、类型化且语义文档化的函数统一暴露站点能力。这些Verbs作为稳定、可组合的单元，代理可发现并合成为简洁程序，并支持前置条件、后置条件等元数据，从而提高可靠性、效率和可验证性。研究提出了其愿景、概念验证实现和案例研究，展示了其相比现有代理的简洁和鲁棒性，并描绘了实现Web规模标准化的路线图。|Suman Nath Team|[2602.17245](http://arxiv.org/abs/2602.17245)|null|
|**2026-02-18**|**EgoScale: Scaling Dexterous Manipulation with Diverse Egocentric Human Data**|人类行为是学习物理智能的宝贵数据源，但如何有效利用大规模人类数据进行精细的灵巧操作仍是挑战。本文提出了EgoScale框架，通过大规模第一人称人类数据实现人机灵巧操作迁移。该研究在一个包含超过20,854小时带动作标签的人类视频数据集上训练了视觉-语言-动作（VLA）模型，揭示了数据规模与验证损失之间的对数线性关系，且验证损失与真实机器人性能强相关。通过两阶段迁移策略：大规模人类预训练和轻量级人机对齐中期训练，最终策略使22自由度机械手在无预训练基线上平均成功率提高了54%，并能有效迁移到低自由度机械手，表明大规模人类运动提供了可复用、与实体无关的运动先验。|Linxi Fan Team|[2602.16710](http://arxiv.org/abs/2602.16710)|null|
|**2026-02-18**|**MALLVI: a multi agent framework for integrated generalized robotics manipulation**|大型语言模型（LLM）在机器人操作任务规划中面临依赖特定模型、微调以及开放循环操作导致在动态环境中脆弱的问题。本文提出了MALLVi，一个多智能体大语言和视觉框架，旨在实现闭环反馈驱动的机器人操作。MALLVi通过协调专门的智能体（如Decomposer、Localizer、Thinker、Reflector）来管理感知、定位、推理和高级规划，并利用VLM评估环境反馈进行决策。实验结果表明，在模拟和真实世界环境中，这种迭代闭环多智能体协调显著提高了零样本操作任务的泛化能力和成功率。|Babak Khalaj Team|[2602.16898](http://arxiv.org/abs/2602.16898)|null|
|**2026-02-17**|**Learning to Retrieve Navigable Candidates for Efficient Vision-and-Language Navigation**|基于大语言模型（LLM）的视觉-语言导航（VLN）方法常因LLM需反复解释指令并处理冗余候选而导致决策效率低下。本文提出一个检索增强框架，通过在指令和步骤两个互补层面引入检索来提升LLM-based VLN的效率和稳定性，且无需修改或微调底层LLM。在情节层面，指令级嵌入检索器提供任务特定先验；在步骤层面，模仿学习的候选检索器修剪不相关导航方向。在R2R基准上的实验结果显示，该方法在成功率和SPL上均取得一致改进，且两类检索模块对全局指导和分步决策效率贡献互补。|Lina Yao Team|[2602.15724](http://arxiv.org/abs/2602.15724)|null|
|**2026-02-17**|**The Next Paradigm Is User-Centric Agent, Not Platform-Centric Service**|现代数字服务普遍采用平台中心模式，优先优化平台指标而非用户真实需求，导致平台利益与用户利益冲突。本文认为，未来数字服务应转向用户中心智能体，其核心在于优先保护隐私、与用户目标对齐并赋予用户对其偏好和行为的控制权。随着大语言模型和设备端智能的进步，这一愿景变得可行。文章探讨了向用户中心智能体转变的机遇与挑战，提出了一种实用的设备-云管道实现方案，并讨论了实现其广泛采用所需的治理和生态系统结构。|Enhong Chen Team|[2602.15682](http://arxiv.org/abs/2602.15682)|null|
|**2026-02-17**|**CARE Drive A Framework for Evaluating Reason-Responsiveness of Vision Language Models in Automated Driving**|在自动驾驶领域，基础模型（包括视觉语言模型）的现有评估主要侧重于结果表现，而非决策是否反映人类相关的考量，导致其解释可能并非真正基于原因的决策。为解决此问题，本文提出了CARE Drive框架，这是一个与模型无关的上下文感知理由评估框架，用于评估自动驾驶中VLM的理由响应能力。该框架通过受控的上下文变化比较基线和理由增强的模型决策，以评估人类理由对决策行为的因果影响。实验结果表明，明确的人类理由显著影响模型决策，提高了与专家建议行为的一致性，但响应性因上下文因素而异。|Arkady Zgonnikov Team|[2602.15645](http://arxiv.org/abs/2602.15645)|null|
|**2026-02-17**|**World Action Models are Zero-shot Policies**|当前视觉-语言-动作（VLA）模型在语义泛化上表现出色，但在新环境中对未见过的物理运动泛化能力不足。本文引入了DreamZero，一个基于预训练视频扩散骨干的“世界动作模型”（WAM），通过预测未来世界状态和动作来学习物理动力学，利用视频作为世界演变的密集表示。DreamZero能从异构机器人数据中有效学习多样技能，无需重复演示。实验表明，DreamZero在真实机器人任务和新环境中的泛化能力比现有VLA模型提升两倍以上，并实现了14B视频扩散模型在7Hz下的实时闭环控制，同时展示了跨实体迁移能力，在极少量数据下实现了显著的性能提升和少样本实体适应。|Joel Jang Team|[2602.15922](http://arxiv.org/abs/2602.15922)|**[link](https://dreamzero0.github.io/)**|
|**2026-02-17**|**VLM-DEWM: Dynamic External World Model for Verifiable and Resilient Vision-Language Planning in Manufacturing**|视觉语言模型（VLM）在智能制造高层规划中面临无状态操作导致世界状态漂移和不透明推理难以诊断故障两大挑战。本文提出了VLM-DEWM认知架构，通过一个持久、可查询的动态外部世界模型（DEWM）将VLM推理与世界状态管理解耦。每个VLM决策被结构化为可外部化推理轨迹，并在执行前通过DEWM验证。故障发生时，系统能通过状态差异分析实现有针对性的恢复。实验结果显示，VLM-DEWM显著提高了多站点装配等任务中的状态跟踪准确率（从56%提升至93%）和恢复成功率（从低于5%提升至95%），并降低了计算开销。|Ning Ji Team|[2602.15549](http://arxiv.org/abs/2602.15549)|null|
|**2026-02-17**|**Selective Perception for Robot: Task-Aware Attention in Multimodal VLA**|机器人领域的视觉-语言-动作（VLA）模型普遍采用静态信息融合，对所有视觉输入进行统一处理，导致不必要的计算开销和任务无关信息的干扰。本文提出了一种动态信息融合框架，引入轻量级自适应路由架构，该架构实时分析文本提示和腕部摄像机观察，预测多摄像机视图的任务相关性。通过有条件地衰减低信息效用视图的计算，并选择性地向策略网络提供必要视觉特征，实现了计算效率与任务相关性成正比。此外，研究还利用VLM建立了自动化标签流水线以降低数据收集和标注成本。实验结果表明，该方法在真实机器人操作场景中显著提升了推理效率和控制性能。|Soo-Chul Lim Team|[2602.15543](http://arxiv.org/abs/2602.15543)|null|
|**2026-02-17**|**One Agent to Guide Them All: Empowering MLLMs for Vision-and-Language Navigation via Explicit World Representation**|为解决可导航智能体中多模态大语言模型（MLLMs）紧耦合设计导致的性能限制，本研究提出了一种解耦设计，将低级空间状态估计与高级语义规划分离。通过引入交互式度量世界表示，MLLMs能在此基础上进行决策推理，并结合反事实推理以增强其能力，同时确保动作的物理有效性。实验结果表明，该方法在R2R-CE和RxR-CE基准测试中达到了新的零样本SOTA，成功率分别达到48.8%和42.2%，并在多种机器人平台（如TurtleBot 4和无人机）上实现了零样本模拟到真实世界的迁移。|Qi Wu Team|[2602.15400](http://arxiv.org/abs/2602.15400)|null|
|**2026-02-17**|**ActionCodec: What Makes for Good Action Tokenizers**|针对视觉-语言-动作（VLA）模型中动作tokenization设计仅关注重建保真度，忽视其对VLA优化直接影响的问题，本研究从VLA优化角度建立了设计原则，包括最大化时间token重叠、最小化词汇冗余等，并据此提出了高性能动作分词器ActionCodec。实验结果表明，ActionCodec显著提高了训练效率和VLA性能，在LIBERO基准上，未经机器人预训练的SmolVLM2-2.2B模型微调后成功率达到95.5%，通过架构改进进一步提升至97.4%，刷新了VLA模型的SOTA。|Jianye Hao Team|[2602.15397](http://arxiv.org/abs/2602.15397)|null|
|**2026-02-17**|**EAA: Automating materials characterization with vision language model agents**|针对复杂实验显微镜工作流程的自动化需求，本研究提出了实验自动化智能体（EAA），一个由视觉-语言模型驱动的代理系统。EAA集成了多模态推理、工具增强动作和长期记忆，支持自主程序和交互式测量，并基于灵活的任务管理器架构和支持Model Context Protocol (MCP) 的工具生态系统。在Advanced Photon Source成像光束线上的实验展示了EAA在自动化区域板聚焦、自然语言描述特征搜索和交互式数据采集方面的能力，有效提高了光束线效率并降低了用户操作门槛。|Mathew J. Cherukara Team|[2602.15294](http://arxiv.org/abs/2602.15294)|null|
|**2026-02-16**|**Beyond Imitation: Reinforcement Learning-Based Sim-Real Co-Training for VLA Models**|模拟提供了可扩展且低成本的方式来丰富视觉-语言-动作（VLA）训练，减少对昂贵真实机器人演示的依赖，但多数模拟-现实协同训练方法依赖于监督微调（SFT），将模拟视为静态演示源，未充分利用大规模闭环交互，从而限制了真实世界收益和泛化。针对此问题，本文提出了RL-based sim-real Co-training (RL-Co) 框架，该框架利用交互式模拟同时保留真实世界能力。其设计分为两阶段：首先，通过对真实和模拟演示的混合数据进行SFT来预热策略，然后通过在模拟中进行强化学习来微调策略，并添加辅助的真实世界数据监督损失以锚定策略并缓解灾难性遗忘。在四项真实世界桌面操作任务上，RL-Co在两种VLA架构（OpenVLA和π₀.₅）上均持续优于仅真实世界微调和基于SFT的协同训练，例如OpenVLA的真实世界成功率提升24%，π₀.₅提升20%，同时在未见任务变体上表现出更强的泛化能力和显著提升的真实世界数据效率。|Yu Wang Team|[2602.12628](http://arxiv.org/abs/2602.12628)|null|
|**2026-02-16**|**DM0: An Embodied-Native Vision-Language-Action Model towards Physical AI**|针对传统方法将物理接地视为微调后期的限制，本研究提出了DM0，一个为物理AI设计的具身原生视觉-语言-动作（VLA）框架，旨在通过学习异构数据源从一开始就统一具身操作和导航。该方法采用三阶段流程：预训练阶段对VLM进行大规模统一预训练，整合网页文本、自动驾驶场景和具身交互日志；中训练阶段构建流匹配动作专家并采用混合训练策略；后训练阶段引入具身空间支架策略构建空间思维链（CoT）推理。实验结果表明，DM0在RoboChallenge基准测试的Table30上，无论在专业还是通用设置中均达到了最先进的性能。|Tiancai Wang Team|[2602.14974](http://arxiv.org/abs/2602.14974)|**[link](https://github.com/Dexmal/dexbotic)**|
|**2026-02-16**|**DriveFine: Refining-Augmented Masked Diffusion VLA for Precise and Robust Driving**|针对自动驾驶中扩散式和基于Token的VLA规划器各自存在的模态对齐困难、训练效率低或累积因果错误等问题，本研究提出了DriveFine，一种结合了灵活解码与自我修正能力的掩码扩散VLA模型。该模型设计了一个即插即用的block-MoE，将细化专家无缝注入生成专家，通过明确的专家选择和梯度阻断实现专家解耦。此外，引入混合强化学习策略以鼓励细化专家探索并保持训练稳定性。广泛实验表明，DriveFine在NAVSIM v1、v2和Navhard基准测试中表现出强大的效能和鲁棒性。|Yan Wang Team|[2602.14577](http://arxiv.org/abs/2602.14577)|null|
|**2026-02-16**|**Replanning Human-Robot Collaborative Tasks with Vision-Language Models via Semantic and Physical Dual-Correction**|针对人机协作（HRC）中人类指令模糊性及VLM在物理可行性判断上的局限，本研究提出了一个增强VLM推理的双重校正HRC框架。该框架包含一个内部校正模型在动作执行前验证逻辑一致性和任务可行性，以及一个外部校正模型通过执行后反馈检测并纠正物理失败。模拟研究表明该方法显著提高了成功率，真实世界的人形机器人协作装配实验进一步验证了其在根据人类指令进行交互式重新规划方面的有效性。|Kensuke Harada Team|[2602.14551](http://arxiv.org/abs/2602.14551)|null|
|**2026-02-16**|**TikArt: Aperture-Guided Observation for Fine-Grained Visual Reasoning via Reinforcement Learning**|针对多模态大语言模型（MLLMs）在细粒度视觉推理中关键信息易丢失的问题，本研究引入了TikArt（Thinking Aperture），一种光圈引导的智能体。TikArt通过“思考-光圈-观察”循环，交替进行语言生成和两种光圈动作（Zoom和Segment），将局部视觉线索转化为持久的语言记忆。该方法基于Qwen3-VL-8B构建，并使用AGRPO强化学习算法优化推理策略。实验结果显示，TikArt在多项基准测试中均超越基线模型，并在高分辨率推理中展现出可解释的光圈轨迹。|Lei Zhao Team|[2602.14482](http://arxiv.org/abs/2602.14482)|null|
|**2026-02-16**|**Hierarchical Vision-Language Interaction for Facial Action Unit Detection**|为解决面部动作单元（AU）检测在有限标注数据下学习判别性和可泛化表示的挑战，本研究提出了分层视觉-语言交互AU理解（HiVA）方法。HiVA利用大型语言模型生成丰富的AU文本描述作为语义先验，并通过AU感知动态图模块捕获细粒度和整体视觉-语言关联。此外，它采用分层跨模态注意力架构，包含解耦双重交叉注意力（DDCA）和上下文双重交叉注意力（CDCA），以建立细粒度AU特定交互和建模全局AU间依赖。广泛实验表明HiVA持续超越SOTA方法，并能产生语义上有意义的激活模式。|Cuntai Guan Team|[2602.14425](http://arxiv.org/abs/2602.14425)|null|
|**2026-02-16**|**Multi-Turn Adaptive Prompting Attack on Large Vision-Language Models**|针对多轮越狱攻击在视觉-语言模型（LVLMs）上因视觉输入过于恶意而易被防御的问题，本研究提出了MAPA，一种多轮自适应提示攻击方法。MAPA在每轮中交替进行文本-视觉攻击动作以引发最恶意响应，并在跨轮次通过迭代细化逐步放大响应的恶意程度。这种两级设计使MAPA持续优于现有SOTA方法，在Llama-3.2-Vision-11B-Instruct、GPT-4o-mini等主流LVLMs上，攻击成功率提高了11-35%。|Yiliao Song Team|[2602.14399](http://arxiv.org/abs/2602.14399)|null|
|**2026-02-15**|**WoVR: World Models as Reliable Simulators for Post-Training VLA Policies with RL**|为解决强化学习（RL）在视觉-语言-动作（VLA）模型中因真实世界交互需求巨大而难以直接部署的问题，并克服现有世界模型在想象轨迹中存在的幻觉和长期误差积累，本研究提出了WoVR框架。WoVR通过可控的动作条件视频世界模型提高轨迹稳定性，通过关键帧初始化轨迹（Keyframe-Initialized Rollouts）减少有效误差深度，并通过世界模型-策略协同演化保持策略与模拟器对齐。实验结果表明，WoVR在LIBERO基准和真实机器人操作中显著提升了平均成功率，验证了在明确控制幻觉的情况下，学习到的世界模型可作为实用的RL模拟器。|Dongbin Zhao Team|[2602.13977](http://arxiv.org/abs/2602.13977)|null|
|**2026-02-14**|**Semantic-Contact Fields for Category-Level Generalizable Tactile Tool Manipulation**|通用工具操作需要语义规划和精确物理控制，但现有通用机器人策略缺乏高保真物理基础，而接触感知策略又往往实例特定，难以泛化。大规模真实世界触觉数据难以获取，且软传感器复杂动力学使零样本仿真到真实迁移充满挑战。针对此，本文提出了语义-接触场（SCFields），一种融合视觉语义和密集接触估计的统一3D表示。通过两阶段Sim-to-Real接触学习管道实现：首先在大规模模拟数据上预训练以学习通用接触物理，再通过少量真实数据和伪标签进行微调以对齐传感器特性，从而实现对未见工具的物理泛化。SCFields作为扩散策略的密集观测输入，在刮擦、蜡笔画和剥皮任务中表现出鲁棒的类别级泛化能力，显著优于纯视觉和原始触觉基线。|Yan Wu Team|[2602.13833](http://arxiv.org/abs/2602.13833)|null|
|**2026-02-14**|**MOTIF: Learning Action Motifs for Few-shot Cross-Embodiment Transfer**|视觉-语言-动作（VLA）模型在通用机器人学习方面取得进展，但由于运动学异质性和高昂的数据收集成本，跨具身（cross-embodiment）迁移仍具挑战。现有跨具身策略多依赖共享-私有架构，存在私有参数容量有限和缺乏明确适应机制的问题。为解决这些限制，本文提出了MOTIF框架，旨在通过解耦具身无关的时空模式（称为动作基序）实现高效的少样本跨具身迁移。MOTIF首先通过带有进度感知对齐和具身对抗约束的向量量化学习统一基序，确保时空和跨具身一致性；随后，设计轻量级预测器从实时输入中预测基序，并将其与机器人特定状态融合，指导流匹配策略生成新具身动作。在仿真和真实世界环境中的评估均验证了MOTIF的优越性，少样本迁移成功率显著提升。|Heng Tao Shen Team|[2602.13764](http://arxiv.org/abs/2602.13764)|null|
|**2026-02-14**|**HBVLA: Pushing 1-Bit Post-Training Quantization for Vision-Language-Action Models**|视觉-语言-动作（VLA）模型因其巨大的计算和内存开销，难以部署在资源受限的机器人和边缘平台。尽管权重二值化可提高效率，但现有方法无法弥合二值化与全精度权重之间的分布差距，导致长期闭环执行中量化误差累积并严重降低动作质量。针对此，本文提出了HBVLA，一个VLA定制的二值化框架。HBVLA首先利用策略感知的增强Hessian识别对动作生成关键的权重，然后对非显著权重进行稀疏正交变换以引入低熵中间状态，最后在Harr域中对所有权重进行组式1比特量化。实验结果表明，HBVLA在LIBERO和SimplerEnv上，量化模型分别保留了92.2%和93.6%的全精度性能，并显著优于现有最先进的二值化方法，在真实世界评估中也仅造成微小的成功率下降，展示了在严格硬件约束下的鲁棒部署能力。|Ivor Tsang Team|[2602.13710](http://arxiv.org/abs/2602.13710)|null|
|**2026-02-13**|**Steerable Vision-Language-Action Policies for Embodied Reasoning and Hierarchical Control**|预训练的视觉-语言模型（VLMs）虽能提供丰富的常识先验，但将其有效落地到机器人行为仍具挑战，现有分层方法通过自然语言指令连接VLM与VLA，限制了VLM对低层行为的引导。为增强VLM对低层行为的控制，本文引入“可控策略”（Steerable Policies），即在多抽象层次（如子任务、运动、像素坐标）的丰富合成指令上训练VLA模型。这使得VLM能够通过上下文学习引导这些策略，从而解锁其预训练知识并提升任务泛化能力。广泛的真实世界操作实验表明，与现有基于VLM的VLA和分层基线相比，新方法在泛化和长周期任务中表现更优。|Sergey Levine Team|[2602.13193](http://arxiv.org/abs/2602.13193)|null|
|**2026-02-13**|**UniManip: General-Purpose Zero-Shot Robotic Manipulation with Agentic Operational Graph**|通用机器人操作需要机器人无缝连接高层语义意图与低层物理交互，但现有方法在零样本泛化方面存在不足。针对此问题，本文提出了UniManip框架，其核心是双层代理操作图（AOG），旨在统一语义推理与物理接地。该框架通过高层代理层进行任务编排，低层场景层表示动态状态，实现抽象规划与几何约束的持续对齐，从而支持鲁棒的零样本执行。作为一个动态代理循环，UniManip能从非结构化感知中实例化以物体为中心的场景图，通过安全感知局部规划器参数化无碰撞轨迹，并利用结构化记忆自主诊断和从执行失败中恢复。广泛实验证明，该系统在未见过物体和任务上的零样本成功率分别比VLA和分层基线高22.5%和25.0%，并能直接零样本迁移到移动操作任务。|Ziwei Wang Team|[2602.13086](http://arxiv.org/abs/2602.13086)|**[link](https://henryhcliu.github.io/unimanip)**|
|**2026-02-13**|**Learning Native Continuation for Action Chunking Flow Policies**|动作分块能使视觉-语言-动作（VLA）模型实时运行，但简单的分块执行常导致块边界处的不连续性。现有的实时分块（RTC）方法虽能缓解此问题，但由于其在策略外部，会引起虚假的多模态切换和非内在平滑的轨迹。为此，本文提出了Legato，一种针对基于动作分块流的VLA策略的训练时序延续方法。Legato通过将去噪初始化为已知动作和噪声的调度形混合物，使模型接触部分动作信息，并重塑学习到的流动力学以确保训练和推理在每步引导下保持一致性，同时使用随机调度条件训练以支持不同推理延迟和实现可控平滑性。实验证明，Legato能生成更平滑的轨迹，减少执行时的虚假多模态切换，从而减少犹豫和缩短任务完成时间，在五项操作任务中均比RTC表现更优，轨迹平滑度和任务完成时间均提升约10%。|Yang Gao Team|[2602.12978](http://arxiv.org/abs/2602.12978)|**[link](https://lyfeng001.github.io/Legato/)**|
|**2026-02-13**|**ALOE: Action-Level Off-Policy Evaluation for Vision-Language-Action Model Post-Training**|在真实世界中通过在线强化学习（RL）改进大型视觉-语言-动作（VLA）系统时，价值函数估计是关键，但其通常从混合数据源中收集的轨迹片段进行，这本质上是一个离策略评估问题，而现有工作常采用保守的同策略估计，限制了学习效果。为解决这一问题，本文提出了ALOE（Action-Level Off-Policy Evaluation）框架，用于VLA的后期训练。ALOE采用基于分块的时间差分自举法来评估单个动作序列而非预测最终任务结果，这在稀疏奖励下能更好地将信用归因于关键动作分块，并支持稳定的策略改进。在三项真实世界操作任务上的评估表明，ALOE在不影响执行速度的前提下提高了学习效率，验证了离策略RL在VLA后期训练中可被可靠地重新引入。|Maoqing Yao Team|[2602.12691](http://arxiv.org/abs/2602.12691)|null|
|**2026-02-13**|**SignScene: Visual Sign Grounding for Mapless Navigation**|人类可利用导航标识在陌生环境中无需地图进行导航，本研究旨在使机器人也能利用标识实现开放世界中的无图导航。核心挑战在于解释标识：真实世界标识多样复杂，其抽象语义内容需与局部3D场景进行接地。本文将此形式化为标识接地问题，即把标识上的语义指令映射到对应的场景元素和导航动作。考虑到视觉-语言模型（VLMs）具备所需的语义常识和推理能力但对空间表示敏感，我们提出了SignScene，一种以标识为中心的空间-语义表示方法，它捕获导航相关的场景元素和标识信息，并以有利于VLM有效推理的形式呈现。在包含9种不同环境类型、114个查询的数据集上评估，SignScene达到了88%的接地准确率，显著优于基线方法，并能驱动Spot机器人在真实世界中仅依赖标识进行无图导航。|David Hsu Team|[2602.12686](http://arxiv.org/abs/2602.12686)|null|
|**2026-02-13**|**Xiaomi-Robotics-0: An Open-Sourced Vision-Language-Action Model with Real-Time Execution**|本文介绍了小米机器人0号（Xiaomi-Robotics-0），一个针对高性能、快速且平滑实时执行优化的先进视觉-语言-动作（VLA）模型。该方法的核心在于精心设计的训练配方和部署策略。模型首先在大规模跨实体机器人轨迹和视觉-语言数据上进行预训练，赋予其广泛且泛化的动作生成能力，同时避免灾难性遗忘底层VLM的视觉语义知识。在后期训练中，作者提出了多种技术用于异步执行训练，以解决真实机器人执行时的推理延迟问题。部署时，仔细对齐连续预测动作块的时间步，确保实时执行的连续性和无缝性。广泛的模拟基准测试和两个需要精确灵巧双臂操作的真实机器人任务评估表明，该方法在所有模拟基准上均达到了最先进的性能，并在真实机器人任务上使用消费级GPU实现了高成功率和吞吐量，代码和模型检查点已开源。|Quanyun Zhou Team|[2602.12684](http://arxiv.org/abs/2602.12684)|**[link](https://xiaomi-robotics-0.github.io)**|
|**2026-02-13**|**CRAFT: Adapting VLA Models to Contact-rich Manipulation via Force-aware Curriculum Fine-tuning**|视觉-语言-动作（VLA）模型在处理接触密集型操作任务时面临挑战，因为成功需要精确对齐、稳定接触和处理变形物体，而高熵视觉语言输入与低熵但关键的力信号之间存在不平衡，导致模型过度依赖感知并产生不稳定控制。针对此问题，本文引入CRAFT，一个力感知课程微调框架，该框架集成了一个变分信息瓶颈模块，以在早期训练中调节视觉和语言嵌入，鼓励模型优先处理力信号，然后逐步恢复完整的多模态信息。为实现力感知学习，作者设计了一个同源主从遥操作系统，用于收集各种接触密集型任务中同步的视觉、语言和力数据。真实世界实验表明，CRAFT持续提高了任务成功率，泛化到未见物体和新任务变体，并能有效适应不同VLA架构，从而实现鲁棒且可泛化的接触密集型操作。|Jingtao Sun Team|[2602.12532](http://arxiv.org/abs/2602.12532)|null|
|**2026-02-13**|**AsyncVLA: An Asynchronous VLA for Fast and Robust Navigation on the Edge**|当前机器人基础模型虽泛化能力强，但推理延迟高，导致在动态环境中不安全。针对此问题，本文提出了AsyncVLA异步控制框架，将语义推理与反应性执行解耦。该框架通过远程工作站上的大型基础模型提供高层指导，同时由轻量级板载Edge Adapter高频精细化动作，并通过端到端微调协议和轨迹重加权策略弥合异步流间的域间隙。在面对高达6秒通信延迟的真实视觉导航任务中，AsyncVLA的成功率比现有最佳基线高出40%，成功连接了大型模型的语义智能与边缘机器人所需的实时反应能力。|Sergey Levine Team|[2602.13476](http://arxiv.org/abs/2602.13476)|null|
|**2026-02-13**|**FlowHOI: Flow-based Semantics-Grounded Generation of Hand-Object Interactions for Dexterous Robot Manipulation**|现有视觉-语言-动作（VLA）模型在长周期、接触密集型任务中表现不佳，原因在于缺乏对手-物体交互（HOI）结构的明确表示。为解决此问题，本文提出FlowHOI，一个两阶段流匹配框架，可根据自我中心观察、语言指令和3D高斯泼溅场景重建生成语义明确、时间连贯的HOI序列。该方法将以几何为中心的抓取与以语义为中心的操控解耦，并利用3D场景令牌和运动-文本对齐损失进行语义接地，同时通过从大规模自我中心视频重建HOI轨迹的方法弥补高保真HOI监督的稀缺性。实验结果显示，FlowHOI在GRAB和HOT3D基准测试中实现了最高的动作识别准确率，物理模拟成功率比扩散基线高1.7倍，推理速度提升40倍，并成功在真实机器人上执行了灵巧操作任务。|Xingxing Zuo Team|[2602.13444](http://arxiv.org/abs/2602.13444)|**[link](https://huajian-zeng.github.io/projects/flowhoi/)**|
|**2026-02-12**|**Scaling Verification Can Be More Effective than Scaling Policy Learning for Vision-Language-Action Alignment**|通用机器人理解并执行自然语言指令是长期愿景，尽管视觉-语言-动作（VLA）模型已取得显著进展，但其生成动作仍可能与指令不符。为缩小“意图-动作差距”，本文研究了测试时验证方法。通过分析具身指令遵循的测试时缩放定律，发现联合缩放复述指令和生成动作的数量能更有效地增加测试时样本多样性。在此基础上，提出了CoVer，一种用于VLA对齐的对比验证器，该架构能随计算资源和数据的增加而良好扩展。进一步引入“启动时计算”和分层验证推理流程：在部署时预计算多样化的复述指令，为每条指令重复生成动作候选，然后使用验证器选择最优高层提示和低层动作块。实验表明，相比扩展策略预训练，CoVer在SIMPLER基准上实现了22%的分布内和13%的分布外增益，并在真实世界实验中进一步提高了45%，在PolaRiS基准上任务进度和成功率分别提升了14%和9%。|Marco Pavone Team|[2602.12281](http://arxiv.org/abs/2602.12281)|null|
|**2026-02-12**|**GigaBrain-0.5M*: a VLA That Learns From World Model-Based Reinforcement Learning**|针对现有视觉-语言-动作（VLA）模型在场景理解和未来预测方面的局限性，本研究提出GigaBrain-0.5M*，一个通过世界模型强化学习训练的VLA模型。该模型基于已在大量机器人操作数据上预训练的GigaBrain-0.5，并整合了RAMP（Reinforcement learning via world Model-conditioned Policy）以实现鲁棒的跨任务适应。实验结果表明，RAMP在RECAP基线之上取得了显著的性能提升，在洗衣折叠、箱子包装和咖啡制作等挑战性任务上提升约30%，并在真实部署中展示了可靠的长期执行能力，能够无故障完成复杂的操纵任务。|Zheng Zhu Team|[2602.12099](http://arxiv.org/abs/2602.12099)|**[link](https://gigabrain05m.github.io/)**|
|**2026-02-12**|**VLAW: Iterative Co-Improvement of Vision-Language-Action Policy and World Model**|为提升视觉-语言-动作（VLA）模型性能和可靠性，并解决真实世界数据收集成本高及现有世界模型物理保真度不足的问题，本研究提出一个迭代改进算法。该算法利用少量真实世界试错数据提高世界模型的保真度，然后世界模型生成补充合成数据以改进VLA模型。在真实机器人上的实验表明，该方法使先进VLA模型的成功率相较于基础策略绝对提升39.2%，并且通过生成的合成试错数据训练，又额外提升了11.6%。|Chelsea Finn Team|[2602.12063](http://arxiv.org/abs/2602.12063)|null|
|**2026-02-12**|**HoloBrain-0 Technical Report**|针对基础模型研究与可靠机器人真实部署之间的差距，本研究提出了HoloBrain-0，一个全面的视觉-语言-动作（VLA）框架。其核心是一个新颖的VLA架构，通过明确整合机器人具身先验（如多视角相机参数和运动学描述）来增强3D空间推理并支持多样化具身形态。该设计通过“预训练后微调”范式验证，在多个仿真基准和长时程真实世界操作任务中取得领先结果，且一个高效的0.2B参数变体能支持低延迟部署。研究还全面开源了HoloBrain生态系统，旨在加速研究和实际应用。|Zhizhong Su Team|[2602.12062](http://arxiv.org/abs/2602.12062)|null|
|**2026-02-12**|**When would Vision-Proprioception Policies Fail in Robotic Manipulation?**|针对视觉-本体感受策略在机器人运动过渡阶段视觉模态作用受限、策略倾向于利用本体感受信号导致视觉学习受抑制的问题，本研究提出了梯度调整与阶段引导（GAP）算法。该算法通过利用本体感受信息估计运动过渡阶段的概率，并自适应地调整本体感受梯度的幅值，以实现视觉和本体感受模态的动态协同。综合实验表明，GAP算法能够提升视觉-本体感受策略的鲁棒性和泛化性，适用于模拟和真实环境、单臂和双臂设置，并兼容多种VLA模型。|Di Hu Team|[2602.12032](http://arxiv.org/abs/2602.12032)|null|
|**2026-02-12**|**JEPA-VLA: Video Predictive Embedding is Needed for VLA Models**|针对现有视觉-语言-动作（VLA）模型存在的样本效率低和泛化能力有限问题，本研究发现其根源在于预训练视觉表示在环境理解和策略先验方面的知识不足。通过深入分析，研究指出在视频上预训练的预测嵌入，特别是V-JEPA 2，能更有效地捕捉任务相关时态动态并忽略不可预测因素，从而弥补了现有视觉表示的缺陷。在此基础上，提出了JEPA-VLA，一种将预测嵌入自适应整合到现有VLA中的方法。实验证明，JEPA-VLA在LIBERO、LIBERO-plus、RoboTwin2.0和真实机器人任务等一系列基准测试中均取得了显著性能提升。|Mingsheng Long Team|[2602.11832](http://arxiv.org/abs/2602.11832)|null|
|**2026-02-12**|**ABot-N0: Technical Report on the VLA Foundation Model for Versatile Embodied Navigation**|针对具身导航任务碎片化的问题，本研究引入了ABot-N0，一个统一的视觉-语言-动作（VLA）基础模型，旨在实现点目标、物体目标、指令遵循、兴趣点目标和人员跟踪五大核心任务的“大一统”。该模型采用分层“大脑-动作”架构，利用大型语言模型进行语义推理，并结合流匹配专家生成精确轨迹。为支持大规模学习，研究构建了ABot-N0数据引擎，整理了海量专家轨迹和推理样本。实验结果表明，ABot-N0在7个基准测试中取得了最先进的性能，并能通过集成的Agentic导航系统实现动态真实世界环境中的鲁棒、长时程任务执行。|Mu Xu Team|[2602.11598](http://arxiv.org/abs/2602.11598)|**[link](https://amap-cvlab.github.io/ABot-Navigation/ABot-N0/)**|
|**2026-02-12**|**Scaling World Model for Hierarchical Manipulation Policies**|针对视觉-语言-动作（VLA）模型在域外（OOD）设置下泛化能力不足的问题，本研究引入了一个分层VLA框架VISTA。VISTA利用大规模预训练世界模型的泛化能力实现鲁棒且可泛化的视觉子目标任务分解，其中高层世界模型规划任务分解为带有目标图像的子任务序列，低层VLA策略遵循文本和视觉指导生成动作。这些合成的目标图像为低层策略提供了视觉和物理上的详细指导，使其能够泛化到未见过的物体和新场景。实验结果表明，在世界模型生成的指导下，VISTA在大量OOD场景中显著提升了VLA性能，在novel场景中性能从14%提高到69%。|Xinghang Li Team|[2602.10983](http://arxiv.org/abs/2602.10983)|null|
|**2026-02-12**|**ForeAct: Steering Your VLA with Efficient Visual Foresight Planning**|视觉-语言-动作（VLA）模型将高级语言指令转换为具体可执行动作，在开放世界环境中尤具挑战性。本文提出了Visual Foresight Planning (ForeAct)，这是一种通用高效的规划器，通过想象的未来观察和子任务描述逐步指导VLA。该规划器包含一个高效的预测图像生成模块（在0.33秒内预测高质量未来观察）和一个视觉-语言模型，后者负责推理任务并生成子任务描述。重要的是，ForeAct能够无缝集成到现有VLA中，只需扩充视觉输入而无需修改架构。经过百万级跨具身任务的预训练，预测生成器学习了鲁棒的具身动力学。在包含11项多样化、多步骤真实世界任务的基准测试中，ForeAct实现了87.4%的平均成功率，比基线 $π_0$提高了40.9%，比带有文本子任务指导的$π_0$ 提高了30.3%。|Song Han Team|[2602.12322](http://arxiv.org/abs/2602.12322)|null|
|**2026-02-11**|**H-WM: Robotic Task and Motion Planning Guided by Hierarchical World Model**|针对现有世界模型在长时程机器人规划中累积误差的问题，以及传统符号逻辑世界模型缺乏视觉感知接地的局限性，本研究提出分层世界模型（H-WM）。H-WM在一个统一的双层框架内联合预测逻辑和视觉状态转换，将符号推理的鲁棒性与视觉观察的感知基础相结合。为训练H-WM，研究引入了一个对齐机器人运动与符号状态、动作和视觉观察的机器人数据集。实验证明，分层输出为长时程任务提供了稳定一致的中间指导，有效减轻了误差积累，并在VLA控制策略上展示了该方法的有效性和通用性。|Yingxue Zhang Team|[2602.11291](http://arxiv.org/abs/2602.11291)|null|
|**2026-02-11**|**RISE: Self-Improving Robot Policy with Compositional World Model**|针对视觉-语言-动作（VLA）模型在接触密集和动态操作任务中易受执行偏差影响的脆弱性，以及物理世界中在线强化学习（RL）的限制，本研究提出了RISE，一个通过想象进行机器人强化学习的可扩展框架。其核心是一个组合世界模型，该模型能预测多视角未来并通过进度价值模型评估想象结果，从而为策略改进提供信息丰富的优势。这些组件被整合到一个闭环自改进流水线中，在想象空间中持续生成试错并更新策略。在三个真实世界任务中，RISE相对于现有技术取得了显著性能提升，如在动态砖块分类、背包包装和盒子关闭任务中，绝对性能分别提升超过35%、45%和35%。|Hongyang Li Team|[2602.11075](http://arxiv.org/abs/2602.11075)|**[link](https://opendrivelab.com/kai0-rl/)**|
|**2026-02-11**|**RADAR: Benchmarking Vision-Language-Action Generalization via Real-World Dynamics, Spatial-Physical Intelligence, and Autonomous Evaluation**|针对现有视觉-语言-动作（VLA）模型评估主要局限于仿真或高度受限的真实世界，导致现实差距大、泛化能力差的问题，本研究提出RADAR（Real-world Autonomous Dynamics And Reasoning）基准。RADAR旨在系统评估VLA在真实条件下的泛化能力，集成了物理动力学套件、专门测试空间推理和物理理解的任务，以及基于3D指标的全自主评估流程。通过RADAR对多个先进VLA模型进行审计，发现模型在适度物理动态下性能急剧下降，例如在传感器噪声下3D IoU从0.261下降到0.068，且空间推理能力有限，揭示了模型在真实世界条件下的严重脆弱性，强调了RADAR作为可靠泛化评估基准的必要性。|Guangrun Wang Team|[2602.10980](http://arxiv.org/abs/2602.10980)|null|
|**2026-02-11**|**From Representational Complementarity to Dual Systems: Synergizing VLM and Vision-Only Backbones for End-to-End Driving**|VLA驾驶将语言功能引入端到端规划，但其核心变化除准确性-成本权衡外尚不明确。本文通过RecogDrive进行3-RQ分析，比较VLM和纯视觉骨干网络在相同规划器下的表现。研究发现VLM能引入独特子空间，导致在长尾场景下行为差异，VLM更激进而ViT更保守。基于此，本文提出HybridDriveVLA，通过学习评分器融合两者的轨迹，将PDMS提升至92.10。进一步，DualDriveVLA实现了快慢策略，仅在低置信度时调用VLM，在15%场景中调用VLM即可达到91.00 PDMS，并使吞吐量提高3.2倍。|Yan Wang Team|[2602.10719](http://arxiv.org/abs/2602.10719)|null|
|**2026-02-11**|**Say, Dream, and Act: Learning Video World Models for Instruction-Driven Robot Manipulation**|机器人操作需要预测环境对动作的响应，但现有系统常缺乏此能力，导致效率低下。鉴于VLMs无法明确预测未来状态且现有世界模型存在预测短期或空间不一致帧的问题，本文提出了一种快速且预测性的视频条件动作框架。该方法首先适配鲁棒的视频生成模型以确保未来预测的可靠性，接着通过对抗蒸馏实现快速视频生成，最后训练一个动作模型利用生成视频和真实观测纠正空间错误。实验结果表明，该方法生成的视频预测在时间连贯性和空间准确性方面表现优异，显著提升了具身一致性、空间指代能力和任务完成度。|Yanwei Fu Team|[2602.10717](http://arxiv.org/abs/2602.10717)|null|
|**2026-02-11**|**AugVLA-3D: Depth-Driven Feature Augmentation for Vision-Language-Action Models**|当前VLA模型主要依赖2D图像训练，限制了其在复杂3D环境中的空间理解和动作落地。为解决此问题，本文提出了一个将深度估计集成到VLA模型中的新框架，以丰富3D特征表示。该方法利用VGGT从RGB输入中提取3D几何线索，并引入“动作助手”模块，通过动作先验约束3D表示以确保与下游控制任务的一致性。实验结果表明，所提出的方法不仅增强了几何模糊场景中的感知，还显著提高了动作预测准确性，强调了深度驱动数据增强对弥合2D观测与3D决策差距的潜力。|F. Richard Yu Team|[2602.10698](http://arxiv.org/abs/2602.10698)|null|
|**2026-02-11**|**Improving Medical Visual Reinforcement Fine-Tuning via Perception and Reasoning Augmentation**|强化微调（RFT）在语言模型后训练中展现潜力，但在跨模态、以视觉为中心的医学影像领域应用不足，而该领域需要强大的视觉感知和结构化推理。为此，本文提出了VRFT-Aug，一个专为医学领域设计的视觉强化微调框架。VRFT-Aug通过引入先验知识注入、感知驱动策略优化、医学知情奖励塑造和行为模仿等训练策略来增强感知和推理。实验结果表明，该方法在多个医学数据集上持续优于标准监督微调和RFT基线，并提供了可推广到其他医学图像任务的实用训练启发。|Qicheng Lao Team|[2602.10619](http://arxiv.org/abs/2602.10619)|null|
|**2026-02-11**|**LAP: Language-Action Pre-Training Enables Zero-shot Cross-Embodiment Transfer**|机器人学长期目标是实现零样本部署的通用策略，但现有VLA模型与训练实体紧密耦合。针对此问题，本文提出了语言-动作预训练（LAP），一种将低级机器人动作直接用自然语言表示的方法，使动作监督与预训练VLM的输入-输出分布对齐，无需定制化设计或昂贵标注。基于LAP，本文提出了LAP-3B，实现了在未见机器人上的显著零样本迁移，平均成功率超过50%，比现有VLA模型提升约2倍。此外，LAP还支持高效适应、有利扩展，并通过统一的语言-动作格式在协同训练中获得额外收益。|Anirudha Majumdar Team|[2602.10556](http://arxiv.org/abs/2602.10556)|**[link](https://lap-vla.github.io)**|
|**2026-02-11**|**Found-RL: foundation model-enhanced reinforcement learning for autonomous driving**|强化学习在自动驾驶中面临样本效率低和语义可解释性不足问题，而基础模型（特别是VLM）虽能提供丰富知识，但高推理延迟阻碍其在RL训练中的实时部署。为此，本文提出了Found-RL平台，通过异步批量推理框架将VLM推理与仿真循环解耦，解决了延迟瓶颈。该平台引入了价值边际正则化和优势加权动作指导等监督机制，将VLM建议有效提炼到RL策略中。此外，通过条件对比动作对齐解决CLIP的动态盲区问题，实现密集奖励塑造。Found-RL使得轻量级RL模型在保持实时推理（500 FPS）的同时，达到接近十亿参数VLM的性能。|Sikai Chen Team|[2602.10458](http://arxiv.org/abs/2602.10458)|null|
|**2026-02-10**|**Hardware Co-Design Scaling Laws via Roofline Modelling for On-Device LLMs**|VLA模型在资源受限设备部署中面临LLM骨干网络选择挑战，需平衡准确性与推理延迟及硬件效率。本文提出了硬件协同设计法则，将模型训练损失建模为架构超参数的函数，并通过屋脊线模型表征推理延迟，从而建立了准确性-延迟的直接对应关系。通过对1,942个候选架构进行经验评估和训练，拟合出架构与训练损失的缩放定律。将该定律与延迟建模结合，本文确定了硬件协同设计LLM的帕累托前沿，并将架构搜索公式化为精度与性能的联合优化。结果显示，该方法将架构选择时间从数月缩短到数天，并在相同延迟下，其协同设计的架构在WikiText-2上的困惑度降低了19.42%。|Cheng Deng Team|[2602.10377](http://arxiv.org/abs/2602.10377)|null|
|**2026-02-10**|**ST4VLA: Spatially Guided Training for Vision-Language-Action Models**|大型VLM虽擅长多模态理解，但在具身任务中将指令转化为低级运动动作时表现不足。为此，本文引入了ST4VLA，一个双系统VLA框架，通过空间引导训练使动作学习与VLM中的空间先验对齐。ST4VLA包含两阶段：首先是空间基础预训练，利用大规模和机器人数据通过点、框、轨迹预测为VLM注入可迁移先验；其次是空间引导动作后训练，通过空间提示促使模型生成更丰富的空间先验以指导动作生成。实验表明，ST4VLA显著提升了Google Robot和WidowX Robot的性能，在SimplerEnv上创造了SOTA，并展现出对未见物体、转述指令的更强泛化能力及在真实世界中对扰动的鲁棒性。|Jiangmiao Pang Team|[2602.10109](http://arxiv.org/abs/2602.10109)|null|
|**2026-02-10**|**EgoHumanoid: Unlocking In-the-Wild Loco-Manipulation with Robot-Free Egocentric Demonstration**|人形机器人运动-操作任务面临数据需求高和具身差距挑战。针对此，本文提出了EgoHumanoid框架，首次利用大量以自我为中心的人类演示数据与有限机器人数据共同训练视觉-语言-动作策略，使人形机器人在多样真实环境中执行运动-操作。为弥合人类与机器人间的形态和视点差异，本文引入了包含硬件设计、数据处理的系统对齐流程，并开发了便携式人类数据收集系统。其核心是视点对齐和动作对齐，分别减少视觉域差异和将人类动作映射到机器人动作空间。真实世界实验表明，整合人类演示数据显著优于仅用机器人数据的基线（51%），尤其是在未见环境中。|Li Chen Team|[2602.10106](http://arxiv.org/abs/2602.10106)|**[link](https://opendrivelab.com/EgoHumanoid)**|

<p align=right>(<a href=#updated-on-20260222>back to top</a>)</p>

## Humanoid

|Publish Date|Title|Chinese Summary|Authors|PDF|Code|
|---|---|---|---|---|---|
|**2026-02-18**|**Learning Humanoid End-Effector Control for Open-Vocabulary Visual Loco-Manipulation**|针对人形机器人野外任意物体抓取操作中末端执行器精确控制和通用视觉理解的挑战，现有模仿学习方法泛化性受限。本文提出HERO范式，结合大型视觉模型的强大泛化能力与模拟训练的控制性能。通过设计残差感知末端执行器跟踪策略，该策略融合了逆运动学、神经前向模型、目标调整和重规划。实验结果显示，该方法将末端执行器跟踪误差降低3.2倍，并在多样真实环境中实现了对日常物品的可靠操作，验证了设计的有效性。|Saurabh Gupta Team|[2602.16705](http://arxiv.org/abs/2602.16705)|**[link](https://hero-humanoid.github.io/)**|
|**2026-02-18**|**VIGOR: Visual Goal-In-Context Inference for Unified Humanoid Fall Safety**|人形机器人在杂乱环境中跌倒恢复能力不足，现有方法多将跌倒安全分解处理或仅在平坦地形上训练，导致泛化性差。本文提出一种统一的跌倒安全方法，基于人类跌倒姿态的可转移性和整合的感知-运动表示。该方法通过在平坦和复杂地形上训练特权教师模型，并将其蒸馏到仅依赖深度和本体感受的学生模型中，以匹配教师的“目标-情境”潜在表示。模拟和真实世界的实验表明，该方法在多样非平坦环境中实现了鲁棒的零样本跌倒安全，无需真实世界微调。|Stella X. Yu Team|[2602.16511](http://arxiv.org/abs/2602.16511)|null|
|**2026-02-18**|**Humanoid Hanoi: Investigating Shared Whole-Body Control for Skill-Based Box Rearrangement**|人形机器人执行长时程箱子重排任务时，通过任务级技能序列实现。然而，若直接重用预训练的共享全身控制器（WBC），在长时程任务中可能因技能组合导致的状态和指令分布偏移而降低鲁棒性。本文提出一个基于技能的框架，所有技能通过共享WBC执行，并采用数据聚合程序，通过领域随机化下的闭环技能执行轨迹来增强共享WBC训练。在“人形汉诺塔”基准测试和Digit V3人形机器人上的实验表明，该方法实现了扩展时程上的完全自主重排，并验证了共享WBC方法的优势。|Alan Fern Team|[2602.13850](http://arxiv.org/abs/2602.13850)|null|
|**2026-02-17**|**Perceptive Humanoid Parkour: Chaining Dynamic Human Skills via Motion Matching**|针对人形机器人难以在复杂环境中实现敏捷、适应性强的高动态跑酷动作的问题，本文提出了感知型人形跑酷（PHP）模块化框架。该框架首先利用运动匹配技术将原子人类技能组合成长时程运动轨迹，然后训练运动跟踪强化学习专家策略，并将其蒸馏为基于深度感知、多技能的学生策略。实验结果表明，该系统在Unitree G1人形机器人上实现了高动态跑酷技能，包括攀爬高障碍物和在实时扰动下进行长时程多障碍物穿越，展示了其鲁棒性和适应性。|C. Karen Liu Team|[2602.15827](http://arxiv.org/abs/2602.15827)|null|
|**2026-02-17**|**MeshMimic: Geometry-Aware Humanoid Motion Learning through 3D Scene Reconstruction**|现有的人形机器人运动控制框架常因动作捕捉数据缺乏环境几何上下文，导致运动与场景解耦，在地形感知任务中出现物理不一致。本文提出MeshMimic框架，通过3D场景重建与具身智能结合，使机器人能直接从视频中学习耦合的“运动-地形”交互。该方法利用先进3D视觉模型重建人类轨迹和3D地形，并引入运动学一致性优化算法和接触不变重定向方法。实验结果表明，MeshMimic在多样复杂地形上实现了鲁棒高动态性能，证明了仅使用消费级单目传感器便可训练复杂物理交互，为人形机器人自主进化提供可扩展途径。|Yijie Guo Team|[2602.15733](http://arxiv.org/abs/2602.15733)|null|
|**2026-02-16**|**Replanning Human-Robot Collaborative Tasks with Vision-Language Models via Semantic and Physical Dual-Correction**|人机协作（HRC）在装配任务中面临人类指令模糊和未充分指定的问题，现有视觉-语言模型（VLMs）虽能解释指令但存在幻觉推理和无法预测物理执行失败的缺陷。本文提出一种HRC框架，通过双重校正机制增强VLM的推理能力：一个内部模型在执行前验证逻辑和可行性，一个外部模型通过执行后反馈检测并纠正物理失败。仿真和真实世界协作装配任务的实验结果表明，该方法显著提高了成功率，并能有效支持机器人根据人类指令进行交互式重规划。|Kensuke Harada Team|[2602.14551](http://arxiv.org/abs/2602.14551)|null|
|**2026-02-16**|**AdaptManip: Learning Adaptive Whole-Body Object Lifting and Delivery with Online Recurrent State Estimation**|针对人形机器人集成导航、物体举升和递送任务中现有模仿学习方法鲁棒性不足的问题，本文提出了AdaptManip，一个全自主的适应性全身运动-操作框架。该框架通过强化学习训练鲁棒策略，无需人类演示，包含循环物体状态估计器、全身基础策略（用于运动和操作控制）以及基于LiDAR的全局定位器。所有组件均在模拟中训练并零样本部署到真实硬件。实验结果表明，AdaptManip在适应性和成功率上显著优于基线方法，并在真实世界中实现了全自主导航、物体举升和递送。|Sehoon Ha Team|[2602.14363](http://arxiv.org/abs/2602.14363)|**[link](https://morganbyrd03.github.io/adaptmanip/)**|
|**2026-02-15**|**WIMLE: Uncertainty-Aware World Models with IMLE for Sample-Efficient Continuous Control**|模型基强化学习（MBRL）在实际中常因模型误差累积、世界模型无法处理多模态动力学以及预测过度自信而表现不佳。本文引入WIMLE，一种MBRL方法，将隐式最大似然估计（IMLE）扩展到该框架中，以学习随机、多模态的世界模型，并通过集成和潜在采样估计不确定性。训练时，WIMLE根据预测置信度权重化合成转换，衰减不确定预测带来的偏差。实验结果表明，在40个连续控制任务中，WIMLE实现了卓越的样本效率，并在挑战性任务上显著提高了性能，突出了IMLE多模态和不确定性感知加权对稳定MBRL的价值。|Ke Li Team|[2602.14351](http://arxiv.org/abs/2602.14351)|**[link](https://openreview.net/forum?id=mzLOnTb3WH)**|
|**2026-02-15**|**ProAct: A Dual-System Framework for Proactive Embodied Social Agents**|具身社交智能体在生成同步语音和手势方面取得进展，但大多数交互系统仅能对短期感官输入做出反应，缺乏主动社交行为所需的长期上下文推理。本文提出ProAct双系统框架，通过解耦低延迟的“行为系统”和较慢的“认知系统”，协调实时交互的时间尺度冲突。认知系统负责长期社交推理并生成高级主动意图，通过ControlNet条件化的流式匹配模型将其转化为连续非语言行为。真实世界用户研究表明，ProAct在感知主动性、社交存在感和整体参与度方面优于反应式变体，证明了双系统主动控制对具身社交交互的益处。|Libin Liu Team|[2602.14048](http://arxiv.org/abs/2602.14048)|**[link](https://proactrobot.github.io/)**|
|**2026-02-14**|**Impact-Robust Posture Optimization for Aerial Manipulation**|针对力矩控制的运动学冗余机器人在受到冲击时可能出现的显著状态和输入指令尖峰问题，本文提出一种优化机器人姿态的新方法，以提高冲击鲁棒性。该方法基于刚体冲击模型构建配置相关度量，量化冲击前后速度变化，并通过最小化该度量来减少冲击尖峰。为实现实时可行性，将问题重构为嵌入到任务空间逆动力学全身控制器中的基于梯度的运动任务。实验结果表明，该方法使机器人冲击后配置尖峰减少高达51%，有效避免执行器饱和，并证明了运动学冗余对冲击鲁棒性的重要性。|Antonio Franchi Team|[2602.13762](http://arxiv.org/abs/2602.13762)|null|
|**2026-02-14**|**A Kung Fu Athlete Bot That Can Do It All Day: Highly Dynamic, Balance-Challenging Motion Dataset and Autonomous Fall-Resilient Tracking**|针对现有类人机器人运动追踪系统在极端动态行为和故障恢复方面的不足，本文构建了高动态武术运动数据集KungFuAthlete，并发现其运动强度远超现有数据集。在此基础上，研究提出了一种新颖的训练范式，使单个策略能够同时学习高动态运动追踪和跌倒恢复。实验证明，该框架将机器人能力从单纯的运动追踪扩展到具备恢复能力的执行，显著提升了类人机器人在真实世界高动态场景中的鲁棒性和自主性。|Xuesong Li Team|[2602.13656](http://arxiv.org/abs/2602.13656)|null|
|**2026-02-13**|**Adding internal audio sensing to internal vision enables human-like in-hand fabric recognition with soft robotic fingertips**|为解决机器人触觉感知在同时实现高空间分辨率和高时间采样率方面的挑战，研究团队开发了一个集成视觉和听觉触觉信息的系统。该系统包含一个基于摄像头的Minsight传感器和一个基于MEMS麦克风的Minsound传感器，能分别捕获指尖形变和高频振动。机器人通过主动夹持和摩擦织物样本进行感知。实验结果表明，听觉传感器在织物分类中发挥了重要作用，基于Transformer的方法在20种常见织物上达到了97%的分类精度，并且能泛化学习织物的拉伸性、厚度和粗糙度等通用表征。|Katherine J. Kuchenbecker Team|[2602.12918](http://arxiv.org/abs/2602.12918)|null|
|**2026-02-13**|**PMG: Parameterized Motion Generator for Human-like Locomotion Control**|现有类人机器人全身运动控制方法在适应高层命令、处理多样化任务以及实现高效仿真到现实迁移方面存在局限性。为克服这些问题，本文提出了一种基于人类运动结构分析的实时运动生成器（PMG），它能利用少量参数化运动数据和高维控制指令合成参考轨迹。结合模仿学习流水线和基于优化的仿真到现实电机参数识别模块，该方法在类人机器人ZERITH Z1上得到了验证，结果表明PMG能生成自然、类人步态，精确响应高维控制输入（包括VR遥操作），并实现高效、可验证的仿真到现实迁移。|Houde Liu Team|[2602.12656](http://arxiv.org/abs/2602.12656)|null|
|**2026-02-13**|**CLOT: Closed-Loop Global Motion Tracking for Whole-Body Humanoid Teleoperation**|全尺寸类人机器人的长时间全身遥操作面临全局位姿漂移的挑战，现有方法常因缺乏全局反馈而导致不稳定。本文提出了CLOT系统，通过高频定位反馈实现闭环全局运动追踪，从而在长时间跨度上实现无漂移的人机模仿。为避免直接施加全局追踪奖励导致的激进行为，研究提出了一种数据驱动的随机化策略，并结合对抗性运动先验来抑制非自然行为。经过20小时人类运动数据训练和1300 GPU小时策略训练，仿真和真实世界实验验证了该系统在31自由度类人机器人上实现高动态、高精度和强鲁棒性的遥操作。|Yichao Yan Team|[2602.15060](http://arxiv.org/abs/2602.15060)|null|
|**2026-02-12**|**General Humanoid Whole-Body Control via Pretraining and Fast Adaptation**|针对类人机器人在复杂运动分布、快速适应和高动态场景中保持鲁棒平衡的挑战，本文提出了FAST框架，旨在实现快速适应和稳定的运动追踪。FAST引入了Parseval引导的残差策略适应机制，该机制通过正交性和KL散度约束学习轻量级增量动作策略，从而高效适应分布外运动并减轻灾难性遗忘。此外，为增强物理鲁棒性，研究提出了质心感知控制，通过纳入质心相关观测和目标来提升追踪复杂参考运动时的平衡性。仿真和真实世界实验均表明FAST在鲁棒性、适应效率和泛化能力上优于现有基线方法。|Zongqing Lu Team|[2602.11929](http://arxiv.org/abs/2602.11929)|null|
|**2026-02-12**|**HAIC: Humanoid Agile Object Interaction Control via Dynamics-Aware World Model**|针对类人机器人在非结构化环境中与非完全驱动对象（具有独立动力学和非完整约束）进行鲁棒交互的难题，本文提出了HAIC框架。HAIC的核心贡献是一个动力学预测器，它仅通过本体感受历史就能估计高阶对象状态，并将这些预测投射到几何先验上，形成空间接地动态占用图，从而使策略在盲区内推断碰撞边界和接触可供性。通过非对称微调策略，HAIC在类人机器人上的实验表明，其在高成功率下完成了滑板、推拉小车等敏捷任务，并能通过预测多对象动力学，掌握跨越不同地形携带箱子等多对象长时任务。|Renjing Xu Team|[2602.11758](http://arxiv.org/abs/2602.11758)|**[link](https://haic-humanoid.github.io/)**|
|**2026-02-12**|**Future Mining: Learning for Safety and Security**|针对采矿环境中恶劣条件（照明差、无GPS、地形不规则、连接不稳）和网络安全威胁（后门攻击、传感器欺骗、模型中毒）对安全和运行可靠性的严峻挑战，本文提出了一个统一的智能安全与安保架构愿景。该架构集成了多模态感知、安全联邦学习、强化学习、DTN通信和能量感知传感。研究介绍了五个核心模块：矿工定位、多模态态势感知、后门攻击监控、可信联邦学习以及物联网设备健康监控。这些模块共同解决了矿工定位、危险识别、联邦鲁棒性和预测性维护等关键问题，旨在构建一个在对抗条件下仍能维持连续运行的弹性、可信智能采矿系统。|Sanjay Madria Team|[2602.11472](http://arxiv.org/abs/2602.11472)|null|
|**2026-02-12**|**Humanoid Manipulation Interface: Humanoid Whole-Body Manipulation from Robot-Free Demonstrations**|当前人形机器人全身操作方法受限于硬件物流和复杂奖励工程，导致自主技能有限且通常仅限于受控环境。为解决这些问题，本文提出了Humanoid Manipulation Interface (HuMI)，一个便携高效的框架，用于在各种环境中学习多样化的全身操作任务。HuMI通过便携硬件捕捉丰富的全身运动，实现无机器人数据收集，并利用分层学习流程将人类运动转化为灵巧且可行的人形技能。广泛实验表明，HuMI的数据收集效率比遥操作提高3倍，并在未知环境中取得了70%的成功率，有效提升了人形机器人的泛化操作能力。|Yang Gao Team|[2602.06643](http://arxiv.org/abs/2602.06643)|**[link](https://humanoid-manipulation-interface.github.io)**|
|**2026-02-11**|**ExtremControl: Low-Latency Humanoid Teleoperation with Direct Extremity Control**|现有类人机器人遥操作系统因依赖重度预处理和仅位置PD控制，导致高延迟（>200ms），严重限制了响应速度。为解决此问题，本文提出了ExtremControl低延迟全身控制框架：它直接作用于选定刚体链（主要为机器人末端）的SE(3)位姿以避免全身重定向；利用笛卡尔空间映射直接转换人体运动到机器人末端目标；并结合低层速度前馈控制以支持快速变化的控制接口下的高响应行为。理论公式化和仿真与真实实验均验证了其有效性。基于ExtremControl的遥操作系统实现了低至50ms的端到端延迟，使机器人能够执行乒乓球平衡、杂耍等高响应任务，显著超越了现有方法的200ms延迟限制。|Chuang Gan Team|[2602.11321](http://arxiv.org/abs/2602.11321)|**[link](https://owenowl.github.io/extremcontrol)**|
|**2026-02-11**|**APEX: Learning Adaptive High-Platform Traversal for Humanoid Robots**|尽管深度强化学习在类人机器人不平坦地形行走方面取得了进展，但突破腿长限制的高平台攀爬仍具挑战，现有强化学习方法常生成高冲击、扭矩受限且不安全的跳跃方案。本文提出了APEX系统，一个感知式攀爬高平台穿越方案，它组合了多种地形条件行为。核心在于引入了一个广义棘轮进度奖励机制，用于学习接触密集型、目标导向的动作，该机制提供密集但无速度的监督并结合强安全正则化。通过LiDAR感知训练全身动作策略，并通过仿真训练中建模感知伪影和部署时对高程图进行滤波与修复来缩小仿真到现实的感知差距。最终将六种技能提炼成单一策略，并在Unitree G1类人机器人上验证了其能零样本仿真到现实穿越0.8米高平台（约腿长的114%），表现出对平台高度和初始姿态的鲁棒适应以及平滑稳定的多技能转换。|Ding Zhao Team|[2602.11143](http://arxiv.org/abs/2602.11143)|**[link](https://apex-humanoid.github.io/)**|
|**2026-02-11**|**Towards Learning a Generalizable 3D Scene Representation from 2D Observations**|针对现有神经辐射场（NeRF）方法在机器人应用中因相机中心坐标系操作而难以直接应用于机器人操作的问题，本文提出了一种可泛化的神经辐射场方法。该模型直接在全局工作空间坐标系中构建占用表示，使其可以直接应用于机器人操作任务。该方法能够集成灵活的源视图，并无需场景特定的微调即可泛化到未见过的物体排列。在类人机器人上的实验验证表明，模型在40个真实场景下训练后，重建误差为26毫米（包括遮挡区域），证明了其超越传统立体视觉方法推断完整3D占用空间的能力。|Stefan Wermter Team|[2602.10943](http://arxiv.org/abs/2602.10943)|null|
|**2026-02-11**|**MOSAIC: Bridging the Sim-to-Real Gap in Generalist Humanoid Motion Tracking and Teleoperation with Rapid Residual Adaptation**|鉴于通用人形运动追踪器在模拟中表现优异，但在实际硬件持续遥操作时易受接口和动力学误差影响，本文提出了开源全栈系统MOSAIC。该系统首先通过强化学习在多源运动库上训练面向遥操作的通用运动追踪器，采用自适应重采样和强调世界坐标系运动一致性的奖励。为弥合模拟到真实世界的接口差距，MOSAIC通过快速残差适应，使用少量接口特定数据训练一个接口特定策略，并通过加性残差模块将其蒸馏到通用追踪器中，优于传统微调方法。实验结果（包括系统消融、分布外基准测试和真实机器人实验）证明，MOSAIC在实际延迟和噪声下能实现稳健的离线运动回放和在线长周期遥操作。|Alois Knoll Team|[2602.08594](http://arxiv.org/abs/2602.08594)|null|
|**2026-02-11**|**MotionWeaver: Holistic 4D-Anchored Framework for Multi-Humanoid Image Animation**|鉴于现有角色图像动画方法难以泛化到涉及多样人形形式、复杂交互和频繁遮挡的多人场景，本文提出了MotionWeaver框架。该框架引入统一的运动表示，提取与身份无关的运动并明确绑定到角色，以泛化到多样人形并扩展到多人场景。同时，提出了整体4D锚定范式，构建共享4D空间融合运动与视频潜空间，并通过分层4D级别监督强化交互和遮挡处理。为支持此研究，构建了46小时多人视频数据集和300视频基准。定量和定性实验结果表明，MotionWeaver在自建基准上达到SOTA，并能有效泛化至复杂多人场景。|Weizhan Zhang Team|[2602.13326](http://arxiv.org/abs/2602.13326)|null|
|**2026-02-10**|**EgoHumanoid: Unlocking In-the-Wild Loco-Manipulation with Robot-Free Egocentric Demonstration**|针对人形机器人动作操作对数据需求高，而现有方法未能充分利用人类演示数据，且存在人机体现差异的问题，本文提出了EgoHumanoid框架。该框架首次利用大量自我中心人类演示和少量机器人数据共同训练视觉-语言-动作策略，使人形机器人能在多样真实世界环境中执行动作操作。通过硬件设计到数据处理的系统对齐流水线，包括视图对齐和动作对齐，成功弥合了人机之间的形态和视角差异。广泛的真实世界实验表明，整合无机器人自我中心数据相比仅机器人基线性能显著提升51%，尤其是在未见过的环境中，且分析揭示了行为有效迁移及扩展人类数据的潜力。|Li Chen Team|[2602.10106](http://arxiv.org/abs/2602.10106)|**[link](https://opendrivelab.com/EgoHumanoid)**|
|**2026-02-10**|**Humanoid Factors: Design Principles for AI Humanoids in Human Worlds**|随着人形机器人开始与人类共享空间，传统人因工程需要扩展，不仅考虑人类因素，也要考虑人形机器人因素。当前人形机器人带来了人类行为、沟通和社会存在的期望，重塑了可用性、信任和安全。本文引入“人形机器人因素”框架，围绕物理、认知、社会和伦理四大支柱，指导人形机器人开发，使其能有效与人类共存和协作，并表征了人机能力间的重叠与差异。通过评估真实人形机器人控制算法，该框架揭示了传统机器人任务指标如何忽视关键人类认知和交互原则，为设计、评估和管理持续人机共存提供了基础性框架。|Lixiao Huang Team|[2602.10069](http://arxiv.org/abs/2602.10069)|null|
|**2026-02-10**|**TeleGate: Whole-Body Humanoid Teleoperation via Gated Expert Selection with Motion Prior**|针对人形机器人实时全身遥操作中，现有方法通过知识蒸馏将多专家策略整合，常导致高动态运动性能下降的挑战，本文提出了TeleGate统一遥操作框架。其核心思想是训练一个轻量级门控网络，根据本体感知状态和参考轨迹实时动态激活领域特定专家策略，从而保留其完整能力，避免知识蒸馏的性能损失。此外，引入基于VAE的运动先验模块，从历史观测中提取未来运动意图，实现预期控制。在模拟和Unitree G1机器人上的实验表明，TeleGate仅需2.5小时训练数据，即在跑步、跌倒恢复和跳跃等多样动态运动中实现了高精度实时遥操作，显著优于基线方法。|Rongyun Cao Team|[2602.09628](http://arxiv.org/abs/2602.09628)|null|
|**2026-02-09**|**Characteristics, Management, and Utilization of Muscles in Musculoskeletal Humanoids: Empirical Study on Kengoro and Musashi**|当前肌肉骨骼人形机器人研究中，对其生物仿生结构固有的多样属性及其管理利用方式缺乏统一讨论。本研究基于作者团队开发的Kengoro和Musashi机器人，将肌肉骨骼结构特征归纳为冗余性、独立性、各向异性、可变力臂和非线性弹性五大属性。文章进一步探讨了这些属性组合所带来的优势与劣势，并重点讨论了身体图式学习、反射控制、肌肉分组及身体图式适应等机制。最后，研究阐述了通过集成系统实现运动的实践，并展望了未来的研究挑战。|Masayuki Inaba Team|[2602.08518](http://arxiv.org/abs/2602.08518)|null|
|**2026-02-09**|**Learning Human-Like Badminton Skills for Humanoid Robots**|人形机器人实现羽毛球等高强度运动的类人表现面临巨大挑战，尤其是在运动学模仿与功能性、物理感知击打之间难以兼顾自然风格。为解决此问题，本文提出了Imitation-to-Interaction渐进式强化学习框架，旨在使机器人从“模仿者”进化为“击球手”。该方法通过人类数据建立运动先验，蒸馏到模型化状态表示中，并利用对抗性先验稳定动力学，同时引入流形扩展策略以应对稀疏的专家演示。实验结果显示，该框架在仿真中掌握了多样羽毛球技能，并首次实现了类人羽毛球技能从仿真到真实机器人的零样本迁移，展示了物理世界中的优雅和精准打击。|Peng Lu Team|[2602.08370](http://arxiv.org/abs/2602.08370)|null|
|**2026-02-07**|**VividFace: Real-Time and Realistic Facial Expression Shadowing for Humanoid Robots**|人形机器人面部表情实时模仿对于实现逼真、情感丰富的人机交互至关重要，但现有方法常因离线推理和细节捕捉不足而难以同时达到实时性和逼真性。为解决这些局限，本文提出了VividFace，一个实时且逼真的人形机器人面部表情阴影系统。该系统通过优化模仿框架X2CNet++，并引入特征适应训练策略，显著增强了表情表现力；同时，通过视频流兼容推理管线和基于异步I/O的工作流，实现了高效的实时模仿。广泛的真实世界演示验证了VividFace在0.05秒内模仿人类表情并生成生动人形面部的实用能力，且能泛化至多种面部配置。|Yang Zhang Team|[2602.07506](http://arxiv.org/abs/2602.07506)|null|
|**2026-02-07**|**TextOp: Real-time Interactive Text-Driven Humanoid Robot Motion Generation and Control**|现有的人形机器人全身控制器在灵活性和自主性方面存在局限，难以实现实时和交互式驱动。为解决这一问题，本文提出了TextOp，一个实时文本驱动的人形运动生成与控制框架，支持流式语言指令和即时修改。TextOp采用两级架构：高级运动扩散模型根据文本生成短时域轨迹，低级运动跟踪策略则在机器人上执行这些轨迹。广泛的真实机器人实验和离线评估表明，TextOp实现了即时响应、平滑全身运动和精确控制，在舞蹈、跳跃等复杂行为中展现出自由形式的意图表达和流畅过渡。|Xuelong Li Team|[2602.07439](http://arxiv.org/abs/2602.07439)|**[link](https://text-op.github.io/)**|
|**2026-02-07**|**Bridging Speech, Emotion, and Motion: a VLM-based Multimodal Edge-deployable Framework for Humanoid Robots**|为解决多数人形机器人缺乏协调的语音、面部表情和手势，以及在设备上自主运行的需求，本文提出了SeM²，一个基于视觉语言模型的框架。SeM²通过多模态感知模块捕捉用户上下文，结合思维链推理规划响应，并利用语义序列对齐机制确保言语内容与物理表达的精确时间协调，从而实现情感一致的多模态交互。研究实现了云端及边缘部署版本，其中边缘版本通过知识蒸馏高效运行。综合评估显示，SeM²在自然度、情感清晰度和模态一致性方面显著优于单模态基线，推动了社交表达性人形机器人在多样现实环境中的应用。|Miao Li Team|[2602.07434](http://arxiv.org/abs/2602.07434)|null|
|**2026-02-06**|**Cerebellar-Inspired Residual Control for Fault Recovery: From Inference-Time Adaptation to Structural Consolidation**|针对机器人策略在真实世界部署中常遇到的训练后故障，且不便重新训练的问题，本文提出了一种推理时、受小脑启发的残差控制框架。该框架通过在线校正动作增强冻结的强化学习策略，无需修改基础策略参数即可实现故障恢复。它实例化了小脑核心原理，如高维模式分离、并行残差路径和局部误差驱动可塑性，并通过保守的元适应调节残差权限。实验结果表明，在MuJoCo基准测试中，该框架在执行器、动力学和环境扰动下，对HalfCheetah-v5和Humanoid-v5在适度故障下性能显著提升，并在严重故障下表现出优雅的性能下降。|Amit Ranjan Trivedi Team|[2602.07227](http://arxiv.org/abs/2602.07227)|null|
|**2026-02-06**|**DynaRetarget: Dynamically-Feasible Retargeting using Sampling-Based Trajectory Optimization**|将人类运动重定向到人形机器人控制策略并确保其动态可行性是一项挑战。本文介绍了DynaRetarget，一个将人类运动重定向到人形控制策略的完整流程。其核心是新颖的基于采样的轨迹优化（SBTO）框架，该框架能将不完善的运动学轨迹细化为动态可行的运动，并通过逐步推进优化范围来处理长时域任务。DynaRetarget在重定向数百个人形-物体演示中取得了比现有技术更高的成功率，并能泛化到不同物体属性的场景，为生成大规模人形局部操作轨迹合成数据集提供了可能。|Majid Khadiv Team|[2602.06827](http://arxiv.org/abs/2602.06827)|null|
|**2026-02-06**|**ECO: Energy-Constrained Optimization with Reinforcement Learning for Humanoid Walking**|为解决仿人机器人节能稳定运动中现有方法需大量超参数调优且易导致次优策略的问题，本研究提出了ECO（Energy-Constrained Optimization）约束强化学习框架。该框架将能量指标明确定义为不等式约束，通过拉格朗日法强制执行能量消耗和参考运动约束，从而实现稳定、对称且节能的行走。ECO在仿真和真实机器人（BRUCE）上的实验结果表明，在保持鲁棒行走性能的同时，其能耗显著低于MPC、标准RL及其他SOTA约束RL方法，实现了仿人机器人节能运动的实质性进步。|Yao Su Team|[2602.06445](http://arxiv.org/abs/2602.06445)|null|
|**2026-02-06**|**Now You See That: Learning End-to-End Humanoid Locomotion from Raw Pixels**|为解决基于视觉的仿人机器人鲁棒运动中模拟到真实差距产生的感知噪声和多样地形下学习目标冲突的挑战，本研究提出了一个端到端的视觉驱动运动框架。为实现鲁棒的sim-to-real迁移，该框架开发了高保真深度传感器模拟，并提出了视觉感知行为蒸馏方法。为适应多样地形，引入了地形特定奖励塑形，并结合多评论家和多判别器学习。在配备不同立体深度摄像头的两个仿人机器人平台上，该策略展现出在多样环境中的鲁棒性能，能处理极端挑战和精细任务，包括双向长期楼梯遍历。|Zongwu Xie Team|[2602.06382](http://arxiv.org/abs/2602.06382)|null|

<p align=right>(<a href=#updated-on-20260222>back to top</a>)</p>
