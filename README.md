## Updated on 2026.02.13

## Categories

- [Manipulation](#manipulation)
- [World Model](#world-model)
- [VLM](#vlm)
- [VLA](#vla)
- [Humanoid](#humanoid)

## Manipulation

|Publish Date|Title|Chinese Summary|Authors|PDF|Code|
|---|---|---|---|---|---|
|**2026-02-12**|**FAIL: Flow Matching Adversarial Imitation Learning for Image Generation**||Weidi Xie Team|[2602.12155](http://arxiv.org/abs/2602.12155)|null|
|**2026-02-12**|**GigaBrain-0.5M*: a VLA That Learns From World Model-Based Reinforcement Learning**||Zheng Zhu Team|[2602.12099](http://arxiv.org/abs/2602.12099)|**[link](https://gigabrain05m.github.io/)**|
|**2026-02-12**|**Affordance-Graphed Task Worlds: Self-Evolving Task Generation for Scalable Embodied Learning**||Changshui Zhang Team|[2602.12065](http://arxiv.org/abs/2602.12065)|null|
|**2026-02-12**|**HoloBrain-0 Technical Report**||Zhizhong Su Team|[2602.12062](http://arxiv.org/abs/2602.12062)|null|
|**2026-02-12**|**When would Vision-Proprioception Policies Fail in Robotic Manipulation?**||Di Hu Team|[2602.12032](http://arxiv.org/abs/2602.12032)|null|
|**2026-02-12**|**Accelerating Robotic Reinforcement Learning with Agent Guidance**||Yaodong Yang Team|[2602.11978](http://arxiv.org/abs/2602.11978)|null|
|**2026-02-12**|**Robot-DIFT: Distilling Diffusion Features for Geometrically Consistent Visuomotor Control**||Georgia Chalvatzaki Team|[2602.11934](http://arxiv.org/abs/2602.11934)|null|
|**2026-02-12**|**JEPA-VLA: Video Predictive Embedding is Needed for VLA Models**||Mingsheng Long Team|[2602.11832](http://arxiv.org/abs/2602.11832)|null|
|**2026-02-12**|**Clutt3R-Seg: Sparse-view 3D Instance Segmentation for Language-grounded Grasping in Cluttered Scenes**||Ayoung Kim Team|[2602.11660](http://arxiv.org/abs/2602.11660)|null|
|**2026-02-12**|**ViTaS: Visual Tactile Soft Fusion Contrastive Learning for Visuomotor Learning**||Huazhe Xu Team|[2602.11643](http://arxiv.org/abs/2602.11643)|null|
|**2026-02-12**|**EasyMimic: A Low-Cost Framework for Robot Imitation Learning from Human Videos**||Qin Jin Team|[2602.11464](http://arxiv.org/abs/2602.11464)|null|
|**2026-02-12**|**Scaling World Model for Hierarchical Manipulation Policies**||Xinghang Li Team|[2602.10983](http://arxiv.org/abs/2602.10983)|null|
|**2026-02-11**|**Human Preference Modeling Using Visual Motion Prediction Improves Robot Skill Learning from Egocentric Human Video**||Christopher G. Atkeson Team|[2602.11393](http://arxiv.org/abs/2602.11393)|null|
|**2026-02-11**|**MolmoSpaces: A Large-Scale Open Ecosystem for Robot Navigation and Manipulation**||Ranjay Krishna Team|[2602.11337](http://arxiv.org/abs/2602.11337)|null|
|**2026-02-11**|**YOR: Your Own Mobile Manipulator for Generalizable Robotics**||Zichen Jeff Cui Team|[2602.11150](http://arxiv.org/abs/2602.11150)|null|
|**2026-02-11**|**ABot-M0: VLA Foundation Model for Robotic Manipulation with Action Manifold Learning**||Mu Xu Team|[2602.11236](http://arxiv.org/abs/2602.11236)|**[link](https://amap-cvlab.github.io/ABot-Manipulation/)**|
|**2026-02-11**|**OSIL: Learning Offline Safe Imitation Policies with Safety Inferred from Non-preferred Trajectories**||Balaraman Ravindran Team|[2602.11018](http://arxiv.org/abs/2602.11018)|null|
|**2026-02-11**|**Towards Learning a Generalizable 3D Scene Representation from 2D Observations**||Stefan Wermter Team|[2602.10943](http://arxiv.org/abs/2602.10943)|null|
|**2026-02-11**|**Semi-Supervised Cross-Domain Imitation Learning**||Ping-Chun Hsieh Team|[2602.10793](http://arxiv.org/abs/2602.10793)|null|
|**2026-02-11**|**Say, Dream, and Act: Learning Video World Models for Instruction-Driven Robot Manipulation**||Yanwei Fu Team|[2602.10717](http://arxiv.org/abs/2602.10717)|null|

<p align=right>(<a href=#updated-on-20260213>back to top</a>)</p>

## World Model

|Publish Date|Title|Chinese Summary|Authors|PDF|Code|
|---|---|---|---|---|---|
|**2026-02-12**|**The Observer Effect in World Models: Invasive Adaptation Corrupts Latent Physics**||Barbara Hammer Team|[2602.12218](http://arxiv.org/abs/2602.12218)|null|
|**2026-02-12**|**LDA-1B: Scaling Latent Dynamics Action Model via Universal Embodied Data Ingestion**||He Wang Team|[2602.12215](http://arxiv.org/abs/2602.12215)|**[link](https://pku-epic.github.io/LDA)**|
|**2026-02-12**|**DreamID-Omni: Unified Framework for Controllable Human-Centric Audio-Video Generation**||Xiangwang Hou Team|[2602.12160](http://arxiv.org/abs/2602.12160)|**[link](https://guoxu1233.github.io/DreamID-Omni/)**|
|**2026-02-12**|**It's TIME: Towards the Next Generation of Time Series Forecasting Benchmarks**||Chenghao Liu Team|[2602.12147](http://arxiv.org/abs/2602.12147)|null|
|**2026-02-12**|**Commencing-Student Enrolment Forecasting Under Data Sparsity with Time Series Foundation Models**||Surangika Ranathunga Team|[2602.12120](http://arxiv.org/abs/2602.12120)|null|
|**2026-02-12**|**The Pensieve Paradigm: Stateful Language Models Mastering Their Own Context**||Yan Wang Team|[2602.12108](http://arxiv.org/abs/2602.12108)|null|
|**2026-02-12**|**GigaBrain-0.5M*: a VLA That Learns From World Model-Based Reinforcement Learning**||Zheng Zhu Team|[2602.12099](http://arxiv.org/abs/2602.12099)|**[link](https://gigabrain05m.github.io/)**|
|**2026-02-12**|**Affordance-Graphed Task Worlds: Self-Evolving Task Generation for Scalable Embodied Learning**||Changshui Zhang Team|[2602.12065](http://arxiv.org/abs/2602.12065)|null|
|**2026-02-12**|**VLAW: Iterative Co-Improvement of Vision-Language-Action Policy and World Model**||Chelsea Finn Team|[2602.12063](http://arxiv.org/abs/2602.12063)|null|
|**2026-02-12**|**HoloBrain-0 Technical Report**||Zhizhong Su Team|[2602.12062](http://arxiv.org/abs/2602.12062)|null|
|**2026-02-12**|**FedGRPO: Privately Optimizing Foundation Models with Group-Relative Rewards from Domain Client**||Yuxing Han Team|[2602.12014](http://arxiv.org/abs/2602.12014)|null|
|**2026-02-12**|**Accelerating Robotic Reinforcement Learning with Agent Guidance**||Yaodong Yang Team|[2602.11978](http://arxiv.org/abs/2602.11978)|null|
|**2026-02-12**|**Where Bits Matter in World Model Planning: A Paired Mixed-Bit Study for Efficient Spatial Reasoning**||Vaishak Menon Team|[2602.11882](http://arxiv.org/abs/2602.11882)|null|
|**2026-02-12**|**PuYun-LDM: A Latent Diffusion Model for High-Resolution Ensemble Weather Forecasts**||Bin Wang Team|[2602.11807](http://arxiv.org/abs/2602.11807)|null|
|**2026-02-12**|**HAIC: Humanoid Agile Object Interaction Control via Dynamics-Aware World Model**||Renjing Xu Team|[2602.11758](http://arxiv.org/abs/2602.11758)|**[link](https://haic-humanoid.github.io/)**|
|**2026-02-12**|**ABot-N0: Technical Report on the VLA Foundation Model for Versatile Embodied Navigation**||Mu Xu Team|[2602.11598](http://arxiv.org/abs/2602.11598)|**[link](https://amap-cvlab.github.io/ABot-Navigation/ABot-N0/)**|
|**2026-02-12**|**Brain4FMs: A Benchmark of Foundation Models for Electrical Brain Signal**||Yang Yang Team|[2602.11558](http://arxiv.org/abs/2602.11558)|null|
|**2026-02-12**|**TS-Memory: Plug-and-Play Memory for Time Series Foundation Models**||Yuxuan Liang Team|[2602.11550](http://arxiv.org/abs/2602.11550)|null|
|**2026-02-12**|**Budget-Constrained Agentic Large Language Models: Intention-Based Planning for Costly Tool Use**||Qi Qi Team|[2602.11541](http://arxiv.org/abs/2602.11541)|null|
|**2026-02-12**|**Vascular anatomy-aware self-supervised pre-training for X-ray angiogram analysis**||Zeng-Guang Hou Team|[2602.11536](http://arxiv.org/abs/2602.11536)|null|

<p align=right>(<a href=#updated-on-20260213>back to top</a>)</p>

## VLM

|Publish Date|Title|Chinese Summary|Authors|PDF|Code|
|---|---|---|---|---|---|
|**2026-02-12**|**Scaling Verification Can Be More Effective than Scaling Policy Learning for Vision-Language-Action Alignment**||Marco Pavone Team|[2602.12281](http://arxiv.org/abs/2602.12281)|null|
|**2026-02-12**|**ExStrucTiny: A Benchmark for Schema-Variable Structured Information Extraction from Document Images**||Manuela Veloso Team|[2602.12203](http://arxiv.org/abs/2602.12203)|null|
|**2026-02-12**|**3DGSNav: Enhancing Vision-Language Model Reasoning for Object Navigation via Active 3D Gaussian Splatting**||Xinyi Yu Team|[2602.12159](http://arxiv.org/abs/2602.12159)|null|
|**2026-02-12**|**Affordance-Graphed Task Worlds: Self-Evolving Task Generation for Scalable Embodied Learning**||Changshui Zhang Team|[2602.12065](http://arxiv.org/abs/2602.12065)|null|
|**2026-02-12**|**Can Local Vision-Language Models improve Activity Recognition over Vision Transformers? -- Case Study on Newborn Resuscitation**||Øyvind Meinich-Bache Team|[2602.12002](http://arxiv.org/abs/2602.12002)|null|
|**2026-02-12**|**Benchmarking Vision-Language Models for French PDF-to-Markdown Conversion**||Nicolas Mery Team|[2602.11960](http://arxiv.org/abs/2602.11960)|null|
|**2026-02-12**|**Are Two LLMs Better Than One? A Student-Teacher Dual-Head LLMs Architecture for Pharmaceutical Content Optimization**||Anubhav Girdhar Team|[2602.11957](http://arxiv.org/abs/2602.11957)|null|
|**2026-02-12**|**LAMP: Implicit Language Map for Robot Navigation**||Sunwook Choi Team|[2602.11862](http://arxiv.org/abs/2602.11862)|**[link](https://lab-of-ai-and-robotics.github.io/LAMP/)**|
|**2026-02-12**|**JEPA-VLA: Video Predictive Embedding is Needed for VLA Models**||Mingsheng Long Team|[2602.11832](http://arxiv.org/abs/2602.11832)|null|
|**2026-02-12**|**Revis: Sparse Latent Steering to Mitigate Object Hallucination in Large Vision-Language Models**||Zhou Yang Team|[2602.11824](http://arxiv.org/abs/2602.11824)|null|
|**2026-02-12**|**Adaptive Debiasing Tsallis Entropy for Test-Time Adaptation**||Jianfeng Lu Team|[2602.11743](http://arxiv.org/abs/2602.11743)|null|
|**2026-02-12**|**Adapting Vision-Language Models for E-commerce Understanding at Scale**||Shahram Khadivi Team|[2602.11733](http://arxiv.org/abs/2602.11733)|null|
|**2026-02-12**|**STVG-R1: Incentivizing Instance-Level Reasoning and Grounding in Videos via Reinforcement Learning**||Qing Li Team|[2602.11730](http://arxiv.org/abs/2602.11730)|null|
|**2026-02-12**|**ScalSelect: Scalable Training-Free Multimodal Data Selection for Efficient Visual Instruction Tuning**||Kai Chen Team|[2602.11636](http://arxiv.org/abs/2602.11636)|**[link](https://github.com/ChangtiWu/ScalSelect}{ScalSelect})**|
|**2026-02-12**|**SkillRater: Untangling Capabilities in Multimodal Data**||Akshat Shrivastava Team|[2602.11615](http://arxiv.org/abs/2602.11615)|null|
|**2026-02-12**|**Chatting with Images for Introspective Visual Thinking**||Tieniu Tan Team|[2602.11073](http://arxiv.org/abs/2602.11073)|null|
|**2026-02-11**|**Hierarchical Concept Embedding & Pursuit for Interpretable Image Classification**||René Vidal Team|[2602.11448](http://arxiv.org/abs/2602.11448)|null|
|**2026-02-11**|**Beyond VLM-Based Rewards: Diffusion-Native Latent Reward Modeling**||Wenhan Luo Team|[2602.11146](http://arxiv.org/abs/2602.11146)|**[link](https://github.com/HKUST-C4G/diffusion-rm)**|
|**2026-02-11**|**Active Zero: Self-Evolving Vision-Language Models through Active Environment Exploration**||Tat-Seng Chua Team|[2602.11241](http://arxiv.org/abs/2602.11241)|null|
|**2026-02-11**|**Safe mobility support system using crowd mapping and avoidance route planning using VLM**||Koichi Ozaki Team|[2602.10910](http://arxiv.org/abs/2602.10910)|null|

<p align=right>(<a href=#updated-on-20260213>back to top</a>)</p>

## VLA

|Publish Date|Title|Chinese Summary|Authors|PDF|Code|
|---|---|---|---|---|---|
|**2026-02-12**|**Scaling Verification Can Be More Effective than Scaling Policy Learning for Vision-Language-Action Alignment**||Marco Pavone Team|[2602.12281](http://arxiv.org/abs/2602.12281)|null|
|**2026-02-12**|**GigaBrain-0.5M*: a VLA That Learns From World Model-Based Reinforcement Learning**||Zheng Zhu Team|[2602.12099](http://arxiv.org/abs/2602.12099)|**[link](https://gigabrain05m.github.io/)**|
|**2026-02-12**|**VLAW: Iterative Co-Improvement of Vision-Language-Action Policy and World Model**||Chelsea Finn Team|[2602.12063](http://arxiv.org/abs/2602.12063)|null|
|**2026-02-12**|**HoloBrain-0 Technical Report**||Zhizhong Su Team|[2602.12062](http://arxiv.org/abs/2602.12062)|null|
|**2026-02-12**|**When would Vision-Proprioception Policies Fail in Robotic Manipulation?**||Di Hu Team|[2602.12032](http://arxiv.org/abs/2602.12032)|null|
|**2026-02-12**|**JEPA-VLA: Video Predictive Embedding is Needed for VLA Models**||Mingsheng Long Team|[2602.11832](http://arxiv.org/abs/2602.11832)|null|
|**2026-02-12**|**ABot-N0: Technical Report on the VLA Foundation Model for Versatile Embodied Navigation**||Mu Xu Team|[2602.11598](http://arxiv.org/abs/2602.11598)|**[link](https://amap-cvlab.github.io/ABot-Navigation/ABot-N0/)**|
|**2026-02-12**|**Scaling World Model for Hierarchical Manipulation Policies**||Xinghang Li Team|[2602.10983](http://arxiv.org/abs/2602.10983)|null|
|**2026-02-11**|**H-WM: Robotic Task and Motion Planning Guided by Hierarchical World Model**||Yingxue Zhang Team|[2602.11291](http://arxiv.org/abs/2602.11291)|null|
|**2026-02-11**|**RISE: Self-Improving Robot Policy with Compositional World Model**||Hongyang Li Team|[2602.11075](http://arxiv.org/abs/2602.11075)|**[link](https://opendrivelab.com/kai0-rl/)**|
|**2026-02-11**|**RADAR: Benchmarking Vision-Language-Action Generalization via Real-World Dynamics, Spatial-Physical Intelligence, and Autonomous Evaluation**||Guangrun Wang Team|[2602.10980](http://arxiv.org/abs/2602.10980)|null|
|**2026-02-11**|**From Representational Complementarity to Dual Systems: Synergizing VLM and Vision-Only Backbones for End-to-End Driving**||Yan Wang Team|[2602.10719](http://arxiv.org/abs/2602.10719)|null|
|**2026-02-11**|**Say, Dream, and Act: Learning Video World Models for Instruction-Driven Robot Manipulation**||Yanwei Fu Team|[2602.10717](http://arxiv.org/abs/2602.10717)|null|
|**2026-02-11**|**AugVLA-3D: Depth-Driven Feature Augmentation for Vision-Language-Action Models**||F. Richard Yu Team|[2602.10698](http://arxiv.org/abs/2602.10698)|null|
|**2026-02-11**|**Improving Medical Visual Reinforcement Fine-Tuning via Perception and Reasoning Augmentation**||Qicheng Lao Team|[2602.10619](http://arxiv.org/abs/2602.10619)|null|
|**2026-02-11**|**LAP: Language-Action Pre-Training Enables Zero-shot Cross-Embodiment Transfer**||Anirudha Majumdar Team|[2602.10556](http://arxiv.org/abs/2602.10556)|**[link](https://lap-vla.github.io)**|
|**2026-02-11**|**Found-RL: foundation model-enhanced reinforcement learning for autonomous driving**||Sikai Chen Team|[2602.10458](http://arxiv.org/abs/2602.10458)|null|
|**2026-02-10**|**Hardware Co-Design Scaling Laws via Roofline Modelling for On-Device LLMs**||Cheng Deng Team|[2602.10377](http://arxiv.org/abs/2602.10377)|null|
|**2026-02-10**|**ST4VLA: Spatially Guided Training for Vision-Language-Action Models**||Jiangmiao Pang Team|[2602.10109](http://arxiv.org/abs/2602.10109)|null|
|**2026-02-10**|**EgoHumanoid: Unlocking In-the-Wild Loco-Manipulation with Robot-Free Egocentric Demonstration**||Li Chen Team|[2602.10106](http://arxiv.org/abs/2602.10106)|**[link](https://opendrivelab.com/EgoHumanoid)**|

<p align=right>(<a href=#updated-on-20260213>back to top</a>)</p>

## Humanoid

|Publish Date|Title|Chinese Summary|Authors|PDF|Code|
|---|---|---|---|---|---|
|**2026-02-12**|**General Humanoid Whole-Body Control via Pretraining and Fast Adaptation**||Zongqing Lu Team|[2602.11929](http://arxiv.org/abs/2602.11929)|null|
|**2026-02-12**|**HAIC: Humanoid Agile Object Interaction Control via Dynamics-Aware World Model**||Renjing Xu Team|[2602.11758](http://arxiv.org/abs/2602.11758)|**[link](https://haic-humanoid.github.io/)**|
|**2026-02-12**|**Future Mining: Learning for Safety and Security**||Sanjay Madria Team|[2602.11472](http://arxiv.org/abs/2602.11472)|null|
|**2026-02-12**|**Humanoid Manipulation Interface: Humanoid Whole-Body Manipulation from Robot-Free Demonstrations**||Yang Gao Team|[2602.06643](http://arxiv.org/abs/2602.06643)|**[link](https://humanoid-manipulation-interface.github.io)**|
|**2026-02-11**|**ExtremControl: Low-Latency Humanoid Teleoperation with Direct Extremity Control**||Chuang Gan Team|[2602.11321](http://arxiv.org/abs/2602.11321)|**[link](https://owenowl.github.io/extremcontrol)**|
|**2026-02-11**|**APEX: Learning Adaptive High-Platform Traversal for Humanoid Robots**||Ding Zhao Team|[2602.11143](http://arxiv.org/abs/2602.11143)|**[link](https://apex-humanoid.github.io/)**|
|**2026-02-11**|**Towards Learning a Generalizable 3D Scene Representation from 2D Observations**||Stefan Wermter Team|[2602.10943](http://arxiv.org/abs/2602.10943)|null|
|**2026-02-11**|**MOSAIC: Bridging the Sim-to-Real Gap in Generalist Humanoid Motion Tracking and Teleoperation with Rapid Residual Adaptation**||Alois Knoll Team|[2602.08594](http://arxiv.org/abs/2602.08594)|null|
|**2026-02-10**|**EgoHumanoid: Unlocking In-the-Wild Loco-Manipulation with Robot-Free Egocentric Demonstration**||Li Chen Team|[2602.10106](http://arxiv.org/abs/2602.10106)|**[link](https://opendrivelab.com/EgoHumanoid)**|
|**2026-02-10**|**Humanoid Factors: Design Principles for AI Humanoids in Human Worlds**||Lixiao Huang Team|[2602.10069](http://arxiv.org/abs/2602.10069)|null|
|**2026-02-10**|**TeleGate: Whole-Body Humanoid Teleoperation via Gated Expert Selection with Motion Prior**||Rongyun Cao Team|[2602.09628](http://arxiv.org/abs/2602.09628)|null|
|**2026-02-09**|**Characteristics, Management, and Utilization of Muscles in Musculoskeletal Humanoids: Empirical Study on Kengoro and Musashi**||Masayuki Inaba Team|[2602.08518](http://arxiv.org/abs/2602.08518)|null|
|**2026-02-09**|**Learning Human-Like Badminton Skills for Humanoid Robots**||Peng Lu Team|[2602.08370](http://arxiv.org/abs/2602.08370)|null|
|**2026-02-07**|**VividFace: Real-Time and Realistic Facial Expression Shadowing for Humanoid Robots**||Yang Zhang Team|[2602.07506](http://arxiv.org/abs/2602.07506)|null|
|**2026-02-07**|**TextOp: Real-time Interactive Text-Driven Humanoid Robot Motion Generation and Control**||Xuelong Li Team|[2602.07439](http://arxiv.org/abs/2602.07439)|**[link](https://text-op.github.io/)**|
|**2026-02-07**|**Bridging Speech, Emotion, and Motion: a VLM-based Multimodal Edge-deployable Framework for Humanoid Robots**||Miao Li Team|[2602.07434](http://arxiv.org/abs/2602.07434)|null|
|**2026-02-06**|**Cerebellar-Inspired Residual Control for Fault Recovery: From Inference-Time Adaptation to Structural Consolidation**||Amit Ranjan Trivedi Team|[2602.07227](http://arxiv.org/abs/2602.07227)|null|
|**2026-02-06**|**DynaRetarget: Dynamically-Feasible Retargeting using Sampling-Based Trajectory Optimization**||Majid Khadiv Team|[2602.06827](http://arxiv.org/abs/2602.06827)|null|
|**2026-02-06**|**ECO: Energy-Constrained Optimization with Reinforcement Learning for Humanoid Walking**||Yao Su Team|[2602.06445](http://arxiv.org/abs/2602.06445)|null|
|**2026-02-06**|**Now You See That: Learning End-to-End Humanoid Locomotion from Raw Pixels**||Zongwu Xie Team|[2602.06382](http://arxiv.org/abs/2602.06382)|null|

<p align=right>(<a href=#updated-on-20260213>back to top</a>)</p>
