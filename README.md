## Updated on 2026.02.16

## Categories

- [Manipulation](#manipulation)
- [World Model](#world-model)
- [VLM](#vlm)
- [VLA](#vla)
- [Humanoid](#humanoid)

## Manipulation

|Publish Date|Title|Chinese Summary|Authors|PDF|Code|
|---|---|---|---|---|---|
|**2026-02-13**|**Imitating What Works: Simulation-Filtered Modular Policy Learning from Human Videos**|为解决机器人通过人类视频学习操作技能时，预抓取行为学习不足以及现有抓取生成器生成的抓取与任务不兼容的问题，本文提出了Perceive-Simulate-Imitate (PSI) 框架。该框架通过在模拟中对人类视频运动数据进行抓取轨迹过滤，并为轨迹数据添加抓取适用性标签，从而实现任务导向型抓取能力的监督学习。真实世界实验证明，PSI框架无需机器人数据即可高效学习精确操作技能，且性能显著优于简单使用抓取生成器的方法。|Wei-Chiu Ma Team|[2602.13197](http://arxiv.org/abs/2602.13197)|null|
|**2026-02-13**|**UniManip: General-Purpose Zero-Shot Robotic Manipulation with Agentic Operational Graph**|针对现有机器人操作方法在零样本泛化方面的不足，即端到端VLA模型缺乏精度而传统规划器语义刚性问题，本文提出了UniManip框架。该框架基于双层Agentic Operational Graph (AOG)，通过高层Agentic层进行任务编排和低层Scene层表示动态状态，实现语义推理与物理接地的统一，并以动态智能体循环方式主动实例化场景图、规划无碰撞轨迹并自主恢复失败。实验结果表明，UniManip在未见对象和任务上展现出鲁棒的零样本能力，成功率显著高于现有VLA和分层基线，且支持从固定基座到移动操作的零样本迁移。|Ziwei Wang Team|[2602.13086](http://arxiv.org/abs/2602.13086)|**[link](https://henryhcliu.github.io/unimanip)**|
|**2026-02-13**|**How Swarms Differ: Challenges in Collective Behaviour Comparison**|针对群体行为分析中数值特征集通常缺乏通用性且难以定量衡量行为相似性的问题，本研究深入探讨了特征集对集体行为的影响。我们从现有群体机器人学工作中筛选出特征集和相似性度量，并评估了它们在特定行为背景外的鲁棒性。研究发现，特征集和相似性度量的相互作用决定了区分相似行为群体的有效性，并提出了一种基于自组织图的方法来识别特征空间中行为难以区分的区域。|Jonas Kuckling Team|[2602.13016](http://arxiv.org/abs/2602.13016)|null|
|**2026-02-13**|**SafeFlowMPC: Predictive and Safe Trajectory Planning for Robot Manipulators with Learning-based Policies**|面对机器人日益融入日常生活对灵活性和实时反应能力的需求，以及学习方法缺乏安全保证和优化方法泛化能力不足的挑战，本文提出了SafeFlowMPC框架。该框架结合了流匹配与在线优化，旨在融合学习和优化方法的优势，并通过次优模型预测控制公式，实时确保操作安全性。在KUKA 7自由度机械臂上的真实世界实验（包括抓取和人机交接任务）中，SafeFlowMPC展现了强大的性能。|Andreas Kugi Team|[2602.12794](http://arxiv.org/abs/2602.12794)|null|
|**2026-02-13**|**Scaling Single Human Demonstrations for Imitation Learning using Generative Foundational Models**|鉴于模仿学习中收集机器人演示数据的困难以及人类演示到机器人转移的挑战，本文提出了Real2Gen框架，仅通过单个人类演示来训练操作策略。Real2Gen从人类演示中提取关键信息并传输到模拟环境，利用可编程专家智能体生成无限量的训练数据来学习流匹配策略。实验结果表明，Real2Gen平均成功率提高了26.6%，并且由于训练数据的丰富性，训练出的策略具有更强的泛化能力，实现了纯模拟训练策略的零样本真实世界部署。|Abhinav Valada Team|[2602.12734](http://arxiv.org/abs/2602.12734)|null|
|**2026-02-13**|**$\mathcal{X}$-KD: General Experiential Knowledge Distillation for Large Language Models**|针对大语言模型知识蒸馏（KD）中，现有方法常忽视教师模型原始学习环境的问题，本文提出了Experiential Knowledge Distillation ($\mathcal{X}$-KD) 框架。受经验学习理论和逆强化学习启发，$\mathcal{X}$-KD采用Approximated Variational Reward Imitation Learning (AVRIL) 框架，联合建模教师的原始奖励函数并执行策略蒸馏，使学生模型能在教师的原始学习环境中学习。实验证明，$\mathcal{X}$ -KD在抽象摘要、机器翻译和算术推理任务上均优于基线方法，并实现了更好的性能-多样性权衡和数据效率。|Yuyu Yuan Team|[2602.12674](http://arxiv.org/abs/2602.12674)|null|
|**2026-02-13**|**PMG: Parameterized Motion Generator for Human-like Locomotion Control**|为解决人形机器人运动中，现有全身体参考引导方法对高级命令接口适应性差、对数据和校准敏感等实际挑战，本文提出了Parameterized Motion Generator (PMG)。PMG是一种基于人类运动结构分析的实时运动生成器，通过紧凑的参数化运动数据和高维控制命令合成参考轨迹，并结合模仿学习流水线和仿真到现实电机参数识别模块。实验证明，该集成系统能生成自然、类人运动，精确响应高维控制输入（如VR远程操作），并实现高效、可验证的仿真到现实迁移。|Houde Liu Team|[2602.12656](http://arxiv.org/abs/2602.12656)|null|
|**2026-02-13**|**Real-to-Sim for Highly Cluttered Environments via Physics-Consistent Inter-Object Reasoning**|为解决从单视图观测重建物理有效3D场景时，现有方法常忽略物理约束导致无效状态，进而影响下游模拟可靠性的问题，本文提出了一种新颖的物理约束Real-to-Sim管道。该管道能够从单视图RGB-D数据重建物理一致的3D场景，其核心是一个可微分优化管道，通过接触图建模空间依赖，并利用可微分刚体模拟联合优化物体姿态和物理属性。实验结果表明，重建场景具有高物理保真度，能忠实复现真实世界接触动力学，从而实现稳定可靠的接触密集型操作。|Jun Ma Team|[2602.12633](http://arxiv.org/abs/2602.12633)|**[link](https://physics-constrained-real2sim.github.io)**|
|**2026-02-12**|**FAIL: Flow Matching Adversarial Imitation Learning for Image Generation**|鉴于流匹配模型后训练与模仿学习的数学等同性，以及监督微调无法纠正策略漂移而偏好优化成本高昂的问题，本文提出了Flow Matching Adversarial Imitation Learning (FAIL) 框架。该框架通过对抗训练最小化策略与专家之间的散度，无需明确奖励或成对比较，并推导出了FAIL-PD和FAIL-PG两种算法。实验证明，FAIL在仅使用少量演示数据的情况下，能在提示遵循和美学基准上取得竞争性性能，并能有效泛化至离散图像和视频生成，同时作为鲁棒正则化器减轻奖励欺骗。|Weidi Xie Team|[2602.12155](http://arxiv.org/abs/2602.12155)|null|
|**2026-02-12**|**GigaBrain-0.5M*: a VLA That Learns From World Model-Based Reinforcement Learning**|针对传统VLA模型在场景理解和未来预测上的局限性，本研究提出了GigaBrain-0.5M*，一个基于世界模型强化学习的VLA模型。该模型在预训练的GigaBrain-0.5基础上，通过RAMP（Reinforcement leArning via world Model-conditioned Policy）进一步整合了世界模型强化学习，以实现鲁棒的跨任务适应性。实验结果表明，RAMP在洗衣折叠、箱子包装和意式浓缩咖啡制作等复杂任务中，相较于RECAP基线性能提升了约30%，并且在实际部署中展示了可靠的长期执行能力，能够无故障完成复杂操作任务。|Zheng Zhu Team|[2602.12099](http://arxiv.org/abs/2602.12099)|**[link](https://gigabrain05m.github.io/)**|
|**2026-02-12**|**Affordance-Graphed Task Worlds: Self-Evolving Task Generation for Scalable Embodied Learning**|鉴于在真实世界中训练机器人策略成本高昂且难以扩展，而现有生成模拟方法难以生成逻辑连贯的长时任务并处理动态物理不确定性，本研究提出了Affordance-Graphed Task Worlds (AGT-World) 框架。该框架能够根据真实世界观测自主构建交互式模拟环境和相应的机器人任务策略，通过将任务空间形式化为结构化图实现复杂目标的精确分层分解，并引入结合视觉-语言模型推理和几何验证的混合反馈自进化机制来完善策略。广泛实验证明，AGT-World在成功率和泛化能力上显著优于现有方法，实现了提议、执行和修正的自改进循环，以支持可扩展的机器人学习。|Changshui Zhang Team|[2602.12065](http://arxiv.org/abs/2602.12065)|null|
|**2026-02-12**|**HoloBrain-0 Technical Report**|为了弥合基础模型研究与可靠的真实世界机器人部署之间的差距，本研究引入了HoloBrain-0，一个全面的视觉-语言-动作 (VLA) 框架。该框架的核心是一个新颖的VLA架构，明确融入了机器人本体先验信息（如多视图相机参数和运动学描述），以增强3D空间推理并支持多样化的本体。通过“预训练-后训练”范式进行验证，该系统在RoboTwin 2.0、LIBERO和GenieSim等模拟基准测试中取得了最先进的成果，并在长时程真实世界操作任务中表现出色。值得注意的是，其高效的0.2B参数变体能与大得多的基线媲美，并支持低延迟的设备部署。为加速研究和实际应用，HoloBrain生态系统已完全开源。|Zhizhong Su Team|[2602.12062](http://arxiv.org/abs/2602.12062)|null|
|**2026-02-12**|**When would Vision-Proprioception Policies Fail in Robotic Manipulation?**|针对视觉-本体感受策略在复杂任务中泛化能力不一致的问题，本研究发现，在机器人运动转换的子阶段，视觉模态的作用有限，策略倾向于更简洁的本体感受信号，抑制了视觉学习。为此，我们提出了梯度调整与阶段引导 (GAP) 算法，通过利用本体感受估计运动转换阶段的概率，并据此自适应地调节本体感受梯度的幅度，从而实现视觉与本体感受的动态协作。综合实验表明，GAP算法在模拟和真实世界环境、单臂和双臂设置以及不同模型类型中均适用，并能形成鲁棒且可泛化的视觉-本体感受策略。|Di Hu Team|[2602.12032](http://arxiv.org/abs/2602.12032)|null|
|**2026-02-12**|**Accelerating Robotic Reinforcement Learning with Agent Guidance**|强化学习在机器人操作中面临严重的样本效率问题，而现有人机协作 (HIL) 方法虽能加速训练，但受限于可扩展性、操作员疲劳和不一致的人类专业知识。为解决此问题，本研究提出了Agent-guided Policy Search (AGPS) 框架，通过多模态智能体取代人工监督者，实现训练流程自动化。其核心思想是将智能体视为语义世界模型，注入内在价值先验来结构化物理探索，并利用可执行工具通过纠正性路点和空间约束提供精确指导。实验证明，AGPS在样本效率方面优于HIL方法，从而实现了无劳动力的可扩展机器人学习路径。|Yaodong Yang Team|[2602.11978](http://arxiv.org/abs/2602.11978)|null|
|**2026-02-12**|**Robot-DIFT: Distilling Diffusion Features for Geometrically Consistent Visuomotor Control**|本研究认为机器人操作泛化性的瓶颈在于当前视觉骨干网络与闭环控制物理需求之间的结构性不匹配，尤其在于现有模型缺乏精细的几何敏感性。鉴于生成扩散模型内在地编码了几何依赖性，但其随机性和延迟阻碍了直接应用，我们提出了Robot-DIFT框架。该框架通过流形蒸馏 (Manifold Distillation) 将冻结的扩散教师模型蒸馏到确定性空间-语义特征金字塔网络 (S2-FPN) 中，从而在保持生成模型丰富几何先验的同时，确保了时间稳定性、实时执行和抗漂移鲁棒性。在DROID数据集上的预训练结果表明，Robot-DIFT在几何一致性和控制性能上均优于领先的判别式基线，验证了视觉学习方式对机器人行为能力的关键影响。|Georgia Chalvatzaki Team|[2602.11934](http://arxiv.org/abs/2602.11934)|null|
|**2026-02-12**|**JEPA-VLA: Video Predictive Embedding is Needed for VLA Models**|现有VLA模型在机器人操作中仍面临样本效率低和泛化能力有限的问题，本研究认为这与预训练视觉表示在环境理解和策略先验方面知识不足有关。通过深入分析，我们发现现有VLA中常用的视觉表示未能有效捕获关键任务相关信息及诱导有效策略先验，而通过视频预训练的预测嵌入，特别是V-JEPA 2，能够灵活地处理不可预测因素并编码任务相关的时间动态。基于此，我们提出了JEPA-VLA，一种将预测嵌入自适应集成到现有VLA的简单有效方法。实验证明，JEPA-VLA在LIBERO、LIBERO-plus、RoboTwin2.0和真实机器人任务等基准测试中均取得了显著的性能提升。|Mingsheng Long Team|[2602.11832](http://arxiv.org/abs/2602.11832)|null|
|**2026-02-12**|**Clutt3R-Seg: Sparse-view 3D Instance Segmentation for Language-grounded Grasping in Cluttered Scenes**|针对杂乱环境中3D实例分割在遮挡、有限视角和噪声掩码下的挑战，本研究提出了Clutt3R-Seg，一种用于语言引导抓取的零样本鲁棒3D实例分割流水线。该方法的核心思想是引入语义线索的分层实例树，通过跨视图分组和条件替换，将噪声掩码作为信息线索，从而抑制过分割和欠分割，产生视图一致的掩码和鲁棒的3D实例。为应对多阶段任务中的场景变化，该方法还引入了一致性感知更新机制。在合成和真实世界数据集及真实机器人上的验证表明，Clutt3R-Seg在杂乱和稀疏视图场景中持续优于现有最先进基线，尤其在重度杂乱序列中表现出超过2.2倍的性能提升。|Ayoung Kim Team|[2602.11660](http://arxiv.org/abs/2602.11660)|null|
|**2026-02-12**|**ViTaS: Visual Tactile Soft Fusion Contrastive Learning for Visuomotor Learning**|针对现有机器人操作中视觉与触觉信息融合方法在遮挡场景下效果不佳、未能充分利用两种模态互补性且集成机制多为直接拼接的问题，本研究提出了ViTaS框架。该框架旨在结合视觉和触觉信息指导智能体行为，并引入了软融合对比学习（Soft Fusion Contrastive Learning）以及一个CVAE模块，以更好地利用视觉-触觉表示中的对齐和互补性。在12个模拟环境和3个真实世界环境中的实验验证表明，ViTaS显著优于现有基线，证明了其在利用多模态信息方面的有效性。|Huazhe Xu Team|[2602.11643](http://arxiv.org/abs/2602.11643)|null|
|**2026-02-12**|**EasyMimic: A Low-Cost Framework for Robot Imitation Learning from Human Videos**|鉴于大规模真实世界数据采集成本高昂阻碍了机器人模仿学习，尤其对低成本家用机器人而言，本研究提出了EasyMimic框架。该框架是一个低成本、可复制的解决方案，使机器人能通过标准RGB相机捕获的人类视频演示快速学习操作策略。其方法首先从视频中提取3D手部轨迹，并利用动作对齐模块将其映射到低成本机器人的夹持器控制空间；为弥合人机领域差距，引入了简单的手部视觉增强策略，并通过协同训练方法在处理过的人类数据和少量机器人数据上微调模型。实验证明，EasyMimic在LeRobot平台上在多种操作任务中取得了高性能，显著减少了对昂贵机器人数据采集的依赖，为智能机器人进入家庭提供了实用途径。|Qin Jin Team|[2602.11464](http://arxiv.org/abs/2602.11464)|null|
|**2026-02-12**|**Scaling World Model for Hierarchical Manipulation Policies**|视觉-语言-动作（VLA）模型在通用机器人操作中虽有前景，但在有限真实机器人数据下，其在分布外（OOD）场景中的泛化能力仍脆弱。本研究提出VISTA，一个分层视觉-语言-动作框架，利用大规模预训练世界模型的泛化能力实现鲁棒且可泛化的视觉子目标任务分解。该框架将世界模型作为高层规划器来生成带有目标图像的子任务序列，VLA作为低层执行器遵循指导生成动作。实验结果表明，与原始文本目标相比，合成的目标图像提供了更具视觉和物理细节的指导，使低层策略能泛化到新物体和场景，将VLA在OOD场景中的性能从14%提升至69%。|Xinghang Li Team|[2602.10983](http://arxiv.org/abs/2602.10983)|null|
|**2026-02-12**|**Self-Refining Vision Language Model for Robotic Failure Detection and Reasoning**|针对机器人故障推理中，真实世界故障的复杂性及丰富推理标签获取成本高昂的问题，本文提出了ARMOR框架。该框架将故障检测和推理建模为一个多任务自细化过程，模型通过迭代预测检测结果和自然语言推理，并从大规模稀疏二元标签和少量丰富推理标注的异构监督中学习。实验结果表明，ARMOR在故障检测率上比现有方法提升高达30%，在LLM模糊匹配分数测量的推理能力上提升高达100%，展现了对异构监督和开放式推理的鲁棒性。|Yesh Dattatreya Team|[2602.12405](http://arxiv.org/abs/2602.12405)|null|
|**2026-02-11**|**Human Preference Modeling Using Visual Motion Prediction Improves Robot Skill Learning from Egocentric Human Video**|当前从第一视角人类视频中学习机器人的方法，在度量视觉状态长期价值时存在假设限制，且在跨实体和环境时需要迁移学习到的价值函数。本研究提出一种新方法，通过预测后续图像中跟踪点的运动来建模人类偏好，并将奖励函数定义为机器人行为中预测与观察到的物体运动的一致性。随后，利用改进的Soft Actor Critic (SAC) 算法（通过10次机器人演示初始化）来估计此奖励的价值函数，并在机器人上优化策略。结果显示，该方法在真实机器人上有效，并且在模拟和真实机器人上的多项任务中，其学习到的策略与现有工作相当或超越。|Christopher G. Atkeson Team|[2602.11393](http://arxiv.org/abs/2602.11393)|null|
|**2026-02-11**|**MolmoSpaces: A Large-Scale Open Ecosystem for Robot Navigation and Manipulation**|机器人大规模部署要求对日常情境具备鲁棒性，但现有基准测试缺乏足够多样化的场景。本研究引入了MolmoSpaces，一个支持机器人策略大规模基准测试的开放生态系统，包含超过23万个多样化室内环境和13万个丰富注释的物体资产，且与模拟器无关。同时设计了MolmoSpaces-Bench基准套件。实验证明，MolmoSpaces-Bench具有强大的模拟到真实关联性，验证了新型零样本策略的优越性，并揭示了对提示措辞、初始关节位置和相机遮挡的关键敏感性，为机器人学习研究提供了可扩展的基础。|Ranjay Krishna Team|[2602.11337](http://arxiv.org/abs/2602.11337)|null|
|**2026-02-11**|**YOR: Your Own Mobile Manipulator for Generalizable Robotics**|随着机器人学习的发展，低成本机器人平台日益增多，但移动操作的最佳外形仍然是待解问题。本研究推出YOR，一个开源、低成本的移动操纵器，集成了全向基座、伸缩式垂直升降器和带抓手的双臂，以实现全身移动和操作。其设计强调模块化、易于组装和经济性（物料清单成本低于1万美元）。YOR在需要协调全身控制、双臂操作和自主导航的任务中展现出其能力，以远低于现有平台的成本提供了具有竞争力的移动操作研究功能。|Zichen Jeff Cui Team|[2602.11150](http://arxiv.org/abs/2602.11150)|null|
|**2026-02-11**|**ABot-M0: VLA Foundation Model for Robotic Manipulation with Action Manifold Learning**|在异构硬件上构建通用具身智能体是机器人学的一大挑战，面临数据碎片化、表示不一致和训练目标不匹配等问题。本研究提出了ABot-M0框架，它通过建立系统的数据整理管道，同时优化模型架构和训练策略，将异构原始数据转换为统一、高效的表示。通过构建大规模UniACT数据集，统一预训练提高了跨平台和任务的知识迁移和泛化能力。为提升动作预测效率和稳定性，引入了动作流形学习（AML）。此外，框架通过双流机制实现模块化感知，整合VLM语义与几何先验和多视角输入。实验结果表明各组件独立且具有累加效益。|Mu Xu Team|[2602.11236](http://arxiv.org/abs/2602.11236)|**[link](https://amap-cvlab.github.io/ABot-Manipulation/)**|
|**2026-02-11**|**OSIL: Learning Offline Safe Imitation Policies with Safety Inferred from Non-preferred Trajectories**|离线安全模仿学习面临在缺乏每时间步安全成本或奖励信息的演示数据中学习安全和奖励最大化策略的难题。本研究针对此问题提出了一种新颖的离线安全模仿学习算法OSIL，它通过非首选轨迹推断安全性。OSIL将安全策略学习表述为约束马尔可夫决策过程（CMDP），并通过推导奖励最大化目标的下界并学习一个估计非首选行为可能性的成本模型来重新构建CMDP问题，从而无需明确的安全成本和奖励标注。实验证明，OSIL能学习到更安全的策略，满足成本约束而不降低奖励性能，优于现有基线方法。|Balaraman Ravindran Team|[2602.11018](http://arxiv.org/abs/2602.11018)|null|
|**2026-02-11**|**Towards Learning a Generalizable 3D Scene Representation from 2D Observations**|现有方法在相机坐标系中构建表示，限制了其在机器人操作中的直接应用。本研究引入了一种可泛化的神经辐射场方法，用于根据以机器人自我为中心的观察来预测3D工作空间占用。该模型在全局工作空间框架中构建占用表示，使其直接适用于机器人操作，并能集成灵活的源视图，无需场景特定微调即可泛化到未见物体布局。在人形机器人上的实验证明，该模型在40个真实场景上训练后，实现了26毫米的重建误差（包括遮挡区域），验证了其推断完整3D占用（超越传统立体视觉方法）的能力。|Stefan Wermter Team|[2602.10943](http://arxiv.org/abs/2602.10943)|null|
|**2026-02-11**|**Semi-Supervised Cross-Domain Imitation Learning**|跨域模仿学习（CDIL）通过领域知识迁移加速策略学习，但在专家数据收集成本高昂时尤为重要。现有CDIL方法或依赖监督（不稳定），或无监督（不稳健）。本研究提出了半监督CDIL（SS-CDIL）设置及其首个具有理论依据的算法。该方法仅使用少量目标专家演示和未标记的不完善轨迹等离线数据。为解决域差异，提出一种新颖的跨域损失函数来学习域间状态-动作映射，并设计自适应权重函数以平衡源域和目标域知识。实验表明，该方法在MuJoCo和Robosuite上始终优于基线，实现了最小监督下稳定且数据高效的策略学习。|Ping-Chun Hsieh Team|[2602.10793](http://arxiv.org/abs/2602.10793)|null|
|**2026-02-11**|**Say, Dream, and Act: Learning Video World Models for Instruction-Driven Robot Manipulation**|机器人操作通常缺乏预测环境响应动作的能力，导致错误和低效，且现有视觉-语言模型（VLM）和世界模型在预测未来状态或生成空间一致帧方面存在局限。本研究提出一种用于快速且可预测的视频条件动作框架。该方法首先选择并适应一个鲁棒的视频生成模型以确保可靠的未来预测，然后应用对抗蒸馏进行快速、少步骤的视频生成，最后训练一个动作模型，利用生成的视频和真实观察来纠正空间误差。大量实验表明，该方法生成的时间连贯、空间准确的视频预测直接支持精确操作，在具身一致性、空间指代能力和任务完成度方面显著优于现有基线。|Yanwei Fu Team|[2602.10717](http://arxiv.org/abs/2602.10717)|null|

<p align=right>(<a href=#updated-on-20260216>back to top</a>)</p>

## World Model

|Publish Date|Title|Chinese Summary|Authors|PDF|Code|
|---|---|---|---|---|---|
|**2026-02-13**|**Imitating What Works: Simulation-Filtered Modular Policy Learning from Human Videos**|针对机器人通过观察人类视频学习抓取操作技能时，传统方法难以有效学习与任务兼容的抓取行为的问题，本研究提出了Perceive-Simulate-Imitate (PSI) 框架。该框架利用仿真中的抓取轨迹过滤技术，对人类视频数据进行处理，并生成带有抓取适用性标签的扩展轨迹数据，从而实现面向任务的抓取能力监督学习。真实世界实验表明，PSI无需任何机器人数据即可高效学习精确的操纵技能，并且相比简单使用抓取生成器的方法，性能显著提升，鲁棒性更强。|Wei-Chiu Ma Team|[2602.13197](http://arxiv.org/abs/2602.13197)|null|
|**2026-02-13**|**Order Matters in Retrosynthesis: Structure-aware Generation via Reaction-Center-Guided Discrete Flow Matching**|为解决现有免模板逆合成方法学习效率低和半模板方法泛化受限的问题，本研究提出了一种结构感知的免模板框架，核心在于利用原子排序信息。该方法将反应中心原子置于序列头部，通过位置归纳偏差编码化学反应的两阶段特性，并采用RetroDiT骨干网络与离散流匹配相结合。实验结果表明，该方法在USPTO-50k和USPTO-Full数据集上取得了SOTA性能，且在预测反应中心下，性能超越了使用更多数据训练的基础模型，并验证了结构先验的重要性。|Tianshu Yu Team|[2602.13136](http://arxiv.org/abs/2602.13136)|null|
|**2026-02-13**|**A Calibrated Memorization Index (MI) for Detecting Training Data Leakage in Generative MRI Models**|鉴于图像生成模型可能复制训练数据，尤其在医学图像生成中引发隐私问题，本研究提出了一种校准的逐样本度量方法来检测训练数据的记忆化和重复。该方法利用MRI基础模型提取图像特征，聚合多层白化最近邻相似度，并映射为有界的“过拟合/新颖性指数”（ONI）和“记忆化指数”（MI）分数。在多个MRI数据集上的实验结果表明，该度量能稳健检测重复数据，并提供一致的度量值，在样本级别实现了近乎完美的重复项检测。|Ibrahim Habli Team|[2602.13066](http://arxiv.org/abs/2602.13066)|null|
|**2026-02-13**|**INHerit-SG: Incremental Hierarchical Semantic Scene Graphs with RAG-Style Retrieval**|针对现有语义场景图在机器人导航中难以支持可解释的人类意图推理的问题，本研究提出了INHerit-SG框架。该框架将地图定义为RAG-ready的知识库，通过引入自然语言描述作为语义锚点对齐人类意图，并采用异步双进程架构和分层结构解耦几何分割与语义推理，通过事件触发机制保持地图长期一致性。实验在新建数据集和真实世界环境中进行，结果表明INHerit-SG在复杂查询上达到了最先进性能，并提高了检索成功率和可靠性，展现了其在下游导航任务中的可扩展性。|Yang Gao Team|[2602.12971](http://arxiv.org/abs/2602.12971)|null|
|**2026-02-13**|**Information-theoretic analysis of world models in optimal reward maximizers**|为量化最优行为对世界内部表示的需求，本研究考虑了一个具有n个状态和m个动作的受控马尔可夫过程，并假设转移动态存在均匀先验。研究证明，观察一个对任何非恒定奖励函数最优的确定性策略，可以精确地传达n log m比特关于环境的信息。具体来说，环境与最优策略之间的互信息为n log m比特。这些发现为实现最优性所需的“隐式世界模型”提供了精确的信息理论下限，适用于多种奖励最大化目标。|Alex Altair Team|[2602.12963](http://arxiv.org/abs/2602.12963)|null|
|**2026-02-13**|**Scaling Single Human Demonstrations for Imitation Learning using Generative Foundational Models**|鉴于机器人模仿学习中收集演示数据耗时且不易从人类演示直接迁移，本研究提出Real2Gen框架，旨在从单个“人类”演示中训练机器人操纵策略。该方法从人类演示中提取必要信息并传输至仿真环境，在仿真中利用可编程专家智能体生成无限数据来训练流匹配策略。实验结果显示，Real2Gen在三个真实世界任务上成功率平均提升26.6%，并且由于训练数据丰富多样，训练策略的泛化能力显著提高，纯仿真训练的策略还能零样本部署到真实世界。|Abhinav Valada Team|[2602.12734](http://arxiv.org/abs/2602.12734)|null|
|**2026-02-13**|**MedXIAOHE: A Comprehensive Recipe for Building Medical MLLMs**|为提升真实世界临床应用中的通用医学理解和推理能力，本研究提出了医疗视觉-语言基础模型MedXIAOHE。该模型采用实体感知持续预训练框架，组织异构医学语料以拓宽知识覆盖并减少长尾问题；通过强化学习和工具增强的代理训练，整合多样化医学推理模式以支持带可验证决策轨迹的多步骤诊断推理；并融合用户偏好规则、证据推理和低幻觉长文本报告生成，提高真实世界使用的可靠性。MedXIAOHE在多项医学基准测试中取得了最先进的性能，并超越了领先的闭源多模态系统。|Zhixiong Yang Team|[2602.12705](http://arxiv.org/abs/2602.12705)|null|
|**2026-02-13**|**RelBench v2: A Large-Scale Benchmark and Repository for Relational Data**|为推动关系深度学习（RDL）的发展，并应对日益增长的模型规模需求，本研究引入了RelBench v2，一个大规模、真实的关系数据库基准扩展。RelBench v2新增了四个大型数据集和“自动完成任务”，旨在直接推理关系表中缺失的属性值，并整合了外部基准和评估框架以实现统一的关系-时间评估。实验结果表明，RDL模型在自动完成、预测和推荐任务中始终优于单表基线，突出了显式建模关系结构的重要性。|Jure Leskovec Team|[2602.12606](http://arxiv.org/abs/2602.12606)|**[link](https://relbench.stanford.edu)**|
|**2026-02-13**|**The Constant Eye: Benchmarking and Bridging Appearance Robustness in Autonomous Driving**|针对自动驾驶算法在OOD条件下易受外观变化影响，且难以区分外观与结构场景变化导致规划器失效的问题，本研究建立了Navdream，一个高保真鲁棒性基准。该基准利用生成式像素对齐风格迁移，隔离外观变化对驾驶性能的影响。为弥合这一差距，研究提出了一种通用感知接口，利用冻结的视觉基础模型（DINOv3）提取外观不变特征作为规划器的稳定接口。实验表明，现有规划算法在OOD外观下性能显著下降，而该即插即用解决方案在各种规划范式中实现了卓越的零样本泛化，在极端外观变化下仍保持一致性能。|Yiyi Liao Team|[2602.12563](http://arxiv.org/abs/2602.12563)|null|
|**2026-02-13**|**Self-Supervised JEPA-based World Models for LiDAR Occupancy Completion and Forecasting**|鉴于自动驾驶需要世界模型来支持长期规划，且模型学习需具备自监督的可扩展性，本研究提出AD-LiST-JEPA，一个基于联合嵌入预测架构（JEPA）的自监督世界模型。该模型旨在利用JEPA框架从激光雷达数据预测未来时空演变。通过下游基于激光雷达的占用完成和预测（OCF）任务评估学习到的表示质量，概念验证实验表明，经过JEPA世界模型学习预训练后的编码器在OCF性能上有所提升，证明了该方法在感知和预测联合任务中的潜力。|Anna Choromanska Team|[2602.12540](http://arxiv.org/abs/2602.12540)|null|
|**2026-02-13**|**Multi-Agent Model-Based Reinforcement Learning with Joint State-Action Learned Embeddings**|在部分可观察和高度动态环境中，多智能体协调学习面临表示学习和数据效率挑战。为此，本文提出了一种新颖的基于模型的强化学习框架，该框架将联合状态-动作表示学习与想象式展开相结合。作者设计了一个使用变分自编码器训练的世界模型，并利用学习到的状态-动作嵌入（SALE）进行增强，将其注入到预测未来展开的想象模块和估计联合动作值函数的联合智能体网络中。在星际争霸II微管理、多智能体MuJoCo和基于级别的觅食挑战等基准测试中，该方法在有限真实环境交互下，通过将想象轨迹与基于SALE的动作值相结合，显著优于基线算法，验证了其在多智能体模型范式中学习联合状态-动作嵌入的有效性。|David Meger Team|[2602.12520](http://arxiv.org/abs/2602.12520)|null|
|**2026-02-12**|**The Observer Effect in World Models: Invasive Adaptation Corrupts Latent Physics**|判断神经网络模型是内化了物理定律还是仅利用统计捷径，尤其是在分布外（OOD）变化下，仍是一个难题。传统的适应性评估方法（如微调或高容量探针）可能改变被测量的表示，从而混淆自监督学习（SSL）期间的真实学习内容。为解决此问题，本文提出了一种非侵入性评估协议PhyIP，该协议基于线性表示假设，通过测试物理量能否从冻结表示中线性解码来评估。在流体动力学和轨道力学任务中，实验发现当SSL错误率较低时，潜在结构变得线性可访问，PhyIP在OOD测试中成功恢复了内能和牛顿反平方标度（ρ>0.90）。相比之下，基于适应性的评估可能使这种结构崩溃（ρ≈0.05）。这表明低容量探针能更准确地评估物理世界模型，而适应性评估可能掩盖潜在结构。|Barbara Hammer Team|[2602.12218](http://arxiv.org/abs/2602.12218)|null|
|**2026-02-12**|**LDA-1B: Scaling Latent Dynamics Action Model via Universal Embodied Data Ingestion**|当前机器人基础模型多依赖大规模行为克隆，忽视了异构具身数据中可迁移的动力学知识，而现有统一世界模型（UWM）因粗糙数据使用和碎片化数据集难以扩展。为此，本文提出了LDA-1B，一个通过通用具身数据摄取进行扩展的机器人基础模型，它联合学习动力学、策略和视觉预测，并为不同质量数据分配不同角色。为支持大规模训练，作者构建并标准化了EI-30k数据集（超过3万小时的人类和机器人轨迹）。通过在结构化的DINO潜在空间中进行预测，实现了异构数据的可扩展动力学学习，避免了冗余的像素空间外观建模，并采用多模态扩散Transformer处理异步视觉和动作流，实现了10亿参数规模的稳定训练。实验结果表明，LDA-1B在接触密集型、灵巧型和长程任务上分别比现有方法（如π0.5）提高了21%、48%和23%，并能通过利用30%通常有害且被丢弃的低质量轨迹，实现数据高效微调，性能提升10%。|He Wang Team|[2602.12215](http://arxiv.org/abs/2602.12215)|**[link](https://pku-epic.github.io/LDA)**|
|**2026-02-12**|**DreamID-Omni: Unified Framework for Controllable Human-Centric Audio-Video Generation**|尽管基础模型在音视频生成方面取得进展，但以人物为中心的多任务（如参考音视频生成、视频编辑、音频驱动动画）仍被孤立处理，且难以在单一框架内实现对多角色身份和音色的精确解耦控制。本文提出了DreamID-Omni，一个统一的可控人物中心音视频生成框架。作者设计了一个对称条件扩散Transformer，通过对称条件注入方案整合异构条件信号。为解决多人物场景中普遍存在的身份-音色绑定失败和说话人混淆问题，提出了双层解耦策略：在信号层面采用同步RoPE确保严格的注意力空间绑定，在语义层面采用结构化字幕建立显式属性-主题映射。此外，还设计了多任务渐进训练方案，利用弱约束生成先验来正则化强约束任务。大量实验证明，DreamID-Omni在视频、音频和音视频一致性方面均达到了全面的最先进性能，甚至超越了领先的商业模型。|Xiangwang Hou Team|[2602.12160](http://arxiv.org/abs/2602.12160)|**[link](https://guoxu1233.github.io/DreamID-Omni/)**|
|**2026-02-12**|**It's TIME: Towards the Next Generation of Time Series Forecasting Benchmarks**|时间序列基础模型（TSFMs）正在革新预测领域，但现有基准在数据构成、数据完整性、任务制定和分析视角上存在局限。为解决这些问题，本文提出了TIME，一个新一代任务中心基准，包含50个全新数据集和98个预测任务，专为严格的零样本TSFM评估而设计，避免数据泄露。通过整合大型语言模型和人类专业知识，建立了严格的人机协作基准构建流程，确保高数据完整性，并根据真实操作需求和变量可预测性重新定义任务。此外，作者提出了一种新颖的模式级评估视角，超越了基于静态元标签的传统数据集级评估，通过利用结构化时间序列特征表征内在时间属性，为模型能力提供了更具普适性的见解。对12个代表性TSFMs进行评估，并建立了一个多粒度排行榜，以促进深入分析和可视化检查。|Chenghao Liu Team|[2602.12147](http://arxiv.org/abs/2602.12147)|null|
|**2026-02-12**|**Commencing-Student Enrolment Forecasting Under Data Sparsity with Time Series Foundation Models**|许多大学面临日益增长的财政压力，亟需准确预测新生入学人数，然而高等教育入学预测通常数据稀疏，年度序列短且受报告变化和体制转变影响。流行的经典方法因短样本导致参数估计和模型选择不稳定，以及结构性中断导致外推能力下降而不可靠。近期，TSFMs在泄漏受限的协变量构建下，为年度、数据稀疏的机构预测提供了强大的零样本先验。本文在零样本设置下，对多种TSFM家族进行了基准测试，并测试了一组紧凑、防泄漏的协变量集。作者引入了“机构运营状况指数”（IOCI），这是一个从时间戳文件证据中提取的可转移的0-100区间状态协变量，并结合了具有稳定特征工程的Google Trends需求代理。使用严格对齐的回溯测试，结果表明，在没有机构特定训练的情况下，条件化TSFMs的表现与经典基准相当，具体表现差异因群体和模型而异。|Surangika Ranathunga Team|[2602.12120](http://arxiv.org/abs/2602.12120)|null|
|**2026-02-12**|**The Pensieve Paradigm: Stateful Language Models Mastering Their Own Context**|当前的大型语言模型（LLMs）虽有强大的数据库和检索系统，但缺乏主动管理自身状态和记忆的机制，被动接受手动提供的上下文，受限于固定窗口大小。本研究引入了StateLM，一种新型基础模型，它具备内部推理循环来管理自身状态。该模型配备了一系列记忆工具（如上下文剪枝、文档索引、笔记）并被训练主动管理这些工具，从而摆脱固定窗口的架构限制。实验结果表明，StateLM在所有模型规模的长文档问答任务中持续优于标准LLMs，在聊天记忆任务中绝对准确率提升10%至20%，在深度研究任务BrowseComp-Plus上更是达到52%的准确率，远超标准LLM的5%左右。这使得LLMs从被动预测器转变为状态感知代理，实现了状态化和可管理的推理过程。|Yan Wang Team|[2602.12108](http://arxiv.org/abs/2602.12108)|null|
|**2026-02-12**|**GigaBrain-0.5M*: a VLA That Learns From World Model-Based Reinforcement Learning**|直接从当前观察预测多步动作块的视觉-语言-动作（VLA）模型，因场景理解和未来预测能力受限而存在不足。相比之下，预训练于网络规模视频语料库的视频世界模型具有强大的时空推理和准确的未来预测能力，为增强VLA学习提供了基础。本研究提出了GigaBrain-0.5M*，一个通过世界模型驱动强化学习训练的VLA模型，其基于在超过1万小时机器人操作数据上预训练的GigaBrain-0.5。GigaBrain-0.5M*通过RAMP（通过世界模型条件策略进行强化学习）整合强化学习，以实现鲁棒的跨任务适应。实验结果表明，RAMP相对于RECAP基线取得了显著性能提升，在“叠衣服”、“装箱”和“制作浓缩咖啡”等挑战性任务上提升了约30%，并且在真实世界部署中展示了可靠的长时序复杂操作能力。|Zheng Zhu Team|[2602.12099](http://arxiv.org/abs/2602.12099)|**[link](https://gigabrain05m.github.io/)**|
|**2026-02-12**|**Affordance-Graphed Task Worlds: Self-Evolving Task Generation for Scalable Embodied Learning**|在现实世界中直接训练机器人策略成本高昂且难以扩展，而生成式仿真虽能合成大规模数据，但现有方法常难以生成逻辑连贯的长时序任务，且因开环执行难以应对动态物理不确定性。本研究提出了Affordance-Graphed Task Worlds (AGT-World)，一个统一框架，能够根据现实世界观察自主构建交互式仿真环境及相应的机器人任务策略。AGT-World通过将任务空间形式化为结构化图，实现复杂目标的精确分层分解为原子原语，并引入结合视觉-语言模型推理和几何验证的混合反馈自进化机制来自主优化策略。大量实验证明，AGT-World在成功率和泛化能力上显著优于现有方法，实现了提议、执行和纠错的自改进循环，从而促进可扩展的机器人学习。|Changshui Zhang Team|[2602.12065](http://arxiv.org/abs/2602.12065)|null|
|**2026-02-12**|**VLAW: Iterative Co-Improvement of Vision-Language-Action Policy and World Model**|视觉-语言-动作（VLA）模型的性能和可靠性可通过迭代在线交互提升，但现实世界策略回放数据收集昂贵。虽然学习到的模拟器（动作条件视频生成模型）有望生成额外回放数据，但现有世界模型缺乏物理保真度，难以精确建模接触密集物体操纵中的关键物理细节。本研究提出一种简单的迭代改进算法，利用真实世界回放数据提升世界模型的保真度，然后该世界模型再用于生成补充合成数据以改进VLA模型。在真实机器人实验中，该方法显著提升了最先进VLA模型在多个下游任务上的性能，相比基础策略，成功率绝对提升39.2%，相比使用生成合成回放数据进行训练，提升了11.6%。|Chelsea Finn Team|[2602.12063](http://arxiv.org/abs/2602.12063)|null|
|**2026-02-12**|**HoloBrain-0 Technical Report**|基础模型研究与可靠的真实世界机器人部署之间存在差距。为弥合此差距，本研究引入了HoloBrain-0，一个全面的视觉-语言-动作（VLA）框架。其核心是一个新颖的VLA架构，明确融入了机器人具身先验（如多视角相机参数和运动学描述），以增强3D空间推理并支持多样化的具身形态。该设计通过可扩展的“预训练-后训练”范式进行验证，在RoboTwin 2.0、LIBERO和GenieSim等仿真基准测试中取得了最先进的结果，并在具有挑战性的长时序真实世界操作任务中表现出色。值得一提的是，其高效的0.2B参数变体能与更大的基线模型相媲美，支持低延迟的设备端部署。为加速研究，HoloBrain生态系统已完全开源，包括预训练VLA基础模型、后训练检查点和全栈VLA基础设施RoboOrchard。|Zhizhong Su Team|[2602.12062](http://arxiv.org/abs/2602.12062)|null|
|**2026-02-12**|**FedGRPO: Privately Optimizing Foundation Models with Group-Relative Rewards from Domain Client**|针对联邦基础模型(FedFMs)中现有知识迁移方法存在的训练成本高、通信开销大及隐私风险问题，本研究将其重构为强化学习式评估过程。提出了FedGRPO框架，通过构建轻量级置信图进行能力专家选择，并利用"组相对"概念将问题与解决方案打包为候选策略，仅聚合客户端返回的标量奖励信号，而非直接交换数据或模型更新。实验结果表明，FedGRPO在保证隐私和降低通信开销的同时，实现了优于传统FedFMs基线的下游任务精度和通信效率。|Yuxing Han Team|[2602.12014](http://arxiv.org/abs/2602.12014)|null|
|**2026-02-12**|**Accelerating Robotic Reinforcement Learning with Agent Guidance**|强化学习(RL)在机器人操作技能学习中面临样本效率低下，而现有的人在环(HIL)方法受限于可扩展性与人类监督的低效性。本研究提出了Agent-guided Policy Search (AGPS)框架，用多模态智能体取代人类监督者，将智能体视为语义世界模型，注入内在价值先验来指导物理探索。通过可执行工具，智能体提供精确的纠正航点和空间约束来修剪探索。实验结果表明，AGPS在精细插入和变形物体操作任务中，样本效率优于HIL方法，从而实现了自动化监督，为可扩展的机器人学习提供了新途径。|Yaodong Yang Team|[2602.11978](http://arxiv.org/abs/2602.11978)|null|
|**2026-02-12**|**Where Bits Matter in World Model Planning: A Paired Mixed-Bit Study for Efficient Spatial Reasoning**|为实现高效空间推理，本研究探讨了低比特规划行为在有限精度预算下，是否主要由总比特位宽决定，或由比特位在模型模块间的分配方式决定。通过在DINO-WM上进行混合位评估，观察到8比特和6比特设置接近FP16，3比特设置性能崩溃，而4比特设置对位宽分配敏感。在4比特过渡区，保留编码器精度可提升规划性能。这些发现表明，模块感知、预算感知的量化策略对于高效空间推理具有重要研究意义。|Vaishak Menon Team|[2602.11882](http://arxiv.org/abs/2602.11882)|null|
|**2026-02-12**|**PuYun-LDM: A Latent Diffusion Model for High-Resolution Ensemble Weather Forecasts**|潜在扩散模型(LDMs)在 <=0.25° 的高分辨率集合天气预报中存在扩散性受限问题，且现有频率方法因缺乏对变量间频谱异质性的考虑而导致正则化强度不均。本研究提出了PuYun-LDM模型，结合3D掩码自编码器(3D-MAE)编码天气状态演化特征作为扩散模型的额外条件，并采用变量感知掩码频率建模(VA-MFM)策略自适应选择阈值。实验证明，PuYun-LDM增强了潜在扩散性，在短时效预报中表现优于ENS，长时效预报中与ENS相当，并能在短时间内高效生成全球预报。|Bin Wang Team|[2602.11807](http://arxiv.org/abs/2602.11807)|null|
|**2026-02-12**|**HAIC: Humanoid Agile Object Interaction Control via Dynamics-Aware World Model**|类人机器人在处理非结构化环境中复杂全身任务时，现有方法难以应对具有独立动力学和非完整约束的欠驱动物体，且缺乏外部状态估计。本研究提出了HAIC统一框架，通过仅利用本体感受历史数据估计高阶物体状态的动力学预测器，并将预测投影至静态几何先验形成空间接地动态占用图，使策略能在盲区推断碰撞边界和接触可供性。通过非对称微调，世界模型持续适应学生策略。实验表明，HAIC在敏捷任务和多物体长时序任务中均取得了高成功率，有效应对惯性扰动并预测多物体动力学。|Renjing Xu Team|[2602.11758](http://arxiv.org/abs/2602.11758)|**[link](https://haic-humanoid.github.io/)**|
|**2026-02-12**|**ABot-N0: Technical Report on the VLA Foundation Model for Versatile Embodied Navigation**|具身导航领域长期受限于任务特定的架构。本研究引入了ABot-N0，一个统一的视觉-语言-动作(VLA)基础模型，旨在实现点目标、物体目标、指令跟随、POI目标和人物跟随五项核心导航任务的“大一统”。模型采用分层“大脑-行动”架构，将基于大型语言模型的认知大脑与基于流匹配的动作专家结合。为支持大规模学习，研究团队开发了ABot-N0数据引擎， curating 16.9M专家轨迹和5.0M推理样本。ABot-N0在7个基准测试中取得了新的SOTA性能，并整合规划器与分层拓扑记忆，实现动态真实世界环境中的鲁棒长时序任务。|Mu Xu Team|[2602.11598](http://arxiv.org/abs/2602.11598)|**[link](https://amap-cvlab.github.io/ABot-Navigation/ABot-N0/)**|
|**2026-02-12**|**Brain4FMs: A Benchmark of Foundation Models for Electrical Brain Signal**|脑基础模型(BFMs)在神经科学领域快速发展，但缺乏统一的方法理解和标准化评估框架。本研究旨在填补这一空白，通过自监督学习(SSL)分类法组织BFMs，并从数据集角度总结常见下游任务，整理临床和以人为中心神经技术应用的代表性公共数据集。在此基础上，推出了Brain4FMs开放评估平台，集成了15个代表性BFM和18个公共数据集。该平台支持标准化比较和分析预训练数据、SSL策略和架构对泛化和下游性能的影响，以指导更准确、可迁移的BFMs开发。|Yang Yang Team|[2602.11558](http://arxiv.org/abs/2602.11558)|null|
|**2026-02-12**|**TS-Memory: Plug-and-Play Memory for Time Series Foundation Models**|时间序列基础模型(TSFMs)在分布偏移下的下游领域适应面临挑战，现有参数化适应易导致灾难性遗忘，而非参数化检索则引入高推理延迟。本研究提出了参数化记忆蒸馏方法并实现为TS-Memory，一个轻量级记忆适配器。TS-Memory分两阶段训练：首先构建离线、无信息泄露的kNN教师模型合成置信度感知的量化目标；其次通过置信度门控监督将检索诱导的分布校正蒸馏到记忆适配器中。实验表明，TS-Memory在多种TSFMs和基准测试上持续改进点预测和概率预测性能，且效率与冻结骨干网络相当，实现了无检索部署。|Yuxuan Liang Team|[2602.11550](http://arxiv.org/abs/2602.11550)|null|
|**2026-02-12**|**Budget-Constrained Agentic Large Language Models: Intention-Based Planning for Costly Tool Use**|在预算约束下，大型语言模型调用外部工具解决多步骤任务的工具增强型智能体面临巨大状态-动作空间、高结果方差和过高探索成本导致直接规划难以处理的问题。本研究提出了INTENT，一个推理时规划框架，利用意图感知的层次世界模型来预测未来的工具使用和风险校准成本，并在线指导决策。实验证明，INTENT在成本增强型StableToolBench上严格遵循硬预算可行性，显著提高了任务成功率，并在工具价格和预算动态变化下表现出鲁棒性。|Qi Qi Team|[2602.11541](http://arxiv.org/abs/2602.11541)|null|
|**2026-02-12**|**Vascular anatomy-aware self-supervised pre-training for X-ray angiogram analysis**|X射线血管造影图像分析受限于带标注数据稀缺，大规模自监督学习(SSL)在该领域潜力未被充分挖掘。本研究引入了VasoMIM框架，通过解剖引导的掩码策略强制模型学习血管语义，并利用解剖一致性损失保留血管结构一致性，从而增强学习表征的判别力。同时，策展了迄今最大的X射线血管造影预训练数据集XA-170K。VasoMIM在多个下游任务中展现出卓越的可迁移性和领先的性能，凸显了其作为X射线血管造影分析基础模型的巨大潜力。|Zeng-Guang Hou Team|[2602.11536](http://arxiv.org/abs/2602.11536)|null|
|**2026-02-12**|**Semantic-aware Adversarial Fine-tuning for CLIP**|当前研究表明，通过对抗性微调CLIP图像编码器可增强其零样本分类的对抗鲁棒性，但生成对抗样本（AEs）时仅依赖图像与单一手动模板的余弦相似度，不足以衡量图文对的语义相似性，导致微调后的模型鲁棒性不足。为解决此问题，本文提出了一种语义集成攻击方法，通过最小化原始图像与一组精炼文本描述（由基础模型生成并去除了幻觉）之间的平均相似度来生成语义感知的AEs。在此基础上，作者提出了语义感知对抗微调（SAFT）框架。实验结果表明，SAFT在16个数据集上的零样本对抗鲁棒性方面显著优于现有方法，实现了实质性提升。|Feng Liu Team|[2602.12461](http://arxiv.org/abs/2602.12461)|null|
|**2026-02-12**|**Stabilizing Native Low-Rank LLM Pretraining**|基础模型日益增长的参数量带来了巨大的计算和内存挑战，而低秩分解是降低成本的潜在途径，但从头开始仅使用低秩权重训练模型且性能匹配全秩模型仍缺乏稳定的方法。本文研究表明，无需先验方法的“全秩”辅助指导，大型语言模型（LLMs）可以从头开始仅使用低秩分解权重训练所有非嵌入矩阵。作者发现权重矩阵更新中谱范数（最大奇异值）的失控增长是导致原生低秩训练不稳定和损失尖峰的主要因素，并提出Spectron方法：通过正交化进行谱重归一化，根据因子当前的谱范数动态限制所得权重更新。实验证明，Spectron实现了稳定、端到端的低秩训练，开销可忽略不计，并为原生低秩Transformer建立了计算最优的缩放定律，展示了可预测的幂律行为和相对于全秩模型改进的推理效率。|Eugene Belilovsky Team|[2602.12429](http://arxiv.org/abs/2602.12429)|null|
|**2026-02-12**|**Policy4OOD: A Knowledge-Guided World Model for Policy Intervention Simulation against the Opioid Overdose Crisis**|阿片类药物危机是美国严重的公共卫生问题，但由于政策互动复杂且系统动态，评估干预措施极具挑战。本文提出Policy4OOD，一个知识引导的时空世界模型，旨在整合预测、反事实推理和优化三种关键能力来有效评估阿片类政策。该模型通过策略知识图谱、州级空间依赖性及社会经济时间序列的联合编码，构建一个策略条件化的Transformer来预测阿片类药物相关结果。训练完成后，世界模型可作为模拟器，通过前向传播进行预测，通过替换历史策略编码进行反事实分析，并通过蒙特卡洛树搜索进行策略优化。实验结果表明，空间依赖性和结构化策略知识显著提高了预测准确性，验证了该模型在数据驱动的公共卫生决策支持中的潜力。|Yanfang Ye Team|[2602.12373](http://arxiv.org/abs/2602.12373)|null|
|**2026-02-12**|**Free Lunch in Medical Image Foundation Model Pre-training via Randomized Synthesis and Disentanglement**|医学图像基础模型（MIFMs）在临床任务中展现巨大潜力，但其发展受限于大规模标注数据集的稀缺、异质性和高成本。本文提出RaSD（Randomized Synthesis and Disentanglement），一个可扩展的框架，可完全利用合成数据预训练MIFMs。RaSD通过随机高斯分布模拟解剖结构和外观变异，使模型接触足够的多尺度结构和外观扰动，从而迫使其依赖不变和任务相关的解剖线索而非数据集特有纹理，实现鲁棒和可迁移的表示学习。在120万3D体和960万2D图像上进行预训练后，RaSD模型在6种成像模态、48个数据集和56个下游任务中，持续优于从零开始训练的模型，在17个任务上取得了最佳性能，并在大多数其他任务上与使用大型真实数据集预训练的模型表现相当。这些结果证明了仅合成数据即可驱动鲁棒表示学习的能力，为医学AI领域带来了范式转变。|Hao Chen Team|[2602.12317](http://arxiv.org/abs/2602.12317)|null|

<p align=right>(<a href=#updated-on-20260216>back to top</a>)</p>

## VLM

|Publish Date|Title|Chinese Summary|Authors|PDF|Code|
|---|---|---|---|---|---|
|**2026-02-13**|**Steerable Vision-Language-Action Policies for Embodied Reasoning and Hierarchical Control**|预训练视觉语言模型(VLMs)拥有丰富的常识先验知识，但在机器人控制中有效落地仍面临挑战，现有分层方法中VLM对低层行为的引导受限于自然语言接口。为此，研究者提出了“可操控策略”(Steerable Policies)，通过在不同抽象级别（如子任务、动作、像素坐标）的丰富合成指令上训练视觉语言动作模型(VLA)，以提升低层可控性并释放VLM的预训练知识。实验结果表明，无论是通过学习型高层具身推理器还是即插即用VLM控制，Steerable Policies在真实的机器人操作实验中均优于现有基线，尤其在泛化和长任务方面表现出色。|Sergey Levine Team|[2602.13193](http://arxiv.org/abs/2602.13193)|null|
|**2026-02-13**|**Implicit-Scale 3D Reconstruction for Multi-Food Volume Estimation from Monocular Images**|现有膳食评估方法多依赖单图像分析或基于外观的推断，缺乏明确几何推理且对尺度模糊敏感，难以在真实用餐场景中准确估计食物份量。本研究提出了Implicit-Scale 3D Reconstruction from Monocular Multi-Food Images基准数据集，将食物份量估计重构为单目观测下的隐式尺度3D重建问题，通过餐盘、餐具等上下文线索而非显式度量来推断尺度，并着重于复杂的多食物场景。实验结果显示，几何重建方法相比强大的视觉语言基线，在准确性和鲁棒性上均有提升，最佳方法在体积估计上达到了0.21 MAPE，几何精度为5.7 L1 Chamfer Distance。|Jiangpeng He Team|[2602.13041](http://arxiv.org/abs/2602.13041)|**[link](https://www.kaggle.com/competitions/3d-reconstruction-from-monocular-multi-food-images/data)**|
|**2026-02-13**|**Training-Free Acceleration for Document Parsing Vision-Language Model with Hierarchical Speculative Decoding**|文档解析是多模态理解中的核心任务，但基于视觉语言模型(VLM)的端到端方法在处理长文档时，常因自回归生成长序列而导致显著的推理延迟。针对这一问题，本研究提出了一种免训练且高效的加速方法，借鉴推测解码的思想，使用轻量级文档解析流水线作为草稿模型预测未来批次token，并由更精确的VLM并行验证。此外，该方法还利用文档的布局结构将页面划分为独立区域进行并行解码。实验结果表明，该方法在通用OmniDocBench上为dots.ocr模型提供了2.42倍的无损加速，在长文档解析任务上加速高达4.89倍。|Lianwen Jin Team|[2602.12957](http://arxiv.org/abs/2602.12957)|null|
|**2026-02-13**|**HoRAMA: Holistic Reconstruction with Automated Material Assignment for Ray Tracing using NYURay**|下一代无线网络需要精确的特定站点确定性信道传播预测，而无线射线追踪(RT)依赖高精度3D环境模型和材料属性，手动建模耗时且传统视觉3D重建方法缺乏RT兼容性。为此，本研究提出了HoRAMA（Holistic Reconstruction with Automated Material Assignment）系统，该系统能利用智能手机捕获的RGB视频，结合MASt3R-SLAM的密集点云生成和视觉语言模型辅助的材料分配，自动生成RT兼容的3D模型。实验结果表明，HoRAMA的射线追踪预测在匹配多径分量功率预测方面与手动创建的3D模型基线表现相当（2.28 dB RMSE vs 2.18 dB），同时将3D重建时间从两个月缩短到16小时，显著提高了效率。|Theodore S. Rappaport Team|[2602.12942](http://arxiv.org/abs/2602.12942)|null|
|**2026-02-13**|**RoadscapesQA: A Multitask, Multimodal Dataset for Visual Question Answering on Indian Roads**|理解道路场景对自动驾驶至关重要，但现有数据集可能无法充分覆盖印度复杂多样的驾驶环境。本研究推出了Roadscapes，一个多任务多模态数据集，包含多达9,000张在印度不同驾驶环境中拍摄的图像，并附带手动验证的边界框。该数据集利用基于规则的启发式方法推断场景属性，并生成用于对象定位、推理和场景理解的问答对，涵盖了印度城市和乡村的多种昼夜场景。Roadscapes旨在推动非结构化环境中视觉场景理解的研究，并提供了使用视觉语言模型进行图像问答任务的初步基线。|Jyothikamalesh S Team|[2602.12877](http://arxiv.org/abs/2602.12877)|null|
|**2026-02-13**|**Thinking Like a Radiologist: A Dataset for Anatomy-Guided Interleaved Vision Language Reasoning in Chest X-ray Interpretation**|放射学诊断涉及视觉检查与语言推理的反复交织，但现有医学大型视觉语言模型(LVLMs)多依赖纯文本思维链推理，易产生幻觉，且现有伪视觉解决方案仍缺乏丰富的视觉细节。为此，本研究提出了MMRad-IVL-22K，这是首个专为胸部X射线解读中原生交织的视觉语言推理设计的大规模数据集，反映了放射科医生反复推理和视觉检查的工作流程。实验结果表明，多模态思维链引导的报告生成在临床准确性和报告质量方面显著优于纯文本思维链（RadGraph指标提高6%），证实高保真交织视觉语言证据是可靠医疗AI不可替代的组成部分。在MMRad-IVL-22K上微调的模型在推理一致性和报告质量方面也优于通用和医学专用LVLMs。|Wei Shen Team|[2602.12843](http://arxiv.org/abs/2602.12843)|null|
|**2026-02-13**|**X-SYS: A Reference Architecture for Interactive Explanation Systems**|可解释AI (XAI) 方法虽多，但将其部署为交互式系统仍面临挑战，因其需要兼顾算法与系统能力以维持解释可用性。本研究将可解释性视为一个信息系统问题，提出了X-SYS，一个交互式解释系统的参考架构。X-SYS围绕STAR（可伸缩性、可追溯性、响应性、适应性）四个质量属性，并指定了包含XUI服务、解释服务等五个组件的分解结构，将交互模式映射到系统能力以解耦用户界面和后端计算。通过SemanticLens系统实现X-SYS，展示了其如何通过契约化服务边界实现独立演进、通过离线/在线分离确保响应性以及通过持久状态管理支持可追溯性，为在操作约束下设计交互式解释系统提供了可重用蓝图和具体实例。|Sebastian Lapuschkin Team|[2602.12748](http://arxiv.org/abs/2602.12748)|null|
|**2026-02-13**|**SignScene: Visual Sign Grounding for Mapless Navigation**|导航标志能帮助人类在陌生环境中无地图导航，但机器人如何利用标志进行无地图导航是一个挑战，核心在于如何解释复杂多样的标志及其抽象语义内容，并将其与局部3D场景匹配。本研究将此形式化为“标志接地”(sign grounding)问题，即把标志上的语义指令映射到对应的场景元素和导航动作。研究者利用视觉语言模型(VLMs)的语义常识和推理能力，并提出了SignScene，一种以标志为中心的空-语义表示，旨在以利于VLM有效推理的形式呈现导航相关场景元素和标志信息。在包含九种环境类型、114个查询的数据集上，该方法实现了88%的接地准确率，显著优于基线，并成功使Spot机器人在真实世界中仅依赖标志进行无地图导航。|David Hsu Team|[2602.12686](http://arxiv.org/abs/2602.12686)|null|
|**2026-02-13**|**IndicFairFace: Balanced Indian Face Dataset for Auditing and Mitigating Geographical Bias in Vision-Language Models**|视觉语言模型(VLMs)常从训练数据中继承并放大社会偏见，印度群体尤其被错误代表，现有公平性数据集将印度视为单一类别，忽视了其内部地理多样性。为解决这一局限，本研究提出了IndicFairFace，一个包含14,400张图像的新颖且平衡的人脸数据集，旨在代表印度的地理多样性，图像伦理获取并在各邦和性别间均匀平衡。通过IndicFairFace，研究量化了基于CLIP的VLM中存在的印度国内地理偏见，并利用后验迭代零空间投影去偏方法成功减少了这种偏见。实验证明，该去偏方法对现有嵌入空间的影响很小，基准数据集上的检索准确率平均下降不到1.5%，确立了IndicFairFace作为研究印度背景下VLM地理偏见的第一个基准。|Jiechao Gao Team|[2602.12659](http://arxiv.org/abs/2602.12659)|null|
|**2026-02-13**|**PISHYAR: A Socially Intelligent Smart Cane for Indoor Social Navigation and Multimodal Human-Robot Interaction for Visually Impaired People**|现有智能助行设备多侧重物理导航，但缺乏社交智能和多模态人机交互能力。本研究提出了PISHYAR，一款结合社交感知导航和多模态人机交互的智能拐杖。该系统包含社交导航框架（集成RGB-D感知、对象检测、活动识别、路径规划和触觉反馈）和代理式多模态LLM-VLM交互框架（集成语音识别、VLM、LLM和文本转语音，并支持动态模式路由）。通过仿真、真实世界实验和用户研究，PISHYAR在避障和社会顺从性导航中表现可靠，整体准确率约80%，集体活动识别稳健。初步用户研究显示，视障用户对其交互框架的可用性、信任度和感知社交性给予高度评价，突显了PISHYAR作为多模态辅助移动设备在提供社交互动支持方面的潜力。|Alireza Taheri Team|[2602.12597](http://arxiv.org/abs/2602.12597)|null|
|**2026-02-13**|**On Robustness and Chain-of-Thought Consistency of RL-Finetuned VLMs**|在视觉语言模型（VLM）中，尽管强化学习（RL）微调能提升推理任务性能，但仍面临视觉基础薄弱、幻觉和过度依赖文本线索的问题。研究发现，简单的文本扰动（如误导性描述）会显著降低模型的鲁棒性和置信度，并揭示了RL微调中存在的准确性与忠实性之间的权衡。具体来说，微调提高了基准准确性，却可能损害推理链的可靠性和模型对上下文变化的鲁棒性。这些结果强调了仅凭准确性评估的局限性，并呼吁在训练和评估中同时关注正确性、鲁棒性以及视觉基础推理的忠实性。|Arnab Mondal Team|[2602.12506](http://arxiv.org/abs/2602.12506)|null|
|**2026-02-13**|**Layer-Specific Fine-Tuning for Improved Negation Handling in Medical Vision-Language Models**|由于视觉语言模型（VLM）在区分肯定和否定医疗陈述方面存在局限性，本研究引入了一个放射学诊断基准来系统评估VLM对极性的敏感性。为解决此问题，我们构建了一个包含结构化声明和属性级否定上下文临床否定数据集，并提出了一种名为否定感知选择性训练（NAST）的自适应方法。NAST利用因果追踪效应（CTE）根据各层对否定处理的因果贡献来调整梯度更新。实验结果表明，NAST在不损害通用视觉-语言对齐的情况下，显著提高了VLM对肯定和否定临床陈述的辨别能力，凸显了因果可解释性在安全关键医疗领域中进行有针对性模型适应的价值。|Rahmatollah Beheshti Team|[2602.12498](http://arxiv.org/abs/2602.12498)|null|
|**2026-02-12**|**Scaling Verification Can Be More Effective than Scaling Policy Learning for Vision-Language-Action Alignment**|针对视觉-语言-动作（VLA）模型在执行自然语言指令时存在的"意图-动作差距"问题，本研究探索了测试时验证方法。我们首先揭示了具身指令遵循的测试时标度定律，发现联合扩展复述指令和生成动作数量能更高效地恢复正确动作。为利用这些规律，我们提出了CoVer，一个对比验证器，并引入了“启动时计算”和分层验证推理管道。在部署时，该框架预先计算VLM生成的复述指令，生成动作候选项，然后使用验证器选择最优提示和动作块。实验结果表明，CoVer在SIMPLER基准上取得了显著提升（分布内22%，分布外13%），并在真实世界实验中进一步提升45%，在PolaRiS基准上任务进展提升14%，成功率提升9%。|Marco Pavone Team|[2602.12281](http://arxiv.org/abs/2602.12281)|null|
|**2026-02-12**|**ExStrucTiny: A Benchmark for Schema-Variable Structured Information Extraction from Document Images**|尽管通用视觉语言模型（VLM）在传统文档理解基准上表现良好，但它们在多样化文档类型和灵活模式下进行整体、细粒度结构化信息提取的能力仍未充分研究。现有数据集在实体本体、查询复杂度或文档类型上存在局限。为解决这些不足，本研究引入了ExStrucTiny，一个新的文档图像结构化信息提取（IE）基准数据集，它统一了关键实体提取、关系提取和视觉问答的特性。该数据集通过结合人工和合成并经过人工验证的样本构建，涵盖了更广泛的文档类型和提取场景。对开放和封闭式VLM在该基准上的分析揭示了模式适应、查询欠规范和答案定位等挑战，为未来改进通用模型在文档结构化信息提取方面的能力奠定了基础。|Manuela Veloso Team|[2602.12203](http://arxiv.org/abs/2602.12203)|null|
|**2026-02-12**|**3DGSNav: Enhancing Vision-Language Model Reasoning for Object Navigation via Active 3D Gaussian Splatting**|针对零样本对象导航（ZSON）中视觉语言模型（VLM）决策受低级感知准确性限制的问题，本研究提出了3DGSNav框架。3DGSNav利用3D高斯泼溅（3DGS）作为VLM的持久记忆来增强空间推理能力。通过主动感知，该框架逐步构建环境的3DGS表示，从而实现轨迹引导的、边界感知的第一人称视角的自由视点渲染。此外，研究设计了结构化视觉提示并结合思维链（CoT）提示来进一步提升VLM的推理能力。在导航过程中，实时对象检测器用于过滤潜在目标，而VLM驱动的主动视点切换则进行目标再验证，确保高效可靠的识别。在多个基准测试和真实世界四足机器人实验中，3DGSNav展现出鲁棒且具竞争力的性能。|Xinyi Yu Team|[2602.12159](http://arxiv.org/abs/2602.12159)|null|
|**2026-02-12**|**Affordance-Graphed Task Worlds: Self-Evolving Task Generation for Scalable Embodied Learning**|鉴于在真实世界中训练机器人策略成本高昂且现有生成模拟难以生成逻辑连贯的长周期任务并处理动态物理不确定性，本研究提出了Affordance-Graphed Task Worlds (AGT-World) 统一框架。该框架基于真实世界观测自主构建交互式模拟环境和机器人任务策略，通过将任务空间形式化为结构化图，实现复杂目标的精确分层分解，并引入了结合VLM推理和几何验证的自演化机制来优化策略。实验结果表明，该方法在成功率和泛化能力上显著优于现有方法，实现了提议、执行和修正的自我改进循环，以支持可扩展的机器人学习。|Changshui Zhang Team|[2602.12065](http://arxiv.org/abs/2602.12065)|null|
|**2026-02-12**|**Can Local Vision-Language Models improve Activity Recognition over Vision Transformers? -- Case Study on Newborn Resuscitation**|鉴于新生儿复苏活动准确记录的重要性及其在实践中的不足，以及现有方法在识别细粒度活动上的挑战，本研究探索了生成式AI（GenAI）方法，特别是结合大型语言模型（LLM）的局部视觉-语言模型（VLM），以改进新生儿复苏视频中的活动识别。研究将几种零样本VLM策略和微调VLM（包括LoRA）与分类头进行评估，并与监督式TimeSFormer基线进行比较。结果表明，小型VLM虽然容易产生幻觉，但经过LoRA微调后，F1分数可达0.91，显著超越TimeSFormer的0.70。|Øyvind Meinich-Bache Team|[2602.12002](http://arxiv.org/abs/2602.12002)|null|
|**2026-02-12**|**Benchmarking Vision-Language Models for French PDF-to-Markdown Conversion**|针对PDF到Markdown转换作为RAG管道关键步骤，以及现有基准多侧重英文或中文并可能过度惩罚无关格式差异的问题，本研究引入了一个专门针对法语文档、通过模型不一致采样挑选的挑战性新基准。该基准涵盖手写表格、复杂布局、密集表格和富含图形的页面，并采用单元测试式检查和类别特定归一化进行评估，以减少无关的呈现差异。实验结果显示，在15个模型中，最强的专有模型在手写和表格处理上表现出更高的鲁棒性，同时一些开源系统在标准打印布局上仍具竞争力。|Nicolas Mery Team|[2602.11960](http://arxiv.org/abs/2602.11960)|null|
|**2026-02-12**|**Are Two LLMs Better Than One? A Student-Teacher Dual-Head LLMs Architecture for Pharmaceutical Content Optimization**|针对大型语言模型（LLM）在制药等受监管领域内容创作中面临的科学准确性和合规性挑战，以及手动质量控制（QC）低效易错的问题，本研究引入了LRBTC，一个模块化的LLM和VLM驱动的QC架构。该架构涵盖语言、法规、品牌、技术和内容结构检查，并结合了师生双模型架构、人工干预（HITL）工作流和瀑布式规则过滤机制。实验结果表明，LRBTC在AIReg-Bench上取得了83.0%的F1和97.5%的召回率，相比Gemini 2.5 Pro，遗漏违规减少5倍；在CSpelling上平均准确率提高26.7%。误差分析揭示当前模型擅长检测拼写错误，但在复杂医学语法和标点错误识别方面仍有提升空间。|Anubhav Girdhar Team|[2602.11957](http://arxiv.org/abs/2602.11957)|null|
|**2026-02-12**|**LAMP: Implicit Language Map for Robot Navigation**|为解决现有视觉-语言模型（VLM）零样本导航方法在大环境中因显式存储语言向量而导致的内存需求过高和细粒度规划分辨率有限的问题，本研究提出了LAMP（Language Map）框架。该框架通过学习连续的、语言驱动的隐式神经语言场来编码语言特征，并结合稀疏图实现高效的粗略路径规划，进而通过梯度优化在学习场中细化目标姿态。该方法还采用贝叶斯框架建模嵌入不确定性以增强泛化能力，并通过图采样策略扩展到大环境。实验证明，LAMP在内存效率和细粒度目标到达精度方面均优于现有显式方法。|Sunwook Choi Team|[2602.11862](http://arxiv.org/abs/2602.11862)|**[link](https://lab-of-ai-and-robotics.github.io/LAMP/)**|
|**2026-02-12**|**JEPA-VLA: Video Predictive Embedding is Needed for VLA Models**|针对现有视觉-语言-动作（VLA）模型在机器人操作中存在的样本效率低和泛化能力有限的问题，本研究认为这与被忽视的预训练视觉表征在环境理解和策略先验方面知识不足有关。研究发现，视频上预训练的预测嵌入（特别是V-JEPA 2）能有效捕捉任务相关的时态动态并过滤不可预测的环境因素，从而弥补现有视觉表征的不足。基于此，论文提出了JEPA-VLA，一种将预测嵌入自适应集成到现有VLA中的简单而有效的方法。实验结果表明，JEPA-VLA在一系列基准和真实机器人任务中均取得了显著的性能提升。|Mingsheng Long Team|[2602.11832](http://arxiv.org/abs/2602.11832)|null|
|**2026-02-12**|**Revis: Sparse Latent Steering to Mitigate Object Hallucination in Large Vision-Language Models**|为解决大型视觉-语言模型（LVLMs）普遍存在的物体幻觉问题，该问题源于视觉特征和预训练文本表示在网络深层中的纠缠，本研究提出了REVIS，一个无需训练的框架。REVIS基于潜在空间几何原理，通过正交投影精确提取纯视觉信息向量，并采用校准策略，仅在视觉信息被抑制的精确深度进行稀疏干预，从而有效恢复视觉信息。经验评估结果显示，REVIS与最先进的基线相比，将目标幻觉率降低了约19%，同时保持了模型原有的通用推理能力。|Zhou Yang Team|[2602.11824](http://arxiv.org/abs/2602.11824)|null|
|**2026-02-12**|**Adaptive Debiasing Tsallis Entropy for Test-Time Adaptation**|现有主流的视觉-语言模型测试时自适应（TTA）方法依赖香农熵（SE）衡量预测不确定性，但由于预训练数据偏差，SE会产生有偏的不确定性估计。为解决此问题，研究发现并证明Tsallis熵（TE）通过引入非广延参数q，天然适合表征有偏分布。在此基础上，本文提出了自适应去偏Tsallis熵（ADTE），为每个类别定制类特定的q^l参数，该参数通过对持续传入的测试实例估计标签偏差进行归一化得到。实验结果表明，ADTE在ImageNet及其五种变体上超越了最先进的方法，并在10个跨域基准测试中取得了最高的平均性能，证实了其有效性。|Jianfeng Lu Team|[2602.11743](http://arxiv.org/abs/2602.11743)|null|
|**2026-02-12**|**Adapting Vision-Language Models for E-commerce Understanding at Scale**|电商产品理解需要文本、图像和结构化属性等多模态的强大理解能力。通用视觉-语言模型（VLMs）虽能提供泛化能力，但如何将其有效地适应电商数据中以属性为中心、多图像和噪声多的特性，同时不牺牲通用性能，是一个未被充分探索的问题。本文通过大规模实验研究，展示了对通用VLM进行有针对性的适配，可以在大幅提升电商任务性能的同时，保持其广泛的多模态能力。此外，本文还提出了一个新颖且全面的评估套件，涵盖了深度产品理解、严格指令遵循和动态属性提取等任务。|Shahram Khadivi Team|[2602.11733](http://arxiv.org/abs/2602.11733)|null|
|**2026-02-12**|**STVG-R1: Incentivizing Instance-Level Reasoning and Grounding in Videos via Reinforcement Learning**|在视觉-语言模型（VLMs）中，文本描述与视觉坐标之间的不匹配常导致幻觉问题，在时空视频定位（STVG）等密集预测任务中尤为突出。现有方法通常增强视觉-文本对齐或添加辅助解码器，但这会引入额外的可训练模块，导致高昂的标注和计算成本。本文提出了一种新颖的视觉提示范式，通过将每帧坐标预测重新表述为实例级识别问题，为每个对象分配唯一且时间一致的ID，并将其嵌入视频作为视觉提示。此外，研究引入了STVG-R1，这是首个用于STVG的强化学习框架，通过任务驱动奖励联合优化时间精度、空间一致性和结构化格式正则化。实验结果表明，STVG-R1在六个基准测试中表现出色，在HCSTVG-v2上m_IoU比基线Qwen2.5-VL-7B高出20.9%，达到SOTA，并展现出强大的零样本泛化能力。|Qing Li Team|[2602.11730](http://arxiv.org/abs/2602.11730)|null|
|**2026-02-12**|**ScalSelect: Scalable Training-Free Multimodal Data Selection for Efficient Visual Instruction Tuning**|大规模视觉指令微调（VIT）是提升视觉-语言模型（VLMs）在多模态任务中性能的关键范式，但其在大型数据集上的训练因数据冗余而计算昂贵且低效。现有的数据选择方法要么需要昂贵的训练或梯度计算，要么依赖代理模型或指令无关表示且复杂度高，限制了可扩展性。本文提出了ScalSelect，一种可扩展的无训练多模态数据选择方法，其复杂度与样本数量呈线性关系，无需外部模型或辅助数据集。ScalSelect通过提取指令令牌在目标VLM中最受关注的视觉特征来构建样本表示，然后识别其表示最能近似整个数据集主导子空间的样本，从而实现无需成对比较的可扩展重要性评分。广泛的实验表明，ScalSelect仅使用16%的数据即可达到全数据集训练97.5%以上的性能，在某些设置下甚至超越了全数据训练。|Kai Chen Team|[2602.11636](http://arxiv.org/abs/2602.11636)|**[link](https://github.com/ChangtiWu/ScalSelect}{ScalSelect})**|
|**2026-02-12**|**SkillRater: Untangling Capabilities in Multimodal Data**|传统数据筛选方法通常只给样本分配一个单一的质量分数，但这在模型需要学习多种不同能力时显得局限。本文认为质量应是多维的，每个维度对应模型需掌握的一种能力。为此，我们提出了SkillRater框架，将数据过滤分解为多个专业的评估器，每个评估器负责一种能力，并通过元学习在独立验证目标上进行训练。这些评估器的分数通过渐进式选择规则组合：在每个训练阶段，只要任一评估器将样本排名高于随时间收紧的阈值，该样本即被保留，以在早期保持多样性，后期聚焦高价值样本。在视觉语言模型上的验证表明，SkillRater在视觉理解、OCR和STEM推理能力上均显著优于未过滤基线，且学习到的评估器信号几乎正交，证实了多维度分解的有效性。|Akshat Shrivastava Team|[2602.11615](http://arxiv.org/abs/2602.11615)|null|
|**2026-02-12**|**Chatting with Images for Introspective Visual Thinking**|当前大型视觉-语言模型（LVLMs）通常依赖基于单次视觉编码的纯文本推理，这常导致细粒度视觉信息丢失，尤其是在处理跨区域或多图像的视觉语义或几何关系推理时。为解决这些挑战，本文提出了“与图像对话”这一新框架，将视觉操作重新定义为语言引导的特征调制。在该框架下，模型在富有表现力的语言提示指导下，动态地对多个图像区域进行联合再编码，从而实现语言推理与视觉状态更新之间更紧密的耦合。我们通过ViLaVT实例化了这一范式，它是一个配备动态视觉编码器的新型LVLM，并采用两阶段课程（监督微调和强化学习）进行训练。广泛的实验表明，ViLaVT在八个基准测试中取得了显著且一致的改进，尤其在复杂的多图像和基于视频的空间推理任务上表现突出。|Tieniu Tan Team|[2602.11073](http://arxiv.org/abs/2602.11073)|null|
|**2026-02-12**|**Self-Refining Vision Language Model for Robotic Failure Detection and Reasoning**|针对机器人系统故障推理面临的挑战，即真实世界故障的复杂性及丰富推理标签获取成本高昂，本研究提出了ARMOR模型。ARMOR将故障检测与自然语言推理视为一个多任务自完善过程，通过迭代预测和基于历史输出的条件推理进行训练。它利用大规模稀疏二元标签和少量丰富推理标注的异构监督，并通过离线和在线模仿学习进行优化。在推理阶段，ARMOR生成多条完善轨迹并利用自确定性指标选择最可靠的预测。实验结果显示，ARMOR在故障检测率上比现有方法提升了30%，在LLM模糊匹配分数衡量的推理能力上提升了100%，证明了其在异构监督下的鲁棒性及超越预定义故障模式的开放式推理能力。|Yesh Dattatreya Team|[2602.12405](http://arxiv.org/abs/2602.12405)|null|
|**2026-02-12**|**What does RL improve for Visual Reasoning? A Frankenstein-Style Analysis**|为了理解强化学习（RL）在视觉语言模型（VLM）视觉推理中相比监督微调（IN）的具体贡献，本研究提出了一种“弗兰肯斯坦式”分析框架。该框架通过因果探测定位功能、参数比较表征更新以及模型合并测试可迁移性。研究发现，RL主要通过在模型的中间到后期层诱导一致的推理时段转移来提升性能，并且这些中间到后期的优化对于RL的性能增益是可迁移和必要的。这些结果表明，RL对视觉推理的可靠贡献并非统一增强视觉感知，而是系统地细化了Transformer中间到后期层的计算，从而改善了视觉到推理的对齐和推理性能，揭示了仅通过基准评估来理解多模态推理改进的局限性。|Tianyi Zhou Team|[2602.12395](http://arxiv.org/abs/2602.12395)|null|
|**2026-02-12**|**Synthetic Image Detection with CLIP: Understanding and Assessing Predictive Cues**|鉴于现代生成模型能产生近乎真实的照片，合成图像检测（SID）的泛化能力及其在实际应用中的表现面临挑战，特别是在新生成模型面前。本研究旨在探究CLIP作为SID基础模型的有效性及其内在线索。为此，我们构建了SynthCLIC数据集以减少语义偏差，并利用可解释的线性分类器和文本概念模型分析CLIP特征。结果显示，CLIP检测器在GAN基准上表现优异，但在高质量扩散数据集SynthCLIC上性能略有下降，且跨不同生成器家族的泛化能力显著受限。研究发现，检测器主要依赖高层摄影属性而非明显的生成器伪影。这些发现强调了持续模型更新和更广泛训练暴露的必要性，并肯定了CLIP作为更通用、鲁棒SID方法的强大基础。|Michael Graber Team|[2602.12381](http://arxiv.org/abs/2602.12381)|null|
|**2026-02-12**|**ForeAct: Steering Your VLA with Efficient Visual Foresight Planning**|在开放世界环境中，视觉-语言-动作（VLA）模型将高级语言指令转换为具体可执行动作面临挑战。本研究提出了视觉预见规划器（ForeAct），一个通用高效的规划器，它通过想象未来观察和子任务描述来逐步指导VLA。ForeAct包含一个高效的预见图像生成模块，能从当前视觉输入和语言指令在0.33秒内预测高质量的未来观察，并结合一个视觉-语言模型生成子任务描述。最先进的VLA模型无需修改架构，只需通过增强视觉输入即可无缝集成ForeAct。预见生成器在超过100万个多任务、跨具身情景中进行预训练，学习了鲁棒的具身动力学。在包含11个真实世界任务的基准测试中，ForeAct取得了87.4%的平均成功率，比基线模型有显著提升。|Song Han Team|[2602.12322](http://arxiv.org/abs/2602.12322)|null|
|**2026-02-12**|**LatentAM: Real-Time, Large-Scale Latent Gaussian Attention Mapping via Online Dictionary Learning**|为解决开放词汇机器人感知中从流式RGB-D观测构建可扩展潜在特征地图的挑战，并克服传统VLM嵌入方法缺乏通用性和依赖预训练的问题，本研究提出了LatentAM框架。LatentAM是一种在线3D高斯泼溅（3DGS）映射框架，它采用模型无关且无需预训练的在线字典学习方法，实现了与不同VLM的即插即用集成。该方法将高斯基元与紧凑查询向量关联，通过带有可学习字典的注意力机制转换为近似VLM嵌入。字典在流式观测中高效初始化并在线优化，同时结合基于体素哈希的地图管理策略以实现大规模环境下的GPU内存有界使用。实验结果表明，LatentAM在特征重建保真度上显著优于现有方法，并在评估数据集上实现了接近实时的速度（12-35 FPS）。|Yulun Tian Team|[2602.12314](http://arxiv.org/abs/2602.12314)|null|
|**2026-02-11**|**Hierarchical Concept Embedding & Pursuit for Interpretable Image Classification**|可解释设计模型在计算机视觉中日益受到关注，因为它们能为其预测提供忠实的解释。在图像分类中，这类模型通常从图像中提取人类可解释的概念并用于分类。稀疏概念恢复方法利用视觉-语言模型的潜在空间将图像嵌入表示为概念嵌入的稀疏组合，但这类方法忽视了概念的层次结构，可能导致预测正确但解释与层次结构不一致。为解决此问题，本文提出了分层概念嵌入与追踪（HCEP）框架，它在潜在空间中诱导概念嵌入的层次结构，并利用分层稀疏编码来恢复图像中存在的概念。实验结果表明，HCEP在概念精度和召回率上均优于基线，同时保持了有竞争力的分类准确率，并且在样本数量有限时展现出卓越的分类和概念恢复性能，证明了将层次结构融入稀疏编码能产生更可靠和可解释的图像分类模型。|René Vidal Team|[2602.11448](http://arxiv.org/abs/2602.11448)|null|
|**2026-02-11**|**Beyond VLM-Based Rewards: Diffusion-Native Latent Reward Modeling**|扩散模型和流匹配模型的偏好优化需要既具有判别鲁棒性又计算高效的奖励函数。视觉-语言模型（VLMs）已成为主要的奖励提供者，利用其丰富的多模态先验来指导对齐。然而，它们的计算和内存成本很高，并且通过像素空间奖励优化潜在扩散生成器会引入域不匹配，使对齐复杂化。本文提出了DiNa-LRM，一种扩散原生的潜在奖励模型，它直接在噪声扩散状态上构建偏好学习。DiNa-LRM引入了一种噪声校准的Thurstone似然，其不确定性依赖于扩散噪声，并利用预训练的潜在扩散骨干网络与时间步条件奖励头。实验结果显示，DiNa-LRM在图像对齐基准测试中显著优于现有基于扩散的奖励基线，并以极低的计算成本实现了与最先进VLM相当的性能，显著改善了偏好优化动态，实现了更快、更资源高效的模型对齐。|Wenhan Luo Team|[2602.11146](http://arxiv.org/abs/2602.11146)|**[link](https://github.com/HKUST-C4G/diffusion-rm)**|
|**2026-02-11**|**Active Zero: Self-Evolving Vision-Language Models through Active Environment Exploration**|自博弈已使大型语言模型（LLMs）通过自生成挑战实现自主提升。然而，现有视觉-语言模型（VLMs）的自博弈方法依赖与静态图像集的被动交互，导致对初始数据集的强依赖和学习效率低下。为解决这些局限性，本文提出了Active-Zero框架，将模型学习从被动交互转向主动探索视觉环境。Active-Zero采用三个共同进化的智能体：一个Searcher根据模型能力前沿从开放世界存储库检索图像；一个Questioner合成校准的推理任务；一个Solver通过准确性奖励进行优化。这种闭环机制实现了自我脚手架的自动课程学习。在Qwen2.5-VL-7B-Instruct的12个基准测试中，Active-Zero在推理任务上平均准确率达到53.97%（提升5.7%），在通用理解上达到59.77%（提升3.9%），持续优于现有自博弈基线，表明主动探索是可扩展和自适应自进化视觉-语言系统的关键要素。|Tat-Seng Chua Team|[2602.11241](http://arxiv.org/abs/2602.11241)|null|
|**2026-02-11**|**Safe mobility support system using crowd mapping and avoidance route planning using VLM**|自主移动机器人在动态、尤其是拥挤环境中安全有效导航仍面临挑战。本文提出了一种新颖的框架，该框架集成了视觉-语言模型（VLM）和高斯过程回归（GPR），用于生成动态人群密度图（“抽象图”），以实现自主机器人导航。我们的方法利用VLM识别抽象环境概念（如人群密度）的能力，并通过GPR以概率方式表示这些概念。在大学校园进行的真实世界试验结果表明，机器人成功地生成了避开静态障碍物和动态人群的路径，从而增强了导航的安全性和适应性。|Koichi Ozaki Team|[2602.10910](http://arxiv.org/abs/2602.10910)|null|

<p align=right>(<a href=#updated-on-20260216>back to top</a>)</p>

## VLA

|Publish Date|Title|Chinese Summary|Authors|PDF|Code|
|---|---|---|---|---|---|
|**2026-02-13**|**Steerable Vision-Language-Action Policies for Embodied Reasoning and Hierarchical Control**|现有预训练视觉-语言模型（VLMs）在机器人控制中面临知识落地挑战，传统的分层方法（VLM负责高层指令，VLA负责低层策略）受限于自然语言接口，限制了VLM推理对低层行为的引导。为解决此问题，本研究提出了“可控策略”（Steerable Policies），即在多种抽象层次（如子任务、动作、像素坐标）的丰富合成指令上训练VLA。通过提升低层可控性，该方法能够解锁VLM的预训练知识，从而增强任务泛化能力。在大量真实世界操作实验中，这些可控策略结合学习到的高层具身推理器和通过上下文学习进行抽象推理的VLM，均显著优于现有具身推理VLA和基于VLM的分层基线模型，尤其在具有挑战性的泛化和长时程任务上表现突出。|Sergey Levine Team|[2602.13193](http://arxiv.org/abs/2602.13193)|null|
|**2026-02-13**|**UniManip: General-Purpose Zero-Shot Robotic Manipulation with Agentic Operational Graph**|通用机器人操作面临语义意图与低层物理交互难以在非结构化环境中无缝衔接的挑战，现有方法在零样本泛化方面表现不佳，端到端视觉-语言-动作（VLA）模型缺乏长时程任务所需的精度，而传统分层规划器在开放世界变体中语义僵化。本文提出了UniManip框架，其基于双层智能体操作图（AOG），融合了语义推理和物理接地。该系统将用于任务编排的高层智能体层与用于动态状态表示的低层场景层相结合，持续使抽象规划与几何约束对齐，实现了稳健的零样本执行。通过大量实验，UniManip在未见物体和任务上展现出强大的零样本能力，相比现有VLA和分层基线模型，成功率分别提高了22.5%和25.0%，并实现了从固定基座到移动操作的零样本迁移。|Ziwei Wang Team|[2602.13086](http://arxiv.org/abs/2602.13086)|**[link](https://henryhcliu.github.io/unimanip)**|
|**2026-02-13**|**Learning Native Continuation for Action Chunking Flow Policies**|动作分块能使视觉-语言-动作（VLA）模型实时运行，但朴素的分块执行常在边界处产生不连续性。实时分块（RTC）虽能缓解此问题，但其作为外部机制，导致虚假的多模态切换和非本质平滑的轨迹。为此，本文提出了Legato，一种针对动作分块流式VLA策略的训练时连续性方法。Legato通过将去噪过程初始化为已知动作和噪声的调度混合，使模型接触部分动作信息，并重塑学习到的流动力学以确保训练和推理过程中的一致性。实验结果表明，Legato能生成更平滑的轨迹，减少执行中的虚假多模态切换，从而降低犹豫并缩短任务完成时间。在五项真实世界操作任务中，Legato持续优于RTC，在轨迹平滑度和任务完成时间上均提升约10%。|Yang Gao Team|[2602.12978](http://arxiv.org/abs/2602.12978)|**[link](https://lyfeng001.github.io/Legato/)**|
|**2026-02-13**|**ALOE: Action-Level Off-Policy Evaluation for Vision-Language-Action Model Post-Training**|在大规模基础视觉-语言-动作（VLA）系统中，通过在线强化学习（RL）进行性能提升时，价值函数估计是核心。由于训练数据混合了历史策略和人类干预，价值函数估计本质上是一个离策略评估问题，而现有方法常采用保守的同策略估计，限制了学习效率。本文提出了ALOE，一个用于VLA后训练的动作级离策略评估框架。ALOE利用基于分块的时序差分自举法来评估单个动作序列而非最终任务结果。该设计在稀疏奖励下能更有效地进行关键动作分块的信用分配，并支持稳定的策略改进。在三项真实世界操作任务中，ALOE在不影响执行速度的前提下提高了学习效率，验证了离策略强化学习在VLA后训练中可稳定重新引入。|Maoqing Yao Team|[2602.12691](http://arxiv.org/abs/2602.12691)|null|
|**2026-02-13**|**SignScene: Visual Sign Grounding for Mapless Navigation**|人类利用导航标志在陌生环境中无需地图即可导航，但机器人尚不具备此能力。解释真实世界标志的挑战在于其多样性、复杂性以及将抽象语义内容与局部3D场景进行接地。本研究将此问题形式化为标志接地。为解决此问题，本文提出了SignScene，一种以标志为中心的空时语义表示，能捕捉导航相关的场景元素和标志信息，并以利于有效推理的形式呈现给视觉-语言模型（VLMs）。实验结果表明，该接地方法在包含九种多样环境的114个查询数据集中取得了88%的接地准确率，显著优于基线。此外，该方法还使Spot机器人在真实世界中仅依靠标志实现了无地图导航。|David Hsu Team|[2602.12686](http://arxiv.org/abs/2602.12686)|null|
|**2026-02-13**|**Xiaomi-Robotics-0: An Open-Sourced Vision-Language-Action Model with Real-Time Execution**|本报告介绍了Xiaomi-Robotics-0，一个为高性能、快速、平滑实时执行而优化的先进视觉-语言-动作（VLA）模型。该方法的关键在于精心设计的训练配方和部署策略。Xiaomi-Robotics-0首先通过大规模跨具身机器人轨迹和视觉-语言数据进行预训练，赋予其广泛和可泛化的动作生成能力，同时避免底层预训练VLM的视觉-语义知识的灾难性遗忘。在后训练阶段，我们提出了多种技术来训练VLA模型以实现异步执行，从而解决真实机器人部署中的推理延迟问题。部署时，模型精确对齐连续预测动作块的时间步，确保实时执行的连续性和无缝性。广泛的模拟基准测试和两项真实机器人灵巧双臂操作任务的实验表明，该方法在所有模拟基准上均达到了最先进的性能，并在真实机器人（消费级GPU）上实现了快速平滑的部署，成功率和吞吐量均较高。|Quanyun Zhou Team|[2602.12684](http://arxiv.org/abs/2602.12684)|**[link](https://xiaomi-robotics-0.github.io)**|
|**2026-02-13**|**RLinf-Co: Reinforcement Learning-Based Sim-Real Co-Training for VLA Models**|模拟能够以低成本方式丰富视觉-语言-动作（VLA）模型的训练，但多数模拟-现实协同训练方法依赖于监督微调（SFT），将模拟视为静态数据源，未能利用大规模闭环交互，从而限制了现实世界的收益和泛化能力。为此，本文提出了RL-Co（基于强化学习的模拟-现实协同训练）框架，该框架采用两阶段设计：首先，通过对真实和模拟演示数据混合进行SFT来预热策略；然后，在模拟环境中通过强化学习进行微调，并辅以真实世界数据的辅助监督损失来锚定策略并减轻灾难性遗忘。在四项真实世界桌面操作任务上，RL-Co在OpenVLA和 $π_{0.5}$ 两种VLA架构上均持续优于仅现实微调和基于SFT的协同训练，显著提升了现实世界成功率、对未见任务变体的泛化能力以及现实世界数据效率。|Yu Wang Team|[2602.12628](http://arxiv.org/abs/2602.12628)|null|
|**2026-02-13**|**CRAFT: Adapting VLA Models to Contact-rich Manipulation via Force-aware Curriculum Fine-tuning**|视觉-语言-动作（VLA）模型在执行通用指令方面能力强大，但在需要精确对齐、稳定接触和处理可变形物体的接触密集型操作任务中表现不佳。主要挑战在于高熵的视觉和语言输入与低熵但关键的力信号之间的不平衡，这往往导致过度依赖感知和不稳定的控制。为解决此问题，本文提出了CRAFT框架，它是一种力感知课程微调框架，通过集成变分信息瓶颈模块在早期训练中调节视觉和语言嵌入，鼓励模型优先处理力信号，然后逐步恢复对完整多模态信息的访问。为实现力感知学习，研究设计了同源主从遥操作系统以收集同步的视觉、语言和力数据。真实世界实验表明，CRAFT持续提高了任务成功率，泛化至未见物体和新任务变体，并有效适应了各种VLA架构，从而实现了鲁棒且可泛化的接触密集型操作。|Jingtao Sun Team|[2602.12532](http://arxiv.org/abs/2602.12532)|null|
|**2026-02-12**|**Scaling Verification Can Be More Effective than Scaling Policy Learning for Vision-Language-Action Alignment**|通用机器人理解并执行自然语言指令是长期愿景，尽管视觉-语言-动作（VLA）模型已取得显著进展，但其生成动作仍可能与指令不符。为缩小“意图-动作差距”，本文研究了测试时验证方法。通过分析具身指令遵循的测试时缩放定律，发现联合缩放复述指令和生成动作的数量能更有效地增加测试时样本多样性。在此基础上，提出了CoVer，一种用于VLA对齐的对比验证器，该架构能随计算资源和数据的增加而良好扩展。进一步引入“启动时计算”和分层验证推理流程：在部署时预计算多样化的复述指令，为每条指令重复生成动作候选，然后使用验证器选择最优高层提示和低层动作块。实验表明，相比扩展策略预训练，CoVer在SIMPLER基准上实现了22%的分布内和13%的分布外增益，并在真实世界实验中进一步提高了45%，在PolaRiS基准上任务进度和成功率分别提升了14%和9%。|Marco Pavone Team|[2602.12281](http://arxiv.org/abs/2602.12281)|null|
|**2026-02-12**|**GigaBrain-0.5M*: a VLA That Learns From World Model-Based Reinforcement Learning**|针对现有视觉-语言-动作（VLA）模型在场景理解和未来预测方面的局限性，本研究提出GigaBrain-0.5M*，一个通过世界模型强化学习训练的VLA模型。该模型基于已在大量机器人操作数据上预训练的GigaBrain-0.5，并整合了RAMP（Reinforcement learning via world Model-conditioned Policy）以实现鲁棒的跨任务适应。实验结果表明，RAMP在RECAP基线之上取得了显著的性能提升，在洗衣折叠、箱子包装和咖啡制作等挑战性任务上提升约30%，并在真实部署中展示了可靠的长期执行能力，能够无故障完成复杂的操纵任务。|Zheng Zhu Team|[2602.12099](http://arxiv.org/abs/2602.12099)|**[link](https://gigabrain05m.github.io/)**|
|**2026-02-12**|**VLAW: Iterative Co-Improvement of Vision-Language-Action Policy and World Model**|为提升视觉-语言-动作（VLA）模型性能和可靠性，并解决真实世界数据收集成本高及现有世界模型物理保真度不足的问题，本研究提出一个迭代改进算法。该算法利用少量真实世界试错数据提高世界模型的保真度，然后世界模型生成补充合成数据以改进VLA模型。在真实机器人上的实验表明，该方法使先进VLA模型的成功率相较于基础策略绝对提升39.2%，并且通过生成的合成试错数据训练，又额外提升了11.6%。|Chelsea Finn Team|[2602.12063](http://arxiv.org/abs/2602.12063)|null|
|**2026-02-12**|**HoloBrain-0 Technical Report**|针对基础模型研究与可靠机器人真实部署之间的差距，本研究提出了HoloBrain-0，一个全面的视觉-语言-动作（VLA）框架。其核心是一个新颖的VLA架构，通过明确整合机器人具身先验（如多视角相机参数和运动学描述）来增强3D空间推理并支持多样化具身形态。该设计通过“预训练后微调”范式验证，在多个仿真基准和长时程真实世界操作任务中取得领先结果，且一个高效的0.2B参数变体能支持低延迟部署。研究还全面开源了HoloBrain生态系统，旨在加速研究和实际应用。|Zhizhong Su Team|[2602.12062](http://arxiv.org/abs/2602.12062)|null|
|**2026-02-12**|**When would Vision-Proprioception Policies Fail in Robotic Manipulation?**|针对视觉-本体感受策略在机器人运动过渡阶段视觉模态作用受限、策略倾向于利用本体感受信号导致视觉学习受抑制的问题，本研究提出了梯度调整与阶段引导（GAP）算法。该算法通过利用本体感受信息估计运动过渡阶段的概率，并自适应地调整本体感受梯度的幅值，以实现视觉和本体感受模态的动态协同。综合实验表明，GAP算法能够提升视觉-本体感受策略的鲁棒性和泛化性，适用于模拟和真实环境、单臂和双臂设置，并兼容多种VLA模型。|Di Hu Team|[2602.12032](http://arxiv.org/abs/2602.12032)|null|
|**2026-02-12**|**JEPA-VLA: Video Predictive Embedding is Needed for VLA Models**|针对现有视觉-语言-动作（VLA）模型存在的样本效率低和泛化能力有限问题，本研究发现其根源在于预训练视觉表示在环境理解和策略先验方面的知识不足。通过深入分析，研究指出在视频上预训练的预测嵌入，特别是V-JEPA 2，能更有效地捕捉任务相关时态动态并忽略不可预测因素，从而弥补了现有视觉表示的缺陷。在此基础上，提出了JEPA-VLA，一种将预测嵌入自适应整合到现有VLA中的方法。实验证明，JEPA-VLA在LIBERO、LIBERO-plus、RoboTwin2.0和真实机器人任务等一系列基准测试中均取得了显著性能提升。|Mingsheng Long Team|[2602.11832](http://arxiv.org/abs/2602.11832)|null|
|**2026-02-12**|**ABot-N0: Technical Report on the VLA Foundation Model for Versatile Embodied Navigation**|针对具身导航任务碎片化的问题，本研究引入了ABot-N0，一个统一的视觉-语言-动作（VLA）基础模型，旨在实现点目标、物体目标、指令遵循、兴趣点目标和人员跟踪五大核心任务的“大一统”。该模型采用分层“大脑-动作”架构，利用大型语言模型进行语义推理，并结合流匹配专家生成精确轨迹。为支持大规模学习，研究构建了ABot-N0数据引擎，整理了海量专家轨迹和推理样本。实验结果表明，ABot-N0在7个基准测试中取得了最先进的性能，并能通过集成的Agentic导航系统实现动态真实世界环境中的鲁棒、长时程任务执行。|Mu Xu Team|[2602.11598](http://arxiv.org/abs/2602.11598)|**[link](https://amap-cvlab.github.io/ABot-Navigation/ABot-N0/)**|
|**2026-02-12**|**Scaling World Model for Hierarchical Manipulation Policies**|针对视觉-语言-动作（VLA）模型在域外（OOD）设置下泛化能力不足的问题，本研究引入了一个分层VLA框架VISTA。VISTA利用大规模预训练世界模型的泛化能力实现鲁棒且可泛化的视觉子目标任务分解，其中高层世界模型规划任务分解为带有目标图像的子任务序列，低层VLA策略遵循文本和视觉指导生成动作。这些合成的目标图像为低层策略提供了视觉和物理上的详细指导，使其能够泛化到未见过的物体和新场景。实验结果表明，在世界模型生成的指导下，VISTA在大量OOD场景中显著提升了VLA性能，在novel场景中性能从14%提高到69%。|Xinghang Li Team|[2602.10983](http://arxiv.org/abs/2602.10983)|null|
|**2026-02-12**|**ForeAct: Steering Your VLA with Efficient Visual Foresight Planning**|视觉-语言-动作（VLA）模型将高级语言指令转换为具体可执行动作，在开放世界环境中尤具挑战性。本文提出了Visual Foresight Planning (ForeAct)，这是一种通用高效的规划器，通过想象的未来观察和子任务描述逐步指导VLA。该规划器包含一个高效的预测图像生成模块（在0.33秒内预测高质量未来观察）和一个视觉-语言模型，后者负责推理任务并生成子任务描述。重要的是，ForeAct能够无缝集成到现有VLA中，只需扩充视觉输入而无需修改架构。经过百万级跨具身任务的预训练，预测生成器学习了鲁棒的具身动力学。在包含11项多样化、多步骤真实世界任务的基准测试中，ForeAct实现了87.4%的平均成功率，比基线 $π_0$提高了40.9%，比带有文本子任务指导的$π_0$ 提高了30.3%。|Song Han Team|[2602.12322](http://arxiv.org/abs/2602.12322)|null|
|**2026-02-11**|**H-WM: Robotic Task and Motion Planning Guided by Hierarchical World Model**|针对现有世界模型在长时程机器人规划中累积误差的问题，以及传统符号逻辑世界模型缺乏视觉感知接地的局限性，本研究提出分层世界模型（H-WM）。H-WM在一个统一的双层框架内联合预测逻辑和视觉状态转换，将符号推理的鲁棒性与视觉观察的感知基础相结合。为训练H-WM，研究引入了一个对齐机器人运动与符号状态、动作和视觉观察的机器人数据集。实验证明，分层输出为长时程任务提供了稳定一致的中间指导，有效减轻了误差积累，并在VLA控制策略上展示了该方法的有效性和通用性。|Yingxue Zhang Team|[2602.11291](http://arxiv.org/abs/2602.11291)|null|
|**2026-02-11**|**RISE: Self-Improving Robot Policy with Compositional World Model**|针对视觉-语言-动作（VLA）模型在接触密集和动态操作任务中易受执行偏差影响的脆弱性，以及物理世界中在线强化学习（RL）的限制，本研究提出了RISE，一个通过想象进行机器人强化学习的可扩展框架。其核心是一个组合世界模型，该模型能预测多视角未来并通过进度价值模型评估想象结果，从而为策略改进提供信息丰富的优势。这些组件被整合到一个闭环自改进流水线中，在想象空间中持续生成试错并更新策略。在三个真实世界任务中，RISE相对于现有技术取得了显著性能提升，如在动态砖块分类、背包包装和盒子关闭任务中，绝对性能分别提升超过35%、45%和35%。|Hongyang Li Team|[2602.11075](http://arxiv.org/abs/2602.11075)|**[link](https://opendrivelab.com/kai0-rl/)**|
|**2026-02-11**|**RADAR: Benchmarking Vision-Language-Action Generalization via Real-World Dynamics, Spatial-Physical Intelligence, and Autonomous Evaluation**|针对现有视觉-语言-动作（VLA）模型评估主要局限于仿真或高度受限的真实世界，导致现实差距大、泛化能力差的问题，本研究提出RADAR（Real-world Autonomous Dynamics And Reasoning）基准。RADAR旨在系统评估VLA在真实条件下的泛化能力，集成了物理动力学套件、专门测试空间推理和物理理解的任务，以及基于3D指标的全自主评估流程。通过RADAR对多个先进VLA模型进行审计，发现模型在适度物理动态下性能急剧下降，例如在传感器噪声下3D IoU从0.261下降到0.068，且空间推理能力有限，揭示了模型在真实世界条件下的严重脆弱性，强调了RADAR作为可靠泛化评估基准的必要性。|Guangrun Wang Team|[2602.10980](http://arxiv.org/abs/2602.10980)|null|
|**2026-02-11**|**From Representational Complementarity to Dual Systems: Synergizing VLM and Vision-Only Backbones for End-to-End Driving**|VLA驾驶将语言功能引入端到端规划，但其核心变化除准确性-成本权衡外尚不明确。本文通过RecogDrive进行3-RQ分析，比较VLM和纯视觉骨干网络在相同规划器下的表现。研究发现VLM能引入独特子空间，导致在长尾场景下行为差异，VLM更激进而ViT更保守。基于此，本文提出HybridDriveVLA，通过学习评分器融合两者的轨迹，将PDMS提升至92.10。进一步，DualDriveVLA实现了快慢策略，仅在低置信度时调用VLM，在15%场景中调用VLM即可达到91.00 PDMS，并使吞吐量提高3.2倍。|Yan Wang Team|[2602.10719](http://arxiv.org/abs/2602.10719)|null|
|**2026-02-11**|**Say, Dream, and Act: Learning Video World Models for Instruction-Driven Robot Manipulation**|机器人操作需要预测环境对动作的响应，但现有系统常缺乏此能力，导致效率低下。鉴于VLMs无法明确预测未来状态且现有世界模型存在预测短期或空间不一致帧的问题，本文提出了一种快速且预测性的视频条件动作框架。该方法首先适配鲁棒的视频生成模型以确保未来预测的可靠性，接着通过对抗蒸馏实现快速视频生成，最后训练一个动作模型利用生成视频和真实观测纠正空间错误。实验结果表明，该方法生成的视频预测在时间连贯性和空间准确性方面表现优异，显著提升了具身一致性、空间指代能力和任务完成度。|Yanwei Fu Team|[2602.10717](http://arxiv.org/abs/2602.10717)|null|
|**2026-02-11**|**AugVLA-3D: Depth-Driven Feature Augmentation for Vision-Language-Action Models**|当前VLA模型主要依赖2D图像训练，限制了其在复杂3D环境中的空间理解和动作落地。为解决此问题，本文提出了一个将深度估计集成到VLA模型中的新框架，以丰富3D特征表示。该方法利用VGGT从RGB输入中提取3D几何线索，并引入“动作助手”模块，通过动作先验约束3D表示以确保与下游控制任务的一致性。实验结果表明，所提出的方法不仅增强了几何模糊场景中的感知，还显著提高了动作预测准确性，强调了深度驱动数据增强对弥合2D观测与3D决策差距的潜力。|F. Richard Yu Team|[2602.10698](http://arxiv.org/abs/2602.10698)|null|
|**2026-02-11**|**Improving Medical Visual Reinforcement Fine-Tuning via Perception and Reasoning Augmentation**|强化微调（RFT）在语言模型后训练中展现潜力，但在跨模态、以视觉为中心的医学影像领域应用不足，而该领域需要强大的视觉感知和结构化推理。为此，本文提出了VRFT-Aug，一个专为医学领域设计的视觉强化微调框架。VRFT-Aug通过引入先验知识注入、感知驱动策略优化、医学知情奖励塑造和行为模仿等训练策略来增强感知和推理。实验结果表明，该方法在多个医学数据集上持续优于标准监督微调和RFT基线，并提供了可推广到其他医学图像任务的实用训练启发。|Qicheng Lao Team|[2602.10619](http://arxiv.org/abs/2602.10619)|null|
|**2026-02-11**|**LAP: Language-Action Pre-Training Enables Zero-shot Cross-Embodiment Transfer**|机器人学长期目标是实现零样本部署的通用策略，但现有VLA模型与训练实体紧密耦合。针对此问题，本文提出了语言-动作预训练（LAP），一种将低级机器人动作直接用自然语言表示的方法，使动作监督与预训练VLM的输入-输出分布对齐，无需定制化设计或昂贵标注。基于LAP，本文提出了LAP-3B，实现了在未见机器人上的显著零样本迁移，平均成功率超过50%，比现有VLA模型提升约2倍。此外，LAP还支持高效适应、有利扩展，并通过统一的语言-动作格式在协同训练中获得额外收益。|Anirudha Majumdar Team|[2602.10556](http://arxiv.org/abs/2602.10556)|**[link](https://lap-vla.github.io)**|
|**2026-02-11**|**Found-RL: foundation model-enhanced reinforcement learning for autonomous driving**|强化学习在自动驾驶中面临样本效率低和语义可解释性不足问题，而基础模型（特别是VLM）虽能提供丰富知识，但高推理延迟阻碍其在RL训练中的实时部署。为此，本文提出了Found-RL平台，通过异步批量推理框架将VLM推理与仿真循环解耦，解决了延迟瓶颈。该平台引入了价值边际正则化和优势加权动作指导等监督机制，将VLM建议有效提炼到RL策略中。此外，通过条件对比动作对齐解决CLIP的动态盲区问题，实现密集奖励塑造。Found-RL使得轻量级RL模型在保持实时推理（500 FPS）的同时，达到接近十亿参数VLM的性能。|Sikai Chen Team|[2602.10458](http://arxiv.org/abs/2602.10458)|null|
|**2026-02-10**|**Hardware Co-Design Scaling Laws via Roofline Modelling for On-Device LLMs**|VLA模型在资源受限设备部署中面临LLM骨干网络选择挑战，需平衡准确性与推理延迟及硬件效率。本文提出了硬件协同设计法则，将模型训练损失建模为架构超参数的函数，并通过屋脊线模型表征推理延迟，从而建立了准确性-延迟的直接对应关系。通过对1,942个候选架构进行经验评估和训练，拟合出架构与训练损失的缩放定律。将该定律与延迟建模结合，本文确定了硬件协同设计LLM的帕累托前沿，并将架构搜索公式化为精度与性能的联合优化。结果显示，该方法将架构选择时间从数月缩短到数天，并在相同延迟下，其协同设计的架构在WikiText-2上的困惑度降低了19.42%。|Cheng Deng Team|[2602.10377](http://arxiv.org/abs/2602.10377)|null|
|**2026-02-10**|**ST4VLA: Spatially Guided Training for Vision-Language-Action Models**|大型VLM虽擅长多模态理解，但在具身任务中将指令转化为低级运动动作时表现不足。为此，本文引入了ST4VLA，一个双系统VLA框架，通过空间引导训练使动作学习与VLM中的空间先验对齐。ST4VLA包含两阶段：首先是空间基础预训练，利用大规模和机器人数据通过点、框、轨迹预测为VLM注入可迁移先验；其次是空间引导动作后训练，通过空间提示促使模型生成更丰富的空间先验以指导动作生成。实验表明，ST4VLA显著提升了Google Robot和WidowX Robot的性能，在SimplerEnv上创造了SOTA，并展现出对未见物体、转述指令的更强泛化能力及在真实世界中对扰动的鲁棒性。|Jiangmiao Pang Team|[2602.10109](http://arxiv.org/abs/2602.10109)|null|
|**2026-02-10**|**EgoHumanoid: Unlocking In-the-Wild Loco-Manipulation with Robot-Free Egocentric Demonstration**|人形机器人运动-操作任务面临数据需求高和具身差距挑战。针对此，本文提出了EgoHumanoid框架，首次利用大量以自我为中心的人类演示数据与有限机器人数据共同训练视觉-语言-动作策略，使人形机器人在多样真实环境中执行运动-操作。为弥合人类与机器人间的形态和视点差异，本文引入了包含硬件设计、数据处理的系统对齐流程，并开发了便携式人类数据收集系统。其核心是视点对齐和动作对齐，分别减少视觉域差异和将人类动作映射到机器人动作空间。真实世界实验表明，整合人类演示数据显著优于仅用机器人数据的基线（51%），尤其是在未见环境中。|Li Chen Team|[2602.10106](http://arxiv.org/abs/2602.10106)|**[link](https://opendrivelab.com/EgoHumanoid)**|

<p align=right>(<a href=#updated-on-20260216>back to top</a>)</p>

## Humanoid

|Publish Date|Title|Chinese Summary|Authors|PDF|Code|
|---|---|---|---|---|---|
|**2026-02-13**|**Adding internal audio sensing to internal vision enables human-like in-hand fabric recognition with soft robotic fingertips**|为解决机器人难以像人类一样区分织物触感的问题，本文提出一个能同时感知高空间分辨率力和高时间采样率振动的系统。该系统利用机器人手指上的Minsight触觉传感器（测量形变和力）和Minsound振动传感器（捕捉振动），并通过摩擦织物样本来收集数据。基于Transformer的方法在20种常见织物数据集上实现了97%的分类准确率，并能泛化学习织物的伸缩性、厚度和粗糙度，同时结合外部麦克风可增强在嘈杂环境中的鲁棒性。|Katherine J. Kuchenbecker Team|[2602.12918](http://arxiv.org/abs/2602.12918)|null|
|**2026-02-13**|**PMG: Parameterized Motion Generator for Human-like Locomotion Control**|针对现有类人机器人全身运动方法在适应高级命令、多样任务和处理速度姿态变化时的局限性，本文提出了参数化运动生成器（PMG）。PMG基于人类运动结构分析，利用紧凑的参数化运动数据和高维控制命令实时合成参考轨迹，并结合模仿学习和仿真到现实电机参数识别模块。实验证明，PMG在ZERITH Z1类人机器人上实现了自然、类人的运动，能精确响应高维控制输入（包括VR遥操作），并支持高效、可验证的仿真到现实迁移，为实用的类人机器人控制提供了新途径。|Houde Liu Team|[2602.12656](http://arxiv.org/abs/2602.12656)|null|
|**2026-02-12**|**General Humanoid Whole-Body Control via Pretraining and Fast Adaptation**|鉴于类人机器人全身控制器在处理多样运动、快速适应和高动态场景下稳定平衡方面的挑战，本文提出了FAST框架以实现快速适应和稳定运动跟踪。FAST通过引入Parseval引导的残差策略适应，在正交性和KL约束下学习轻量级增量动作策略，从而高效适应分布外运动并减轻灾难性遗忘。此外，提出的质心感知控制通过整合质心相关观测和目标，增强了跟踪挑战性参考运动时的平衡性。仿真和真实世界实验表明，FAST在鲁棒性、适应效率和泛化能力上均优于现有先进基线。|Zongqing Lu Team|[2602.11929](http://arxiv.org/abs/2602.11929)|null|
|**2026-02-12**|**HAIC: Humanoid Agile Object Interaction Control via Dynamics-Aware World Model**|针对人形机器人在与具有独立动力学和非完整约束的欠驱动物体交互时遇到的控制挑战，本文提出了HAIC框架。该框架无需外部状态估计，通过一个动力学预测器仅从本体感受历史估计高阶物体状态，并将其投射到静态几何先验上，形成动态占据图以推断盲区中的碰撞边界和接触可供性。实验证明，HAIC通过主动补偿惯性扰动，在敏捷任务（如滑板和推拉手推车）中实现了高成功率，并能通过预测多物体动力学，完成多物体长时任务（如携带箱子穿越不同地形）。|Renjing Xu Team|[2602.11758](http://arxiv.org/abs/2602.11758)|**[link](https://haic-humanoid.github.io/)**|
|**2026-02-12**|**Future Mining: Learning for Safety and Security**|针对AI驱动采矿环境中恶劣条件、网络物理威胁和能量受限传感器带来的安全与操作可靠性挑战，本文提出了一个统一智能安全与安保架构。该架构整合了多模态感知、安全联邦学习、强化学习、DTN通信和能量感知传感。架构包含矿工定位、多模态态势感知、后门攻击监控、可信联邦LFD和物联网设备健康监测五个核心模块。这些模块协同工作，旨在解决矿工定位、危险理解、联邦鲁棒性和预测性维护问题，构建一个在对抗条件下仍能维持运行连续性的弹性可信智能采矿系统。|Sanjay Madria Team|[2602.11472](http://arxiv.org/abs/2602.11472)|null|
|**2026-02-12**|**Humanoid Manipulation Interface: Humanoid Whole-Body Manipulation from Robot-Free Demonstrations**|当前人形机器人全身操作方法受限于硬件物流和复杂奖励工程，导致自主技能有限且通常仅限于受控环境。为解决这些问题，本文提出了Humanoid Manipulation Interface (HuMI)，一个便携高效的框架，用于在各种环境中学习多样化的全身操作任务。HuMI通过便携硬件捕捉丰富的全身运动，实现无机器人数据收集，并利用分层学习流程将人类运动转化为灵巧且可行的人形技能。广泛实验表明，HuMI的数据收集效率比遥操作提高3倍，并在未知环境中取得了70%的成功率，有效提升了人形机器人的泛化操作能力。|Yang Gao Team|[2602.06643](http://arxiv.org/abs/2602.06643)|**[link](https://humanoid-manipulation-interface.github.io)**|
|**2026-02-11**|**ExtremControl: Low-Latency Humanoid Teleoperation with Direct Extremity Control**|为解决现有类人机器人遥操作系统因重度预处理和简单PD控制导致的显著延迟问题，本文提出了ExtremControl低延迟全身控制框架。该框架直接在选定刚性链节（尤其是末端执行器）的SE(3)姿态上操作，避免了全身重定向；利用笛卡尔空间映射直接转换人类运动为机器人目标；并在低层整合速度前馈控制以支持高响应行为。ExtremControl构建的低延迟遥操作系统支持光学运动捕捉和VR跟踪，实现了低至50ms的端到端延迟，显著超越了现有方法的200ms限制，并支持乒乓球平衡和杂耍等高响应任务。|Chuang Gan Team|[2602.11321](http://arxiv.org/abs/2602.11321)|**[link](https://owenowl.github.io/extremcontrol)**|
|**2026-02-11**|**APEX: Learning Adaptive High-Platform Traversal for Humanoid Robots**|针对深度强化学习在人形机器人攀爬高于腿长平台时常收敛于不安全跳跃式方案的挑战，本文提出了APEX系统。APEX通过组合地形条件行为（攀爬、行走、站立等）实现感知驱动的攀爬式高平台穿越，并引入广义棘轮进度奖励来学习接触丰富的目标达成操作，该奖励在强安全正则化下提供密集且无速度的监督。基于LiDAR的全身操作策略通过建模映射伪影和部署时滤波修复来减少仿真到现实的感知差距。实验表明，在Unitree G1人形机器人上实现了对腿长114%平台的零样本穿越，并能鲁棒适应高度和姿态，实现多技能平稳过渡。|Ding Zhao Team|[2602.11143](http://arxiv.org/abs/2602.11143)|**[link](https://apex-humanoid.github.io/)**|
|**2026-02-11**|**Towards Learning a Generalizable 3D Scene Representation from 2D Observations**|针对现有3D占据预测方法多在相机坐标系操作、不直接适用于机器人操控的局限，本文提出了一种可泛化的神经辐射场方法。该模型能从以自我为中心的机器人观测中，在全局工作空间坐标系中构建3D占据表示，使其直接适用于机器人操纵。该方法集成了灵活的源视图，并能无需场景特定微调地泛化到未见过的物体布置。在人形机器人上的实验表明，模型在40个真实场景训练后，实现了26mm的重建误差（包括被遮挡区域），验证了其超越传统立体视觉方法、推断完整3D占据的能力。|Stefan Wermter Team|[2602.10943](http://arxiv.org/abs/2602.10943)|null|
|**2026-02-11**|**MOSAIC: Bridging the Sim-to-Real Gap in Generalist Humanoid Motion Tracking and Teleoperation with Rapid Residual Adaptation**|鉴于通用人形运动跟踪器在仿真中表现良好，但在真实硬件遥操作中常因接口和动力学误差而脆弱，本文提出了MOSAIC，一个用于多接口人形运动跟踪和全身遥操作的开源全栈系统。MOSAIC首先通过强化学习训练通用运动跟踪器，强调世界坐标系运动一致性；随后，通过快速残差适应机制，训练接口特定策略并以附加残差模块蒸馏到通用跟踪器中，以弥合仿真与真实之间的差距。系统消融实验和真实机器人部署证明，MOSAIC在实际延迟和噪声下，仍能实现鲁棒的离线运动回放和在线长时程遥操作。|Alois Knoll Team|[2602.08594](http://arxiv.org/abs/2602.08594)|null|
|**2026-02-10**|**EgoHumanoid: Unlocking In-the-Wild Loco-Manipulation with Robot-Free Egocentric Demonstration**|针对人类演示数据在人形机器人运动-操作这一数据密集型问题中潜力未充分挖掘的现状，本文提出了EgoHumanoid框架。该框架首次将大量以自我为中心的人类演示数据与有限的机器人数据共同训练视觉-语言-动作策略，使人形机器人能在多样化真实环境中执行运动-操作。为弥合人类与机器人的形态和视角差异，框架引入了从硬件设计到数据处理的系统对齐流程，包括视图对齐和动作对齐。真实世界实验表明，整合无机器人自我中心数据相比仅使用机器人数据的基线性能提升51%，尤其在未见过的环境中表现出色。|Li Chen Team|[2602.10106](http://arxiv.org/abs/2602.10106)|**[link](https://opendrivelab.com/EgoHumanoid)**|
|**2026-02-10**|**Humanoid Factors: Design Principles for AI Humanoids in Human Worlds**|鉴于人形机器人将与人类共存和互动，传统人因工程研究需扩展以考虑“人形机器人因素”。本文引入了“人形机器人因素”概念，并构建了一个围绕物理、认知、社会和伦理四个支柱的框架。该框架旨在指导人形机器人开发，使其能与人类有效共存协作，并表征了人类能力与AI基础模型驱动的通用人形机器人能力之间的异同。通过评估一个真实世界的人形机器人控制算法，本文揭示了传统机器人任务完成指标如何忽视了关键人类认知和交互原则，并将人形机器人因素定位为设计、评估和管理人类-人形机器人持续共存的基础框架。|Lixiao Huang Team|[2602.10069](http://arxiv.org/abs/2602.10069)|null|
|**2026-02-10**|**TeleGate: Whole-Body Humanoid Teleoperation via Gated Expert Selection with Motion Prior**|针对人形机器人实时全身遥操作中统一控制器难以鲁棒支持多样动态运动的挑战，本文提出了TeleGate框架。该框架通过训练一个轻量级门控网络，基于本体状态和参考轨迹动态激活领域专家策略，从而避免了知识蒸馏导致的性能下降。此外，为应对实时遥操作中未来轨迹缺失问题，引入了基于VAE的运动先验模块，实现对未来运动意图的预测性控制。实验结果表明，TeleGate在仿真和Unitree G1人形机器人上，仅用少量数据训练，便在跑步、跌倒恢复和跳跃等多样动态运动中实现了高精度实时遥操作，显著优于基线方法。|Rongyun Cao Team|[2602.09628](http://arxiv.org/abs/2602.09628)|null|
|**2026-02-09**|**Characteristics, Management, and Utilization of Muscles in Musculoskeletal Humanoids: Empirical Study on Kengoro and Musashi**|当前肌肉骨骼人形机器人研究中，对其生物仿生结构固有的多样属性及其管理利用方式缺乏统一讨论。本研究基于作者团队开发的Kengoro和Musashi机器人，将肌肉骨骼结构特征归纳为冗余性、独立性、各向异性、可变力臂和非线性弹性五大属性。文章进一步探讨了这些属性组合所带来的优势与劣势，并重点讨论了身体图式学习、反射控制、肌肉分组及身体图式适应等机制。最后，研究阐述了通过集成系统实现运动的实践，并展望了未来的研究挑战。|Masayuki Inaba Team|[2602.08518](http://arxiv.org/abs/2602.08518)|null|
|**2026-02-09**|**Learning Human-Like Badminton Skills for Humanoid Robots**|人形机器人实现羽毛球等高强度运动的类人表现面临巨大挑战，尤其是在运动学模仿与功能性、物理感知击打之间难以兼顾自然风格。为解决此问题，本文提出了Imitation-to-Interaction渐进式强化学习框架，旨在使机器人从“模仿者”进化为“击球手”。该方法通过人类数据建立运动先验，蒸馏到模型化状态表示中，并利用对抗性先验稳定动力学，同时引入流形扩展策略以应对稀疏的专家演示。实验结果显示，该框架在仿真中掌握了多样羽毛球技能，并首次实现了类人羽毛球技能从仿真到真实机器人的零样本迁移，展示了物理世界中的优雅和精准打击。|Peng Lu Team|[2602.08370](http://arxiv.org/abs/2602.08370)|null|
|**2026-02-07**|**VividFace: Real-Time and Realistic Facial Expression Shadowing for Humanoid Robots**|人形机器人面部表情实时模仿对于实现逼真、情感丰富的人机交互至关重要，但现有方法常因离线推理和细节捕捉不足而难以同时达到实时性和逼真性。为解决这些局限，本文提出了VividFace，一个实时且逼真的人形机器人面部表情阴影系统。该系统通过优化模仿框架X2CNet++，并引入特征适应训练策略，显著增强了表情表现力；同时，通过视频流兼容推理管线和基于异步I/O的工作流，实现了高效的实时模仿。广泛的真实世界演示验证了VividFace在0.05秒内模仿人类表情并生成生动人形面部的实用能力，且能泛化至多种面部配置。|Yang Zhang Team|[2602.07506](http://arxiv.org/abs/2602.07506)|null|
|**2026-02-07**|**TextOp: Real-time Interactive Text-Driven Humanoid Robot Motion Generation and Control**|现有的人形机器人全身控制器在灵活性和自主性方面存在局限，难以实现实时和交互式驱动。为解决这一问题，本文提出了TextOp，一个实时文本驱动的人形运动生成与控制框架，支持流式语言指令和即时修改。TextOp采用两级架构：高级运动扩散模型根据文本生成短时域轨迹，低级运动跟踪策略则在机器人上执行这些轨迹。广泛的真实机器人实验和离线评估表明，TextOp实现了即时响应、平滑全身运动和精确控制，在舞蹈、跳跃等复杂行为中展现出自由形式的意图表达和流畅过渡。|Xuelong Li Team|[2602.07439](http://arxiv.org/abs/2602.07439)|**[link](https://text-op.github.io/)**|
|**2026-02-07**|**Bridging Speech, Emotion, and Motion: a VLM-based Multimodal Edge-deployable Framework for Humanoid Robots**|为解决多数人形机器人缺乏协调的语音、面部表情和手势，以及在设备上自主运行的需求，本文提出了SeM²，一个基于视觉语言模型的框架。SeM²通过多模态感知模块捕捉用户上下文，结合思维链推理规划响应，并利用语义序列对齐机制确保言语内容与物理表达的精确时间协调，从而实现情感一致的多模态交互。研究实现了云端及边缘部署版本，其中边缘版本通过知识蒸馏高效运行。综合评估显示，SeM²在自然度、情感清晰度和模态一致性方面显著优于单模态基线，推动了社交表达性人形机器人在多样现实环境中的应用。|Miao Li Team|[2602.07434](http://arxiv.org/abs/2602.07434)|null|
|**2026-02-06**|**Cerebellar-Inspired Residual Control for Fault Recovery: From Inference-Time Adaptation to Structural Consolidation**|针对机器人策略在真实世界部署中常遇到的训练后故障，且不便重新训练的问题，本文提出了一种推理时、受小脑启发的残差控制框架。该框架通过在线校正动作增强冻结的强化学习策略，无需修改基础策略参数即可实现故障恢复。它实例化了小脑核心原理，如高维模式分离、并行残差路径和局部误差驱动可塑性，并通过保守的元适应调节残差权限。实验结果表明，在MuJoCo基准测试中，该框架在执行器、动力学和环境扰动下，对HalfCheetah-v5和Humanoid-v5在适度故障下性能显著提升，并在严重故障下表现出优雅的性能下降。|Amit Ranjan Trivedi Team|[2602.07227](http://arxiv.org/abs/2602.07227)|null|
|**2026-02-06**|**DynaRetarget: Dynamically-Feasible Retargeting using Sampling-Based Trajectory Optimization**|将人类运动重定向到人形机器人控制策略并确保其动态可行性是一项挑战。本文介绍了DynaRetarget，一个将人类运动重定向到人形控制策略的完整流程。其核心是新颖的基于采样的轨迹优化（SBTO）框架，该框架能将不完善的运动学轨迹细化为动态可行的运动，并通过逐步推进优化范围来处理长时域任务。DynaRetarget在重定向数百个人形-物体演示中取得了比现有技术更高的成功率，并能泛化到不同物体属性的场景，为生成大规模人形局部操作轨迹合成数据集提供了可能。|Majid Khadiv Team|[2602.06827](http://arxiv.org/abs/2602.06827)|null|
|**2026-02-06**|**ECO: Energy-Constrained Optimization with Reinforcement Learning for Humanoid Walking**|为解决仿人机器人节能稳定运动中现有方法需大量超参数调优且易导致次优策略的问题，本研究提出了ECO（Energy-Constrained Optimization）约束强化学习框架。该框架将能量指标明确定义为不等式约束，通过拉格朗日法强制执行能量消耗和参考运动约束，从而实现稳定、对称且节能的行走。ECO在仿真和真实机器人（BRUCE）上的实验结果表明，在保持鲁棒行走性能的同时，其能耗显著低于MPC、标准RL及其他SOTA约束RL方法，实现了仿人机器人节能运动的实质性进步。|Yao Su Team|[2602.06445](http://arxiv.org/abs/2602.06445)|null|
|**2026-02-06**|**Now You See That: Learning End-to-End Humanoid Locomotion from Raw Pixels**|为解决基于视觉的仿人机器人鲁棒运动中模拟到真实差距产生的感知噪声和多样地形下学习目标冲突的挑战，本研究提出了一个端到端的视觉驱动运动框架。为实现鲁棒的sim-to-real迁移，该框架开发了高保真深度传感器模拟，并提出了视觉感知行为蒸馏方法。为适应多样地形，引入了地形特定奖励塑形，并结合多评论家和多判别器学习。在配备不同立体深度摄像头的两个仿人机器人平台上，该策略展现出在多样环境中的鲁棒性能，能处理极端挑战和精细任务，包括双向长期楼梯遍历。|Zongwu Xie Team|[2602.06382](http://arxiv.org/abs/2602.06382)|null|

<p align=right>(<a href=#updated-on-20260216>back to top</a>)</p>
