## Updated on 2026.02.25

## Categories

- [Manipulation](#manipulation)
- [World Model](#world-model)
- [VLM](#vlm)
- [VLA](#vla)
- [Humanoid](#humanoid)

## Manipulation

|Publish Date|Title|Chinese Summary|Authors|PDF|Code|
|---|---|---|---|---|---|
|**2026-02-24**|**ActionReasoning: Robot Action Reasoning in 3D Space with LLM for Robotic Brick Stacking**|经典机器人系统因缺乏泛化能力而限制了具身AI的发展。为解决这一问题，本文提出了ActionReasoning框架，该框架利用大型语言模型（LLMs）中已编码的物理先验和真实世界知识，通过多智能体架构进行显式动作推理，为机器人操作生成符合物理原理的决策。在砖块堆叠案例研究中，实验表明该多智能体LLM框架能实现稳定的砖块放置，并将开发重心从低级编码转向高级工具调用和提示，展示了其在机器人操作中整合物理推理与LLM的巨大潜力。|Brian Sheil Team|[2602.21161](http://arxiv.org/abs/2602.21161)|null|
|**2026-02-24**|**HALO: A Unified Vision-Language-Action Model for Embodied Multimodal Chain-of-Thought Reasoning**|视觉-语言-动作（VLA）模型在长时程或分布外场景中因缺乏多模态推理能力而受限。为此，本文提出了HALO模型，一个统一的VLA框架，通过文本任务推理、视觉子目标预测和增强动作预测的顺序过程，实现了具身多模态思维链（EM-CoT）推理。HALO采用Transformer混合架构，并引入自动化流水线生成EM-CoT训练数据。实验结果表明，HALO在模拟和真实环境中均表现优越，在RoboTwin基准测试中成功率提高了34.1%，且在未知环境随机化下展现出强大的泛化能力。|Song Guo Team|[2602.21157](http://arxiv.org/abs/2602.21157)|null|
|**2026-02-24**|**Matching Multiple Experts: On the Exploitability of Multi-Agent Imitation Learning**|多智能体模仿学习在离线场景下，其学习策略与纳什均衡的距离特征描述仍是空白。本文首先证明了在一般n-player马尔可夫博弈中学习低可利用性策略的难度。随后，研究人员展示了如何通过利用专家均衡的策略支配假设来克服这些挑战，具体地，对于主导策略专家均衡情况，其纳什模仿差距可被界定。该研究通过引入最佳响应连续性概念进一步泛化了结果，并指出标准正则化技术能隐式鼓励这种连续性。|Negar Mehr Team|[2602.21020](http://arxiv.org/abs/2602.21020)|null|
|**2026-02-24**|**IG-RFT: An Interaction-Guided RL Framework for VLA Models in Long-Horizon Robotic Manipulation**|VLA模型在复杂长时程真实世界任务中面临泛化挑战，而强化学习在VLA微调中存在探索效率、训练稳定性和样本成本等问题。为解决这些挑战，本文提出了IG-RFT，一个交互引导的强化微调系统，专为基于流的VLA模型设计。该系统引入了交互引导优势加权回归（IG-AWR）算法动态调节探索强度，设计了整合轨迹级和子任务级奖励的混合密集奖励函数，并构建了SFT、离线RL和人机交互RL的三阶段微调系统。真实世界实验表明，IG-RFT平均成功率达85.0%，显著优于现有基线，并验证了其关键组件的贡献。|Huixu Dong Team|[2602.20715](http://arxiv.org/abs/2602.20715)|null|
|**2026-02-24**|**BFA++: Hierarchical Best-Feature-Aware Token Prune for Multi-View Vision Language Action Model**|VLA模型中大量视觉token（尤其多视图输入）对实时机器人操作构成挑战，而现有token剪枝技术在直接应用于VLA模型时常因忽略视图间关系和任务动态性而性能下降。为此，本文提出了BFA++，一个专为VLA模型设计的动态token剪枝框架。BFA++采用两级重要性预测器引导的分层剪枝策略，实现视图内任务相关区域提取和视图间关键视图识别，从而在保留关键视觉信息的同时提高计算效率。实验结果表明，BFA++显著提升了成功率并实现了1.5X至1.8X的加速，证明了上下文敏感和任务感知剪枝策略在提高机器人系统推理速度和操作精度方面的有效性。|Hua Chen Team|[2602.20566](http://arxiv.org/abs/2602.20566)|null|
|**2026-02-24**|**Inner Speech as Behavior Guides: Steerable Imitation of Diverse Behaviors for Human-AI coordination**|当前模仿学习方法难以捕捉人类行为多样性、非马尔可夫性，并缺乏在推理时引导行为的能力。受人类认知中“内部言语”的启发，本文提出了MIMIC框架，该框架利用语言作为行为意图的内部表征。MIMIC创新性地使用视觉-语言模型作为语言支架训练条件变分自编码器，从观察中生成内部言语，并由扩散行为克隆策略根据观察和内部言语选择动作。实验证明，MIMIC显著增强了行为多样性和对人类演示的忠实度，并能在无需额外训练下实现精细的行为引导。|David C Parkes Team|[2602.20517](http://arxiv.org/abs/2602.20517)|null|
|**2026-02-23**|**AdaWorldPolicy: World-Model-Driven Diffusion Policy with Online Adaptive Learning for Robotic Manipulation**|有效机器人操作需要策略能够预测物理结果并适应动态环境。为解决这一问题，本文提出了AdaWorldPolicy，一个世界模型驱动的扩散策略与在线自适应学习的统一框架。其核心思想是世界模型提供强监督信号，结合力矩反馈，实现动态环境中的在线自适应学习。AdaWorldPolicy将世界模型、动作专家和力预测器整合为相互连接的流匹配扩散Transformer，并通过新颖的在线自适应学习策略动态切换模式以驱动各模块的反应性更新。该框架在模拟和真实机器人基准测试中均达到最先进性能，并展现出对分布外场景的动态自适应能力。|Dong Xu Team|[2602.20057](http://arxiv.org/abs/2602.20057)|**[link](https://AdaWorldPolicy.github.io)**|
|**2026-02-23**|**Beyond Mimicry: Toward Lifelong Adaptability in Imitation Learning**|模仿学习（IL）虽有进展，但现有代理在上下文变化或目标演变时表现不佳，本文认为这是因为其优化目标错误。研究团队提出一项研究议程，将IL的成功标准从“完美回放”重新定义为“组合适应性”，旨在实现行为原语的一次性学习和无需重训练即可在新上下文中的重组。该议程建立了组合泛化的度量标准，提出了混合架构，并概述了结合认知科学和文化进化的跨学科研究方向，强调将适应性作为IL核心能力以应对开放世界挑战的重要性。|Odinaldo Rodrigues Team|[2602.19930](http://arxiv.org/abs/2602.19930)|null|
|**2026-02-23**|**Hilbert-Augmented Reinforcement Learning for Scalable Multi-Robot Coverage and Exploration**|多机器人系统在稀疏奖励环境中进行探索和覆盖时常面临效率和冗余问题。为解决此问题，本文提出了一个覆盖框架，将希尔伯特空间填充先验整合到去中心化多机器人学习和执行中。该框架通过希尔伯特空间索引增强DQN和PPO，以结构化探索并减少冗余，并设计了路径点接口将希尔伯特排序转换为可执行的SE(2)轨迹。实验结果表明，该方法显著提高了覆盖效率、降低了冗余并加速了收敛。在Boston Dynamics Spot腿足机器人上的验证进一步证实了其在室内环境中实现可靠且低冗余覆盖的有效性。|Aryya Gangopadhyay Team|[2602.19400](http://arxiv.org/abs/2602.19400)|null|
|**2026-02-23**|**FACTO: Function-space Adaptive Constrained Trajectory Optimization for Robotic Manipulators**|为优化单臂和多臂机械臂的轨迹，本文提出了Function-space Adaptive Constrained Trajectory Optimization (FACTO) 算法。该算法将轨迹参数化为正交基函数的线性组合，并在系数空间中直接进行优化。通过使用指数移动平均的高斯-牛顿近似处理非线性，并通过系数空间映射和Levenberg-Marquardt算法在零空间中执行自适应约束更新来处理轨迹约束。实验结果显示，与现有规划器相比，FACTO在受限单臂和多臂场景中显著提高了解决方案的质量和可行性，并在Franka机器人上验证了其部署可行性。|Minghui Zheng Team|[2602.20225](http://arxiv.org/abs/2602.20225)|null|
|**2026-02-22**|**Seeing Farther and Smarter: Value-Guided Multi-Path Reflection for VLM Policy Optimization**|解决复杂、长期的机器人操作任务需要精确规划和对物理交互的深刻理解。现有VLM反射式规划方法存在效率低下、依赖隐式学习和推理延迟等问题。为此，本研究提出一种新的测试时计算框架，将状态评估与行动生成解耦，通过显式建模行动计划的优势并使用束搜索聚合多条未来路径来生成更鲁棒的行动。该方法在多阶段机器人操作任务上将成功率提高了24.6%，并显著减少了56.5%的推理时间。|Dimitris N. Metaxas Team|[2602.19372](http://arxiv.org/abs/2602.19372)|null|
|**2026-02-22**|**The Price Is Not Right: Neuro-Symbolic Methods Outperform VLAs on Structured Long-Horizon Manipulation Tasks with Significantly Lower Energy Consumption**|视觉-语言-行动（VLA）模型在通用机器人策略方面展现潜力，但其在结构化、长程操作任务上的有效性和效率尚不明确。本研究对比了微调的VLA模型π0与结合PDDL符号规划和低层控制的神经-符号架构在汉诺塔任务中的表现。结果显示，神经-符号模型成功率（95%）远高于VLA模型（34%），且能更好地泛化到未见任务，同时训练能耗显著低于VLA模型。这强调了显式符号结构在提升机器人操作的可靠性、数据效率和能效方面的重要性。|Matthias Scheutz Team|[2602.19260](http://arxiv.org/abs/2602.19260)|null|
|**2026-02-22**|**Human-to-Robot Interaction: Learning from Video Demonstration for Robot Imitation**|现有从视频中学习机器人操作的方法面临视频描述不精确和难以泛化的挑战。本研究提出一种“人到机器人”模仿学习新管道，解耦为视频理解和机器人模仿两个阶段。视频理解阶段结合时移模块和视觉-语言模型提取动作和识别对象；机器人模仿阶段利用TD3深度强化学习执行操作。实验表明，该方法在动作分类和对象识别方面显著优于基线，并在模拟和真实机器人操作中取得了87.5%的平均成功率，在复杂任务中表现良好。|Xiem HoangVan Team|[2602.19184](http://arxiv.org/abs/2602.19184)|null|
|**2026-02-22**|**Global Prior Meets Local Consistency: Dual-Memory Augmented Vision-Language-Action Model for Efficient Robotic Manipulation**|分层视觉-语言-行动（VLA）模型在机器人操作中面临推理效率低和鲁棒性差的瓶颈。为解决这些问题，本研究引入OptimusVLA，一个采用全局先验记忆（GPM）和局部一致性记忆（LCM）的双记忆VLA框架。GPM通过任务级先验取代高斯噪声，缩短生成路径；LCM动态建模行动序列并注入一致性约束以增强时间连贯性。OptimusVLA在三个模拟基准测试中表现优异，并在真实世界任务中实现了更高的成功率和2.9倍的推理加速。|Liqiang Nie Team|[2602.20200](http://arxiv.org/abs/2602.20200)|null|
|**2026-02-21**|**TactEx: An Explainable Multimodal Robotic Interaction Framework for Human-Like Touch and Hardness Estimation**|准确感知物体硬度对于机器人灵巧操作至关重要。本研究提出TactEx，一个可解释的多模态机器人交互框架，它融合视觉、触觉和语言以实现类人硬度估计和交互式指导。该系统通过ResNet50+LSTM模型从触觉数据估算硬度，并通过跨模态对齐模块结合视觉线索和大型语言模型指导。TactEx在水果成熟度评估任务中成功率达90%，能有效区分成熟度等级并泛化到新任务，突显了预训练模型与语言接地相结合在机器人触觉感知和决策中的潜力。|Dandan Zhang Team|[2602.18967](http://arxiv.org/abs/2602.18967)|null|
|**2026-02-21**|**TPRU: Advancing Temporal and Procedural Understanding in Large Multimodal Models**|小型多模态大型语言模型（MLLMs）在理解时序和程序性视觉数据方面存在缺陷，阻碍了其在具身AI中的应用。为解决训练数据中缺乏大规模、程序连贯数据的问题，本研究引入了TPRU数据集，其包含来自机器人操作和GUI导航等场景的数据，并通过时间重排序、下一帧预测和前一帧回顾任务培养时间推理能力。利用强化学习微调后，TPRU-7B模型在TPRU-Test数据集上准确率从50.33%提升至75.70%，显著优于GPT-4o等大型基线模型，并展现了出色的泛化能力。|Yuan Xie Team|[2602.18884](http://arxiv.org/abs/2602.18884)|**[link](https://github.com/Stephen-gzk/TPRU)**|
|**2026-02-21**|**Issues with Measuring Task Complexity via Random Policies in Robotic Tasks**|衡量强化学习（RL）任务复杂性对于基准测试和课程设计至关重要，但在非表格领域仍缺乏可靠指标。本研究评估了基于随机权重猜测（RWG）的策略信息容量（PIC）和策略最优信息容量（POIC）等现有复杂性指标。通过在不同难度机器人操作设置下的实证分析，发现PIC和POIC在某些情况下与RL的典型理解和经验结果相矛盾，例如认为双连杆机械臂比单连杆更简单，或稀疏奖励任务比密集奖励任务更容易。这强调了需要开发超越RWG、更可靠捕捉非表格RL任务复杂性的新指标。|Aditya Gilra Team|[2602.18856](http://arxiv.org/abs/2602.18856)|null|
|**2026-02-21**|**HeRO: Hierarchical 3D Semantic Representation for Pose-aware Object Manipulation**|纯几何策略的机器人模仿学习缺乏部件级语义，这限制了其在姿态感知操作中的能力。本研究提出HeRO，一个基于扩散的策略，通过分层语义场结合几何和语义。HeRO通过密集语义提升，融合DINOv2的几何敏感特征与Stable Diffusion的全局一致对应，生成细粒度且空间一致的特征。这些特征用于构建全局和局部场，并由分层条件模块以置换不变网络架构调节生成去噪器，从而实现连贯的姿态感知控制策略。HeRO在多项任务中达到SOTA，成功率平均提升6.5%。|Shuaicheng Liu Team|[2602.18817](http://arxiv.org/abs/2602.18817)|null|
|**2026-02-21**|**RoboCurate: Harnessing Diversity with Action-Verified Neural Trajectory for Robot Learning**|视频生成模型产生的合成数据在机器人学习中存在行动质量不一致的问题，而VLM在区分物理精确视频和评估行动方面有局限。本研究引入RoboCurate框架，通过在模拟器中重放预测行动并比较其与生成视频的运动一致性来评估和过滤带注释行动的质量。此外，通过图像编辑和视频到视频传输增强观察多样性。结果显示，RoboCurate生成的数据显著提升了机器人任务的成功率，例如在GR-1 Tabletop任务中提高了70.1%，并在ALLEX人形机器人操作中提高了179.9%。|Jinwoo Shin Team|[2602.18742](http://arxiv.org/abs/2602.18742)|**[link](https://seungkukim.github.io/robocurate/)**|
|**2026-02-20**|**MALLVI: A Multi-Agent Framework for Integrated Generalized Robotics Manipulation**|针对大型语言模型（LLMs）在机器人操作任务规划中开环、缺乏环境反馈的脆弱性，本文提出了MALLVi框架。该框架是一个多智能体大语言和视觉系统，通过协调分解器、定位器、思考者和反射器等专用智能体，实现了闭环反馈驱动的机器人操作，其中反射器支持有针对性的错误检测和恢复。实验结果表明，这种迭代闭环多智能体协调显著提高了零样本操作任务的泛化性和成功率。|Babak Khalaj Team|[2602.16898](http://arxiv.org/abs/2602.16898)|null|
|**2026-02-20**|**SimVLA: A Simple VLA Baseline for Robotic Manipulation**|鉴于视觉-语言-动作（VLA）模型研究中因训练方法和实现细节多样而难以识别性能提升来源的问题，本研究引入了SimVLA这一精简基线。SimVLA严格解耦感知与控制，采用标准视觉-语言骨干和轻量级动作头，并标准化了关键训练动态。实验证明，尽管SimVLA参数量仅为0.5B，但在未进行机器人预训练的标准仿真基准上超越了数十亿参数的模型，并在真实机器人性能上与pi0.5相当，为VLA研究提供了透明且可复现的参考基准。|Zhenguo Li Team|[2602.18224](http://arxiv.org/abs/2602.18224)|null|
|**2026-02-20**|**UAOR: Uncertainty-aware Observation Reinjection for Vision-Language-Action Models**|针对视觉-语言-动作（VLA）模型在增强性能时常需额外观察线索或辅助模块，导致数据收集和训练成本高昂的问题，本研究受语言模型FFN启发，提出了一种无需训练、即插即用的“不确定性感知观测重注”（UAOR）模块。该模块在语言模型层表现出高不确定性时，通过注意力检索将关键观测信息重新注入下一层的前馈网络。广泛实验表明，UAOR以最小开销持续提升了多种VLA模型在模拟和真实世界任务中的表现，且无需额外观察线索或模块，具有良好的通用性和实用性。|Liang Wang Team|[2602.18020](http://arxiv.org/abs/2602.18020)|null|
|**2026-02-20**|**Quasi-Periodic Gaussian Process Predictive Iterative Learning Control**|鉴于重复运动任务中机器人性能随时间退化的问题，本研究将准周期高斯过程（QPGPs）引入预测性迭代学习控制（ILC）框架，以高效建模和预测跨迭代的扰动与漂移。通过QPGPs的结构方程表述，该方法实现了O(p^3)的计算复杂度，远低于传统方法。实验结果表明，与标准ILC及传统GP-ILC相比，所提方法在自动驾驶、机械臂和真实世界机器人等任务中能更快收敛，在扰动下保持鲁棒性，并显著降低了计算成本。|Michael Burke Team|[2602.18014](http://arxiv.org/abs/2602.18014)|null|
|**2026-02-20**|**Learning Optimal and Sample-Efficient Decision Policies with Guarantees**|面对高风险应用中强化学习在线交互成本高昂且离线学习受隐蔽混淆变量阻碍的挑战，本研究首先利用工具变量解决了存在混淆变量的离线学习问题，并提出了一种样本高效的条件矩约束算法，具有收敛性和最优性保证。其次，放宽模仿学习中对混淆变量的条件，并调整估计器以学习有效的模仿策略。最后，针对学习线性时序逻辑表达的高级目标，开发了一种可证明最优且样本效率更高的学习算法。实验证明了这些方法在实际决策中的有效性。|Daqian Shao Team|[2602.17978](http://arxiv.org/abs/2602.17978)|null|
|**2026-02-20**|**Graph-Neural Multi-Agent Coordination for Distributed Access-Point Selection in Cell-Free Massive MIMO**|为解决免蜂窝大规模MIMO (CFmMIMO) 系统中接入点选择（APS）的挑战，本研究提出了APS-GNN，一个可扩展的分布式多智能体学习框架。该框架将APS分解为基于图神经网络的智能体，通过局部观测交换和参数共享进行协调，并采用受约束强化学习方法处理冲突目标。为提高训练效率，策略通过监督模仿学习初始化。实验结果表明，APS-GNN在实现目标频谱效率的同时，激活的接入点数量比启发式和集中式多智能体强化学习基线减少50-70%，且推理延迟低一到两个数量级，为大型CFmMIMO网络提供了实用且可扩展的APS解决方案。|Raouf Boutaba Team|[2602.17954](http://arxiv.org/abs/2602.17954)|null|
|**2026-02-20**|**ROCKET: Residual-Oriented Multi-Layer Alignment for Spatially-Aware Vision-Language-Action Models**|针对视觉-语言-动作（VLA）模型在3D空间理解上的不足以及现有表示对齐方法未能充分利用多层信息或可能导致梯度冲突的问题，本研究提出了ROCKET，一个残差导向的多层表示对齐框架。ROCKET通过共享投影器和层不变映射，将VLA骨干的多个层与强大的3D视觉基础模型的多个层进行对齐，并引入Matryoshka风格的稀疏激活方案以平衡多个对齐损失。实验证明，ROCKET在计算开销极小的情况下，在LIBERO、LIBERO-Plus和RoboTwin等任务上取得了98.5%的SOTA成功率，且优于现有设计。|Ang Li Team|[2602.17951](http://arxiv.org/abs/2602.17951)|null|
|**2026-02-20**|**Enhancing Goal Inference via Correction Timing**|人类对机器人的纠正反馈通常被视为新的示范或偏好，但忽略了人类决定干预的时机信息。本研究探讨了纠正时机作为一种有用信号，用于推断机器人任务相关的因素，如进度、期望一致性、动力学等。研究集中于三个应用：识别促使人类纠正的运动特征、快速推断纠正目标以及学习更精确的任务目标约束。结果表明，纠正时机能有效改善前两个应用的学习效果，为机器人学习提供了新的见解。|Tesca Fitzgerald Team|[2602.18603](http://arxiv.org/abs/2602.18603)|null|
|**2026-02-19**|**RoboGene: Boosting VLA Pre-training via Diversity-Driven Agentic Framework for Real-World Task Generation**|针对通用机器人操作面临的真实世界交互数据稀缺和自动化任务生成挑战，本文提出了RoboGene框架。该代理框架通过多样性驱动采样、自我反思机制和人机循环改进，自动生成单臂、双臂和移动机器人的多样化、物理合理的操纵任务。实验证明，RoboGene显著优于现有基础模型，且其生成的数据能提高预训练VLA模型的成功率和泛化能力。|Jian Tang Team|[2602.16444](http://arxiv.org/abs/2602.16444)|null|
|**2026-02-19**|**IRIS: Learning-Driven Task-Specific Cinema Robot Arm for Visuomotor Motion Control**|鉴于工业级机器人摄像系统成本高昂且操作复杂，限制了其普及，本研究提出了智能机器人成像系统（IRIS）。IRIS是一款专为自主、学习驱动的电影级运动控制设计的6自由度机械手，结合了轻量级3D打印硬件与基于Action Chunking with Transformers（ACT）的目标条件视觉运动模仿学习框架，直接从人类演示中学习摄像机轨迹，无需显式编程。该系统成本低于1000美元，支持1.5公斤载荷，并实现了约1毫米的重复精度。真实世界实验验证了其准确的轨迹跟踪、可靠的自主执行以及在多样化电影级运动中的泛化能力。|Ali Bereyhi Team|[2602.17537](http://arxiv.org/abs/2602.17537)|null|
|**2026-02-19**|**Benchmarking the Effects of Object Pose Estimation and Reconstruction on Robotic Grasping Success**|针对3D重建在机器人感知中作为基础，但传统几何评估未能反映其对下游任务（如机器人抓取）性能影响的空白，本研究引入了一个大规模、基于物理的基准。该基准通过在不同重建的3D网格上生成抓取，并在真实模型上执行，来评估6D姿态估计器和3D网格模型在抓取方面的功能有效性。实验结果表明，重建缺陷显著减少了抓取姿态候选数量，但给定准确姿态估计后对抓取性能影响可忽略；抓取成功率与姿态误差的关系主要由空间误差主导，即使简单的平移误差也能为对称物体抓取姿态的成功提供洞察，为理解感知系统与机器人物体操作之间的关系提供了见解。|Torsten Sattler Team|[2602.17101](http://arxiv.org/abs/2602.17101)|null|
|**2026-02-19**|**MePoly: Max Entropy Polynomial Policy Optimization**|针对随机最优控制中传统参数化策略难以表示多模态解决方案，以及基于扩散的策略缺乏明确概率密度的问题，本研究提出了MePoly，一种基于多项式能量模型的策略参数化方法。MePoly提供了明确且可处理的概率密度，从而实现了精确的熵最大化，并基于经典矩问题，利用其对任意分布的通用逼近能力。实验证明MePoly能有效捕获复杂的非凸流形，并在各种基准测试中超越基线模型，展现了其在处理多模态决策问题中的优越性能。|Maani Ghaffari Team|[2602.17832](http://arxiv.org/abs/2602.17832)|null|
|**2026-02-18**|**BPP: Long-Context Robot Imitation Learning by Focusing on Key History Frames**|针对机器人任务中历史观测的重要性及现有策略因虚假关联导致泛化性差的问题，本文提出了大局策略（BPP）。该方法通过视觉-语言模型检测有意义的关键帧，并以此最小关键帧集为条件进行策略学习，从而将多样化的轨迹投射到紧凑的任务相关事件集上，显著减少了训练和部署之间的分布漂移。在真实世界和模拟操作任务中的实验结果表明，BPP的成功率比最佳对比方法高70%。|Aviral Kumar Team|[2602.15010](http://arxiv.org/abs/2602.15010)|null|
|**2026-02-18**|**Learning Humanoid End-Effector Control for Open-Vocabulary Visual Loco-Manipulation**|为解决人形机器人在野外视觉运动操作中末端执行器（EE）控制和场景理解的泛化性限制，本文提出了HERO范式。该方法结合了大型视觉模型的强泛化能力和开放词汇理解与模拟训练的控制性能，并通过设计精确的残差感知EE跟踪策略，结合经典机器人学和机器学习技术。实验表明，HERO将末端执行器跟踪误差降低了3.2倍，并在多样化的真实世界环境中实现了对多种日常物品的可靠操作。|Saurabh Gupta Team|[2602.16705](http://arxiv.org/abs/2602.16705)|**[link](https://hero-humanoid.github.io/)**|
|**2026-02-18**|**VIGOR: Visual Goal-In-Context Inference for Unified Humanoid Fall Safety**|鉴于人形机器人在杂乱环境中跌倒恢复的复杂性和现有方法零碎或泛化性差的问题，本文提出了一种统一的跌倒安全方法。该方法基于人类跌倒姿态的可迁移性以及整合感知-运动表征的需求，通过训练一个特权教师模型并蒸馏到仅依赖自我中心深度和本体感觉的学生模型。仿真和真实Unitree G1人形机器人实验证明，该方法在多样化的非平面环境中实现了鲁棒的零样本跌倒安全，无需真实世界微调。|Stella X. Yu Team|[2602.16511](http://arxiv.org/abs/2602.16511)|null|
|**2026-02-18**|**SparTa: Sparse Graphical Task Models from a Handful of Demonstrations**|针对机器人从演示中高效学习长距离操作任务的挑战，特别是现有方法多关注动作执行而非任务目标的问题，本研究侧重于推断机器人应实现的目标，而非如何实现。该方法通过图形化对象关系表示场景状态，并提出一种演示分割和池化方法，以提取操作图序列并估计任务阶段中对象状态的分布。实验证明，该方法能准确分割演示并从多个演示中有效学习，所构建的任务表示在仿真和真实机器人环境中均支持可靠的执行。|Abhinav Valada Team|[2602.16911](http://arxiv.org/abs/2602.16911)|null|
|**2026-02-17**|**Learning to Retrieve Navigable Candidates for Efficient Vision-and-Language Navigation**|为解决基于大语言模型（LLM）的视觉-语言导航（VLN）在决策效率和稳定性方面的挑战，本文提出了一个检索增强框架。该框架在剧集和步骤两个层面引入检索机制：指令级嵌入检索器提供上下文示例进行全局指导，模仿学习的候选检索器修剪不相关的导航方向以提高步进决策效率。在R2R基准测试上的实验结果表明，该方法在成功率和SPL方面均取得显著提升，验证了检索增强决策支持的有效性。|Lina Yao Team|[2602.15724](http://arxiv.org/abs/2602.15724)|null|
|**2026-02-17**|**Selective Perception for Robot: Task-Aware Attention in Multimodal VLA**|针对现有机器人视觉-语言-动作（VLA）模型静态融合多视图输入导致的计算开销和背景噪声问题，本文提出了一种动态信息融合框架。该方法引入轻量级自适应路由架构，实时评估摄像机视图的任务相关性，有条件地衰减低信息效用视图的计算，从而选择性地提供关键视觉特征。真实世界机器人操作实验结果表明，该方法在推理效率和控制性能上均优于现有VLA模型，验证了其在资源受限实时控制环境中的实用性。|Soo-Chul Lim Team|[2602.15543](http://arxiv.org/abs/2602.15543)|null|
|**2026-02-17**|**Feasibility-aware Imitation Learning from Observation with Multimodal Feedback**|针对模仿学习中示教者动作与机器人物理特性不符导致学习困难的问题，本文提出了“可行性感知行为克隆自观察”（FABCO）框架。FABCO通过结合行为克隆自观察与可行性估计，利用机器人动力学模型评估示教动作的可再现性，并将估计的可行性用于多模态反馈和可行性感知策略学习。实验结果表明，相比无可行性反馈的情况，FABCO将模仿学习性能提高了3.2倍以上。|Takamitsu Matsubara Team|[2602.15351](http://arxiv.org/abs/2602.15351)|null|
|**2026-02-16**|**PhyScensis: Physics-Augmented LLM Agents for Complex Physical Scene Arrangement**|为解决自动化3D环境生成中忽略物体间物理关系导致场景不真实和复杂性不足的问题，本文提出了PhyScensis框架。该框架是一个基于LLM代理并由物理引擎驱动的系统，通过LLM代理迭代提出带有空间和物理谓词的资产，并由求解器利用物理引擎将其实现为3D场景。实验结果表明，PhyScensis在场景复杂度、视觉质量和物理精度方面均优于现有方法，为机器人操作提供了生成复杂物理场景布局的统一管道。|Chuang Gan Team|[2602.14968](http://arxiv.org/abs/2602.14968)|null|
|**2026-02-16**|**Affordance Transfer Across Object Instances via Semantically Anchored Functional Map**|针对传统从示范学习（LfD）数据收集成本高昂，以及如何将示范交互泛化到几何差异大但功能相似物体上的挑战，本文提出了语义锚定功能图（SemFM）框架。该方法从单一视觉示范中，通过识别语义对应功能区域、选择语义锚点并利用功能图传播约束，实现了跨对象的可用性（affordance）转移。实验证明，SemFM以适度的计算成本实现了准确的可用性转移，适用于实际机器人感知-动作流程。|Weiming Zhi Team|[2602.14874](http://arxiv.org/abs/2602.14874)|null|
|**2026-02-16**|**DriveFine: Refining-Augmented Masked Diffusion VLA for Precise and Robust Driving**|鉴于自动驾驶领域视觉-语言-行动（VLA）模型中的生成式规划器（如扩散模型和基于Token的模型）存在的缺点，如模态对齐困难、训练效率低、泛化能力有限及累积因果误差等问题，本研究提出了DriveFine，一个结合灵活解码和自校正能力的掩码扩散VLA模型。该模型引入了即插即用的block-MoE（专家混合）模块，将精炼专家与生成专家无缝结合，并通过推理时的显式专家选择和训练时的梯度阻断实现专家解耦。此外，设计了一种混合强化学习策略来促进精炼专家的有效探索并保持训练稳定性。在NAVSIM v1、v2和Navhard基准上的大量实验证明，DriveFine表现出强大的有效性和鲁棒性。|Yan Wang Team|[2602.14577](http://arxiv.org/abs/2602.14577)|null|
|**2026-02-16**|**RoboSolver: A Multi-Agent Large Language Model Framework for Solving Robotic Arm Problems**|为整合大型语言模型（LLMs）和视觉-语言模型（VLMs）与计算工具的能力以自动化解决机器人操作臂问题，本研究提出了一种智能多智能体框架。该框架能够接受文本和视觉输入，并根据用户查询自动执行正逆运动学计算、关键点速度和加速度计算、机器人3D仿真生成，最终在仿真环境中执行运动控制。通过与GPT-4o、DeepSeek-V3.2和Claude-Sonnet-4.5等模型及Gemini 2.5 Pro VLM结合的基准测试，结果显示该框架显著提高了各种任务的准确性，例如与GPT-4o结合在文本描述下的正运动学计算准确率达到0.97，在视觉输入下达到0.93，且在多类机器人任务中也表现出0.97的准确率，远超单一模型的表现。|Alireza Taheri Team|[2602.14438](http://arxiv.org/abs/2602.14438)|null|
|**2026-02-16**|**A Soft Wrist with Anisotropic and Selectable Stiffness for Robust Robot Learning in Contact-rich Manipulation**|针对非结构化环境中接触密集型操作任务中现有软末端执行器变形范围有限、缺乏定向刚度控制或驱动系统复杂等问题，本研究引入了一种新型柔性腕部机构CLAW（Compliant Leaf-spring Anisotropic soft Wrist）。该机制通过两个正交的板簧和带锁定机构的旋转关节，实现了大范围的六自由度变形、可调的三种模式各向异性刚度，同时保持了轻量化和低成本。在模仿学习的实验评估中，CLAW在插栓任务中的成功率达到76%，优于Fin Ray夹持器（43%）和刚性夹持器（36%），展示了其在处理精密装配和精细物体操作等接触密集型场景中的潜力，有望提高机器人学习的鲁棒性。|Masashi Hamaya Team|[2602.14434](http://arxiv.org/abs/2602.14434)|null|
|**2026-02-16**|**AdaptManip: Learning Adaptive Whole-Body Object Lifting and Delivery with Online Recurrent State Estimation**|针对人形机器人集成导航、物体抓取和递送（运动操作）任务中，现有模仿学习方法依赖人类示教且对干扰脆弱的问题，本研究提出了AdaptManip框架。该框架通过强化学习训练鲁棒的运动操作策略，无需人类示教数据，并由三个耦合组件构成：实时跟踪被操纵物体的循环对象状态估计器、用于稳定运动和操作控制的全身基础策略以及基于激光雷达的机器人全局位姿估计器。所有组件均在仿真中训练并零样本部署到真实硬件。实验结果表明，AdaptManip在适应性和整体成功率上显著优于包括模仿学习在内的基线方法，且精确的对象状态估计即使在遮挡下也能提升操作性能，并在真实世界中成功展示了自主导航、物体抓取和递送能力。|Sehoon Ha Team|[2602.14363](http://arxiv.org/abs/2602.14363)|**[link](https://morganbyrd03.github.io/adaptmanip/)**|
|**2026-02-15**|**GRAIL: Goal Recognition Alignment through Imitation Learning**|鉴于从智能体行为中理解其目标对于AI系统与人类意图对齐至关重要，但现有目标识别方法常依赖最优策略表示而与实际（可能次优）行为不符，本研究提出了通过模仿学习实现目标识别对齐（GRAIL）的方法。GRAIL利用模仿学习和逆强化学习，直接从（可能次优的）示教轨迹中为每个候选目标学习一个目标导向策略。通过对观察到的部分轨迹进行单次前向传播，GRAIL在保留经典目标识别的单次推理能力的同时，能够捕获次优和系统性偏差行为。实验结果显示，GRAIL在系统性偏差最优行为下将F1分数提高超过0.5，在次优行为下提高约0.1-0.3，在带噪声的最优轨迹下提高高达0.4，同时在完全最优设置下保持竞争力。|Reuth Mirsky Team|[2602.14252](http://arxiv.org/abs/2602.14252)|null|
|**2026-02-15**|**Learning Part-Aware Dense 3D Feature Field for Generalizable Articulated Object Manipulation**|鉴于关节物体操作在多样化物体间的泛化挑战，以及现有2D基础特征在提升至3D空间时面临运行时间长、多视角不一致及空间分辨率低等问题，本研究提出了Part-Aware 3D Feature Field (PA3FF)。PA3FF是一种新型的密集3D特征，通过对比学习利用大规模标注数据集中的3D部件提议进行训练，能从点云输入中预测连续的3D特征场，其中点特征的距离反映功能部件的接近程度。在此基础上，研究引入了Part-Aware Diffusion Policy (PADP) 模仿学习框架，以提升样本效率和泛化能力。实验证明，PA3FF在模拟和真实世界的操作任务中始终优于多种2D和3D表征，并能支持对应学习和分割等多种下游任务。|Hao Dong Team|[2602.14193](http://arxiv.org/abs/2602.14193)|**[link](https://pa3ff.github.io)**|
|**2026-02-15**|**RoboAug: One Annotation to Hundreds of Scenes via Region-Contrastive Data Augmentation for Robotic Manipulation**|为解决机器人学习在多样化、未知场景中泛化能力差的问题，同时避免对大规模预训练和完美上游物体检测的依赖，本研究提出了RoboAug，一个生成式数据增强框架。该方法仅需单张图像的边界框标注，即可利用预训练的生成模型进行精确的语义数据增强，并整合即插即用的区域对比损失，使模型聚焦于任务相关区域。在UR-5e、AgileX和Tien Kung 2.0三款机器人上进行的超过3.5万次真实世界实验表明，RoboAug显著优于现有数据增强基线方法。在包含多样化背景、干扰物和光照条件的未知场景中，该方法将UR-5e的成功率从0.09提升至0.47，AgileX从0.16提升至0.60，Tien Kung 2.0从0.19提升至0.67，凸显了其在真实世界操作任务中的卓越泛化能力和有效性。|Jian Tang Team|[2602.14032](http://arxiv.org/abs/2602.14032)|null|
|**2026-02-15**|**WoVR: World Models as Reliable Simulators for Post-Training VLA Policies with RL**|为克服强化学习在机器人VLA模型中因大量真实世界交互需求而难以直接部署，以及世界模型模拟器在长周期想象轨迹中易产生幻觉和误差累积的问题，本研究提出了WoVR框架。WoVR通过可控的动作条件视频世界模型提高轨迹稳定性，利用关键帧初始化轨迹减少有效误差深度，并通过世界模型-策略协同演化保持对齐。实验结果显示，WoVR在LIBERO基准和真实机器人操作任务中实现了稳定的长周期想象轨迹和有效的策略优化，平均成功率在LIBERO上提升了29.3个百分点，在真实机器人上提升了30.0个百分点，证明了在有效控制幻觉的情况下，学习到的世界模型可作为实用的强化学习模拟器。|Dongbin Zhao Team|[2602.13977](http://arxiv.org/abs/2602.13977)|null|
|**2026-02-14**|**Enabling Option Learning in Sparse Rewards with Hindsight Experience Replay**|针对分层强化学习框架（如MOC）在稀疏奖励多目标环境中表现不佳，特别是在对象操作任务中难以发现与对象的交互策略问题，本研究首先提出了MOC-HER，将回溯经验回放（HER）集成到MOC中。在此基础上，为更有效处理对象操作任务，进一步引入了Dual Objectives Hindsight Experience Replay (2HER)，通过同时生成对象最终状态目标和智能体效应器位置目标，奖励智能体与对象的交互和任务完成。实验结果表明，MOC-2HER在机器人操作环境中的成功率高达90%，远高于MOC和MOC-HER的不足11%，验证了双目标重标记策略的有效性。|Gabriel de Oliveira Ramos Team|[2602.13865](http://arxiv.org/abs/2602.13865)|null|
|**2026-02-14**|**Mean Flow Policy with Instantaneous Velocity Constraint for One-step Action Generation**|针对流基策略在强化学习中表达能力与计算负担的权衡问题，本研究提出了一种新的生成式策略函数——平均速度策略（MVP）。MVP通过建模平均速度场实现最快的一步动作生成，并引入瞬时速度约束（IVC）以确保高表达能力。理论上证明IVC作为关键边界条件可提高学习精度和策略表达力。实验结果表明，MVP在Robomimic和OGBench的多个机器人操作任务中取得了最先进的成功率，并在训练和推理速度上显著优于现有流基策略基线。|Shengbo Eben Li Team|[2602.13810](http://arxiv.org/abs/2602.13810)|null|
|**2026-02-14**|**Gaussian Sequences with Multi-Scale Dynamics for 4D Reconstruction from Monocular Casual Videos**|为解决从单目日常视频中进行四维（4D）动态场景重建的ill-posed问题，本研究基于真实世界动态的多尺度规律性，设计了多尺度动态机制以分解复杂运动场。在此基础上，提出了具有多尺度动态的高斯序列，通过多级运动组合构建动态3D高斯表示，显著减轻了重建歧义并促进物理合理性。同时，结合视觉基础模型的多模态先验提供补充监督，进一步约束解空间并提高重建保真度。实验证明，该方法在动态新视角合成任务中，在基准和真实世界操作数据集上均显著优于现有方法，实现了从单目视频中准确且全局一致的4D重建。|Lei Sun Team|[2602.13806](http://arxiv.org/abs/2602.13806)|null|
|**2026-02-14**|**MOTIF: Learning Action Motifs for Few-shot Cross-Embodiment Transfer**|尽管视觉-语言-动作（VLA）模型推动了通用机器人学习，但由于运动学异构性以及收集足够真实世界示范数据进行微调的高成本，跨具身（cross-embodiment）迁移仍然充满挑战。现有跨具身策略通常依赖共享-私有架构，其私有参数容量有限且缺乏明确的适应机制。为解决这些局限性，本文提出了MOTIF框架，旨在实现高效的小样本跨具身迁移，它将具身无关的时空模式（称为动作基序）与异构动作数据解耦。具体而言，MOTIF首先通过带有进度感知对齐和具身对抗约束的矢量量化学习统一的基序，以确保时间和跨具身一致性。然后，设计一个轻量级预测器从实时输入预测这些基序，并将其与机器人特定状态融合，以指导流匹配策略在新的具身上生成动作。模拟和真实世界环境的评估均验证了MOTIF的优越性，在小样本迁移场景中显著优于强基线，模拟中提升6.5%，真实世界中提升43.7%。|Heng Tao Shen Team|[2602.13764](http://arxiv.org/abs/2602.13764)|null|
|**2026-02-14**|**HybridFlow: A Two-Step Generative Policy for Robotic Manipulation**|现有的机器人操作策略受推理延迟限制，缺乏足够的实时环境交互能力。尽管流匹配等更快的生成方法正逐步取代扩散方法，但其精度仍难以满足机器人操作的严格要求。本文关注MeanFlow作为流匹配的单步变体虽然快速但在动作生成精度上的不足。为平衡推理速度和生成质量，本文提出了HybridFlow，这是一种具有2-NFE（函数评估次数）的三阶段方法，包括MeanFlow模式下的全局跳转、用于分布对齐的ReNoise以及ReFlow模式下的局部细化。该方法利用MeanFlow单步生成的快速优势，同时以最少的生成步骤确保动作精度。真实世界实验表明，HybridFlow在成功率上比16步扩散策略高出15-25%，并将推理时间从152毫秒缩短到19毫秒（8倍加速，约52赫兹）；在未见颜色OOD抓取和可变形物体折叠任务上分别达到了70.0%和66.3%的成功率。这些结果表明HybridFlow是一种实用的低延迟方法，能增强机器人操作策略的真实世界交互能力。|Yide Liu Team|[2602.13718](http://arxiv.org/abs/2602.13718)|null|
|**2026-02-14**|**Symmetry-Aware Fusion of Vision and Tactile Sensing via Bilateral Force Priors for Robotic Manipulation**|机器人插入任务需要精密的、富接触的交互，仅凭视觉难以解决。尽管触觉反馈具有直观价值，但现有研究表明，朴素的视觉-触觉融合往往未能持续提供改进。为解决此问题，本文提出了一种用于视觉-触觉融合的跨模态Transformer（CMT），它通过结构化的自注意力与交叉注意力机制整合腕部摄像头观测和触觉信号。为稳定触觉嵌入，本文进一步引入了物理信息正则化，鼓励双边力平衡，反映了人类运动控制的原理。在TacSL基准上的实验表明，带有对称正则化的CMT实现了96.59%的插入成功率，超越了朴素和门控融合基线，并与“腕部+接触力”的优越配置（96.09%）非常接近。这些结果突出表明：触觉感知对于精确对齐不可或缺，以及经过物理信息正则化强化的原则性多模态融合，能够充分发挥视觉和触觉的互补优势，在现实感知条件下接近最优性能。|Tao Yu Team|[2602.13689](http://arxiv.org/abs/2602.13689)|null|
|**2026-02-14**|**Hierarchical Audio-Visual-Proprioceptive Fusion for Precise Robotic Manipulation**|现有机器人操作方法主要依赖视觉和本体感受，在部分可观测的真实世界环境中难以推断接触相关的交互状态。而声学线索能自然编码丰富的接触动态，但在多模态融合中却未被充分利用，且大多数融合方法错误地假设模态作用均一。为实现基于声学信息的精确机器人操作，本文提出一种分层表示融合框架，逐步整合音频、视觉和本体感受。该方法首先将视觉和本体感受表示条件化于声学线索，然后明确建模高阶跨模态交互以捕捉模态间的互补依赖。融合后的表示被扩散策略用于直接从多模态观测生成连续机器人动作。在真实世界机器人操作任务（如倒液体和开柜门）上的广泛实验表明，该方法持续优于现有最先进的多模态融合框架，尤其是在声学线索提供视觉无法轻易获得的任务相关信息时。此外，通过互信息分析解释了音频线索在机器人操作中的作用。|Peng Liu Team|[2602.13640](http://arxiv.org/abs/2602.13640)|null|
|**2026-02-13**|**Imitating What Works: Simulation-Filtered Modular Policy Learning from Human Videos**|通过观看人类视频学习操作技能，有望为机器人学习提供大规模数据的新来源。然而，人类视频在学习抓取后动作方面提供了强信号，但在学习抓取行为方面用处较小，特别是对于没有类似人手的机器人而言，任意稳定的抓取通常不兼容任务。为解决这一挑战，本文提出了Perceive-Simulate-Imitate (PSI) 框架，用于使用经过模拟中成对抓取-轨迹过滤处理的人类视频运动数据训练模块化操作策略。这一模拟步骤通过抓取适用性标签扩展了轨迹数据，从而能够监督学习面向任务的抓取能力。真实世界实验表明，该框架可以无需任何机器人数据高效学习精确操作技能，与简单使用抓取生成器相比，性能显著更鲁棒。|Wei-Chiu Ma Team|[2602.13197](http://arxiv.org/abs/2602.13197)|null|
|**2026-02-13**|**UniManip: General-Purpose Zero-Shot Robotic Manipulation with Agentic Operational Graph**|针对现有机器人操作方法在零样本泛化方面的不足，即端到端VLA模型缺乏精度而传统规划器语义刚性问题，本文提出了UniManip框架。该框架基于双层Agentic Operational Graph (AOG)，通过高层Agentic层进行任务编排和低层Scene层表示动态状态，实现语义推理与物理接地的统一，并以动态智能体循环方式主动实例化场景图、规划无碰撞轨迹并自主恢复失败。实验结果表明，UniManip在未见对象和任务上展现出鲁棒的零样本能力，成功率显著高于现有VLA和分层基线，且支持从固定基座到移动操作的零样本迁移。|Ziwei Wang Team|[2602.13086](http://arxiv.org/abs/2602.13086)|**[link](https://henryhcliu.github.io/unimanip)**|
|**2026-02-13**|**How Swarms Differ: Challenges in Collective Behaviour Comparison**|针对群体行为分析中数值特征集通常缺乏通用性且难以定量衡量行为相似性的问题，本研究深入探讨了特征集对集体行为的影响。我们从现有群体机器人学工作中筛选出特征集和相似性度量，并评估了它们在特定行为背景外的鲁棒性。研究发现，特征集和相似性度量的相互作用决定了区分相似行为群体的有效性，并提出了一种基于自组织图的方法来识别特征空间中行为难以区分的区域。|Jonas Kuckling Team|[2602.13016](http://arxiv.org/abs/2602.13016)|null|
|**2026-02-13**|**SafeFlowMPC: Predictive and Safe Trajectory Planning for Robot Manipulators with Learning-based Policies**|面对机器人日益融入日常生活对灵活性和实时反应能力的需求，以及学习方法缺乏安全保证和优化方法泛化能力不足的挑战，本文提出了SafeFlowMPC框架。该框架结合了流匹配与在线优化，旨在融合学习和优化方法的优势，并通过次优模型预测控制公式，实时确保操作安全性。在KUKA 7自由度机械臂上的真实世界实验（包括抓取和人机交接任务）中，SafeFlowMPC展现了强大的性能。|Andreas Kugi Team|[2602.12794](http://arxiv.org/abs/2602.12794)|null|
|**2026-02-13**|**Scaling Single Human Demonstrations for Imitation Learning using Generative Foundational Models**|鉴于模仿学习中收集机器人演示数据的困难以及人类演示到机器人转移的挑战，本文提出了Real2Gen框架，仅通过单个人类演示来训练操作策略。Real2Gen从人类演示中提取关键信息并传输到模拟环境，利用可编程专家智能体生成无限量的训练数据来学习流匹配策略。实验结果表明，Real2Gen平均成功率提高了26.6%，并且由于训练数据的丰富性，训练出的策略具有更强的泛化能力，实现了纯模拟训练策略的零样本真实世界部署。|Abhinav Valada Team|[2602.12734](http://arxiv.org/abs/2602.12734)|null|
|**2026-02-13**|**$\mathcal{X}$-KD: General Experiential Knowledge Distillation for Large Language Models**|针对大语言模型知识蒸馏（KD）中，现有方法常忽视教师模型原始学习环境的问题，本文提出了Experiential Knowledge Distillation ($\mathcal{X}$-KD) 框架。受经验学习理论和逆强化学习启发，$\mathcal{X}$-KD采用Approximated Variational Reward Imitation Learning (AVRIL) 框架，联合建模教师的原始奖励函数并执行策略蒸馏，使学生模型能在教师的原始学习环境中学习。实验证明，$\mathcal{X}$ -KD在抽象摘要、机器翻译和算术推理任务上均优于基线方法，并实现了更好的性能-多样性权衡和数据效率。|Yuyu Yuan Team|[2602.12674](http://arxiv.org/abs/2602.12674)|null|
|**2026-02-13**|**PMG: Parameterized Motion Generator for Human-like Locomotion Control**|为解决人形机器人运动中，现有全身体参考引导方法对高级命令接口适应性差、对数据和校准敏感等实际挑战，本文提出了Parameterized Motion Generator (PMG)。PMG是一种基于人类运动结构分析的实时运动生成器，通过紧凑的参数化运动数据和高维控制命令合成参考轨迹，并结合模仿学习流水线和仿真到现实电机参数识别模块。实验证明，该集成系统能生成自然、类人运动，精确响应高维控制输入（如VR远程操作），并实现高效、可验证的仿真到现实迁移。|Houde Liu Team|[2602.12656](http://arxiv.org/abs/2602.12656)|null|
|**2026-02-13**|**Real-to-Sim for Highly Cluttered Environments via Physics-Consistent Inter-Object Reasoning**|为解决从单视图观测重建物理有效3D场景时，现有方法常忽略物理约束导致无效状态，进而影响下游模拟可靠性的问题，本文提出了一种新颖的物理约束Real-to-Sim管道。该管道能够从单视图RGB-D数据重建物理一致的3D场景，其核心是一个可微分优化管道，通过接触图建模空间依赖，并利用可微分刚体模拟联合优化物体姿态和物理属性。实验结果表明，重建场景具有高物理保真度，能忠实复现真实世界接触动力学，从而实现稳定可靠的接触密集型操作。|Jun Ma Team|[2602.12633](http://arxiv.org/abs/2602.12633)|**[link](https://physics-constrained-real2sim.github.io)**|
|**2026-02-13**|**FlowHOI: Flow-based Semantics-Grounded Generation of Hand-Object Interactions for Dexterous Robot Manipulation**|近期视觉-语言-动作（VLA）模型能够生成看似合理的末端执行器运动，但在长程、富接触的任务中常常失败，因为缺乏对手-物体交互（HOI）结构的显式表示。为解决此问题，本文提出了FlowHOI，一个两阶段流匹配框架，它能根据第一人称观测、语言指令和3D高斯飞溅（3DGS）场景重建，生成语义接地、时间连贯的HOI序列，包括手部姿态、物体姿态和手-物体接触状态。该框架将以几何为中心的抓取与以语义为中心的操作解耦，后者通过紧凑的3D场景令牌进行条件化，并采用运动-文本对齐损失来语义化生成的交互。为解决高保真HOI监督数据稀缺的问题，本文引入了一个重建流水线，从大规模第一人称视频中恢复对齐的手-物体轨迹和网格，为鲁棒生成提供了HOI先验。FlowHOI在GRAB和HOT3D基准上实现了最高的动作识别精度和比最强扩散基线高1.7倍的物理模拟成功率，同时推理速度提升了40倍。此外，通过将生成的HOI表示重定向到真实机器人执行流程，本文在四个灵巧操作任务上验证了真实机器人执行的可行性。|Xingxing Zuo Team|[2602.13444](http://arxiv.org/abs/2602.13444)|**[link](https://huajian-zeng.github.io/projects/flowhoi/)**|
|**2026-02-12**|**FAIL: Flow Matching Adversarial Imitation Learning for Image Generation**|鉴于流匹配模型后训练与模仿学习的数学等同性，以及监督微调无法纠正策略漂移而偏好优化成本高昂的问题，本文提出了Flow Matching Adversarial Imitation Learning (FAIL) 框架。该框架通过对抗训练最小化策略与专家之间的散度，无需明确奖励或成对比较，并推导出了FAIL-PD和FAIL-PG两种算法。实验证明，FAIL在仅使用少量演示数据的情况下，能在提示遵循和美学基准上取得竞争性性能，并能有效泛化至离散图像和视频生成，同时作为鲁棒正则化器减轻奖励欺骗。|Weidi Xie Team|[2602.12155](http://arxiv.org/abs/2602.12155)|null|
|**2026-02-12**|**GigaBrain-0.5M*: a VLA That Learns From World Model-Based Reinforcement Learning**|针对传统VLA模型在场景理解和未来预测上的局限性，本研究提出了GigaBrain-0.5M*，一个基于世界模型强化学习的VLA模型。该模型在预训练的GigaBrain-0.5基础上，通过RAMP（Reinforcement leArning via world Model-conditioned Policy）进一步整合了世界模型强化学习，以实现鲁棒的跨任务适应性。实验结果表明，RAMP在洗衣折叠、箱子包装和意式浓缩咖啡制作等复杂任务中，相较于RECAP基线性能提升了约30%，并且在实际部署中展示了可靠的长期执行能力，能够无故障完成复杂操作任务。|Zheng Zhu Team|[2602.12099](http://arxiv.org/abs/2602.12099)|**[link](https://gigabrain05m.github.io/)**|
|**2026-02-12**|**Affordance-Graphed Task Worlds: Self-Evolving Task Generation for Scalable Embodied Learning**|鉴于在真实世界中训练机器人策略成本高昂且难以扩展，而现有生成模拟方法难以生成逻辑连贯的长时任务并处理动态物理不确定性，本研究提出了Affordance-Graphed Task Worlds (AGT-World) 框架。该框架能够根据真实世界观测自主构建交互式模拟环境和相应的机器人任务策略，通过将任务空间形式化为结构化图实现复杂目标的精确分层分解，并引入结合视觉-语言模型推理和几何验证的混合反馈自进化机制来完善策略。广泛实验证明，AGT-World在成功率和泛化能力上显著优于现有方法，实现了提议、执行和修正的自改进循环，以支持可扩展的机器人学习。|Changshui Zhang Team|[2602.12065](http://arxiv.org/abs/2602.12065)|null|
|**2026-02-12**|**HoloBrain-0 Technical Report**|为了弥合基础模型研究与可靠的真实世界机器人部署之间的差距，本研究引入了HoloBrain-0，一个全面的视觉-语言-动作 (VLA) 框架。该框架的核心是一个新颖的VLA架构，明确融入了机器人本体先验信息（如多视图相机参数和运动学描述），以增强3D空间推理并支持多样化的本体。通过“预训练-后训练”范式进行验证，该系统在RoboTwin 2.0、LIBERO和GenieSim等模拟基准测试中取得了最先进的成果，并在长时程真实世界操作任务中表现出色。值得注意的是，其高效的0.2B参数变体能与大得多的基线媲美，并支持低延迟的设备部署。为加速研究和实际应用，HoloBrain生态系统已完全开源。|Zhizhong Su Team|[2602.12062](http://arxiv.org/abs/2602.12062)|null|
|**2026-02-12**|**When would Vision-Proprioception Policies Fail in Robotic Manipulation?**|针对视觉-本体感受策略在复杂任务中泛化能力不一致的问题，本研究发现，在机器人运动转换的子阶段，视觉模态的作用有限，策略倾向于更简洁的本体感受信号，抑制了视觉学习。为此，我们提出了梯度调整与阶段引导 (GAP) 算法，通过利用本体感受估计运动转换阶段的概率，并据此自适应地调节本体感受梯度的幅度，从而实现视觉与本体感受的动态协作。综合实验表明，GAP算法在模拟和真实世界环境、单臂和双臂设置以及不同模型类型中均适用，并能形成鲁棒且可泛化的视觉-本体感受策略。|Di Hu Team|[2602.12032](http://arxiv.org/abs/2602.12032)|null|
|**2026-02-12**|**Accelerating Robotic Reinforcement Learning with Agent Guidance**|强化学习在机器人操作中面临严重的样本效率问题，而现有人机协作 (HIL) 方法虽能加速训练，但受限于可扩展性、操作员疲劳和不一致的人类专业知识。为解决此问题，本研究提出了Agent-guided Policy Search (AGPS) 框架，通过多模态智能体取代人工监督者，实现训练流程自动化。其核心思想是将智能体视为语义世界模型，注入内在价值先验来结构化物理探索，并利用可执行工具通过纠正性路点和空间约束提供精确指导。实验证明，AGPS在样本效率方面优于HIL方法，从而实现了无劳动力的可扩展机器人学习路径。|Yaodong Yang Team|[2602.11978](http://arxiv.org/abs/2602.11978)|null|
|**2026-02-12**|**Robot-DIFT: Distilling Diffusion Features for Geometrically Consistent Visuomotor Control**|本研究认为机器人操作泛化性的瓶颈在于当前视觉骨干网络与闭环控制物理需求之间的结构性不匹配，尤其在于现有模型缺乏精细的几何敏感性。鉴于生成扩散模型内在地编码了几何依赖性，但其随机性和延迟阻碍了直接应用，我们提出了Robot-DIFT框架。该框架通过流形蒸馏 (Manifold Distillation) 将冻结的扩散教师模型蒸馏到确定性空间-语义特征金字塔网络 (S2-FPN) 中，从而在保持生成模型丰富几何先验的同时，确保了时间稳定性、实时执行和抗漂移鲁棒性。在DROID数据集上的预训练结果表明，Robot-DIFT在几何一致性和控制性能上均优于领先的判别式基线，验证了视觉学习方式对机器人行为能力的关键影响。|Georgia Chalvatzaki Team|[2602.11934](http://arxiv.org/abs/2602.11934)|null|
|**2026-02-12**|**JEPA-VLA: Video Predictive Embedding is Needed for VLA Models**|现有VLA模型在机器人操作中仍面临样本效率低和泛化能力有限的问题，本研究认为这与预训练视觉表示在环境理解和策略先验方面知识不足有关。通过深入分析，我们发现现有VLA中常用的视觉表示未能有效捕获关键任务相关信息及诱导有效策略先验，而通过视频预训练的预测嵌入，特别是V-JEPA 2，能够灵活地处理不可预测因素并编码任务相关的时间动态。基于此，我们提出了JEPA-VLA，一种将预测嵌入自适应集成到现有VLA的简单有效方法。实验证明，JEPA-VLA在LIBERO、LIBERO-plus、RoboTwin2.0和真实机器人任务等基准测试中均取得了显著的性能提升。|Mingsheng Long Team|[2602.11832](http://arxiv.org/abs/2602.11832)|null|
|**2026-02-12**|**Clutt3R-Seg: Sparse-view 3D Instance Segmentation for Language-grounded Grasping in Cluttered Scenes**|针对杂乱环境中3D实例分割在遮挡、有限视角和噪声掩码下的挑战，本研究提出了Clutt3R-Seg，一种用于语言引导抓取的零样本鲁棒3D实例分割流水线。该方法的核心思想是引入语义线索的分层实例树，通过跨视图分组和条件替换，将噪声掩码作为信息线索，从而抑制过分割和欠分割，产生视图一致的掩码和鲁棒的3D实例。为应对多阶段任务中的场景变化，该方法还引入了一致性感知更新机制。在合成和真实世界数据集及真实机器人上的验证表明，Clutt3R-Seg在杂乱和稀疏视图场景中持续优于现有最先进基线，尤其在重度杂乱序列中表现出超过2.2倍的性能提升。|Ayoung Kim Team|[2602.11660](http://arxiv.org/abs/2602.11660)|null|
|**2026-02-12**|**ViTaS: Visual Tactile Soft Fusion Contrastive Learning for Visuomotor Learning**|针对现有机器人操作中视觉与触觉信息融合方法在遮挡场景下效果不佳、未能充分利用两种模态互补性且集成机制多为直接拼接的问题，本研究提出了ViTaS框架。该框架旨在结合视觉和触觉信息指导智能体行为，并引入了软融合对比学习（Soft Fusion Contrastive Learning）以及一个CVAE模块，以更好地利用视觉-触觉表示中的对齐和互补性。在12个模拟环境和3个真实世界环境中的实验验证表明，ViTaS显著优于现有基线，证明了其在利用多模态信息方面的有效性。|Huazhe Xu Team|[2602.11643](http://arxiv.org/abs/2602.11643)|null|
|**2026-02-12**|**EasyMimic: A Low-Cost Framework for Robot Imitation Learning from Human Videos**|鉴于大规模真实世界数据采集成本高昂阻碍了机器人模仿学习，尤其对低成本家用机器人而言，本研究提出了EasyMimic框架。该框架是一个低成本、可复制的解决方案，使机器人能通过标准RGB相机捕获的人类视频演示快速学习操作策略。其方法首先从视频中提取3D手部轨迹，并利用动作对齐模块将其映射到低成本机器人的夹持器控制空间；为弥合人机领域差距，引入了简单的手部视觉增强策略，并通过协同训练方法在处理过的人类数据和少量机器人数据上微调模型。实验证明，EasyMimic在LeRobot平台上在多种操作任务中取得了高性能，显著减少了对昂贵机器人数据采集的依赖，为智能机器人进入家庭提供了实用途径。|Qin Jin Team|[2602.11464](http://arxiv.org/abs/2602.11464)|null|
|**2026-02-12**|**Scaling World Model for Hierarchical Manipulation Policies**|视觉-语言-动作（VLA）模型在通用机器人操作中虽有前景，但在有限真实机器人数据下，其在分布外（OOD）场景中的泛化能力仍脆弱。本研究提出VISTA，一个分层视觉-语言-动作框架，利用大规模预训练世界模型的泛化能力实现鲁棒且可泛化的视觉子目标任务分解。该框架将世界模型作为高层规划器来生成带有目标图像的子任务序列，VLA作为低层执行器遵循指导生成动作。实验结果表明，与原始文本目标相比，合成的目标图像提供了更具视觉和物理细节的指导，使低层策略能泛化到新物体和场景，将VLA在OOD场景中的性能从14%提升至69%。|Xinghang Li Team|[2602.10983](http://arxiv.org/abs/2602.10983)|null|
|**2026-02-12**|**Self-Refining Vision Language Model for Robotic Failure Detection and Reasoning**|针对机器人故障推理中，真实世界故障的复杂性及丰富推理标签获取成本高昂的问题，本文提出了ARMOR框架。该框架将故障检测和推理建模为一个多任务自细化过程，模型通过迭代预测检测结果和自然语言推理，并从大规模稀疏二元标签和少量丰富推理标注的异构监督中学习。实验结果表明，ARMOR在故障检测率上比现有方法提升高达30%，在LLM模糊匹配分数测量的推理能力上提升高达100%，展现了对异构监督和开放式推理的鲁棒性。|Yesh Dattatreya Team|[2602.12405](http://arxiv.org/abs/2602.12405)|null|
|**2026-02-11**|**Human Preference Modeling Using Visual Motion Prediction Improves Robot Skill Learning from Egocentric Human Video**|当前从第一视角人类视频中学习机器人的方法，在度量视觉状态长期价值时存在假设限制，且在跨实体和环境时需要迁移学习到的价值函数。本研究提出一种新方法，通过预测后续图像中跟踪点的运动来建模人类偏好，并将奖励函数定义为机器人行为中预测与观察到的物体运动的一致性。随后，利用改进的Soft Actor Critic (SAC) 算法（通过10次机器人演示初始化）来估计此奖励的价值函数，并在机器人上优化策略。结果显示，该方法在真实机器人上有效，并且在模拟和真实机器人上的多项任务中，其学习到的策略与现有工作相当或超越。|Christopher G. Atkeson Team|[2602.11393](http://arxiv.org/abs/2602.11393)|null|
|**2026-02-11**|**MolmoSpaces: A Large-Scale Open Ecosystem for Robot Navigation and Manipulation**|机器人大规模部署要求对日常情境具备鲁棒性，但现有基准测试缺乏足够多样化的场景。本研究引入了MolmoSpaces，一个支持机器人策略大规模基准测试的开放生态系统，包含超过23万个多样化室内环境和13万个丰富注释的物体资产，且与模拟器无关。同时设计了MolmoSpaces-Bench基准套件。实验证明，MolmoSpaces-Bench具有强大的模拟到真实关联性，验证了新型零样本策略的优越性，并揭示了对提示措辞、初始关节位置和相机遮挡的关键敏感性，为机器人学习研究提供了可扩展的基础。|Ranjay Krishna Team|[2602.11337](http://arxiv.org/abs/2602.11337)|null|
|**2026-02-11**|**YOR: Your Own Mobile Manipulator for Generalizable Robotics**|随着机器人学习的发展，低成本机器人平台日益增多，但移动操作的最佳外形仍然是待解问题。本研究推出YOR，一个开源、低成本的移动操纵器，集成了全向基座、伸缩式垂直升降器和带抓手的双臂，以实现全身移动和操作。其设计强调模块化、易于组装和经济性（物料清单成本低于1万美元）。YOR在需要协调全身控制、双臂操作和自主导航的任务中展现出其能力，以远低于现有平台的成本提供了具有竞争力的移动操作研究功能。|Zichen Jeff Cui Team|[2602.11150](http://arxiv.org/abs/2602.11150)|null|
|**2026-02-11**|**ABot-M0: VLA Foundation Model for Robotic Manipulation with Action Manifold Learning**|在异构硬件上构建通用具身智能体是机器人学的一大挑战，面临数据碎片化、表示不一致和训练目标不匹配等问题。本研究提出了ABot-M0框架，它通过建立系统的数据整理管道，同时优化模型架构和训练策略，将异构原始数据转换为统一、高效的表示。通过构建大规模UniACT数据集，统一预训练提高了跨平台和任务的知识迁移和泛化能力。为提升动作预测效率和稳定性，引入了动作流形学习（AML）。此外，框架通过双流机制实现模块化感知，整合VLM语义与几何先验和多视角输入。实验结果表明各组件独立且具有累加效益。|Mu Xu Team|[2602.11236](http://arxiv.org/abs/2602.11236)|**[link](https://amap-cvlab.github.io/ABot-Manipulation/)**|
|**2026-02-11**|**OSIL: Learning Offline Safe Imitation Policies with Safety Inferred from Non-preferred Trajectories**|离线安全模仿学习面临在缺乏每时间步安全成本或奖励信息的演示数据中学习安全和奖励最大化策略的难题。本研究针对此问题提出了一种新颖的离线安全模仿学习算法OSIL，它通过非首选轨迹推断安全性。OSIL将安全策略学习表述为约束马尔可夫决策过程（CMDP），并通过推导奖励最大化目标的下界并学习一个估计非首选行为可能性的成本模型来重新构建CMDP问题，从而无需明确的安全成本和奖励标注。实验证明，OSIL能学习到更安全的策略，满足成本约束而不降低奖励性能，优于现有基线方法。|Balaraman Ravindran Team|[2602.11018](http://arxiv.org/abs/2602.11018)|null|
|**2026-02-11**|**Towards Learning a Generalizable 3D Scene Representation from 2D Observations**|现有方法在相机坐标系中构建表示，限制了其在机器人操作中的直接应用。本研究引入了一种可泛化的神经辐射场方法，用于根据以机器人自我为中心的观察来预测3D工作空间占用。该模型在全局工作空间框架中构建占用表示，使其直接适用于机器人操作，并能集成灵活的源视图，无需场景特定微调即可泛化到未见物体布局。在人形机器人上的实验证明，该模型在40个真实场景上训练后，实现了26毫米的重建误差（包括遮挡区域），验证了其推断完整3D占用（超越传统立体视觉方法）的能力。|Stefan Wermter Team|[2602.10943](http://arxiv.org/abs/2602.10943)|null|
|**2026-02-11**|**Semi-Supervised Cross-Domain Imitation Learning**|跨域模仿学习（CDIL）通过领域知识迁移加速策略学习，但在专家数据收集成本高昂时尤为重要。现有CDIL方法或依赖监督（不稳定），或无监督（不稳健）。本研究提出了半监督CDIL（SS-CDIL）设置及其首个具有理论依据的算法。该方法仅使用少量目标专家演示和未标记的不完善轨迹等离线数据。为解决域差异，提出一种新颖的跨域损失函数来学习域间状态-动作映射，并设计自适应权重函数以平衡源域和目标域知识。实验表明，该方法在MuJoCo和Robosuite上始终优于基线，实现了最小监督下稳定且数据高效的策略学习。|Ping-Chun Hsieh Team|[2602.10793](http://arxiv.org/abs/2602.10793)|null|
|**2026-02-11**|**Say, Dream, and Act: Learning Video World Models for Instruction-Driven Robot Manipulation**|机器人操作通常缺乏预测环境响应动作的能力，导致错误和低效，且现有视觉-语言模型（VLM）和世界模型在预测未来状态或生成空间一致帧方面存在局限。本研究提出一种用于快速且可预测的视频条件动作框架。该方法首先选择并适应一个鲁棒的视频生成模型以确保可靠的未来预测，然后应用对抗蒸馏进行快速、少步骤的视频生成，最后训练一个动作模型，利用生成的视频和真实观察来纠正空间误差。大量实验表明，该方法生成的时间连贯、空间准确的视频预测直接支持精确操作，在具身一致性、空间指代能力和任务完成度方面显著优于现有基线。|Yanwei Fu Team|[2602.10717](http://arxiv.org/abs/2602.10717)|null|

<p align=right>(<a href=#updated-on-20260225>back to top</a>)</p>

## World Model

|Publish Date|Title|Chinese Summary|Authors|PDF|Code|
|---|---|---|---|---|---|
|**2026-02-24**|**Brewing Stronger Features: Dual-Teacher Distillation for Multispectral Earth Observation**|地球观测（EO）领域的基础模型面临传感器多样性挑战，高效的跨模态知识迁移至关重要。现有EO预训练主要依赖掩码图像建模，但对全局语义结构控制有限。本文提出一种用于多光谱图像的双教师对比蒸馏框架，将学生模型的预训练目标与现代光学视觉基础模型（VFMs）的对比自蒸馏范式对齐，通过结合多光谱教师和光学VFM教师实现一致的跨模态表示学习。实验结果表明，该模型在不影响光学数据性能的情况下适应多光谱数据，在语义分割、变化检测和分类任务中均取得显著提升，平均F1分数分别提高3.64、1.2和1.31个百分点。|Luka Čehovin Zajc Team|[2602.19863](http://arxiv.org/abs/2602.19863)|null|
|**2026-02-24**|**Skullptor: High Fidelity 3D Head Reconstruction in Seconds with Multi-View Normal Prediction**|从图像重建高保真3D头部几何对广泛应用至关重要，但现有方法在细节、相机要求和计算成本之间存在权衡。本研究提出一种混合方法，通过引入多视图表面法线预测模型，将单目基础模型与跨视图注意力结合，生成几何一致的法线。随后，这些法线作为强几何先验，在逆渲染优化框架中用于恢复高频表面细节。实验结果表明，该方法在减少相机和计算成本的同时，实现了与密集视图摄影测量相当的高保真重建，性能优于当前最先进的单图像和多视图方法。|Abdallah Dib Team|[2602.21100](http://arxiv.org/abs/2602.21100)|null|
|**2026-02-24**|**OmniOCR: Generalist OCR for Ethnic Minority Languages**|尽管光学字符识别（OCR）取得了显著进展，但由于书写系统复杂、标注稀缺，少数民族语言的OCR仍然面临挑战。针对此问题，本研究提出了OmniOCR框架，通过引入动态低秩适应（Dynamic LoRA）在不同层和文字间有效分配模型容量并保留知识。此外，稀疏正则化用于剪枝冗余更新，确保高效适应而不增加推理成本。在TibetanMNIST、Shui、古彝文和东巴文上的评估显示，OmniOCR优于零样本基础模型和标准后训练方法，以更高的参数效率实现了最先进的准确性，在这些数据集上准确率提高了39%-66%。|Ying Cai Team|[2602.21042](http://arxiv.org/abs/2602.21042)|null|
|**2026-02-24**|**Cycle-Consistent Tuning for Layered Image Decomposition**|在真实图像中解耦视觉层，尤其是涉及非线性全局耦合的商标-物体分解，是一个长期挑战。本研究提出了一个利用大型扩散基础模型的上下文图像分解框架。该方法通过轻量级LoRA适应微调预训练扩散模型，并引入循环一致性调整策略，共同训练分解和合成模型，以强制分解与重构图像之间的一致性。此外，还引入了渐进式自改进过程，通过高质量模型生成示例迭代扩充训练集。实验证明，该方法能实现准确连贯的分解，并有效泛化到其他分解类型，展现了其作为统一框架的潜力。|Hui Huang Team|[2602.20989](http://arxiv.org/abs/2602.20989)|**[link](https://vcc.tech/research/2026/ImgDecom)**|
|**2026-02-24**|**Estimation of Confidence Bounds in Binary Classification using Wilson Score Kernel Density Estimation**|近年来，深度学习二元分类器的性能和易用性显著提升，为自动化关键检测任务带来了潜力，但其在关键操作中的应用依赖于可靠的置信区间估计。本研究提出了一种新颖的基于核的置信区间估计方法——Wilson Score核密度分类。其核心是Wilson Score核密度估计器，用于估计条件成功概率变化的二项式实验中的置信区间。在四个数据集上对选择性分类任务的评估表明，该方法作为任何特征提取器（包括视觉基础模型）的分类头，表现出与高斯过程分类相似的性能，但计算复杂度更低。|Frederik Hagelskjær Team|[2602.20947](http://arxiv.org/abs/2602.20947)|null|
|**2026-02-24**|**SpatiaLQA: A Benchmark for Evaluating Spatial Logical Reasoning in Vision-Language Models**|视觉-语言模型（VLMs）在理解和推理方面表现出色，但在复杂真实环境中做出合理决策，即空间逻辑推理能力仍有所欠缺。为了弥补这一差距，本研究引入了SpatiaLQA基准，旨在评估VLM的空间逻辑推理能力，该基准包含来自241个真实室内场景的9,605个问答对。针对现有模型的不足，研究提出了一种递归场景图辅助推理方法，利用视觉基础模型逐步将复杂场景分解为任务相关的场景图。实验结果显示，即使是最先进的VLM也难以处理空间逻辑推理，而所提出的方法则显著增强了VLM的空间逻辑推理能力，优于所有现有方法。|Jie Song Team|[2602.20901](http://arxiv.org/abs/2602.20901)|null|
|**2026-02-24**|**VGGDrive: Empowering Vision-Language Models with Cross-View Geometric Grounding for Autonomous Driving**|自动驾驶中跨视图3D几何建模能力至关重要，但现有视觉-语言模型（VLMs）普遍缺乏此能力，导致性能受限。本研究提出VGGDrive架构，旨在将VLMs与成熟3D基础模型的跨视图几何基础相结合，以弥补这一关键能力差距。具体来说，通过引入即插即用的跨视图3D几何使能器（CVGE），VGGDrive能够将冻结视觉3D模型中的跨视图3D几何特征与VLM的2D视觉特征桥接，通过分层自适应注入机制赋能VLM。广泛实验证明，VGGDrive在五个自动驾驶基准测试（包括跨视图风险感知、运动预测和轨迹规划）上提升了基础VLM的性能。|Long Chen Team|[2602.20794](http://arxiv.org/abs/2602.20794)|null|
|**2026-02-24**|**OrthoDiffusion: A Generalizable Multi-Task Diffusion Foundation Model for Musculoskeletal MRI Interpretation**|肌肉骨骼疾病的MRI诊断因其复杂性和对专业知识的高要求而充满挑战。本研究开发了OrthoDiffusion，一个统一的基于扩散的基础模型，用于多任务肌肉骨骼MRI解读。该框架使用三个针对特定方向的3D扩散模型，通过自监督方式在15,948张未标记膝关节MRI扫描上预训练，学习鲁棒的解剖特征，并整合这些视图特定表示以支持解剖分割和多标签诊断。评估结果显示，OrthoDiffusion在膝关节结构分割和异常检测方面表现出色，且在不同临床中心和MRI场强下展现了卓越的鲁棒性，仅用少量标记数据即可保持高诊断精度，并且所学特征可迁移至其他关节疾病诊断。|Dingyu Wang Team|[2602.20752](http://arxiv.org/abs/2602.20752)|null|
|**2026-02-24**|**RAYNOVA: 3D-Geometry-Free Auto-Regressive Driving World Modeling with Unified Spatio-Temporal Representation**|当前世界基础模型在模拟真实世界演化时，通常独立处理时空相关性并强制强3D几何先验，限制了泛化能力。本研究提出RAYNOVA，一个无几何世界模型，采用双因果自回归框架，遵循尺度感知和时间拓扑顺序，并利用全局注意力进行统一的4D时空推理。RAYNOVA通过相对Plücker射线位置编码构建各向同性时空表示，实现跨视图、帧和尺度的鲁棒泛化，并引入循环训练范式以缓解长时视频生成中的分布漂移。RAYNOVA在nuScenes上实现了最先进的多视图视频生成结果，具有高吞吐量和强大可控性，无需显式3D场景表示即可泛化到新颖视图和相机配置。|Wei Zhan Team|[2602.20685](http://arxiv.org/abs/2602.20685)|**[link](http://yichen928.github.io/raynova)**|
|**2026-02-24**|**UrbanFM: Scaling Urban Spatio-Temporal Foundation Models**|城市系统产生的时空数据流编码了城市演化的规律，但现有城市计算模型因“场景特定”而泛化能力受限。本研究以“规模化”为核心，系统探讨了城市时空基础模型的构建。通过数据规模化，构建了十亿级语料库WorldST，统一标准化了100多个全球城市的多种物理信号。通过计算规模化，引入MiniST单元以统一网格和传感器观测表示。通过架构规模化，提出了极简主义自注意力架构UrbanFM，旨在从海量数据中自主学习动态时空依赖。同时，建立了最大规模的城市时空基准EvalST。实验证明，UrbanFM在未见城市和任务上实现了卓越的零样本泛化能力，为大规模城市时空基础模型奠定了基础。|Yuxuan Liang Team|[2602.20677](http://arxiv.org/abs/2602.20677)|null|
|**2026-02-24**|**Recursive Belief Vision Language Model**|当前的视觉-语言-动作（VLA）模型在部分可观测下的长周期操作中表现不佳，主要因其观察驱动特性导致任务进度丢失、动作重复及高延迟。本研究提出了RB-VLA，一种以信念为中心的架构，通过自监督世界模型目标进行训练，维护一个紧凑的潜在状态，编码与任务相关的历史、动态和物体交互。该模型只需一次VLM查询即可获取高级意图，信念模块负责跟踪任务进度并在部分可观测下实现阶段感知、因果导向的控制。RB-VLA在长周期基准测试中表现优异，相比现有VLA模型显著提高了多阶段抓取放置和堆叠任务的成功率，并大幅降低了推理延迟和内存消耗，证明了基于信念的状态表示对长周期VLA策略的有效性。|Nirav Patel Team|[2602.20659](http://arxiv.org/abs/2602.20659)|null|
|**2026-02-24**|**The Finite Primitive Basis Theorem for Computational Imaging: Formal Foundations of the OperatorGraph Representation**|计算成像的前向模型传统上是模态特异的单一代码，本文证明了包括线性和非线性模态在内的广泛操作符类别Cimg中的所有前向模型，都可以通过11个基本原语（如Propagate, Modulate等）组成的有向无环图进行近似表示。研究还分析了成像物理学中的非线性特性，并提供了将任意Cimg模型转换为DAG的算法。在31种线性模态和9种非线性模态上的经验验证显示，该方法实现了低于0.01的相对操作符误差，为物理世界模型（PWM）框架奠定了数学基础。|Chengshuai Yang Team|[2602.20550](http://arxiv.org/abs/2602.20550)|null|
|**2026-02-24**|**Progressive Per-Branch Depth Optimization for DEFOM-Stereo and SAM3 Joint Analysis in UAV Forestry Applications**|自主无人机修剪树木需要精确的单分支3D重建，但复杂森林冠层中的立体视差图噪声过大。本文提出了一个渐进式流程，整合了DEFOM-Stereo基础模型视差估计、SAM3实例分割和多阶段深度优化，以生成鲁棒的单分支点云。该流程通过形态学操作、颜色验证及一个五阶段的深度噪声过滤方案，系统性地解决了掩模边界污染、分割不准确性和深度噪声等问题。在辐射松立体图像上的评估显示，该流程将平均单分支深度标准差降低了82%，同时保持了边缘保真度，生成了适用于自主修剪工具定位的几何一致3D点云。|Richard Green Team|[2602.20539](http://arxiv.org/abs/2602.20539)|null|
|**2026-02-24**|**Actor-Curator: Co-adaptive Curriculum Learning via Policy-Improvement Bandits for RL Post-Training**|大型基础模型在强化学习后训练中依赖大规模异构数据集，使有效的课程学习成为挑战。本文提出了ACTOR-CURATOR，一个可扩展且全自动的强化学习课程学习框架，用于大型语言模型（LLMs）的后训练。该框架学习一个神经策展器，通过优化预期策略性能改进，将问题选择公式化为非平稳随机多臂老虎机问题，从而动态选择训练问题。经验结果表明，ACTOR-CURATOR在多个推理基准上显著优于现有基线，在AIME2024和ARC-1D上分别取得了28.6%和30.5%的相对增益，并实现了高达80%的速度提升，证明了其在LLM可扩展后训练中的有效性。|Yisong Yue Team|[2602.20532](http://arxiv.org/abs/2602.20532)|null|
|**2026-02-24**|**Probing and Bridging Geometry-Interaction Cues for Affordance Reasoning in Vision Foundation Models**|视觉系统对“可供性”的真正理解依赖于几何感知和交互感知两种互补能力。本文通过系统探测视觉基础模型（VFMs），发现DINO模型本身编码了部分级别的几何结构，而Flux等生成模型包含丰富的、动词条件的（verb-conditioned）空间注意力图作为隐式交互先验。研究证明，通过免训练、零样本地融合DINO的几何原型和Flux的交互图，即可实现与弱监督方法相当的可供性估计。这些结果证实了几何感知和交互感知是VFM中理解可供性的基本组成部分，为感知如何基础行动提供了机制解释。|Jing Zhang Team|[2602.20501](http://arxiv.org/abs/2602.20501)|null|
|**2026-02-24**|**SegSEM: Enabling and Enhancing SAM2 for SEM Contour Extraction**|从扫描电子显微镜（SEM）图像中高保真提取2D轮廓对校准光学邻近效应修正（OPC）模型至关重要，但在数据稀缺的专业领域适应基础模型如SAM2面临挑战。本文提出了SegSEM框架，旨在少样本设置下将SAM2适应于SEM轮廓提取。该框架采用数据高效的微调策略，仅训练模型编码器，并整合传统算法作为置信度感知的备用方案，构成一个鲁棒的混合架构。在包含60张生产图像的小型数据集上的实验验证了该方法的可行性，为数据受限的工业应用中利用基础模型提供了一种有效方法。|Mingxuan Yuan Team|[2602.20471](http://arxiv.org/abs/2602.20471)|null|
|**2026-02-23**|**AdaWorldPolicy: World-Model-Driven Diffusion Policy with Online Adaptive Learning for Robotic Manipulation**|有效的机器人操作需要策略能够预测物理结果并适应动态环境。本文提出了AdaWorldPolicy框架，一个由世界模型驱动的扩散策略，结合在线自适应学习，以最少的人工干预增强动态条件下的机器人操作。其核心思想是世界模型提供强监督信号，支持在线自适应学习，并通过力矩反馈缓解动态力偏移。AdaWorldPolicy将世界模型、动作专家和力预测器整合为相互连接的流匹配扩散Transformer，并提出在线自适应学习策略，动态切换“动作生成”和“未来想象”模式以驱动模块更新。在模拟和真实机器人基准测试中，AdaWorldPolicy实现了最先进的性能，展现了对分布外场景的动态自适应能力。|Dong Xu Team|[2602.20057](http://arxiv.org/abs/2602.20057)|**[link](https://AdaWorldPolicy.github.io)**|
|**2026-02-23**|**Make Some Noise: Unsupervised Remote Sensing Change Detection Using Latent Space Perturbations**|遥感中的无监督变化检测（UCD）在不依赖标注数据的情况下定位语义变化，但现有方法因依赖预设变化类型假设而泛化能力受限。本文提出了MaSoN（Make Some Noise），一个端到端的UCD框架，在训练期间直接在潜在特征空间中合成多样化变化。MaSoN根据目标数据的特征统计动态估计并生成变化，确保了多样性并与目标域对齐，且易于扩展到新模态如SAR。在五个基准测试中，MaSoN取得了最先进的性能，平均F1分数提高了14.1个百分点，展现了强大的泛化能力。|Luka Čehovin Zajc Team|[2602.19881](http://arxiv.org/abs/2602.19881)|null|
|**2026-02-23**|**MAS-FIRE: Fault Injection and Reliability Evaluation for LLM-Based Multi-Agent Systems**|基于LLM的多智能体系统（MAS）的可靠性保障是一个挑战，因其通过非结构化自然语言协调易发生语义故障且难以察觉。现有端到端任务成功评估缺乏对故障原因和恢复能力的深入洞察。为解决此问题，本文提出了MAS-FIRE，一个系统性故障注入和可靠性评估框架，定义了包含15种故障类型的分类法，涵盖智能体内认知错误和智能体间协调失败，并通过三种非侵入性机制注入故障。将MAS-FIRE应用于三种代表性MAS架构，揭示了四层故障容忍行为，并发现更强大的基础模型并不总是提高鲁棒性，而迭代、闭环架构能有效中和导致系统崩溃的故障。MAS-FIRE提供了过程级可观察性和可操作性指导，以系统改进MAS。|Zibin Zheng Team|[2602.19843](http://arxiv.org/abs/2602.19843)|null|
|**2026-02-23**|**Open-vocabulary 3D scene perception in industrial environments**|生产、内部物流或制造环境中的自主视觉应用需要超越固定类别集之外的开放词汇感知能力。然而，现有开放词汇方法利用2D视觉-语言基础模型 (VLFMs)，但常依赖在非工业数据集上预训练的与类别无关的分割模型，导致在常见工业对象上性能不佳。为此，本研究提出了一种免训练的开放词汇3D感知管道，以克服这一局限。该方法不使用预训练模型生成实例提议，而是通过根据语义特征合并预先计算的超点来直接生成掩码。随后，将领域自适应的VLFM "IndustrialCLIP" 应用于代表性的3D工业车间场景进行开放词汇查询。定性结果表明，该管道能够成功分割工业对象，解决了现有方法在工业场景中的泛化性问题。|Thorsten Schüppstuhl Team|[2602.19823](http://arxiv.org/abs/2602.19823)|null|
|**2026-02-23**|**OpenClaw, Moltbook, and ClawdLab: From Agent-Only Social Networks to Autonomous Scientific Research**|针对自主AI-to-AI交互生态系统中出现的集体现象、安全漏洞和反复出现的架构故障模式，本研究进行了一项多声部文献综述。背景是开源代理框架OpenClaw和社交网络Moltbook于2026年1月生成的大规模自主AI交互数据集引发了广泛关注。为响应这些架构故障模式，本研究提出了ClawdLab，一个用于自主科学研究的开源平台。ClawdLab通过硬性角色限制、结构化对抗性批评、PI主导的治理、多模型编排以及将领域特定证据要求编码为协议约束等方法来解决识别出的故障模式，将验证过程基于计算工具输出而非社会共识。该架构通过结构性设计实现了新兴的女巫攻击抵抗力，并提出了三层分类法来区分单智能体管道、预定多智能体工作流和完全去中心化系统。研究指出，ClawdLab的可组合三层架构，其基础模型、能力、治理和证据要求均可独立修改，能够随着更广泛AI生态系统的进步而实现复合改进。|Aakaash Meduri Team|[2602.19810](http://arxiv.org/abs/2602.19810)|null|
|**2026-02-23**|**Compositional Planning with Jumpy World Models**|智能决策的核心在于利用时间抽象进行规划，即通过组合预训练策略来解决复杂任务。然而，由于长远预测中的复合误差，估计策略序列诱导的访问分布具有挑战性，使得这种组合规划难以实现。受几何策略组合框架启发，本研究提出了通过学习多步动态的预测模型，即“跳跃式世界模型”，来应对这些挑战。该模型以离线方式捕获预训练策略在多个时间尺度上诱导的状态占据。在Temporal Difference Flows的基础上，本研究通过新颖的一致性目标增强这些模型，对齐跨时间尺度的预测，从而提高长远预测精度。实验结果表明，使用跳跃式世界模型的组合规划在各种基本策略下的零样本性能显著提高，在挑战性的操作和导航任务上，对于长时任务，相对于原始动作规划平均提高了200%。|Ahmed Touati Team|[2602.19634](http://arxiv.org/abs/2602.19634)|null|
|**2026-02-23**|**Seeing Clearly, Reasoning Confidently: Plug-and-Play Remedies for Vision Language Model Blindness**|视觉语言模型 (VLMs) 在广泛的视觉理解方面取得了显著成功，但在稀有对象的以对象为中心的推理方面仍面临挑战，原因在于预训练数据中此类实例稀缺。现有方法通过检索额外数据或引入更强的视觉编码器来缓解此问题，但这些方法在微调VLMs时计算密集，且未能充分利用原始训练数据。为此，本研究提出了一种高效的即插即用模块，通过精炼视觉token和丰富输入文本提示来显著改善VLMs对稀有对象的推理，而无需微调VLMs。具体而言，该方法通过利用视觉基础模型的先验知识和同义词增强的文本描述，学习稀有对象的多模态类别嵌入，以弥补训练示例的局限。这些嵌入通过轻量级基于注意力的增强模块精炼VLMs中的视觉token，改善细粒度对象细节。此外，学习到的嵌入还被用作对象感知检测器，生成信息提示注入文本提示中，以引导VLM关注相关图像区域。实验结果表明，该方法在稀有对象识别和推理方面为预训练的VLMs带来了持续且显著的提升。|Zhengming Ding Team|[2602.19615](http://arxiv.org/abs/2602.19615)|null|
|**2026-02-23**|**Satellite-Based Detection of Looted Archaeological Sites Using Machine Learning**|考古遗址盗掘对文化遗产构成严重威胁，但监测成千上万个偏远地点在操作上仍然困难。为解决这一问题，本研究提出了一个可扩展的基于卫星的考古遗址盗掘检测管道。该方法利用PlanetScope月度影像和阿富汗1943个考古遗址的整理数据集，比较了两种主要方法：(i) 在原始RGB补丁上训练的端到端CNN分类器，以及 (ii) 在手工制作的光谱/纹理特征和最新遥感基础模型嵌入上训练的传统机器学习 (ML) 方法。实验结果表明，ImageNet预训练的CNN结合空间掩模达到了0.926的F1分数，明显优于最强的传统ML设置（F1分数为0.710）。消融研究进一步证实，ImageNet预训练（即使存在领域偏移）和空间掩模均能显著提高性能，而地理空间基础模型嵌入与手工特征表现相当，暗示盗掘特征具有极强的局部性。|Juan Lavista Ferres Team|[2602.19608](http://arxiv.org/abs/2602.19608)|null|
|**2026-02-23**|**HOCA-Bench: Beyond Semantic Perception to Predictive World Modeling via Hegelian Ontological-Causal Anomalies**|视频大语言模型 (Video-LLMs) 在语义感知方面持续改进，但在预测性世界建模方面仍显不足，而这对于基于物理的智能至关重要。为评估这一能力，本研究引入了HOCA-Bench基准测试，该基准从黑格尔视角将物理异常分为本体论异常（实体违反自身定义或持久性）和因果异常（交互违反物理关系）。研究人员使用最先进的生成式视频模型作为对抗性模拟器，构建了一个包含1,439个视频和3,470对问答的测试平台。对17个Video-LLMs的评估结果显示出明显的认知滞后：模型通常能识别静态的本体论违规（如形状突变），但在处理因果机制（如重力或摩擦力）时却表现困难，因果任务的性能下降超过20%。虽然系统2“思维”模式能改善推理，但未能弥合这一差距，表明当前架构识别视觉模式比应用基本物理定律更容易。|Zhiping Cai Team|[2602.19571](http://arxiv.org/abs/2602.19571)|null|
|**2026-02-23**|**CIBER: A Comprehensive Benchmark for Security Evaluation of Code Interpreter Agents**|LLM代码解释器代理在关键工作流中日益普及，但其代码执行能力引入的安全风险，尤其是在动态执行、工具交互和多轮上下文方面，尚未得到充分研究。为此，研究人员提出了CIBER，一个自动化的基准测试平台，通过动态攻击生成、隔离沙盒和状态感知评估，系统性地评估代理对抗直接/间接提示注入、内存投毒和基于提示的后门等四类主要对抗攻击的脆弱性。实验结果表明，解释器架构和模型对齐是安全基线，结构集成使专用模型优于通用SOTA模型；高智能反而因指令遵循性强而更易受复杂对抗提示影响；自然语言作为输入模式比显式代码更有效，能绕过语法防御；代理对显式威胁防御稳健，但对隐式语义危害却灾难性失败，揭示了当前模式匹配保护方法的盲点。|Songze Li Team|[2602.19547](http://arxiv.org/abs/2602.19547)|null|
|**2026-02-23**|**A Text-Guided Vision Model for Enhanced Recognition of Small Instances**|随着无人机目标检测技术的发展，用户对精确识别特定目标的需求日益增长，尤其是在小目标检测方面。为满足此需求，本研究提出了一种改进的YOLO-World模型，旨在增强文本引导的小目标检测能力。具体方法是将YOLOv8骨干网络中的C2f层替换为C3k2层，以更精确地捕捉局部特征，并采用并行处理优化来提高速度和效率，同时实现轻量化设计。在VisDrone数据集上的比较实验表明，该模型在精度、召回率、F1分数和mAP@0.5等指标上均优于原始YOLO-World模型，且参数量和FLOPs有所减少，验证了其在准确性和轻量化方面的提升，为无人机精确目标检测提供了实用解决方案。|Hyun-Ki Jung Team|[2602.19503](http://arxiv.org/abs/2602.19503)|null|
|**2026-02-23**|**Forgetting-Resistant and Lesion-Aware Source-Free Domain Adaptive Fundus Image Analysis with Vision-Language Model**|源域无关域适应（SFDA）旨在仅利用未标记目标域数据和源模型在目标域进行模型适应，但传统方法在域偏移下易出错。近期虽有研究尝试结合视觉-语言（ViL）基础模型，但存在目标模型优越预测遗忘和忽略ViL模型细粒度知识的问题。为此，本文提出了一种新型的抗遗忘和病灶感知（FRLA）方法，用于基于ViL模型的眼底图像诊断SFDA。该方法包含一个抗遗忘适应模块以保留模型置信预测，以及一个病灶感知适应模块利用ViL模型的像素级预测来引导目标模型感知病灶区域并利用其细粒度知识。大量实验证实，该方法不仅显著优于ViL模型，且超越了最先进的SFDA方法。|Xiaomeng Li Team|[2602.19471](http://arxiv.org/abs/2602.19471)|null|
|**2026-02-23**|**MACE-POLAR-1: A Polarisable Electrostatic Foundation Model for Molecular Chemistry**|计算化学中，静电相互作用和电荷转移的精确建模至关重要，但大多数机器学习原子间势（MLIPs）难以捕捉长程静电效应。本研究提出一种新的分子化学静电基础模型，通过明确处理长程相互作用和静电感应，扩展了MACE架构。该方法结合局部多体几何特征与非自洽场形式，通过可极化迭代更新可学习的电荷和自旋密度，并通过可学习的Fukui函数进行全局电荷平衡。模型在OMol25数据集上训练，实现了化学精度，并在热化学、反应势垒、构象能量和过渡金属络合物方面与混合DFT相当。引入长程静电作用显著改善了非共价相互作用和超分子络合物的描述，在分子晶体形成能和蛋白质-配体相互作用上表现出色。该模型因其处理可变电荷和自旋状态、响应外部场及提供可解释电荷密度的能力，成为计算分子化学和药物发现的通用工具。|Gábor Csányi Team|[2602.19411](http://arxiv.org/abs/2602.19411)|null|
|**2026-02-23**|**Hilbert-Augmented Reinforcement Learning for Scalable Multi-Robot Coverage and Exploration**|在分散式多机器人系统中，稀疏奖励环境下的覆盖探索效率和冗余是一个重要挑战。本研究提出一个覆盖框架，将Hilbert空间填充先验整合到分散式多机器人学习和执行中。该方法通过Hilbert-based空间索引增强DQN和PPO算法，以结构化探索并减少冗余。此外，还设计了一个路径点接口，将Hilbert排序转换为曲率受限、时间参数化的SE(2)轨迹，以确保在资源受限机器人上的板载可行性。实验结果表明，该方法在覆盖效率、冗余和收敛速度上均优于DQN/PPO基线。在Boston Dynamics Spot机器人的实际验证中，生成的轨迹实现了可靠且低冗余的室内覆盖，证明了几何先验能够显著提升群体和腿式机器人的自主性和可扩展性。|Aryya Gangopadhyay Team|[2602.19400](http://arxiv.org/abs/2602.19400)|null|
|**2026-02-23**|**In-context Pre-trained Time-Series Foundation Models adapt to Unseen Tasks**|时间序列基础模型（TSFMs）虽然泛化能力强，但在未经微调的情况下难以适应未见任务。为解决此问题，本文提出了情境时间序列预训练（ICTP）框架，通过情境学习（ICL）增强TSFMs的能力。ICTP重构原始预训练数据，使主干TSFM能够通过动态适应情境中提供的输入-输出关系进行测试时推理，从而适应未见任务。实验结果表明，ICTP在未见任务上将最先进的TSFMs性能提高了约11.4%，且无需进行微调，显著提升了模型的泛化能力。|B. Aditya Prakash Team|[2602.20307](http://arxiv.org/abs/2602.20307)|null|
|**2026-02-22**|**Adaptive Data Augmentation with Multi-armed Bandit: Sample-Efficient Embedding Calibration for Implicit Pattern Recognition**|在现代AI应用中，识别隐式视觉和文本模式至关重要，但长尾模式识别任务对LLMs和VLMs等预训练基础模型仍具挑战。虽然微调可提高精度，但通常因训练数据稀缺和计算开销大而不可行。针对此问题，本文提出了ADAMAB，一个高效的嵌入校准框架，用于少样本模式识别。该框架在固定嵌入模型上训练嵌入器无关的轻量级校准器，无需访问模型参数以降低计算成本。同时，引入基于多臂老虎机（MAB）机制的自适应数据增强策略，通过改进的UPPER CONFIDENCE BOUND算法减少梯度偏移，并在少样本训练中提供理论上的收敛保证。多模态实验证明，当每个类别仅用少于5个初始数据样本进行训练时，ADAMAB可将准确率提升高达40%，显示出其优越性能。|Taha Belkhouja Yujia Bao Team|[2602.19385](http://arxiv.org/abs/2602.19385)|null|
|**2026-02-22**|**Detector-in-the-Loop Tracking: Active Memory Rectification for Stable Glottic Opening Localization**|在视频喉镜检查中，由于单帧检测器缺乏时间上下文和基础模型跟踪器存在记忆漂移，声门开口定位的瞬时稳定性面临挑战，特别是在紧急情况下，快速组织变形、遮挡和视觉模糊要求鲁棒、时间感知的解决方案。为此，研究团队提出了闭环记忆校正（CL-MC）框架，这是一个检测器在环的框架，它通过置信度对齐的状态决策和主动记忆修正来监督Segment Anything Model 2 (SAM2)。高置信度检测能触发语义重置，覆盖损坏的跟踪器记忆，有效减轻复杂内窥镜场景中无训练基础跟踪器的漂移累积。在紧急插管视频上的实验表明，CL-MC实现了最先进的性能，与SAM2变体和开环方法相比，显著降低了漂移和漏检率，证实了记忆校正对可靠临床视频跟踪的重要性。|Jenq-Neng Hwang Team|[2602.19380](http://arxiv.org/abs/2602.19380)|null|
|**2026-02-22**|**Vid2Sid: Videos Can Help Close the Sim2Real Gap**|校准机器人模拟器物理参数以匹配真实硬件通常依赖手动或黑盒优化器，这些方法虽能减少误差但无法解释物理差异来源，且在仅限于外部摄像头的感知下，问题因感知噪声和缺乏直接测量而更复杂。本研究提出Vid2Sid，一个视频驱动的系统识别流程，它将基础模型感知与循环中的VLM优化器相结合，通过分析模拟与真实视频，诊断具体失配，并提供带自然语言解释的物理参数更新建议。在腱驱动手指和可变形连续触手上的实验表明，Vid2Sid在未见过的sim2real控制中取得最佳平均排名，媲美或超越黑盒优化器，并独特地提供可解释推理。Sim2sim验证证实Vid2Sid最准确地恢复了真实参数，且消融分析揭示，VLM引导优化在感知清晰且模拟器表达力强时表现卓越，但在挑战性设置中受模型类别限制。|Josie Hughes Team|[2602.19359](http://arxiv.org/abs/2602.19359)|null|
|**2026-02-22**|**US-JEPA: A Joint Embedding Predictive Architecture for Medical Ultrasound**|超声（US）成像因其固有的噪声采集过程，对表征学习构成独特挑战，低信噪比和随机散斑模式阻碍了依赖像素级重建的标准自监督学习方法。联合嵌入预测架构（JEPAs）通过预测掩码潜表示而非原始像素来解决此问题，但标准方法依赖于超参数敏感且计算成本高的在线教师模型。本研究提出US-JEPA，一个自监督框架，采用静态教师非对称潜训练（SALT）目标，通过使用冻结的、领域特定教师模型提供稳定的潜在目标，解耦学生-教师优化并促使学生扩展教师的语义先验。此外，本研究首次在UltraBench上严格比较了所有公开的最先进超声基础模型。在各种分类任务的线性探测下，US-JEPA实现了与领域特定和通用视觉基础模型基线相当或更优的性能，表明掩码潜在预测为稳健超声表征提供了一条稳定高效的途径。|William Speier Team|[2602.19322](http://arxiv.org/abs/2602.19322)|null|
|**2026-02-22**|**WildOS: Open-Vocabulary Object Search in the Wild**|在复杂、非结构化的室外环境中进行自主导航时，机器人需要在无预设地图和有限深度感知下进行远距离操作。在这种情况下，仅依靠几何边界进行探索往往不足，语义推理对于确定安全可遍历区域和目的地至关重要。本文提出了WildOS，一个用于长距离、开放词汇对象搜索的统一系统，它结合了安全几何探索与语义视觉推理。WildOS通过构建稀疏导航图来维护空间记忆，并利用基于基础模型的视觉模块ExploRFM对图的边界节点进行评分，ExploRFM能实时预测图像空间中的可遍历性、视觉边界和对象相似性，从而实现实时的板载语义导航。此外，系统还引入基于粒子滤波器的方法，用于开放词汇目标查询的粗略定位，以估计超出机器人即时深度范围的候选目标位置，实现对远距离目标的有效规划。在多种越野和城市地形的闭环现场实验表明，WildOS在效率和自主性方面显著优于纯几何和纯视觉基线，突出了视觉基础模型在驱动语义信息和几何基础的开放世界机器人行为方面的潜力。|Patrick Spieler Team|[2602.19308](http://arxiv.org/abs/2602.19308)|null|
|**2026-02-20**|**Generated Reality: Human-centric World Simulation using Interactive Video Generation with Hand and Camera Control**|现有扩展现实（XR）中的视频世界模型缺乏对用户精确实时动作的响应能力，限制了其在具身交互中的应用。为此，本研究提出了一个以人为中心的视频世界模型，该模型以跟踪到的头部姿态和关节级手部姿态为条件，并引入了一种有效的3D头手控制机制以实现灵巧的手物交互。通过训练和蒸馏双向视频扩散模型，最终形成一个可交互的因果系统，生成第一人称视角的虚拟环境。实验结果表明，该系统显著提升了任务执行性能，并提高了用户对其动作的感知控制水平，优于现有基线方法。|Gordon Wetzstein Team|[2602.18422](http://arxiv.org/abs/2602.18422)|**[link](https://codeysun.github.io/generated-reality)**|
|**2026-02-20**|**JPmHC Dynamical Isometry via Orthogonal Hyper-Connections**|Hyper-Connections (HC) 虽提升了深度学习性能，但其牺牲了残差连接的恒等映射性质，导致训练不稳定和可扩展性问题。为解决此挑战，本文提出了JPmHC (Jacobian-spectrum Preserving manifold-constrained Hyper-Connections) 框架，用可训练的线性混合器替代恒等跳跃连接，并将其约束在算子范数有界流形上，以控制梯度条件、防止梯度病理并增强稳定性。该方法通过自由概率分析、内存高效的隐式微分和基于Cayley变换的Stiefel约束混合器实现。在ARC-AGI基准上的实验证明，JPmHC比现有双随机基线收敛更快，准确性更高，计算成本更低，为深度学习提供了频谱感知、稳定且高效的新途径。|Leo Brunswic Team|[2602.18308](http://arxiv.org/abs/2602.18308)|null|
|**2026-02-20**|**On the Adversarial Robustness of Discrete Image Tokenizers**|离散图像tokenizer在多模态系统中日益普及，但其对对抗性攻击的脆弱性尚未被深入研究。本研究首次探讨了这一问题，提出了一系列计算高效、与应用无关的攻击方法，这些攻击能有效扰动tokenizer提取的特征并改变生成的token，在分类、多模态检索和图像描述任务中均表现出效力。为增强防御，研究团队受鲁棒CLIP编码器启发，通过无监督对抗训练对现有tokenizer进行微调，且保持其他组件不变。实验结果显示，该无监督且与任务无关的方法显著提升了模型对无监督和端到端监督攻击的鲁棒性，并展现出良好的跨任务和跨数据泛化能力，其利用未标记图像的特性使其更具通用性。本研究强调了tokenizer鲁棒性在下游任务中的关键作用，并为开发安全的多模态基础模型迈出了重要一步。|Francesco Croce Team|[2602.18252](http://arxiv.org/abs/2602.18252)|null|
|**2026-02-20**|**Comparative Assessment of Multimodal Earth Observation Data for Soil Moisture Estimation**|农场级高分辨率土壤湿度（SM）估算对于精准农业至关重要，但现有卫星产品分辨率不足。本研究提出了一个针对欧洲植被区的高分辨率（10米）SM估算框架，融合了Sentinel-1 SAR、Sentinel-2光学图像和ERA-5再分析数据，并运用机器学习方法。通过空间交叉验证，研究比较了不同模态组合与时间参数化的效果，并评估了IBM-NASA Prithvi模型的基础模型嵌入是否优于传统手工光谱特征。结果显示，Sentinel-2当日与Sentinel-1降轨数据结合的混合时间匹配方案取得了R^2=0.514，配合10天ERA5回溯窗口可提升至R^2=0.518。值得注意的是，Prithvi基础模型嵌入对性能的提升微乎其微，表明在稀疏数据回归任务中，领域特定的光谱指数结合基于树的集成方法仍是一种实用且计算高效的泛欧农场尺度SM监测方案。|Charalampos Kontoes Team|[2602.18083](http://arxiv.org/abs/2602.18083)|null|
|**2026-02-20**|**Towards LLM-centric Affective Visual Customization via Efficient and Precise Emotion Manipulating**|现有视觉定制研究多侧重客观内容对齐，忽视了图像的主观情感，并缺乏通用情感视觉定制的基础模型。本文提出了以LLM为中心的情感视觉定制（L-AVC）任务，旨在通过多模态LLM生成并修改图像的主观情感。针对情感语义转换的有效对齐和情感无关内容的精确保留这两个挑战，研究团队提出了一种高效精确情感操控（EPEM）方法，其中包含高效跨情感转换（EIC）模块以实现语义情感转换的对齐，以及精确情感无关内容保留（PER）模块以保留情感无关内容。在构建的L-AVC数据集上的全面实验证明，所提出的EPEM方法在L-AVC任务上显著优于现有SOTA基线，突显了情感信息对该任务的重要性及其操纵方法的有效性。|Jiahong Lu Team|[2602.18016](http://arxiv.org/abs/2602.18016)|null|
|**2026-02-20**|**ROCKET: Residual-Oriented Multi-Layer Alignment for Spatially-Aware Vision-Language-Action Models**|视觉-语言-动作（VLA）模型在机器人操作中指令遵循表现出色，但由于主要在2D数据上预训练，其3D空间理解能力不足。尽管表征对齐是弥补这一缺陷的有效方法，但现有方法通常仅在单层应用监督，未能充分利用深层信息，且多层朴素对齐易引发梯度冲突。为解决此问题，本文提出了ROCKET框架，它将多层对齐表述为将一个残差流与另一个对齐，并利用共享投影仪通过层不变映射对齐VLA骨干网络与3D视觉基础模型的多个层，从而有效减少梯度冲突。理论分析与实验证明了共享投影仪的有效性，并进一步提出了Matryoshka风格的稀疏激活方案以平衡多重对齐损失。实验结果表明，ROCKET结合免训练层选择策略，仅需约4%的计算预算，却能在LIBERO上达到98.5%的SOTA成功率，并在多个VLA模型和基准上展现了卓越性能。|Ang Li Team|[2602.17951](http://arxiv.org/abs/2602.17951)|null|
|**2026-02-19**|**Reverso: Efficient Time Series Foundation Models for Zero-shot Forecasting**|时间序列基础模型在零样本预测方面潜力巨大，但现有大型Transformer模型参数量庞大，导致实际应用效率和成本问题。本研究提出一种学习高效零样本时间序列预测基础模型的简单方法，其规模远小于现有模型。研究发现，通过结合长卷积层和线性RNN层（特别是DeltaNet层）的小型混合模型，性能可与大型Transformer模型媲美，但体积却小一百多倍。此外，论文还介绍了多种数据增强和推理策略以进一步提升性能。这些结合产生了Reverso系列高效时间序列零样本预测基础模型，显著提升了性能-效率的帕累托前沿。|Yoon Kim Team|[2602.17634](http://arxiv.org/abs/2602.17634)|null|
|**2026-02-19**|**AI Gamestore: Scalable, Open-Ended Evaluation of Machine General Intelligence with Human Games**|在快速发展的技术时代，严格评估机器智能的通用性变得愈发重要和困难，而传统AI基准往往过于狭隘且易饱和。为此，本研究提出通过通用游戏玩法来评估AI系统的类人通用智能，即衡量AI系统在“所有可想象的人类游戏”中与人类玩家的对比表现。作为实现此愿景的第一步，研究引入了AI GameStore，一个可扩展、开放式的平台，它利用LLM与人类协作，通过从流行数字游戏平台获取并改编标准化、容器化的游戏环境，合成新的代表性人类游戏。概念验证基于Apple App Store和Steam排行榜生成了100款游戏，并评估了七个前沿视觉-语言模型（VLM）。结果显示，即使是最好的VLM在大多数游戏上的人类平均得分也不足10%，尤其在世界模型学习、记忆和规划等挑战性任务中表现不佳，这为衡量和推动机器类人通用智能的发展提供了实践途径。|Joshua B. Tenenbaum Team|[2602.17594](http://arxiv.org/abs/2602.17594)|null|
|**2026-02-19**|**Systematic Evaluation of Single-Cell Foundation Model Interpretability Reveals Attention Captures Co-Expression Rather Than Unique Regulatory Signal**|为了评估单细胞基础模型的机制可解释性，本研究提出了一个包含37项分析、153项统计测试的系统评估框架。通过该框架对scGPT和Geneformer进行评估，发现注意力模式编码了具有层级结构的生物信息，但这种结构对扰动预测没有额外价值，基因层面的基线表现更优。为解决注意力特有的扩展失效问题，研究引入了细胞状态分层可解释性（CSSI），成功将基因调控网络恢复能力提升了1.85倍，并为该领域建立了可重用的质量控制标准。|Ihor Kendiukhov Team|[2602.17532](http://arxiv.org/abs/2602.17532)|null|
|**2026-02-19**|**Dataless Weight Disentanglement in Task Arithmetic via Kronecker-Factored Approximate Curvature**|任务算术为基础模型提供了模块化、可扩展的适应方式，但组合任务向量可能导致跨任务干扰，引起表示漂移并降低性能。现有的正则化方法通常需要外部任务数据，与模块化原则和数据可用性相冲突。为此，本研究提出了一种无数据方法，将表示漂移正则化视为曲率矩阵近似问题，并采用Kronecker分解近似曲率技术。该方法在任务添加和否定方面取得了最先进的成果，且计算复杂度与任务数量无关，增强了对任务向量缩放的鲁棒性，无需额外的调优。|Simone Calderara Team|[2602.17385](http://arxiv.org/abs/2602.17385)|null|
|**2026-02-19**|**Computer-Using World Model**|在复杂的软件环境中，智能体需要对操作后果进行推理，因为即使是单一的错误UI操作也可能破坏长期的工作流。针对真实执行无法支持反事实探索的问题，本研究引入了计算机使用世界模型（CUWM），一个用于桌面软件的世界模型。CUWM采用UI动态的两阶段分解：首先预测与智能体相关的文本描述状态变化，然后将其可视化以合成下一个屏幕截图。CUWM在Microsoft Office真实应用程序的离线UI转换数据上进行训练，并通过轻量级强化学习进行微调，实验结果表明，在测试时行动搜索中，CUWM能够提高决策质量和执行鲁棒性。|Dongmei Zhang Team|[2602.17365](http://arxiv.org/abs/2602.17365)|null|
|**2026-02-19**|**FRAPPE: Infusing World Modeling into Generalist Policies via Multiple Future Representation Alignment**|使视觉语言动作（VLA）模型能够预测环境动态（即世界建模）对于提高机器人推理和泛化至关重要。当前方法面临像素级重建过度强调导致语义学习受限，以及推理时依赖预测未来观测导致误差累积的问题。为此，本研究提出了通过并行渐进扩展的未来表示对齐（FRAPPE）方法，采用两阶段微调策略，模型在中期学习预测未来观测的潜在表示，后期并行扩展计算工作量并与多个视觉基础模型对齐。该方法显著提高了微调效率并减少了对动作标注数据的依赖，在RoboTwin基准和真实世界任务中均优于现有方法，展现出强大的长周期和未见场景泛化能力。|Donglin Wang Team|[2602.17259](http://arxiv.org/abs/2602.17259)|**[link](https://h-zhao1997.github.io/frappe)**|
|**2026-02-19**|**Structured Prototype-Guided Adaptation for EEG Foundation Models**|脑电图（EEG）基础模型（EFMs）在完全微调下表现出色，但在真实临床场景常见的受试者级别监督受限时泛化能力差，这源于噪声数据与EFMs高可塑性参数空间之间的结构不匹配。为解决此问题，本研究提出了SCOPE，一个结构化置信度感知原型引导的EFM微调框架。SCOPE采用两阶段流程：首先通过学习几何正则化任务先验、构建平衡的类级别原型并生成置信度感知伪标签来构建可靠的外部监督；其次引入ProAdapter，通过基于结构化原型的轻量级适配器来适应冻结的EFM。实验结果表明，SCOPE在标签受限的跨受试者设置下，在三项EEG任务和五种基础模型骨干上均能持续实现出色的性能和效率。|Mengling Feng Team|[2602.17251](http://arxiv.org/abs/2602.17251)|null|
|**2026-02-19**|**Decoding the Human Factor: High Fidelity Behavioral Prediction for Strategic Foresight**|预测高风险环境中的人类决策是人工智能的核心挑战，大型语言模型难以生成一致且个性化的行为。本研究引入了大型行为模型（LBM），一个经过微调的行为基础模型，用于高保真预测个体战略选择。LBM通过将结构化、高维度的特质概况作为条件进行行为嵌入，而非瞬态提示。模型在一个连接稳定倾向、动机状态和情境约束与观察到的选择的专有数据集上训练。在评估中，LBM在行为预测方面优于未经调整的Llama-3.1-8B-Instruct，且在条件设定为大五人格特质时与前沿基线模型表现相当，且随着额外特质维度的提供，性能持续提升，证明了其高保真行为模拟的可扩展性。|Shula Grinapol Team|[2602.17222](http://arxiv.org/abs/2602.17222)|null|
|**2026-02-19**|**Continual learning and refinement of causal models through dynamic predicate invention**|智能体在复杂环境中高效导航需要内化世界的底层逻辑，然而传统的建立世界模型的方法常面临样本效率低下、缺乏透明度及扩展性差的问题。本研究提出了一种框架，通过将连续模型学习与修复集成到智能体的决策循环中，利用元解释学习和谓词发明来在线构建符号因果世界模型。该方法能够发现语义上有意义且可重用的抽象，使智能体能够从观测中构建一个分层的、解耦的高质量概念。实验证明，该提升推理方法在具有复杂关系动态的领域中能够有效扩展，且样本效率比基于PPO神经网络的基线方法高出几个数量级。|Peter Flach Team|[2602.17217](http://arxiv.org/abs/2602.17217)|null|
|**2026-02-19**|**JEPA-DNA: Grounding Genomic Foundation Models through Joint-Embedding Predictive Architectures**|基因组基础模型（GFMs）主要依赖掩码语言建模（MLM）或下一令牌预测（NTP）来学习生命语言。这些范式擅长捕捉局部基因组语法和精细基序模式，但往往未能捕捉更广泛的功能上下文，导致其表示缺乏全局生物学视角。本研究引入了JEPA-DNA，一种结合了联合嵌入预测架构（JEPA）与传统生成目标的预训练框架。JEPA-DNA通过将令牌级恢复与潜在空间中的预测目标耦合，监督一个CLS令牌来预测掩码基因组片段的高级功能嵌入，从而引入了潜在接地。实验结果表明，JEPA-DNA在各种基因组基准测试中的监督和零样本任务中均表现优于纯生成基线模型，提供了更稳健且生物学上更接地气的表示，为理解基因组字母及其底层功能逻辑的基础模型提供了可扩展的路径。|Yoli Shavit Team|[2602.17162](http://arxiv.org/abs/2602.17162)|null|
|**2026-02-19**|**AudioChat: Unified Audio Storytelling, Editing, and Understanding with Transfusion Forcing**|尽管最近取得了突破，音频基础模型在处理复杂的多源声学场景（即音频故事，包含多说话者和背景/前景音效）时仍面临挑战。为解决这一问题，本研究提出了AudioChat框架，旨在开发能生成、编辑和理解音频故事的音频基础模型。AudioChat引入了一种新范式，利用基于LLM的工具调用智能体模拟用户与系统间的交互，生成训练数据，并引入了一种新颖的Audio Transfusion Forcing目标函数进行模型训练，使其能够同时通过结构化的思维链推理分解高级指令，并执行交互式多轮音频理解/生成。为评估生成和编辑性能，我们开发了三项新的直接衡量任务表现的指标。|Zeyu Jin Team|[2602.17097](http://arxiv.org/abs/2602.17097)|null|
|**2026-02-19**|**MantisV2: Closing the Zero-Shot Gap in Time Series Classification with Synthetic Data and Test-Time Strategies**|为时间序列分类开发基础模型具有高度实用价值，但现有模型（如Mantis）在零样本和微调编码器之间存在显著性能差距。本研究引入了多项创新以显著强化时间序列的零样本特征提取能力。首先，提出了Mantis+，一个完全基于合成时间序列预训练的Mantis变体。其次，通过架构精炼，得到了更轻量、性能更优的MantisV2。第三，开发了一种增强型测试时方法，利用中间层表示并改进输出token聚合。此外，研究还展示了通过自集成和跨模型嵌入融合可进一步提升性能。在UCR、UEA、HAR和EEG等多个基准数据集上的广泛实验证明，MantisV2和Mantis+均持续优于先前的时间序列基础模型，实现了最先进的零样本性能。|Ievgen Redko Team|[2602.17868](http://arxiv.org/abs/2602.17868)|null|
|**2026-02-19**|**Enabling Training-Free Text-Based Remote Sensing Segmentation**|现有视觉语言模型（VLM）和视觉基础模型（VFM）为遥感图像的零样本文本引导分割带来了机遇，但多数方法仍依赖额外的可训练组件。本研究旨在探讨在不进行额外训练的情况下，仅依赖现有基础模型实现文本基遥感分割的潜力。研究提出了一种简单有效的方法，将对比和生成式VLM与Segment Anything Model (SAM) 集成，从而实现完全免训练或轻量级LoRA微调的管道。其中，对比方法使用CLIP作为SAM网格提案的掩码选择器，在完全零样本设置下实现了最先进的开放词汇语义分割（OVSS）。生成方法则利用GPT-5在零样本设置下为SAM生成点击提示，或通过LoRA微调的Qwen-VL模型实现推理和参照分割，后者取得了最佳结果。在19个遥感基准测试（涵盖开放词汇、参照和基于推理的任务）上的广泛实验证明了该方法的强大能力。|Djamila Aouada Team|[2602.17799](http://arxiv.org/abs/2602.17799)|null|
|**2026-02-19**|**GeneZip: Region-Aware Compression for Long Context DNA Modeling**|基因组序列包含数十亿个碱基对，对基因组规模的基础模型构成了巨大挑战，现有方法多通过扩展小型模型或依赖多GPU并行来规避。本研究提出了GeneZip，一个利用基因组信息高度不平衡（编码区信息密集，非编码区稀疏）的DNA压缩模型。GeneZip将HNet风格的动态路由与区域感知压缩比目标相结合，实现了基因组区域间表示预算的自适应分配。结果显示，GeneZip实现了137.6倍的压缩率，困惑度仅增加0.31。在下游长上下文基准测试中，GeneZip在接触图预测、表达数量性状基因座预测和增强子-靶基因预测方面表现相当或更优，从而解锁了上下文和容量的同时扩展，使得在单个A100 80GB GPU上训练1M-bp上下文的636M参数模型成为可能。|Jian Tang Team|[2602.17739](http://arxiv.org/abs/2602.17739)|null|
|**2026-02-18**|**Parameter-free representations outperform single-cell foundation models on downstream benchmarks**|单细胞RNA测序（scRNA-seq）数据展现出可重现的统计结构，促使了基于Transformer的大规模基础模型（如TranscriptFormer）发展，用于下游任务。本研究探讨了是否无需计算密集型深度学习表示也能实现类似性能。通过采用依赖精心归一化和线性方法的简单、可解释的流水线，研究在多个单细胞基础模型常用基准测试中达到了SOTA或接近SOTA的性能，甚至在涉及训练数据中未出现的新细胞类型和生物体的域外任务中超越了基础模型。研究结果强调了严格基准测试的重要性，并表明细胞身份的生物学特性可通过单细胞基因表达数据的简单线性表示来捕获。|Pankaj Mehta Team|[2602.16696](http://arxiv.org/abs/2602.16696)|null|
|**2026-02-18**|**Are Object-Centric Representations Better At Compositional Generalization?**|组合泛化能力是人类认知的基本要素，也是机器学习的重大挑战，而以对象为中心的（OC）表示被认为能支持此能力，但视觉丰富环境中的系统性证据有限。本研究引入了跨三个受控视觉世界的视觉问答基准，以评估带有或不带OC偏置的视觉编码器在泛化到未见过的对象属性组合时的表现。研究通过严谨控制训练数据多样性、样本量和计算等因素，对DINOv2和SigLIP2及其OC版本进行了比较。结果表明，在更复杂的组合泛化任务中，OC方法表现更优；原始密集表示仅在更简单的任务中超越OC且需要更多计算；OC模型在样本效率上更高，在数据集大小、训练数据多样性或下游计算受限时提供了更强的组合泛化能力。|Andrea Dittadi Team|[2602.16689](http://arxiv.org/abs/2602.16689)|null|
|**2026-02-18**|**Scaling Open Discrete Audio Foundation Models with Interleaved Semantic, Acoustic, and Text Tokens**|当前音频语言模型多以文本为先，限制了通用音频建模。本文提出并系统研究了原生音频基础模型，通过大规模的下一令牌预测，联合建模语义内容、声学细节和文本，以支持通用音频生成和跨模态能力。研究者系统探索了数据源、文本混合比例等设计选择，并首次对离散音频模型进行了缩放定律研究。基于这些发现，他们训练了SODA模型套件，并在保留说话者声音的语音-语音翻译等任务中展示了其作为灵活骨干网络的有效性。|Diyi Yang Team|[2602.16687](http://arxiv.org/abs/2602.16687)|null|
|**2026-02-18**|**Retrieval-Augmented Foundation Models for Matched Molecular Pair Transformations to Recapitulate Medicinal Chemistry Intuition**|针对药物化学中类似物设计缺乏高效、可控的分子编辑方法，现有机器学习方法存在局限性的问题，本文提出了一种基于大规模匹配分子对转换（MMPTs）的变异到变异（variable-to-variable）类似物生成基础模型。该模型通过提示机制实现用户对转换模式的精确控制，并引入MMPT-RAG框架利用外部参考类似物进行上下文指导。实验证明，该方法在通用化学语料库和专利数据集上显著提升了生成多样性、新颖性和可控性，能在实际发现场景中恢复出真实的类似物结构。|Liang Zhao Team|[2602.16684](http://arxiv.org/abs/2602.16684)|null|
|**2026-02-18**|**Learning Situated Awareness in the Real World**|多模态基础模型（MFMs）现有基准多侧重环境中心空间关系，忽视了以观察者为中心的、需推理智能体视角和动作的情境感知。为填补此空白，研究者引入了SAW-Bench，一个基于Ray-Ban Meta智能眼镜录制真实世界视频的新型基准，用于评估自我中心情境感知。该基准包含786个视频和2071个问答对，通过六项感知任务探测模型的观察者中心理解。评估显示，即使是最佳MFM，与人类仍有显著差距，模型常未能推断出连贯的相机几何导致空间推理错误，突显了超越被动观察、理解以观察者为中心的物理动态的重要性。|Xin Eric Wang Team|[2602.16682](http://arxiv.org/abs/2602.16682)|null|
|**2026-02-18**|**VETime: Vision Enhanced Zero-Shot Time Series Anomaly Detection**|时间序列异常检测（TSAD）需要识别点异常和上下文异常，但现有基础模型在精细点定位和全局上下文理解之间存在权衡。为解决这一困境，本文提出了VETime，首个通过细粒度视觉-时间对齐和动态融合统一时间与视觉模态的TSAD框架。VETime引入可逆图像转换和补丁级时间对齐模块，以建立共享的视觉-时间轴并保留细节。此外，它设计了异常窗口对比学习和任务自适应多模态融合机制。大量实验表明，VETime在零样本场景中显著优于先进模型，实现了更高的定位精度和更低的计算开销。|Chen Zhang Team|[2602.16681](http://arxiv.org/abs/2602.16681)|null|
|**2026-02-18**|**Learning to unfold cloth: Scaling up world models to deformable object manipulation**|机器人布料操作因其复杂的物理特性而极具挑战性，需要通用策略以适应不同布料特性。本文提出一种改进的DreamerV2强化学习架构应用于空中布料操作，通过引入表面法线作为输入，并优化回放缓冲区及数据增强程序，增强了机器人使用的世界模型来应对物理复杂性。在仿真和物理机器人的零样本部署实验中，该方法成功实现了多种布料的空中展开，证明了所提出架构在泛化性能上的显著优势。|Subramanian Ramamoorthy Team|[2602.16675](http://arxiv.org/abs/2602.16675)|null|
|**2026-02-18**|**A Systematic Evaluation of Sample-Level Tokenization Strategies for MEG Foundation Models**|神经影像数据的基础模型需将连续神经时间序列数据“令牌化”，但不同令牌化策略的影响尚不明确。本文对应用于脑磁图（MEG）数据的基于Transformer的大型神经影像模型（LNMs）的样本级令牌化策略进行了系统评估。通过比较可学习（引入基于自编码器的新方法）和不可学习令牌器在信号重建保真度、基础建模性能及下游任务上的表现，研究发现在多个MEG数据集上，两者均实现了高重建精度和大致相当的性能，表明简单的固定样本级令牌化策略足以支持神经基础模型的开发。|Mark W. Woolrich Team|[2602.16626](http://arxiv.org/abs/2602.16626)|null|
|**2026-02-18**|**Why Thinking Hurts? Diagnosing and Rectifying the Reasoning Shift in Foundation Recommender Models**|将思维链（CoT）推理整合到语义ID推荐基础模型中，常导致推荐性能下降，究其原因在于“通用子空间”中冗长推理的文本惯性，使得模型忽视关键语义ID。为解决此问题，本文提出一个训练无关的“推理时子空间对齐”框架。该方法通过压缩推理链和应用偏差减去的对比解码，有效缓解了无根据的文本漂移。实验证明，此框架能有效校准推理过程，使基础模型在利用推理能力的同时，不牺牲基于ID的推荐准确性。|Enhong Chen Team|[2602.16587](http://arxiv.org/abs/2602.16587)|null|
|**2026-02-18**|**Arc2Morph: Identity-Preserving Facial Morphing with Arc2Face**|人脸融合攻击对电子身份文档中的人脸识别系统构成严峻威胁，尤其是在护照注册过程中缺乏实时监督采集时。本文提出一种基于身份条件化人脸基础模型Arc2Face的新型人脸融合技术，能够从紧凑的身份表示合成逼真的人脸图像。通过在多个大规模数据集上与现有先进方法进行比较，实验结果表明，所提出的深度学习方法在融合攻击潜力方面达到了与传统上最具挑战性的基于地标技术相当的水平，证实了其在融合生成过程中有效保留和管理身份信息的能力。|Davide Maltoni Team|[2602.16569](http://arxiv.org/abs/2602.16569)|null|
|**2026-02-18**|**MMA: Multimodal Memory Agent**|长时程多模态智能体在依赖外部记忆时，常因检索到过时、低可信或冲突信息而产生过度自信的错误。针对此问题，本文提出了多模态记忆智能体（MMA），该智能体结合来源可信度、时间衰减和冲突感知网络共识，动态评估每个记忆项的可靠性，并利用此信号加权证据或在支持不足时弃权。同时，引入了MMA-Bench基准来研究信念动态。实验结果表明，在FEVER数据集上，MMA在保持基线准确率的同时，将方差降低了35.2%；在LoCoMo上，提高了操作准确性并减少了错误答案；在MMA-Bench上，视觉模式下MMA的准确率显著优于基线，并揭示了RAG智能体中潜在的“视觉安慰剂效应”。|Hao Tang Team|[2602.16493](http://arxiv.org/abs/2602.16493)|null|
|**2026-02-18**|**RoboGene: Boosting VLA Pre-training via Diversity-Driven Agentic Framework for Real-World Task Generation**|通用机器人操作受限于稀缺且成本高昂的真实世界交互数据，且现有任务策划方法不可扩展或易产生不可行指令。为解决这一问题，本文提出了RoboGene框架，旨在自动化生成多样化且物理上可行的单臂、双臂和移动机器人操作任务。RoboGene包含多样性驱动采样、自反思机制以强制物理约束以及人机协作精炼等核心组件。实验结果表明，RoboGene在定量分析和大规模真实世界实验中显著优于现有基础模型，且使用RoboGene预训练的VLA模型展现出更高的成功率和泛化能力，强调了高质量任务生成的重要性。|Jian Tang Team|[2602.16444](http://arxiv.org/abs/2602.16444)|null|
|**2026-02-18**|**Automated Histopathology Report Generation via Pyramidal Feature Extraction and the UNI Foundation Model**|从千兆像素级的组织病理学全玻片图像（WSIs）生成精确的诊断文本面临巨大挑战。本文提出了一个分层视觉语言框架，将一个冻结的病理学基础模型与Transformer解码器相结合进行报告生成。该方法通过多分辨率金字塔补丁选择和背景伪影去除技术处理WSI，并利用UNI Vision Transformer提取特征，随后由Transformer解码器生成文本，并使用BioGPT进行分词。为提高可靠性，该框架还引入了一个基于检索的验证步骤，通过比较生成报告与参考语料库来修正报告内容。|Serkan Sokmen Team|[2602.16422](http://arxiv.org/abs/2602.16422)|null|
|**2026-02-18**|**CADEvolve: Creating Realistic CAD via Program Evolution**|计算机辅助设计（CAD）的AI自动化受限于缺乏复杂操作和设计意图的数据集，导致现有方法难以生成工业级程序。为此，本文提出了CADEvolve，一个基于进化的管道和数据集，它从简单基元开始，通过VLM引导的编辑和验证，逐步生成工业级复杂性的CAD程序。该方法生成了8k个复杂零件作为可执行的CadQuery参数化生成器，并经过处理和增强后，形成了一个包含130万个脚本的统一数据集。实验结果表明，在CADEvolve上微调的VLM在DeepCAD、Fusion 360和MCB基准测试的Image2CAD任务上均达到了最先进的性能。|Dmitrii Zhemchuzhnikov Team|[2602.16317](http://arxiv.org/abs/2602.16317)|null|
|**2026-02-18**|**AFFMAE: Scalable and Efficient Vision Pretraining for Desktop Graphics Cards**|自监督预训练虽提高了计算机视觉效率，但高分辨率训练仍需服务器级基础设施，限制了基础模型的开发。传统MAE与分层架构结合时面临密集网格和掩码感知设计挑战。本文提出了AFFMAE，一个基于自适应、非网格token合并的掩码友好型分层预训练框架，通过丢弃被掩码的token并仅对可见token执行动态合并，消除了密集网格假设并保持了分层可扩展性。实验结果表明，在相同参数量下，AFFMAE在高分辨率电子显微镜分割任务上匹配了ViT-MAE的性能，同时将FLOPs减少高达7倍，内存使用减半，并在单个RTX 5090上实现了更快的训练。|Behzad Najafian Team|[2602.16249](http://arxiv.org/abs/2602.16249)|null|
|**2026-02-18**|**EasyControlEdge: A Foundation-Model Fine-Tuning for Edge Detection**|在实际边缘检测中，清晰度和数据效率至关重要，但用有限数据生成清晰边缘图极具挑战，且图像生成基础模型在边缘检测领域的潜力尚未充分挖掘。本文提出了EasyControlEdge，旨在将图像生成基础模型适应于边缘检测任务，以实现高清晰度和数据高效性。该方法通过引入边缘导向目标和高效像素空间损失来专门化基础模型，并在推理时利用基于无条件动力学的引导，实现通过引导尺度控制边缘密度。实验结果显示，EasyControlEdge在多个基准测试中持续优于现有方法，特别是在无后处理清晰度评估和有限训练数据条件下表现突出。|Tadahiro Taniguchi Team|[2602.16238](http://arxiv.org/abs/2602.16238)|null|
|**2026-02-18**|**Factored Latent Action World Models**|从无动作视频中学习潜在动作是构建可控世界模型的强大范式，但现有方法通常依赖单一动力学模型控制整个场景，难以应对多实体复杂环境。为解决此问题，本文提出了因子化潜在动作模型（FLAM），该框架将场景分解为独立因子，每个因子推断其自身潜在动作并预测下一时刻状态。这种因子化结构能够更准确地建模复杂的多实体动力学。实验结果表明，FLAM在模拟和真实世界多实体数据集上的预测准确性和表示质量均优于现有方法，改善了视频生成质量，并促进了下游策略学习。|Peter Stone Team|[2602.16229](http://arxiv.org/abs/2602.16229)|null|
|**2026-02-18**|**World Model Failure Classification and Anomaly Detection for Autonomous Inspection**|自主检查机器人可降低工业现场监测的成本与风险，但遮挡、视角受限等问题使准确读数充满挑战。本文提出了一个混合框架，结合监督式故障分类与异常检测，将检查任务分类为成功、已知故障或异常情况。该方法以带有压缩视频输入的世界模型为骨干，并通过共形预测阈值确定的两个决策函数在人类观察者之前进行分类。在办公室和工业现场仪表检查上的实验结果显示，该框架在区分成功、故障和OOD情况方面准确率超过90%，且分类发生时间早于人类观察者，展现了其在自主检查中实现鲁棒、预见性故障检测的潜力。|Shayegan Omidshafiei Team|[2602.16182](http://arxiv.org/abs/2602.16182)|null|
|**2026-02-18**|**BrainRVQ: A High-Fidelity EEG Foundation Model via Dual-Domain Residual Quantization and Hierarchical Autoregression**|由于低信噪比和复杂的时频谱非平稳性，开发脑电图（EEG）基础模型并重建精细信息仍面临挑战。为此，本研究提出了BrainRVQ，一个通用的EEG基础模型，在大规模临床EEG数据上预训练。该模型采用双域残差向量量化（DD-RVQ）分词器将时域波形和频谱模式解耦为分层离散代码，并通过分层自回归预训练目标和“重要性引导的课程掩码策略”从粗到细地重建这些代码。在8个下游数据集上的广泛实验表明，BrainRVQ持续优于现有先进基线，验证了其学习鲁棒和可泛化神经表示的有效性。|Luca Mainardi Team|[2602.16951](http://arxiv.org/abs/2602.16951)|null|
|**2026-02-18**|**How should AI knowledge be governed? Epistemic authority, structural transparency, and the case for open cognitive graphs**|教育AI系统在实际应用中拥有认知权威，但缺乏类似人类教育者的制度化问责机制，这构成了结构性治理难题。本研究将教育AI重新定义为公共教育认知基础设施，并提出了“开放认知图谱（OCG）”作为技术接口，通过显式表示概念、前提、错误认知和支架，使AI行为的认知逻辑可被审查和修改。在此基础上，研究引入了“主干-分支”治理模型来组织认知权威，并通过社区治理的教育基础模型案例研究，展示了如何通过制度化流程整合分布式专业知识，从而实现教育AI与民主问责制的对齐。|Yi Hua Team|[2602.16949](http://arxiv.org/abs/2602.16949)|null|
|**2026-02-18**|**BEMEval-Doc2Schema: Benchmarking Large Language Models for Structured Data Extraction in Building Energy Modeling**|大语言模型（LLM）等基础模型为自动化建筑能耗建模（BEM）提供了新机遇，但缺乏公开数据集和标准化评估指标限制了系统性评估。本研究提出了BEMEval基准框架，其中BEMEval-Doc2Schema专注于从建筑文档中提取结构化数据，并引入“键值重叠率（KVOR）”指标以量化模型输出与真实图式的一致性。利用该框架，在三个数据集上评估了GPT-5和Gemini 2.5，结果显示Gemini 2.5持续优于GPT-5，且少样本提示均能提高准确性，同时简单图式表现更佳。该研究为AI辅助BEM工作流奠定了首个社区驱动的评估基准。|Liang Zhang Team|[2602.16926](http://arxiv.org/abs/2602.16926)|null|
|**2026-02-18**|**Beyond the Flag: A Framework for Integrating Cybersecurity Competitions into K-12 Education for Cognitive Apprenticeship and Ethical Skill Development**|夺旗（CTF）竞赛是弥补网络安全人才缺口的有效教学工具，但在K-12教育中面临教师准备不足和公平性等实施障碍。本研究提出了“网络安全伦理认知学徒（ECAC）”框架，该模型整合了认知学徒理论与嵌入式伦理发展，分为五个阶段，旨在提供“低门槛，高上限”的学习路径，以扩大包括少数族裔和女性在内的不同学生群体的参与。通过将教育者重新定义为“首席学习者”，ECAC还提供了解决教师专业知识差距的可持续方案，旨在将CTF转变为培养更专业、更具伦理素养和更多样化网络安全人才的综合学习体验。|Nam Son Nguyen Team|[2602.16921](http://arxiv.org/abs/2602.16921)|null|
|**2026-02-18**|**StereoAdapter-2: Globally Structure-Consistent Underwater Stereo Depth Estimation**|水下立体深度估计受光衰减、散射和折射导致的严重域偏移影响，现有GRU迭代精修方法在处理大视差和无纹理区域时性能受限。本研究提出了StereoAdapter-2，将传统ConvGRU更新器替换为基于选择性状态空间模型的新型ConvSS2D算子，该算子通过四向扫描策略与极线几何对齐，从而在一个更新步骤内以线性计算复杂度实现高效长距离空间传播。此外，研究构建了大规模合成水下立体数据集UW-StereoDepth-80K。结合动态LoRA适应，该框架在TartanAir-UW和SQUID基准测试中实现了17%和7.2%的性能提升，并通过真实机器人平台验证了其鲁棒性。|Hao Tang Team|[2602.16915](http://arxiv.org/abs/2602.16915)|null|
|**2026-02-18**|**SparTa: Sparse Graphical Task Models from a Handful of Demonstrations**|机器人学习中高效学习长时程操作任务是一项核心挑战，现有方法多关注学习动作而非任务目标。本研究提出通过一系列图形化的对象关系来表示不断变化的场景状态，并引入一种演示分割和池化方法，以提取操作图并估计跨任务阶段的对象状态分布。与以往仅捕获局部交互的基于图的方法不同，本方法能捕获从控制开始到操作结束的完整对象交互，并通过使用预训练视觉特征进行对象匹配以提高学习的鲁棒性。实验证明了方法在演示分割准确性上的有效性，以及学习到任务模型支持在仿真和真实机器人上跨环境可靠执行的能力。|Abhinav Valada Team|[2602.16911](http://arxiv.org/abs/2602.16911)|null|
|**2026-02-18**|**Position: Why a Dynamical Systems Perspective is Needed to Advance Time Series Modeling**|时间序列（TS）建模已发展至TS基础模型，但其进展尚不完全明朗，领域需要动力系统（DS）视角以实现进一步突破。本研究强调了动力系统重建（DSR）的潜力，该方法旨在从数据中推断底层动力系统模型，从而不仅能实现短期预测，还能预测系统的长期统计行为。DS理论还能提供领域无关的见解，揭示TS生成机制、性能上限、未知区域的泛化能力及潜在控制策略。本研究回顾了DS理论和DSR的核心概念，并提出了将这些见解转化为TS建模的具体建议，以期实现更优预测和更低的计算与内存开销。|Lukas Eisenmann Team|[2602.16864](http://arxiv.org/abs/2602.16864)|null|
|**2026-02-17**|**MedProbCLIP: Probabilistic Adaptation of Vision-Language Foundation Model for Reliable Radiograph-Report Retrieval**|视觉-语言基础模型在多模态理解方面潜力巨大，但其确定性嵌入难以满足高风险生物医学应用对可靠性的要求。本文提出了MedProbCLIP，一个用于胸部X射线和放射学报告表示学习及双向检索的概率视觉-语言学习框架。MedProbCLIP通过概率对比目标将图像和文本表示建模为高斯嵌入，显式捕获不确定性和多对多对应关系，并利用变分信息瓶颈减轻过度自信预测。实验结果表明，在MIMIC-CXR数据集上，MedProbCLIP在检索和零样本分类方面均优于现有基线，并展现出卓越的校准性、风险-覆盖行为、选择性检索可靠性以及对临床相关损坏的鲁棒性，提升了放射学图像-文本检索系统的可信度和安全性。|Gongbo Liang Team|[2602.16019](http://arxiv.org/abs/2602.16019)|null|
|**2026-02-17**|**Neural Scaling Laws for Boosted Jet Tagging**|大型语言模型（LLMs）的成功表明计算规模扩展是性能提升的关键，然而高能物理（HEP）领域最先进模型的训练计算量远低于工业界。针对该背景，本文研究了使用公共JetClass数据集进行增压射流分类的神经网络扩展定律。研究推导了计算最优的扩展定律，并识别出一个可通过增加计算持续接近的有效性能极限。研究结果表明，增加计算能可靠地推动性能接近渐近极限，且更具表达能力的低级特征可以提高性能极限并在固定数据集大小下改善结果，同时量化了数据重复对有效数据集大小的增益影响。|Lukas Heinrich Team|[2602.15781](http://arxiv.org/abs/2602.15781)|null|
|**2026-02-16**|**EditCtrl: Disentangled Local and Global Control for Real-Time Generative Video Editing**|背景：高保真生成式视频编辑依赖预训练视频基础模型，但其计算成本高昂，即使是局部编辑也需处理整个视频上下文。方法：本文提出EditCtrl，一个高效的视频修复控制框架。它引入了新颖的局部视频上下文模块，仅对掩码标记进行操作，使计算成本与编辑区域大小成正比。同时，一个轻量级的时间全局上下文嵌入器确保视频整体上下文的一致性。结果：EditCtrl比现有先进方法计算效率高10倍，并提升了编辑质量。该方法还支持多区域文本提示编辑和自回归内容传播。|Caleb Leak Team|[2602.15031](http://arxiv.org/abs/2602.15031)|**[link](https://yehonathanlitman.github.io/edit_ctrl)**|
|**2026-02-16**|**Generalization from Low- to Moderate-Resolution Spectra with Neural Networks for Stellar Parameter Estimation: A Case Study with DESI**|背景：在恒星光谱分析中，跨巡天泛化能力（特别是从低分辨率到中分辨率光谱的迁移）是一个关键挑战。方法：本文研究了使用预训练多层感知机（MLPs）解决此问题，以LAMOST低分辨率光谱到DESI中分辨率光谱的迁移为例。作者在LAMOST低分辨率光谱或其嵌入上预训练MLPs，并在DESI光谱上进行微调，比较了直接在光谱上训练的MLPs与基于Transformer模型嵌入训练的MLPs，并评估了不同的微调策略。结果：预训练在LAMOST低分辨率光谱上的MLPs表现出色，即便不微调也能获得良好性能，适度微调可进一步提升。研究表明，简单预训练MLPs能提供有竞争力的跨巡天泛化能力，但光谱基础模型在跨巡天恒星参数估计中的作用仍需深入探索。|Viska Wei Team|[2602.15021](http://arxiv.org/abs/2602.15021)|null|
|**2026-02-16**|**Cold-Start Personalization via Training-Free Priors from Structured World Models**|背景：冷启动个性化（即在无用户历史数据时推断用户偏好）是一个挑战，因为用户只关心少数偏好维度，且关键维度因人而异。现有强化学习方法在多轮交互中难以有效利用偏好数据的分因子结构。方法：本文提出Pep（Preference Elicitation with Priors）框架，将冷启动偏好获取分解为离线结构学习和在线贝叶斯推理。Pep离线从完整用户档案中学习偏好相关性的结构化世界模型，然后在线进行免训练的贝叶斯推理，以选择信息丰富的提问并预测完整的偏好档案。结果：Pep在生成响应与用户偏好的一致性方面达到80.8%，远高于强化学习的68.5%，且交互次数减少3-5倍。它仅用约1万参数就实现此效果，而强化学习需80亿参数，突显了利用偏好数据分因子结构的重要性。|Asli Celikyilmaz Team|[2602.15012](http://arxiv.org/abs/2602.15012)|null|
|**2026-02-16**|**PDE foundation models are skillful AI weather emulators for the Martian atmosphere**|背景：为火星大气层构建熟练的预测天气模拟器，面临训练数据和计算资源不足的挑战。方法：本文展示了如何将预训练在多源偏微分方程数值解上的AI基础模型（Poseidon PDE基础模型）适配并微调，以构建火星大气的预测天气模拟器。研究扩展了Poseidon模型从二维到三维的方法，同时保留了预训练信息，并探讨了在稀疏初始条件下的模型性能。结果：通过预训练与模型扩展的结合，模型在独立验证年份的性能提升了34.4%。这表明偏微分方程基础模型不仅能近似其他偏微分方程的解，还能作为解决实际世界复杂交互问题的锚定模型，尤其是在训练数据或计算预算有限的情况下。|Juan Bernabe-Moreno Team|[2602.15004](http://arxiv.org/abs/2602.15004)|null|
|**2026-02-16**|**Use What You Know: Causal Foundation Models with Partial Graphs**|背景：传统的因果量估计依赖于为特定假设定制的估计器。新兴的因果基础模型（CFMs）提供统一方法，但目前无法融入领域知识，导致预测次优。方法：本文提出将因果信息（如完整因果图或部分祖先信息）条件化到CFMs中的方法。研究系统评估了不同的条件化策略，发现将可学习偏差注入注意力机制是利用完整和部分因果信息最有效的方法。结果：通过条件化，通用CFM的性能可以与针对特定因果结构训练的专用模型相媲美。这一方法克服了构建一体化因果基础模型的核心障碍，使其能够以数据驱动的方式回答因果查询，同时有效利用任何程度的领域专业知识。|Bernhard Schölkopf Team|[2602.14972](http://arxiv.org/abs/2602.14972)|null|
|**2026-02-16**|**Model Context Protocol (MCP) Tool Descriptions Are Smelly! Towards Improving AI Agent Efficiency with Augmented MCP Tool Descriptions**|背景：基于基础模型（FM）的智能体通过工具描述与外部系统交互，但这些自然语言描述中的缺陷可能误导FM，其普遍性和影响尚不明确。方法：本文对103个MCP服务器上的856个工具进行了大规模实证研究。通过识别工具描述的六个组成部分，开发了评分标准并据此形式化了“工具描述异味”。利用FM-based扫描器进行操作化评估，并增强描述以评估其对智能体性能的影响。结果：97.1%的工具描述至少含有一种“异味”，其中56%未能清晰阐明目的。尽管增强所有组件的描述可使任务成功率中位数提升5.85个百分点，但执行步骤也增加了67.46%，并导致性能下降，揭示了性能与成本之间的权衡。|Ahmed E. Hassan Team|[2602.14878](http://arxiv.org/abs/2602.14878)|null|
|**2026-02-16**|**World Models for Policy Refinement in StarCraft II**|背景：尽管大型语言模型（LLMs）展现出强大的推理能力，但现有基于LLM的《星际争霸II》（SC2）智能体主要关注策略优化，缺乏可学习的、动作条件化的转移模型来辅助决策。方法：本文提出StarWM，这是首个在部分可观测环境下预测SC2未来观测的世界模型。为学习SC2的混合动态，作者引入了一种将观测分解为五个语义模块的结构化文本表示，并构建了首个用于SC2动态预测的指令微调数据集SC2-Dynamics-50k。StarWM被集成到“生成-模拟-细化”决策循环中，形成StarWM-Agent。结果：StarWM在资源预测准确性等指标上比零样本基线提升近60%。在线评估中，StarWM-Agent对不同难度级别（Hard、Harder、VeryHard）的胜率分别提升了30%、15%和30%，同时改善了宏观管理稳定性和战术风险评估。|Bo Xu Team|[2602.14857](http://arxiv.org/abs/2602.14857)|null|
|**2026-02-16**|**SAILS: Segment Anything with Incrementally Learned Semantics for Task-Invariant and Training-Free Continual Learning**|背景：持续学习在类增量语义分割（CISS）中面临重复训练、高计算成本和灾难性遗忘的限制，制约了其实际应用。方法：本文提出SAILS（Segment Anything with Incrementally Learned Semantics），一个免训练的CISS框架，它将CISS解耦为两个阶段：首先利用Segment Anything Model (SAM)进行零样本区域提取，然后通过固定特征空间中的原型进行语义关联。SAILS通过选择性类内聚类，为每个类生成多个原型以更好地建模类内变异性。结果：SAILS无需增量训练，但在标准CISS数据集上通常超越了现有的基于训练的方法，特别是在遗忘问题严重的长期和挑战性任务序列中。SAILS完全消除了遗忘，保持了任务不变的一致性能，并展现出正向反向迁移。|René Schuster Team|[2602.14767](http://arxiv.org/abs/2602.14767)|null|
|**2026-02-16**|**Depth Completion as Parameter-Efficient Test-Time Adaptation**|背景：现有深度补全方法通常通过训练任务特定编码器来利用辅助输入，但易过拟合且泛化性差。而3D基础模型可提供更强的几何先验。方法：本文提出CAPA，一个参数高效的测试时优化框架，用于利用稀疏几何线索对预训练3D基础模型进行深度补全。CAPA冻结基础模型骨干，仅通过参数高效微调（如LoRA或VPT）更新少量参数，并利用推理时稀疏观测直接计算梯度进行指导。对于视频，CAPA引入序列级参数共享以利用时间相关性并强制多帧一致性。结果：CAPA与任何基于ViT的基础模型兼容，并在室内外数据集的各种条件下取得了最先进的结果。它有效地将基础模型的几何先验与场景特定测量相结合，修正了畸变。|Shengyu Huang Team|[2602.14751](http://arxiv.org/abs/2602.14751)|null|
|**2026-02-16**|**WebWorld: A Large-Scale World Model for Web Agent Training**|背景：网页智能体需要大量轨迹以实现泛化，但真实世界训练受网络延迟、速率限制和安全风险制约。现有模拟器局限于封闭环境。方法：本文推出WebWorld系列，首个大规模训练的开放网络模拟器。它利用可扩展数据管道在超过100万次开放网络交互中进行训练，支持推理、多格式数据和超过30步的长周期模拟。结果：WebWorld在WebWorld-Bench上实现了与Gemini-3-Pro相当的模拟性能。在WebWorld合成轨迹上训练的Qwen3-14B在WebArena上性能提升9.2%，达到与GPT-4o相当的水平。WebWorld作为世界模型，在推理时搜索方面超越了GPT-5。此外，它还展现出跨领域泛化能力，为构建世界模型提供了可复现的方法。|Zuozhu Liu Team|[2602.14721](http://arxiv.org/abs/2602.14721)|null|
|**2026-02-16**|**Arbor: A Framework for Reliable Navigation of Critical Conversation Flows**|针对大型语言模型在医疗分诊等高风险领域难以遵循结构化工作流程的问题，以及单一提示方法在长提示下易导致指令依从性下降的挑战，本文提出了Arbor框架。该框架将决策树导航分解为节点级任务，通过基于DAG的编排机制动态检索和评估转换，并将响应生成解耦。实验结果表明，与单一提示基线相比，Arbor在真实临床分诊对话中将平均轮次准确率提高了29.4个百分点，同时显著降低了延迟和成本，证明了架构分解能有效提升模型性能并降低对模型固有能力的依赖。|Luís Ungaro Team|[2602.14643](http://arxiv.org/abs/2602.14643)|null|
|**2026-02-16**|**Tabular Foundation Models Can Learn Association Rules**|传统的关联规则挖掘（ARM）方法存在规则爆炸和可扩展性差的问题，而现有神经方法在低数据量下性能不佳，但表格基础模型（TFMs）为解决这些局限性提供了基础。为此，本文提出了一个模型无关的关联规则学习框架，能够利用TFMs从任何条件概率模型中提取关联规则，并实例化了TabProbe。实验结果显示，TabProbe利用TFMs作为条件概率估计器，无需频繁项集挖掘即可生成简洁、高质量的关联规则，在低数据设置下仍保持强大的预测性能和鲁棒性。|Victoria Degeler Team|[2602.14622](http://arxiv.org/abs/2602.14622)|null|
|**2026-02-16**|**MATEO: A Multimodal Benchmark for Temporal Reasoning and Planning in LVLMs**|AI智能体在实现复杂目标时需要规划，但现有研究对基础模型时间执行顺序（TEO）的理解有限，多局限于线性近似或纯文本输入。为解决此问题，本文引入了MATEO（MultimodAl Temporal Execution Order）基准，旨在评估和提升大型视觉语言模型（LVLMs）的时间推理能力。通过收集高质量多模态食谱语料库及相应的TEO图注释，作者使用MATEO评估了六个最先进的LVLM，考察了不同模型规模、语言上下文、多模态输入结构和微调策略对时间推理能力的影响。|Giuseppe Riccardi Team|[2602.14589](http://arxiv.org/abs/2602.14589)|null|
|**2026-02-16**|**MedVAR: Towards Scalable and Efficient Medical Image Generation via Next-scale Autoregressive Prediction**|医学图像生成在数据增强和隐私保护中至关重要，但现有方法在架构效率、多器官数据和原则性评估方面存在不足。为此，本文提出了MedVAR，首个基于自回归的医学基础模型，采用“下一尺度预测”范式，实现快速可扩展的医学图像合成。MedVAR以粗到精的方式生成图像，并构建了一个包含约44万张CT和MRI图像的协调数据集。综合实验表明，MedVAR在图像保真度、多样性和可扩展性方面均达到最先进水平，为未来的医学生成基础模型提供了 promising 的架构方向。|Yueming Jin Team|[2602.14512](http://arxiv.org/abs/2602.14512)|null|
|**2026-02-16**|**Covariance-Aware Transformers for Quadratic Programming and Decision Making**|针对Transformer在涉及协方差矩阵的决策问题中的应用潜力，本文首先证明线性注意力机制可通过模拟梯度下降求解无约束二次规划（QP），并扩展至求解L1惩罚/约束QP。在此基础上，本文提出了Time2Decide，一种通过显式输入协方差矩阵来增强时间序列基础模型（TSFM）的通用方法。实验结果表明，Time2Decide在经典的投资组合优化问题上，其性能普遍优于基础TSFM模型，并在特定条件下甚至超越了传统的“预测-优化”流程，证明Transformer通过显式利用二阶统计量能有效解决复杂决策问题。|Samet Oymak Team|[2602.14506](http://arxiv.org/abs/2602.14506)|null|
|**2026-02-16**|**A Soft Wrist with Anisotropic and Selectable Stiffness for Robust Robot Learning in Contact-rich Manipulation**|在非结构化环境中，机器人进行接触密集型操作任务时，现有软末端执行器因形变范围有限、缺乏定向刚度控制或系统复杂而面临挑战。本文介绍了一种名为CLAW（Compliant Leaf-spring Anisotropic soft Wrist）的新型软腕机构，它通过简单的板簧和锁定旋转关节设计，实现了大范围6自由度形变和可调的各向异性刚度，同时保持轻量和低成本。在模仿学习实验中，CLAW在插销任务中实现了76%的成功率，显著优于其他夹具，并在处理高精度装配和精细物体操作等接触密集型场景中表现出强大潜力，预示其能增强机器人学习的鲁棒性。|Masashi Hamaya Team|[2602.14434](http://arxiv.org/abs/2602.14434)|null|
|**2026-02-15**|**WIMLE: Uncertainty-Aware World Models with IMLE for Sample-Efficient Continuous Control**|基于模型的强化学习（MBRL）常因模型误差累积、世界模型处理多模态动力学不佳及预测过度自信而表现受限。本文提出了WIMLE，一种将隐式最大似然估计（IMLE）扩展到MBRL框架的方法，旨在学习随机、多模态的世界模型，并利用集成和潜在采样估计预测不确定性。WIMLE在训练中根据预测置信度加权合成转换，以稳定学习。在40个连续控制任务上的实验结果表明，WIMLE实现了卓越的样本效率和有竞争力的渐近性能，尤其在挑战性任务上显著提升了样本效率，凸显了IMLE基多模态和不确定性感知加权对稳定MBRL的价值。|Ke Li Team|[2602.14351](http://arxiv.org/abs/2602.14351)|**[link](https://openreview.net/forum?id=mzLOnTb3WH)**|
|**2026-02-15**|**Multi-Agent Debate: A Unified Agentic Framework for Tabular Anomaly Detection**|表格异常检测常依赖单一或静态集成检测器，但异构模型在分布漂移、缺失数据和稀有异常下常出现分歧。本文提出了MAD（Multi-Agent Debating）框架，将这种分歧作为核心信号，通过数学协调层解决。框架中每个代理是一个ML检测器，提供异常分数、置信度和证据，并由LLM评论员增强。协调器将消息转换为损失并更新代理影响力，生成最终异常分数和可审计的辩论轨迹。实验表明，MAD在各种表格异常基准测试上提高了鲁棒性，并提供了更清晰的模型分歧追踪。|Sheng Li Team|[2602.14251](http://arxiv.org/abs/2602.14251)|null|
|**2026-02-15**|**Towards Spatial Transcriptomics-driven Pathology Foundation Models**|空间转录组学（ST）能够提供超越组织学评估的分子景观，多模态基础模型也显示了形态分子耦合提升组织学表征的潜力。为整合局部分子信息到病理视觉编码器，本文提出了Spatial Expression-Aligned Learning (SEAL) 框架，作为一种参数高效的视觉-组学自监督微调方法，可应用于现有病理学基础模型。SEAL通过在涵盖14个器官的70多万个配对基因表达点-组织区域示例上进行训练，在38项幻灯片级和15项补丁级下游任务上，持续优于纯视觉和ST预测基线，并展示了强大的域泛化能力和基因到图像检索等跨模态能力，为病理学基础模型的ST引导微调提供了通用且实用的框架。|Faisal Mahmood Team|[2602.14177](http://arxiv.org/abs/2602.14177)|null|
|**2026-02-15**|**ARport: An Augmented Reality System for Markerless Image-Guided Port Placement in Robotic Surgery**|精确的端口放置是机器人辅助手术的关键步骤，但术前规划与术中执行之间存在差距。本文提出了ARport，一个增强现实（AR）系统，旨在自动将预规划的套管布局映射到患者体表，提供直观的术中空间指导。ARport在光学透视头戴式显示器（OST-HMD）上实现，无需外部传感器或标记，通过基础模型提取患者体表并进行无标记配准，实现术前解剖模型与患者体表的对齐，从而现场可视化套管布局。全尺寸人体模型实验表明，ARport能够准确叠加预规划的套管位置，实现虚拟规划与真实解剖之间的一致空间对应，为临床工作流程的无缝集成提供了高效且极简的解决方案。|Qi Dou Team|[2602.14153](http://arxiv.org/abs/2602.14153)|null|
|**2026-02-13**|**Imitating What Works: Simulation-Filtered Modular Policy Learning from Human Videos**|针对机器人通过观察人类视频学习抓取操作技能时，传统方法难以有效学习与任务兼容的抓取行为的问题，本研究提出了Perceive-Simulate-Imitate (PSI) 框架。该框架利用仿真中的抓取轨迹过滤技术，对人类视频数据进行处理，并生成带有抓取适用性标签的扩展轨迹数据，从而实现面向任务的抓取能力监督学习。真实世界实验表明，PSI无需任何机器人数据即可高效学习精确的操纵技能，并且相比简单使用抓取生成器的方法，性能显著提升，鲁棒性更强。|Wei-Chiu Ma Team|[2602.13197](http://arxiv.org/abs/2602.13197)|null|
|**2026-02-13**|**Order Matters in Retrosynthesis: Structure-aware Generation via Reaction-Center-Guided Discrete Flow Matching**|为解决现有免模板逆合成方法学习效率低和半模板方法泛化受限的问题，本研究提出了一种结构感知的免模板框架，核心在于利用原子排序信息。该方法将反应中心原子置于序列头部，通过位置归纳偏差编码化学反应的两阶段特性，并采用RetroDiT骨干网络与离散流匹配相结合。实验结果表明，该方法在USPTO-50k和USPTO-Full数据集上取得了SOTA性能，且在预测反应中心下，性能超越了使用更多数据训练的基础模型，并验证了结构先验的重要性。|Tianshu Yu Team|[2602.13136](http://arxiv.org/abs/2602.13136)|null|
|**2026-02-13**|**A Calibrated Memorization Index (MI) for Detecting Training Data Leakage in Generative MRI Models**|鉴于图像生成模型可能复制训练数据，尤其在医学图像生成中引发隐私问题，本研究提出了一种校准的逐样本度量方法来检测训练数据的记忆化和重复。该方法利用MRI基础模型提取图像特征，聚合多层白化最近邻相似度，并映射为有界的“过拟合/新颖性指数”（ONI）和“记忆化指数”（MI）分数。在多个MRI数据集上的实验结果表明，该度量能稳健检测重复数据，并提供一致的度量值，在样本级别实现了近乎完美的重复项检测。|Ibrahim Habli Team|[2602.13066](http://arxiv.org/abs/2602.13066)|null|
|**2026-02-13**|**INHerit-SG: Incremental Hierarchical Semantic Scene Graphs with RAG-Style Retrieval**|针对现有语义场景图在机器人导航中难以支持可解释的人类意图推理的问题，本研究提出了INHerit-SG框架。该框架将地图定义为RAG-ready的知识库，通过引入自然语言描述作为语义锚点对齐人类意图，并采用异步双进程架构和分层结构解耦几何分割与语义推理，通过事件触发机制保持地图长期一致性。实验在新建数据集和真实世界环境中进行，结果表明INHerit-SG在复杂查询上达到了最先进性能，并提高了检索成功率和可靠性，展现了其在下游导航任务中的可扩展性。|Yang Gao Team|[2602.12971](http://arxiv.org/abs/2602.12971)|null|
|**2026-02-13**|**Information-theoretic analysis of world models in optimal reward maximizers**|为量化最优行为对世界内部表示的需求，本研究考虑了一个具有n个状态和m个动作的受控马尔可夫过程，并假设转移动态存在均匀先验。研究证明，观察一个对任何非恒定奖励函数最优的确定性策略，可以精确地传达n log m比特关于环境的信息。具体来说，环境与最优策略之间的互信息为n log m比特。这些发现为实现最优性所需的“隐式世界模型”提供了精确的信息理论下限，适用于多种奖励最大化目标。|Alex Altair Team|[2602.12963](http://arxiv.org/abs/2602.12963)|null|
|**2026-02-13**|**Scaling Single Human Demonstrations for Imitation Learning using Generative Foundational Models**|鉴于机器人模仿学习中收集演示数据耗时且不易从人类演示直接迁移，本研究提出Real2Gen框架，旨在从单个“人类”演示中训练机器人操纵策略。该方法从人类演示中提取必要信息并传输至仿真环境，在仿真中利用可编程专家智能体生成无限数据来训练流匹配策略。实验结果显示，Real2Gen在三个真实世界任务上成功率平均提升26.6%，并且由于训练数据丰富多样，训练策略的泛化能力显著提高，纯仿真训练的策略还能零样本部署到真实世界。|Abhinav Valada Team|[2602.12734](http://arxiv.org/abs/2602.12734)|null|
|**2026-02-13**|**MedXIAOHE: A Comprehensive Recipe for Building Medical MLLMs**|为提升真实世界临床应用中的通用医学理解和推理能力，本研究提出了医疗视觉-语言基础模型MedXIAOHE。该模型采用实体感知持续预训练框架，组织异构医学语料以拓宽知识覆盖并减少长尾问题；通过强化学习和工具增强的代理训练，整合多样化医学推理模式以支持带可验证决策轨迹的多步骤诊断推理；并融合用户偏好规则、证据推理和低幻觉长文本报告生成，提高真实世界使用的可靠性。MedXIAOHE在多项医学基准测试中取得了最先进的性能，并超越了领先的闭源多模态系统。|Zhixiong Yang Team|[2602.12705](http://arxiv.org/abs/2602.12705)|null|
|**2026-02-13**|**RelBench v2: A Large-Scale Benchmark and Repository for Relational Data**|为推动关系深度学习（RDL）的发展，并应对日益增长的模型规模需求，本研究引入了RelBench v2，一个大规模、真实的关系数据库基准扩展。RelBench v2新增了四个大型数据集和“自动完成任务”，旨在直接推理关系表中缺失的属性值，并整合了外部基准和评估框架以实现统一的关系-时间评估。实验结果表明，RDL模型在自动完成、预测和推荐任务中始终优于单表基线，突出了显式建模关系结构的重要性。|Jure Leskovec Team|[2602.12606](http://arxiv.org/abs/2602.12606)|**[link](https://relbench.stanford.edu)**|
|**2026-02-13**|**The Constant Eye: Benchmarking and Bridging Appearance Robustness in Autonomous Driving**|针对自动驾驶算法在OOD条件下易受外观变化影响，且难以区分外观与结构场景变化导致规划器失效的问题，本研究建立了Navdream，一个高保真鲁棒性基准。该基准利用生成式像素对齐风格迁移，隔离外观变化对驾驶性能的影响。为弥合这一差距，研究提出了一种通用感知接口，利用冻结的视觉基础模型（DINOv3）提取外观不变特征作为规划器的稳定接口。实验表明，现有规划算法在OOD外观下性能显著下降，而该即插即用解决方案在各种规划范式中实现了卓越的零样本泛化，在极端外观变化下仍保持一致性能。|Yiyi Liao Team|[2602.12563](http://arxiv.org/abs/2602.12563)|null|
|**2026-02-13**|**Self-Supervised JEPA-based World Models for LiDAR Occupancy Completion and Forecasting**|鉴于自动驾驶需要世界模型来支持长期规划，且模型学习需具备自监督的可扩展性，本研究提出AD-LiST-JEPA，一个基于联合嵌入预测架构（JEPA）的自监督世界模型。该模型旨在利用JEPA框架从激光雷达数据预测未来时空演变。通过下游基于激光雷达的占用完成和预测（OCF）任务评估学习到的表示质量，概念验证实验表明，经过JEPA世界模型学习预训练后的编码器在OCF性能上有所提升，证明了该方法在感知和预测联合任务中的潜力。|Anna Choromanska Team|[2602.12540](http://arxiv.org/abs/2602.12540)|null|
|**2026-02-13**|**Multi-Agent Model-Based Reinforcement Learning with Joint State-Action Learned Embeddings**|在部分可观察和高度动态环境中，多智能体协调学习面临表示学习和数据效率挑战。为此，本文提出了一种新颖的基于模型的强化学习框架，该框架将联合状态-动作表示学习与想象式展开相结合。作者设计了一个使用变分自编码器训练的世界模型，并利用学习到的状态-动作嵌入（SALE）进行增强，将其注入到预测未来展开的想象模块和估计联合动作值函数的联合智能体网络中。在星际争霸II微管理、多智能体MuJoCo和基于级别的觅食挑战等基准测试中，该方法在有限真实环境交互下，通过将想象轨迹与基于SALE的动作值相结合，显著优于基线算法，验证了其在多智能体模型范式中学习联合状态-动作嵌入的有效性。|David Meger Team|[2602.12520](http://arxiv.org/abs/2602.12520)|null|
|**2026-02-12**|**The Observer Effect in World Models: Invasive Adaptation Corrupts Latent Physics**|判断神经网络模型是内化了物理定律还是仅利用统计捷径，尤其是在分布外（OOD）变化下，仍是一个难题。传统的适应性评估方法（如微调或高容量探针）可能改变被测量的表示，从而混淆自监督学习（SSL）期间的真实学习内容。为解决此问题，本文提出了一种非侵入性评估协议PhyIP，该协议基于线性表示假设，通过测试物理量能否从冻结表示中线性解码来评估。在流体动力学和轨道力学任务中，实验发现当SSL错误率较低时，潜在结构变得线性可访问，PhyIP在OOD测试中成功恢复了内能和牛顿反平方标度（ρ>0.90）。相比之下，基于适应性的评估可能使这种结构崩溃（ρ≈0.05）。这表明低容量探针能更准确地评估物理世界模型，而适应性评估可能掩盖潜在结构。|Barbara Hammer Team|[2602.12218](http://arxiv.org/abs/2602.12218)|null|
|**2026-02-12**|**LDA-1B: Scaling Latent Dynamics Action Model via Universal Embodied Data Ingestion**|当前机器人基础模型多依赖大规模行为克隆，忽视了异构具身数据中可迁移的动力学知识，而现有统一世界模型（UWM）因粗糙数据使用和碎片化数据集难以扩展。为此，本文提出了LDA-1B，一个通过通用具身数据摄取进行扩展的机器人基础模型，它联合学习动力学、策略和视觉预测，并为不同质量数据分配不同角色。为支持大规模训练，作者构建并标准化了EI-30k数据集（超过3万小时的人类和机器人轨迹）。通过在结构化的DINO潜在空间中进行预测，实现了异构数据的可扩展动力学学习，避免了冗余的像素空间外观建模，并采用多模态扩散Transformer处理异步视觉和动作流，实现了10亿参数规模的稳定训练。实验结果表明，LDA-1B在接触密集型、灵巧型和长程任务上分别比现有方法（如π0.5）提高了21%、48%和23%，并能通过利用30%通常有害且被丢弃的低质量轨迹，实现数据高效微调，性能提升10%。|He Wang Team|[2602.12215](http://arxiv.org/abs/2602.12215)|**[link](https://pku-epic.github.io/LDA)**|
|**2026-02-12**|**DreamID-Omni: Unified Framework for Controllable Human-Centric Audio-Video Generation**|尽管基础模型在音视频生成方面取得进展，但以人物为中心的多任务（如参考音视频生成、视频编辑、音频驱动动画）仍被孤立处理，且难以在单一框架内实现对多角色身份和音色的精确解耦控制。本文提出了DreamID-Omni，一个统一的可控人物中心音视频生成框架。作者设计了一个对称条件扩散Transformer，通过对称条件注入方案整合异构条件信号。为解决多人物场景中普遍存在的身份-音色绑定失败和说话人混淆问题，提出了双层解耦策略：在信号层面采用同步RoPE确保严格的注意力空间绑定，在语义层面采用结构化字幕建立显式属性-主题映射。此外，还设计了多任务渐进训练方案，利用弱约束生成先验来正则化强约束任务。大量实验证明，DreamID-Omni在视频、音频和音视频一致性方面均达到了全面的最先进性能，甚至超越了领先的商业模型。|Xiangwang Hou Team|[2602.12160](http://arxiv.org/abs/2602.12160)|**[link](https://guoxu1233.github.io/DreamID-Omni/)**|
|**2026-02-12**|**It's TIME: Towards the Next Generation of Time Series Forecasting Benchmarks**|时间序列基础模型（TSFMs）正在革新预测领域，但现有基准在数据构成、数据完整性、任务制定和分析视角上存在局限。为解决这些问题，本文提出了TIME，一个新一代任务中心基准，包含50个全新数据集和98个预测任务，专为严格的零样本TSFM评估而设计，避免数据泄露。通过整合大型语言模型和人类专业知识，建立了严格的人机协作基准构建流程，确保高数据完整性，并根据真实操作需求和变量可预测性重新定义任务。此外，作者提出了一种新颖的模式级评估视角，超越了基于静态元标签的传统数据集级评估，通过利用结构化时间序列特征表征内在时间属性，为模型能力提供了更具普适性的见解。对12个代表性TSFMs进行评估，并建立了一个多粒度排行榜，以促进深入分析和可视化检查。|Chenghao Liu Team|[2602.12147](http://arxiv.org/abs/2602.12147)|null|
|**2026-02-12**|**Commencing-Student Enrolment Forecasting Under Data Sparsity with Time Series Foundation Models**|许多大学面临日益增长的财政压力，亟需准确预测新生入学人数，然而高等教育入学预测通常数据稀疏，年度序列短且受报告变化和体制转变影响。流行的经典方法因短样本导致参数估计和模型选择不稳定，以及结构性中断导致外推能力下降而不可靠。近期，TSFMs在泄漏受限的协变量构建下，为年度、数据稀疏的机构预测提供了强大的零样本先验。本文在零样本设置下，对多种TSFM家族进行了基准测试，并测试了一组紧凑、防泄漏的协变量集。作者引入了“机构运营状况指数”（IOCI），这是一个从时间戳文件证据中提取的可转移的0-100区间状态协变量，并结合了具有稳定特征工程的Google Trends需求代理。使用严格对齐的回溯测试，结果表明，在没有机构特定训练的情况下，条件化TSFMs的表现与经典基准相当，具体表现差异因群体和模型而异。|Surangika Ranathunga Team|[2602.12120](http://arxiv.org/abs/2602.12120)|null|
|**2026-02-12**|**The Pensieve Paradigm: Stateful Language Models Mastering Their Own Context**|当前的大型语言模型（LLMs）虽有强大的数据库和检索系统，但缺乏主动管理自身状态和记忆的机制，被动接受手动提供的上下文，受限于固定窗口大小。本研究引入了StateLM，一种新型基础模型，它具备内部推理循环来管理自身状态。该模型配备了一系列记忆工具（如上下文剪枝、文档索引、笔记）并被训练主动管理这些工具，从而摆脱固定窗口的架构限制。实验结果表明，StateLM在所有模型规模的长文档问答任务中持续优于标准LLMs，在聊天记忆任务中绝对准确率提升10%至20%，在深度研究任务BrowseComp-Plus上更是达到52%的准确率，远超标准LLM的5%左右。这使得LLMs从被动预测器转变为状态感知代理，实现了状态化和可管理的推理过程。|Yan Wang Team|[2602.12108](http://arxiv.org/abs/2602.12108)|null|
|**2026-02-12**|**GigaBrain-0.5M*: a VLA That Learns From World Model-Based Reinforcement Learning**|直接从当前观察预测多步动作块的视觉-语言-动作（VLA）模型，因场景理解和未来预测能力受限而存在不足。相比之下，预训练于网络规模视频语料库的视频世界模型具有强大的时空推理和准确的未来预测能力，为增强VLA学习提供了基础。本研究提出了GigaBrain-0.5M*，一个通过世界模型驱动强化学习训练的VLA模型，其基于在超过1万小时机器人操作数据上预训练的GigaBrain-0.5。GigaBrain-0.5M*通过RAMP（通过世界模型条件策略进行强化学习）整合强化学习，以实现鲁棒的跨任务适应。实验结果表明，RAMP相对于RECAP基线取得了显著性能提升，在“叠衣服”、“装箱”和“制作浓缩咖啡”等挑战性任务上提升了约30%，并且在真实世界部署中展示了可靠的长时序复杂操作能力。|Zheng Zhu Team|[2602.12099](http://arxiv.org/abs/2602.12099)|**[link](https://gigabrain05m.github.io/)**|
|**2026-02-12**|**Affordance-Graphed Task Worlds: Self-Evolving Task Generation for Scalable Embodied Learning**|在现实世界中直接训练机器人策略成本高昂且难以扩展，而生成式仿真虽能合成大规模数据，但现有方法常难以生成逻辑连贯的长时序任务，且因开环执行难以应对动态物理不确定性。本研究提出了Affordance-Graphed Task Worlds (AGT-World)，一个统一框架，能够根据现实世界观察自主构建交互式仿真环境及相应的机器人任务策略。AGT-World通过将任务空间形式化为结构化图，实现复杂目标的精确分层分解为原子原语，并引入结合视觉-语言模型推理和几何验证的混合反馈自进化机制来自主优化策略。大量实验证明，AGT-World在成功率和泛化能力上显著优于现有方法，实现了提议、执行和纠错的自改进循环，从而促进可扩展的机器人学习。|Changshui Zhang Team|[2602.12065](http://arxiv.org/abs/2602.12065)|null|
|**2026-02-12**|**VLAW: Iterative Co-Improvement of Vision-Language-Action Policy and World Model**|视觉-语言-动作（VLA）模型的性能和可靠性可通过迭代在线交互提升，但现实世界策略回放数据收集昂贵。虽然学习到的模拟器（动作条件视频生成模型）有望生成额外回放数据，但现有世界模型缺乏物理保真度，难以精确建模接触密集物体操纵中的关键物理细节。本研究提出一种简单的迭代改进算法，利用真实世界回放数据提升世界模型的保真度，然后该世界模型再用于生成补充合成数据以改进VLA模型。在真实机器人实验中，该方法显著提升了最先进VLA模型在多个下游任务上的性能，相比基础策略，成功率绝对提升39.2%，相比使用生成合成回放数据进行训练，提升了11.6%。|Chelsea Finn Team|[2602.12063](http://arxiv.org/abs/2602.12063)|null|
|**2026-02-12**|**HoloBrain-0 Technical Report**|基础模型研究与可靠的真实世界机器人部署之间存在差距。为弥合此差距，本研究引入了HoloBrain-0，一个全面的视觉-语言-动作（VLA）框架。其核心是一个新颖的VLA架构，明确融入了机器人具身先验（如多视角相机参数和运动学描述），以增强3D空间推理并支持多样化的具身形态。该设计通过可扩展的“预训练-后训练”范式进行验证，在RoboTwin 2.0、LIBERO和GenieSim等仿真基准测试中取得了最先进的结果，并在具有挑战性的长时序真实世界操作任务中表现出色。值得一提的是，其高效的0.2B参数变体能与更大的基线模型相媲美，支持低延迟的设备端部署。为加速研究，HoloBrain生态系统已完全开源，包括预训练VLA基础模型、后训练检查点和全栈VLA基础设施RoboOrchard。|Zhizhong Su Team|[2602.12062](http://arxiv.org/abs/2602.12062)|null|
|**2026-02-12**|**FedGRPO: Privately Optimizing Foundation Models with Group-Relative Rewards from Domain Client**|针对联邦基础模型(FedFMs)中现有知识迁移方法存在的训练成本高、通信开销大及隐私风险问题，本研究将其重构为强化学习式评估过程。提出了FedGRPO框架，通过构建轻量级置信图进行能力专家选择，并利用"组相对"概念将问题与解决方案打包为候选策略，仅聚合客户端返回的标量奖励信号，而非直接交换数据或模型更新。实验结果表明，FedGRPO在保证隐私和降低通信开销的同时，实现了优于传统FedFMs基线的下游任务精度和通信效率。|Yuxing Han Team|[2602.12014](http://arxiv.org/abs/2602.12014)|null|
|**2026-02-12**|**Accelerating Robotic Reinforcement Learning with Agent Guidance**|强化学习(RL)在机器人操作技能学习中面临样本效率低下，而现有的人在环(HIL)方法受限于可扩展性与人类监督的低效性。本研究提出了Agent-guided Policy Search (AGPS)框架，用多模态智能体取代人类监督者，将智能体视为语义世界模型，注入内在价值先验来指导物理探索。通过可执行工具，智能体提供精确的纠正航点和空间约束来修剪探索。实验结果表明，AGPS在精细插入和变形物体操作任务中，样本效率优于HIL方法，从而实现了自动化监督，为可扩展的机器人学习提供了新途径。|Yaodong Yang Team|[2602.11978](http://arxiv.org/abs/2602.11978)|null|
|**2026-02-12**|**Where Bits Matter in World Model Planning: A Paired Mixed-Bit Study for Efficient Spatial Reasoning**|为实现高效空间推理，本研究探讨了低比特规划行为在有限精度预算下，是否主要由总比特位宽决定，或由比特位在模型模块间的分配方式决定。通过在DINO-WM上进行混合位评估，观察到8比特和6比特设置接近FP16，3比特设置性能崩溃，而4比特设置对位宽分配敏感。在4比特过渡区，保留编码器精度可提升规划性能。这些发现表明，模块感知、预算感知的量化策略对于高效空间推理具有重要研究意义。|Vaishak Menon Team|[2602.11882](http://arxiv.org/abs/2602.11882)|null|
|**2026-02-12**|**PuYun-LDM: A Latent Diffusion Model for High-Resolution Ensemble Weather Forecasts**|潜在扩散模型(LDMs)在 <=0.25° 的高分辨率集合天气预报中存在扩散性受限问题，且现有频率方法因缺乏对变量间频谱异质性的考虑而导致正则化强度不均。本研究提出了PuYun-LDM模型，结合3D掩码自编码器(3D-MAE)编码天气状态演化特征作为扩散模型的额外条件，并采用变量感知掩码频率建模(VA-MFM)策略自适应选择阈值。实验证明，PuYun-LDM增强了潜在扩散性，在短时效预报中表现优于ENS，长时效预报中与ENS相当，并能在短时间内高效生成全球预报。|Bin Wang Team|[2602.11807](http://arxiv.org/abs/2602.11807)|null|
|**2026-02-12**|**HAIC: Humanoid Agile Object Interaction Control via Dynamics-Aware World Model**|类人机器人在处理非结构化环境中复杂全身任务时，现有方法难以应对具有独立动力学和非完整约束的欠驱动物体，且缺乏外部状态估计。本研究提出了HAIC统一框架，通过仅利用本体感受历史数据估计高阶物体状态的动力学预测器，并将预测投影至静态几何先验形成空间接地动态占用图，使策略能在盲区推断碰撞边界和接触可供性。通过非对称微调，世界模型持续适应学生策略。实验表明，HAIC在敏捷任务和多物体长时序任务中均取得了高成功率，有效应对惯性扰动并预测多物体动力学。|Renjing Xu Team|[2602.11758](http://arxiv.org/abs/2602.11758)|**[link](https://haic-humanoid.github.io/)**|
|**2026-02-12**|**ABot-N0: Technical Report on the VLA Foundation Model for Versatile Embodied Navigation**|具身导航领域长期受限于任务特定的架构。本研究引入了ABot-N0，一个统一的视觉-语言-动作(VLA)基础模型，旨在实现点目标、物体目标、指令跟随、POI目标和人物跟随五项核心导航任务的“大一统”。模型采用分层“大脑-行动”架构，将基于大型语言模型的认知大脑与基于流匹配的动作专家结合。为支持大规模学习，研究团队开发了ABot-N0数据引擎， curating 16.9M专家轨迹和5.0M推理样本。ABot-N0在7个基准测试中取得了新的SOTA性能，并整合规划器与分层拓扑记忆，实现动态真实世界环境中的鲁棒长时序任务。|Mu Xu Team|[2602.11598](http://arxiv.org/abs/2602.11598)|**[link](https://amap-cvlab.github.io/ABot-Navigation/ABot-N0/)**|
|**2026-02-12**|**Brain4FMs: A Benchmark of Foundation Models for Electrical Brain Signal**|脑基础模型(BFMs)在神经科学领域快速发展，但缺乏统一的方法理解和标准化评估框架。本研究旨在填补这一空白，通过自监督学习(SSL)分类法组织BFMs，并从数据集角度总结常见下游任务，整理临床和以人为中心神经技术应用的代表性公共数据集。在此基础上，推出了Brain4FMs开放评估平台，集成了15个代表性BFM和18个公共数据集。该平台支持标准化比较和分析预训练数据、SSL策略和架构对泛化和下游性能的影响，以指导更准确、可迁移的BFMs开发。|Yang Yang Team|[2602.11558](http://arxiv.org/abs/2602.11558)|null|
|**2026-02-12**|**TS-Memory: Plug-and-Play Memory for Time Series Foundation Models**|时间序列基础模型(TSFMs)在分布偏移下的下游领域适应面临挑战，现有参数化适应易导致灾难性遗忘，而非参数化检索则引入高推理延迟。本研究提出了参数化记忆蒸馏方法并实现为TS-Memory，一个轻量级记忆适配器。TS-Memory分两阶段训练：首先构建离线、无信息泄露的kNN教师模型合成置信度感知的量化目标；其次通过置信度门控监督将检索诱导的分布校正蒸馏到记忆适配器中。实验表明，TS-Memory在多种TSFMs和基准测试上持续改进点预测和概率预测性能，且效率与冻结骨干网络相当，实现了无检索部署。|Yuxuan Liang Team|[2602.11550](http://arxiv.org/abs/2602.11550)|null|
|**2026-02-12**|**Budget-Constrained Agentic Large Language Models: Intention-Based Planning for Costly Tool Use**|在预算约束下，大型语言模型调用外部工具解决多步骤任务的工具增强型智能体面临巨大状态-动作空间、高结果方差和过高探索成本导致直接规划难以处理的问题。本研究提出了INTENT，一个推理时规划框架，利用意图感知的层次世界模型来预测未来的工具使用和风险校准成本，并在线指导决策。实验证明，INTENT在成本增强型StableToolBench上严格遵循硬预算可行性，显著提高了任务成功率，并在工具价格和预算动态变化下表现出鲁棒性。|Qi Qi Team|[2602.11541](http://arxiv.org/abs/2602.11541)|null|
|**2026-02-12**|**Vascular anatomy-aware self-supervised pre-training for X-ray angiogram analysis**|X射线血管造影图像分析受限于带标注数据稀缺，大规模自监督学习(SSL)在该领域潜力未被充分挖掘。本研究引入了VasoMIM框架，通过解剖引导的掩码策略强制模型学习血管语义，并利用解剖一致性损失保留血管结构一致性，从而增强学习表征的判别力。同时，策展了迄今最大的X射线血管造影预训练数据集XA-170K。VasoMIM在多个下游任务中展现出卓越的可迁移性和领先的性能，凸显了其作为X射线血管造影分析基础模型的巨大潜力。|Zeng-Guang Hou Team|[2602.11536](http://arxiv.org/abs/2602.11536)|null|
|**2026-02-12**|**Semantic-aware Adversarial Fine-tuning for CLIP**|当前研究表明，通过对抗性微调CLIP图像编码器可增强其零样本分类的对抗鲁棒性，但生成对抗样本（AEs）时仅依赖图像与单一手动模板的余弦相似度，不足以衡量图文对的语义相似性，导致微调后的模型鲁棒性不足。为解决此问题，本文提出了一种语义集成攻击方法，通过最小化原始图像与一组精炼文本描述（由基础模型生成并去除了幻觉）之间的平均相似度来生成语义感知的AEs。在此基础上，作者提出了语义感知对抗微调（SAFT）框架。实验结果表明，SAFT在16个数据集上的零样本对抗鲁棒性方面显著优于现有方法，实现了实质性提升。|Feng Liu Team|[2602.12461](http://arxiv.org/abs/2602.12461)|null|
|**2026-02-12**|**Stabilizing Native Low-Rank LLM Pretraining**|基础模型日益增长的参数量带来了巨大的计算和内存挑战，而低秩分解是降低成本的潜在途径，但从头开始仅使用低秩权重训练模型且性能匹配全秩模型仍缺乏稳定的方法。本文研究表明，无需先验方法的“全秩”辅助指导，大型语言模型（LLMs）可以从头开始仅使用低秩分解权重训练所有非嵌入矩阵。作者发现权重矩阵更新中谱范数（最大奇异值）的失控增长是导致原生低秩训练不稳定和损失尖峰的主要因素，并提出Spectron方法：通过正交化进行谱重归一化，根据因子当前的谱范数动态限制所得权重更新。实验证明，Spectron实现了稳定、端到端的低秩训练，开销可忽略不计，并为原生低秩Transformer建立了计算最优的缩放定律，展示了可预测的幂律行为和相对于全秩模型改进的推理效率。|Eugene Belilovsky Team|[2602.12429](http://arxiv.org/abs/2602.12429)|null|
|**2026-02-12**|**Policy4OOD: A Knowledge-Guided World Model for Policy Intervention Simulation against the Opioid Overdose Crisis**|阿片类药物危机是美国严重的公共卫生问题，但由于政策互动复杂且系统动态，评估干预措施极具挑战。本文提出Policy4OOD，一个知识引导的时空世界模型，旨在整合预测、反事实推理和优化三种关键能力来有效评估阿片类政策。该模型通过策略知识图谱、州级空间依赖性及社会经济时间序列的联合编码，构建一个策略条件化的Transformer来预测阿片类药物相关结果。训练完成后，世界模型可作为模拟器，通过前向传播进行预测，通过替换历史策略编码进行反事实分析，并通过蒙特卡洛树搜索进行策略优化。实验结果表明，空间依赖性和结构化策略知识显著提高了预测准确性，验证了该模型在数据驱动的公共卫生决策支持中的潜力。|Yanfang Ye Team|[2602.12373](http://arxiv.org/abs/2602.12373)|null|
|**2026-02-12**|**Free Lunch in Medical Image Foundation Model Pre-training via Randomized Synthesis and Disentanglement**|医学图像基础模型（MIFMs）在临床任务中展现巨大潜力，但其发展受限于大规模标注数据集的稀缺、异质性和高成本。本文提出RaSD（Randomized Synthesis and Disentanglement），一个可扩展的框架，可完全利用合成数据预训练MIFMs。RaSD通过随机高斯分布模拟解剖结构和外观变异，使模型接触足够的多尺度结构和外观扰动，从而迫使其依赖不变和任务相关的解剖线索而非数据集特有纹理，实现鲁棒和可迁移的表示学习。在120万3D体和960万2D图像上进行预训练后，RaSD模型在6种成像模态、48个数据集和56个下游任务中，持续优于从零开始训练的模型，在17个任务上取得了最佳性能，并在大多数其他任务上与使用大型真实数据集预训练的模型表现相当。这些结果证明了仅合成数据即可驱动鲁棒表示学习的能力，为医学AI领域带来了范式转变。|Hao Chen Team|[2602.12317](http://arxiv.org/abs/2602.12317)|null|

<p align=right>(<a href=#updated-on-20260225>back to top</a>)</p>

## VLM

|Publish Date|Title|Chinese Summary|Authors|PDF|Code|
|---|---|---|---|---|---|
|**2026-02-24**|**Spa3R: Predictive Spatial Field Modeling for 3D Visual Reasoning**|针对视觉-语言模型（VLMs）在三维空间理解方面的局限性，现有方法依赖显式三维模态或部分几何先验，导致可扩展性受限。本文提出Spa3R框架，通过自监督方式从多视图图像中学习统一的、视图不变的空间表示，并引入预测空间场建模（PSFM）范式，实现对底层三维场景的整体理解。将预训练的Spa3R编码器集成到现有VLM中形成Spa3-VLM后，模型在VSI-Bench的3D VQA任务上达到了58.6%的最新准确率，显著优于现有方法，证明了PSFM在提升空间智能方面的可扩展性。|Xinggang Wang Team|[2602.21186](http://arxiv.org/abs/2602.21186)|null|
|**2026-02-24**|**Seeing Through Words: Controlling Visual Retrieval Quality with Language Models**|传统图文检索在处理用户简短、语义模糊的查询时面临挑战，导致检索质量不可控。为此，本文提出一种质量可控的检索新范式，利用生成式语言模型将简短查询扩展为包含姿态、场景、美学等细粒度视觉属性的描述性文本。该框架通过结合相关性和美学评分模型，将查询补全过程与离散的质量水平相关联，从而实现语义丰富且质量感知的检索。实验结果表明，所提方法显著提升了检索效果并提供了有效的质量控制能力，弥合了VLM表达能力与用户简短查询之间的差距。|Yun Fu Team|[2602.21175](http://arxiv.org/abs/2602.21175)|null|
|**2026-02-24**|**LUMEN: Longitudinal Multi-Modal Radiology Model for Prognosis and Diagnosis**|大型视觉-语言模型（VLMs）在临床领域展现出决策支持潜力，尤其是在放射学胸部X光（CXR）图像分析中。然而，手动进行纵向CXR分析耗时且缺乏预后能力。本文提出了LUMEN，一个专为纵向CXR解读优化的新型训练框架，它利用多图像和多任务指令微调来增强模型的诊断和预后性能。通过在MIMIC-CXR等数据集上的实验，LUMEN显著提高了诊断VQA任务的性能，并展现出有前景的预后能力，突出了设计良好的指令微调VLM在实现更准确、临床意义更强的纵向放射学图像解读中的价值。|Marius George Linguraru Team|[2602.21142](http://arxiv.org/abs/2602.21142)|null|
|**2026-02-24**|**VAUQ: Vision-Aware Uncertainty Quantification for LVLM Self-Evaluation**|大型视觉-语言模型（LVLMs）常出现幻觉问题，但现有的LLM自评估方法过度依赖语言先验，不适用于视觉条件下的预测评估。本文提出了VAUQ，一个视觉感知不确定性量化框架，用于LVLM自评估，它显式地衡量模型输出对视觉证据的依赖程度。VAUQ引入了图像信息分数（IS）来量化视觉输入带来的预测不确定性降低，并结合无监督核心区域掩蔽策略。实验证明，VAUQ在多个数据集上始终优于现有自评估方法，有效提升了LVLM的可靠性。|Sharon Li Team|[2602.21054](http://arxiv.org/abs/2602.21054)|null|
|**2026-02-24**|**OCR-Agent: Agentic OCR with Capability and Memory Reflection**|大型视觉-语言模型（VLMs）在多轮修订中缺乏有效的自纠正机制，常陷入重复和低效的尝试，难以持续提升回答质量。本文提出一种新颖的迭代自纠正框架OCR-Agent，赋予模型能力反思和记忆反思两大核心能力。通过能力反思诊断错误并生成纠正方案，再利用记忆反思回顾过往尝试以避免重复并探索新解，最终通过严谨的重新推理优化答案。实验表明，OCR-Agent在OCRBench v2基准上显著超越了当前SOTA模型，并在视觉理解和推理任务中取得了领先结果，证明了结构化、自反思的机制可在无需额外训练的情况下显著增强VLM的推理鲁棒性。|Ying Cai Team|[2602.21053](http://arxiv.org/abs/2602.21053)|null|
|**2026-02-24**|**Not Just What's There: Enabling CLIP to Comprehend Negated Visual Descriptions Without Fine-tuning**|视觉-语言模型（VLMs）如CLIP在理解否定句方面表现不佳，常将肯定和否定语义（如“没有狗”）嵌入得相似，而现有微调方法有过度拟合风险。本文提出了CLIPGlasses，一个即插即用的框架，旨在增强CLIP对否定视觉描述的理解能力。该框架采用双阶段设计：Lens模块负责解耦文本嵌入中的否定语义，而Frame模块预测与上下文相关的排斥强度，并将其整合到修正的相似性计算中，以惩罚与否定语义的对齐。实验证明，CLIP配备CLIPGlasses后在域内性能具有竞争力，并在跨域泛化方面优于SOTA方法，尤其在低资源条件下表现出更强的鲁棒性。|Zejiang He Team|[2602.21035](http://arxiv.org/abs/2602.21035)|null|
|**2026-02-24**|**From Perception to Action: An Interactive Benchmark for Vision Reasoning**|现有视觉-语言模型（VLM）评估多集中于与结构无关的单轮任务，无法有效评估模型对几何、接触和支撑关系的推理能力，这对于理解物理结构至关重要。为弥补这一空白，本文引入CHAIN基准，一个交互式3D物理驱动测试平台，旨在评估模型理解、规划和执行基于物理约束的结构化动作序列的能力。CHAIN将评估从被动感知转向主动问题解决，涵盖机械拼图、3D堆叠等任务。实验结果表明，即使是最先进的VLM和扩散模型，仍难以内化物理结构和因果约束，在生成长程规划和有效行动方面表现不足。|Roy Ka-Wei Lee Team|[2602.21015](http://arxiv.org/abs/2602.21015)|**[link](https://social-ai-studio.github.io/CHAIN/)**|
|**2026-02-24**|**SpatiaLQA: A Benchmark for Evaluating Spatial Logical Reasoning in Vision-Language Models**|尽管视觉-语言模型（VLMs）在通用视觉问答和逻辑推理方面表现出色，但在复杂真实环境中进行合理决策的能力（即空间逻辑推理）仍有欠缺。为弥补这一不足，本文提出了SpatiaLQA基准，专门用于评估VLM的空间逻辑推理能力，该基准包含来自241个真实室内场景的9605个问答对。针对现有模型在此任务上的不足，我们提出了一种递归场景图辅助推理方法，利用视觉基础模型逐步将复杂场景分解为与任务相关的场景图。实验证明，该方法显著优于所有现有方法，有效提升了VLMs的空间逻辑推理能力。|Jie Song Team|[2602.20901](http://arxiv.org/abs/2602.20901)|null|
|**2026-02-24**|**Diagnosing Causal Reasoning in Vision-Language Models via Structured Relevance Graphs**|大型视觉-语言模型（LVLMs）在视觉问答任务中表现优异，但常依赖虚假关联而非真实的因果推理。现有评估主要关注答案正确性，未能区分失败原因是否源于推理能力限制或未能识别因果相关信息。本文引入了视觉-语言因果图（VLCGs），一种结构化、查询条件化的表示，明确编码因果相关的对象、属性、关系及场景假设。基于此，提出了ViLCaR诊断基准，包含因果归因、因果推理和问答任务，并采用与图对齐的评估指标。实验表明，注入结构化相关性信息能显著提升LVLMs的归因和推理一致性，暗示当前因果推理的局限性主要源于结构指导不足，而非推理能力缺乏。|Yihao Ding Team|[2602.20878](http://arxiv.org/abs/2602.20878)|null|
|**2026-02-24**|**MUSE: Harnessing Precise and Diverse Semantics for Few-Shot Whole Slide Image Classification**|计算病理学中，少量样本全玻片图像分类受限于专家标注稀缺，而现有视觉-语言方法将文本语义作为静态类级别先验，限制了视觉-语义对齐的多样性和精确性。为此，本文提出随机多视图语义增强（MUSE）框架，首先通过基于MoE的自适应视觉-语义交互实现样本级细粒度语义增强（SFSE），然后通过随机多视图模型优化（SMMO）构建LLM生成的丰富病理描述知识库，并在训练中随机整合多视图文本作为增强语义监督。实验证明，MUSE在多个WSI基准上持续优于现有VLM基线，强调了有效少量样本病理学习需更丰富的语义源和积极、样本感知的语义优化。|Nankun Mu Team|[2602.20873](http://arxiv.org/abs/2602.20873)|null|
|**2026-02-24**|**On the Explainability of Vision-Language Models in Art History**|在问题背景方面，视觉-语言模型（VLMs）将视觉和文本数据整合到共享嵌入空间中，同时引发了对机器“理解”本质的质疑。本研究旨在通过可解释人工智能（XAI）方法，揭示VLM（特指CLIP）在艺术史背景下的视觉推理过程。研究方法上，论文评估了七种XAI方法，结合了零样本定位实验和人类可解释性研究。实验结果表明，尽管这些方法能够捕捉人类解释的某些方面，但其有效性取决于所检查类别的概念稳定性及其表征可用性。|Stefanie Schneider Team|[2602.20853](http://arxiv.org/abs/2602.20853)|null|
|**2026-02-24**|**GatedCLIP: Gated Multimodal Fusion for Hateful Memes Detection**|针对多模态表情包中仇恨内容的检测难题（因图像与文本复杂交互而产生有害信息），本研究提出了一种名为GatedCLIP的视觉-语言模型。该模型通过专门的架构改进增强了CLIP的多模态能力，具体包括引入学习投影头以将CLIP嵌入映射到任务优化的语义空间、动态门控融合机制以自适应加权视觉和文本特征，以及对比学习目标以保持跨模态语义对齐。在Hateful Memes数据集上的实验结果表明，GatedCLIP的AUROC达到0.66，显著优于CLIP基线（AUROC 0.49），同时仅需35万个可训练参数，保持了计算效率。|Zirong Zeng Team|[2602.20818](http://arxiv.org/abs/2602.20818)|null|
|**2026-02-24**|**VGGDrive: Empowering Vision-Language Models with Cross-View Geometric Grounding for Autonomous Driving**|自动驾驶中跨视角3D几何建模能力至关重要，然而现有视觉-语言模型（VLMs）普遍缺乏此能力，导致性能平庸，且通过问答数据进行辅助训练的现有方法也未能根本解决此问题。为此，本研究提出了一种新范式，旨在将成熟3D基础模型的跨视角几何基础能力融入VLMs。具体方法是，论文提出了VGGDrive架构，并引入了即插即用的跨视角3D几何使能器（CVGE），通过分层自适应注入机制，将冻结的视觉3D模型的跨视角3D几何特征与VLM的2D视觉特征桥接起来，从而赋能VLM具备3D能力。大量实验表明，VGGDrive在包括跨视角风险感知、运动预测和轨迹规划在内的五个自动驾驶基准测试中显著提升了VLM的性能。|Long Chen Team|[2602.20794](http://arxiv.org/abs/2602.20794)|null|
|**2026-02-24**|**NGL-Prompter: Training-Free Sewing Pattern Estimation from a Single Image**|从图像中估计缝纫版型是创建高质量3D服装的实用方法，但缺乏真实世界数据导致现有VLM方法难以泛化，且多限于单层服装。本研究观察到VLM能有效描述服装，但直接回归参数表现不佳。为解决此问题，论文提出了NGL（Natural Garment Language），一种将GarmentCode重构为更易VLM理解的中间语言。在此基础上，引入了NGL-Prompter，一个无需训练的流程，通过查询大型VLM提取结构化服装参数，再确定性地映射到有效的GarmentCode。在Dress4D、CloSe和新收集的约5000张真实时尚图像数据集上的评估显示，该方法在几何度量上达到最先进水平，并在人类和GPT感性评估中获得显著偏好，同时能恢复多层服装并强泛化到真实世界图像，证明无需昂贵模型训练即可实现精确缝纫版型重建。|Michael Black Team|[2602.20700](http://arxiv.org/abs/2602.20700)|null|
|**2026-02-24**|**PromptCD: Test-Time Behavior Enhancement via Polarity-Prompt Contrastive Decoding**|为构建可靠的AI系统，大型语言模型（LLMs）需与人类偏好和价值观对齐，但现有对齐方法依赖大量高质量数据和高昂成本，且对比解码方法应用范围有限。为此，本研究提出了一种名为极性提示对比解码（PromptCD）的测试时行为控制方法，旨在将对比解码推广到更广泛的增强设置。PromptCD通过构建针对目标行为的成对正向和负向引导提示，对比模型响应（LLM中的词元级概率分布和VLM中的视觉注意力模式）以增强期望结果，且无需额外训练。实验结果显示，PromptCD在LLM的“3H”对齐目标（有益性、诚实性和无害性）上持续取得显著改进，在VLM上通过强化行为一致的视觉基础显著提升了VQA性能，证实其作为跨模态行为控制的简单、通用且经济高效的策略。|Xueqi Cheng Team|[2602.20696](http://arxiv.org/abs/2602.20696)|null|
|**2026-02-24**|**How Foundational Skills Influence VLM-based Embodied Agents:A Native Perspective**|视觉-语言模型（VLMs）在具身智能方面展现了巨大潜力，然而现有VLM驱动的具身智能体基准测试常依赖高层命令或离散动作空间，与现实世界控制存在显著差异，且缺乏低层和高层任务的联合评估。为解决这些局限性，本研究提出了NativeEmbodied，一个采用统一、原生低层动作空间的VLM驱动具身智能体挑战性基准。该基准构建于多样化的模拟场景，包含三个复杂场景中的代表性高层任务以评估整体性能，并进一步解耦复杂任务所需的技能，构建了四种针对基本具身技能的低层任务，从而实现跨任务和技能粒度的联合评估。对最先进VLM的实验揭示了其在多个基本具身技能上的明显缺陷，并分析表明这些瓶颈显著限制了高层任务的性能，为未来研究指明了关键挑战和方向。|Tong Xu Team|[2602.20687](http://arxiv.org/abs/2602.20687)|null|
|**2026-02-24**|**Recursive Belief Vision Language Model**|当前视觉-语言-动作（VLA）模型在部分可观测的长期操纵任务中表现不佳，主要因依赖短期观察或反复查询VLM，导致任务进度丢失、感知混叠下重复动作以及高推理延迟，其根本在于缺乏持久的、动作条件化的状态表示以及有限的时空和物理推理能力。本研究提出RB-VLA，一种以信念为中心的架构，通过自监督世界模型目标进行训练，能够维持一个紧凑的潜在状态来编码任务相关的历史、动态和物体交互。该模型仅需一次VLM查询以获取高层意图，而信念模块则跟踪任务进度，并在不存储原始观测或随时间扩展内存的情况下，实现部分可观测条件下的阶段感知和因果接地控制。信念和意图共同条件化一个扩散策略以实现鲁棒的闭环执行。RB-VLA在长期基准测试中优于现有VLA模型，多阶段抓取放置和堆叠任务成功率分别比π0高出52.5%和37.5%，推理延迟降低高达5倍，并消除了现有VLA中随时间增长的内存问题。消融实验表明信念模块是性能提升的主要驱动因素，将成功率从32.5%提高到77.5%。|Nirav Patel Team|[2602.20659](http://arxiv.org/abs/2602.20659)|null|
|**2026-02-24**|**Vision-Language Models for Ergonomic Assessment of Manual Lifting Tasks: Estimating Horizontal and Vertical Hand Distances from RGB Video**|人工搬运任务是工作相关肌肉骨骼疾病的主要原因，有效的工效学风险评估至关重要。修订后的NIOSH搬运方程（RNLE）是一种广泛使用的评估工具，依赖于手部水平（H）和垂直（V）距离等六个任务变量，这些变量通常通过手动测量或专业传感系统获取，难以在真实环境中应用。本研究评估了利用创新视觉-语言模型（VLMs）从RGB视频流中非侵入性估计H和V距离的可行性。为此，开发了两种多阶段VLM管道：文本引导的仅检测管道和检测加分割管道，两者均采用文本引导的兴趣区域定位、视觉特征提取和基于Transformer的时间回归来估计搬运开始和结束时的H和V。在多类搬运任务中，通过留一受试者交叉验证和七种摄像机视角条件进行性能评估。结果显示，基于分割的多视角管道持续产生最小误差（H约6-8厘米，V约5-8厘米的平均绝对误差），且像素级分割相比仅检测管道，将H和V的估计误差分别降低了约20-30%和35-40%。这些发现支持了基于VLM的管道在视频基础上估计RNLE距离参数的可行性。|Maury A. Nussbaum Team|[2602.20658](http://arxiv.org/abs/2602.20658)|null|
|**2026-02-24**|**Efficient and Explainable End-to-End Autonomous Driving via Masked Vision-Language-Action Diffusion**|大型语言模型（LLMs）和视觉-语言模型（VLMs）在端到端自动驾驶中展现潜力，但面临推理延迟、动作精度和可解释性挑战。现有自回归方法生成速度慢，而基于扩散的规划器常依赖缺乏几何结构的冗长通用语言标记。为此，本研究提出MVLAD-AD框架，旨在通过掩码视觉-语言-动作扩散模型弥合高效规划与语义可解释性之间的鸿沟。该方法引入离散动作标记策略，从真实驾驶分布构建运动学上可行的紧凑路点代码本；提出几何感知嵌入学习，确保潜在空间嵌入近似物理几何度量；并引入动作优先解码策略。在nuScenes及衍生基准上的大量实验表明，MVLAD-AD在效率和规划精度上优于最先进的自回归和扩散基线，同时提供高保真和可解释的推理。|Ziran Wang Team|[2602.20577](http://arxiv.org/abs/2602.20577)|null|
|**2026-02-24**|**An interactive enhanced driving dataset for autonomous driving**|随着自动驾驶向完全自动化发展，强大的交互能力成为关键，但现有数据中稀疏的交互场景和不足的多模态对齐限制了视觉-语言-动作（VLA）模型的发展。为此，本研究提出了交互增强驾驶数据集（IEDD）。该方法开发了一个可扩展的管道，基于交互轨迹从自然驾驶数据中挖掘百万级的交互片段，并设计了量化交互过程的指标。此外，通过生成语义动作与结构化语言严格对齐的合成鸟瞰图（BEV）视频，构建了IEDD-VQA数据集。对十个主流视觉语言模型（VLMs）的基准测试结果表明，该数据集在评估和微调自动驾驶模型的推理能力方面具有很高的复用价值。|Lu Xiong Team|[2602.20575](http://arxiv.org/abs/2602.20575)|null|
|**2026-02-23**|**NovaPlan: Zero-Shot Long-Horizon Manipulation via Closed-Loop Video Language Planning**|为解决机器人执行长周期任务时，视觉语言模型（VLM）和视频生成模型缺乏物理基础的问题，本文提出了NovaPlan，一个分层框架。该框架将闭环VLM与视频规划相结合，通过几何接地的机器人执行实现零样本长周期操作。高层VLM规划器分解子任务并闭环监控执行，实现自主重规划和单步失败恢复；低层则从生成视频中提取物体关键点和人手姿态作为运动学先验，并采用切换机制选择最优参考以计算机器人动作，确保稳定执行。实验结果表明，NovaPlan无需预训练或演示即可完成复杂装配任务，并展现出灵巧的错误恢复能力。|George Konidaris Team|[2602.20119](http://arxiv.org/abs/2602.20119)|**[link](https://nova-plan.github.io/)**|
|**2026-02-23**|**StructXLIP: Enhancing Vision-language Models with Multimodal Structural Cues**|针对现有视觉-语言对齐方法在处理长而细节丰富的字幕时，未能充分利用图像结构信息的问题，本文提出了StructXLIP。该方法通过提取图像边缘图作为视觉结构的代理，并过滤字幕以强调结构化线索，构建“以结构为中心”的对齐范式。在标准对齐损失的基础上，StructXLIP额外引入了三个结构中心损失，旨在对齐边缘图与结构化文本、匹配局部边缘区域与文本块、以及连接边缘图与彩色图像以防止表征漂移，从而最大化多模态结构表征间的互信息。实验证明，该方法在通用和专业领域的跨模态检索中超越了现有技术，并可作为即插即用的通用增强方案。|Marco Cristani Team|[2602.20089](http://arxiv.org/abs/2602.20089)|null|
|**2026-02-23**|**Do Large Language Models Understand Data Visualization Principles?**|鉴于数据可视化原则对有效视觉传达的重要性，但将其编码为逻辑规则需要专业知识，本文旨在探究大型语言模型（LLM）和视觉语言模型（VLM）是否能直接推理和执行可视化原则，从而绕过符号规则规范的需求。研究首次系统评估了LLM和VLM在推理可视化原则上的能力，利用从Answer Set Programming (ASP) 推导的精确验证真值，构建了一个包含约2000个Vega-Lite规范和300多个真实世界图表的受控数据集。实验评估了模型在检测和修正原则违反方面的表现。结果表明，大型（视觉-）语言模型在可视化设计验证和编辑方面具有潜力，但在视觉感知更细微之处仍与符号求解器存在差距，并且模型在纠正违规方面的效果往往优于可靠地检测它们。|Emmanuel Iarussi Team|[2602.20084](http://arxiv.org/abs/2602.20084)|null|
|**2026-02-23**|**HeatPrompt: Zero-Shot Vision-Language Modeling of Urban Heat Demand from Satellite Images**|鉴于大多数市政当局缺乏计算准确热需求地图所需详细建筑级数据的问题，本文提出了HeatPrompt，一个零样本视觉-语言能源建模框架。该框架利用卫星图像、基本地理信息系统（GIS）和建筑级特征中提取的语义特征来估算年度热需求。具体而言，它将预训练的大型视觉语言模型（VLM）与领域特定提示结合，使其充当能源规划器，并从RGB卫星图像中提取与热负荷相关的视觉属性（如屋顶年龄、建筑密度等）。实验结果显示，在此类描述上训练的多层感知器（MLP）回归器相比基线模型，其 $R^2$ 提升了93.7%，平均绝对误差（MAE）降低了30%，并能通过高影响令牌识别高需求区域，为数据稀缺地区的热规划提供了轻量级支持。|Veit Hagenmeyer Team|[2602.20066](http://arxiv.org/abs/2602.20066)|null|
|**2026-02-23**|**Contextual Safety Reasoning and Grounding for Open-World Robots**|针对机器人需在开放世界环境中操作，但传统安全方法无法处理情境可变性的问题，本文提出了CORE，一个无需环境先验知识即可实现在线上下文推理、接地和强制执行的安全框架。CORE利用视觉-语言模型（VLM）持续从视觉观测中推理上下文相关的安全规则，将这些规则在物理环境中接地，并通过控制障碍函数强制执行由此产生的空间定义安全集。该方法提供了考虑感知不确定性的概率安全保证。仿真和真实世界实验表明，CORE在未见过环境中能强制执行上下文适当的行为，显著优于缺乏在线上下文推理的语义安全方法，验证了VLM推理和空间接地对于新环境下上下文安全的重要性。|George J. Pappas Team|[2602.19983](http://arxiv.org/abs/2602.19983)|null|
|**2026-02-23**|**BeamVLM for Low-altitude Economy: Generative Beam Prediction via Vision-language Models**|针对低空经济中高移动性无人机与地面基站之间波束预测现有方法泛化能力差、缺乏对动态环境高级语义理解或精细空间语义感知的问题，本文提出了BeamVLM。该框架是一个端到端生成式波束预测模型，将波束预测视为视觉问答任务，利用强大的视觉-语言模型（VLM）。通过将原始视觉补丁直接投射到语言域并巧妙设计指导性提示，BeamVLM使VLM能够联合推理无人机轨迹和环境上下文。在真实世界数据集上的实验结果表明，BeamVLM在预测精度上优于最先进方法，并对车对基础设施（V2I）波束预测等其他场景展现出卓越的泛化能力。|Chengwen Xing Team|[2602.19929](http://arxiv.org/abs/2602.19929)|null|
|**2026-02-23**|**Multi-Modal Representation Learning via Semi-Supervised Rate Reduction for Generalized Category Discovery**|针对广义类别发现（GCD）任务中，现有基于多模态表示学习的方法虽然依赖跨模态对齐，但往往忽视了模态内对齐以生成理想表示结构的问题，本文提出了SSR $^2$ -GCD。该框架是一个新颖有效的多模态表示学习方法，通过半监督率降低机制，旨在强调正确对齐模态内关系，从而学习具有所需结构属性的跨模态表示。此外，为增强知识迁移，该方法利用视觉语言模型提供的模态间对齐，整合了提示候选。在通用和细粒度基准数据集上的广泛实验证明，该方法性能卓越。|Chun-Guang Li Team|[2602.19910](http://arxiv.org/abs/2602.19910)|null|
|**2026-02-23**|**ApET: Approximation-Error Guided Token Compression for Efficient VLMs**|针对近期视觉-语言模型（VLMs）因冗余视觉令牌导致计算开销大和推理效率低的问题，且现有解决方案易引入位置偏差并与高效注意力内核不兼容的局限，本文提出了ApET（Approximation-Error guided Token compression）。该框架摆脱了注意力依赖，从信息论角度出发，旨在通过线性近似用少量基础令牌重建原始视觉令牌，然后利用近似误差识别并丢弃信息量最少的令牌，从而最大程度保留视觉信息。在多个VLM和基准测试上的实验表明，ApET在图像理解和视频理解任务中分别保留了95.2%和100.4%的原始性能，同时将令牌预算压缩了88.9%和87.5%，且其无注意力设计使其能与FlashAttention无缝集成，进一步加速推理，使VLM部署更具实用性。|Hairong Zheng Team|[2602.19870](http://arxiv.org/abs/2602.19870)|null|
|**2026-02-23**|**TraceVision: Trajectory-Aware Vision-Language Model for Human-Like Spatial Understanding**|针对现有大型视觉-语言模型（LVLMs）主要关注全局图像理解，难以模拟人类视觉注意轨迹并解释描述与特定区域关联的问题，本文提出了TraceVision。该模型是一个统一的视觉-语言模型，通过轨迹感知视觉感知（TVP）模块实现视觉特征和轨迹信息的双向融合，从而将轨迹感知空间理解整合到端到端框架中。TraceVision设计了几何简化从原始轨迹中提取语义关键点，并提出三阶段训练流水线，以轨迹引导描述生成和区域定位，并可扩展至轨迹引导分割和视频场景理解。通过构建RILN数据集增强逻辑推理，实验证明TraceVision在轨迹引导字幕生成、文本引导轨迹预测、理解和分割任务上达到最先进性能，为直观空间交互和可解释视觉理解奠定了基础。|Jinqiao Wang Team|[2602.19768](http://arxiv.org/abs/2602.19768)|null|
|**2026-02-23**|**Seeing Clearly, Reasoning Confidently: Plug-and-Play Remedies for Vision Language Model Blindness**|为解决视觉语言模型（VLMs）在稀有物体上进行以物体为中心的推理时，由于预训练数据稀缺而面临挑战，且现有解决方案计算成本高昂的问题，本文提出了一个高效的即插即用模块。该模块在不微调VLM的情况下，通过精炼视觉令牌和丰富输入文本提示，显著提高VLM对稀有物体的推理能力。具体方法是利用视觉基础模型的先验知识和同义词增强文本描述，学习稀有物体的多模态类别嵌入；这些嵌入通过轻量级注意力增强模块精炼VLM的视觉令牌，并作为物体感知检测器生成信息提示，注入文本提示中以引导VLM关注相关图像区域。实验结果显示，预训练VLM在稀有物体识别和推理方面取得了显著提升，并增强了模型对稀有物体的关注和推理能力。|Zhengming Ding Team|[2602.19615](http://arxiv.org/abs/2602.19615)|null|
|**2026-02-23**|**VALD: Multi-Stage Vision Attack Detection for Efficient LVLM Defense**|大型视觉-语言模型（LVLMs）易受对抗性图像攻击，导致输出貌似合理但实则错误。为解决此问题，研究提出了一种通用、高效、免训练的防御机制，结合图像变换与主体数据整合来恢复模型正确行为。该方法包含两阶段检测：首先通过内容保留变换评估图像一致性，随后在文本嵌入空间检查差异；仅在必要时才调用大型语言模型解决分歧并整合多响应。实验证明，该方法在实现最先进准确性的同时保持了显著效率，多数干净图像跳过昂贵处理，即使在大量对抗性样本存在下开销也极小。|Ayellet Tal Team|[2602.19570](http://arxiv.org/abs/2602.19570)|null|
|**2026-02-23**|**Can a Teenager Fool an AI? Evaluating Low-Cost Cosmetic Attacks on Age Estimation Systems**|年龄估计系统作为在线内容守门人，其对化妆修改的鲁棒性未被充分评估。本研究系统性探究了胡须、白发、化妆和模拟皱纹等简单化妆是否会导致AI年龄估计器将未成年人错误分类为成年人。研究人员使用VLM图像编辑器在329张10-21岁面部图像上模拟攻击，并评估了八种模型，引入“攻击转换率”（ACR）衡量未成年预测转变为成年预测的比例。结果显示，合成胡须即可使28%至69%的图像被错误分类，四种攻击组合平均将预测年龄推高7.7岁，ACR高达83%，揭示了年龄验证系统的严重漏洞，强调对抗性鲁棒性评估的必要性。|Simiao Ren Team|[2602.19539](http://arxiv.org/abs/2602.19539)|null|
|**2026-02-23**|**ORION: ORthonormal Text Encoding for Universal VLM AdaptatION**|视觉语言模型（VLMs）的性能受限于其类别的文本原型质量和几何结构，现有方法生成的嵌入可能存在相关或分离度弱的问题，限制了任务辨别能力。本文提出了ORION，一个仅使用类名即可改进预训练VLM的文本编码器微调框架。该方法通过低秩适应优化了一个新颖的损失函数，该函数整合了促进类文本表示间两两正交性和惩罚与初始原型偏差两项。实验在11个基准和三个大型VLM骨干网络上进行，结果表明ORION精炼后的文本嵌入能有效替代标准CLIP原型，作为即插即用模块在零样本、少样本及测试时间适应等多种设置下显著提升了各种SOTA方法的性能。|Ismail Ben Ayed Team|[2602.19530](http://arxiv.org/abs/2602.19530)|null|
|**2026-02-23**|**Forgetting-Resistant and Lesion-Aware Source-Free Domain Adaptive Fundus Image Analysis with Vision-Language Model**|无源域适应（SFDA）在眼底图像诊断中，即使借助视觉-语言（ViL）基础模型，仍面临两个挑战：适应过程中目标模型优越预测的遗忘问题以及现有方法忽视了ViL模型中丰富的细粒度知识。针对此，本研究提出了一种新型的抗遗忘和病灶感知（FRLA）方法。具体而言，抗遗忘模块显式保留目标模型的置信预测，而病灶感知模块则利用ViL模型的逐块预测帮助目标模型感知病灶区域并利用其细粒度知识。大量实验证明，FRLA方法不仅显著优于独立的视觉-语言模型，而且相对于现有先进方法也实现了持续的性能提升。|Xiaomeng Li Team|[2602.19471](http://arxiv.org/abs/2602.19471)|null|
|**2026-02-23**|**Decoupling Vision and Language: Codebook Anchored Visual Adaptation**|大型视觉-语言模型（LVLMs）的视觉编码器在医学图像诊断或细粒度分类等领域特定任务中表现不佳，可能导致错误响应。现有适应方法通过调整编码器与语言模型之间的连续特征接口，但这种耦合设计限制了灵活性。本研究提出CRAFT（Codebook RegulAted Fine-Tuning），一种轻量级方法，通过离散码本微调视觉编码器，将视觉表示锚定到稳定的令牌空间，从而实现领域适应而不修改模型其他部分。这种解耦设计使得适应后的编码器能无缝提升共享相同码本的不同LVLM性能。实验在10个领域特定基准上平均实现了13.51%的性能提升，同时保留了LLM的语言能力，并优于其他处理连续令牌的方法。|Jonathan Wu Team|[2602.19449](http://arxiv.org/abs/2602.19449)|null|
|**2026-02-23**|**UrbanAlign: Post-hoc Semantic Calibration for VLM-Human Preference Alignment**|在领域特定任务中，使视觉-语言模型（VLM）输出与人类偏好对齐通常需要昂贵的微调或强化学习。本研究发现，对于主观感知任务，这种对齐无需模型训练即可实现，因为VLM是强大的概念提取器但校准决策能力较弱，可通过外部手段弥补。研究提出一个免训练的后处理概念瓶颈流程UrbanAlign，包含概念挖掘、多智能体结构化评分和几何校准三个阶段，由端到端维度优化循环统一。在城市感知任务Place Pulse 2.0上，该框架在六个类别中达到72.2%的准确率，超越了最佳监督基线15.1个百分点，且未校准的VLM评分16.3个百分点，同时提供全维度可解释性且无需修改模型权重。|Chunlei Shi Team|[2602.19442](http://arxiv.org/abs/2602.19442)|null|
|**2026-02-23**|**PA-Attack: Guiding Gray-Box Attacks on LVLM Vision Encoders with Prototypes and Attention**|大型视觉-语言模型（LVLMs）对对抗性攻击的脆弱性是一个关键问题，现有白盒攻击泛化性差，黑盒方法效率低下。鉴于视觉编码器在LVLMs间常被共享且稳定，本文提出了PA-Attack（Prototype-Anchored Attentive Attack）。该方法首先利用原型锚定指导，提供稳定的攻击方向以解决传统攻击的属性限制和任务泛化问题；其次，引入两阶段注意力增强机制，利用令牌级注意力分数集中扰动于关键视觉令牌，并自适应重新校准注意力权重以跟踪对抗过程中的注意力演变。大量实验表明，PA-Attack在各种下游任务和LVLM架构上实现了平均75.1%的分数降低率，展现了强大的攻击有效性、效率和任务泛化能力。|Minjing Dong Team|[2602.19418](http://arxiv.org/abs/2602.19418)|null|
|**2026-02-22**|**Seeing Farther and Smarter: Value-Guided Multi-Path Reflection for VLM Policy Optimization**|解决复杂、长周期机器人操作任务需要深入理解物理交互和高层规划，但现有基于反射规划的VLM方法存在效率低下、预测不准确和推理延迟大等局限。本研究提出一种新颖的测试时计算框架，解耦状态评估与动作生成。该框架通过可扩展的评论器明确量化动作计划的优势（与目标距离的减少），并利用束搜索探索多条未来路径以聚合其预期长期回报，从而生成更稳健的动作。此外，引入了基于置信度的轻量级触发器，在直接预测可靠时提前退出，仅在必要时进行反思。在多样化的多阶段机器人操作任务中，该方法成功率提高了24.6%，推理时间减少了56.5%。|Dimitris N. Metaxas Team|[2602.19372](http://arxiv.org/abs/2602.19372)|null|
|**2026-02-22**|**MentalBlackboard: Evaluating Spatial Visualization via Mathematical Transformations**|空间可视化是人类认知中将动作与感知在心理层面连接的重要能力。为探究现有视觉-语言模型（VLMs）是否具备此能力，本研究开发了MentalBlackboard基准，用于纸张折叠和打孔测试的预测和规划任务。预测实验显示，模型难以应用对称变换，即使展开步骤正确；旋转也对其物理情境感知构成显著挑战。规划任务揭示模型在分析对称关系和实施多阶段对称过程方面的局限性，其中Claude Opus 4.1的规划准确率最高，达到10%。表现最佳的模型o3在无需空间可视化但涉及空间数据传输的泛化任务上达到71.6%的峰值性能，但在基于文本的预测任务上仅达到25%的准确率，表明当前VLM在空间可视化能力方面仍存在局限。|Yezhou Yang Team|[2602.19357](http://arxiv.org/abs/2602.19357)|null|
|**2026-02-22**|**TOPReward: Token Probabilities as Hidden Zero-Shot Rewards for Robotics**|尽管视觉-语言-动作（VLA）模型在预训练方面取得了快速进展，但在真实世界场景中，其在强化学习（RL）方面的进步仍受低样本效率和稀疏奖励的阻碍。开发可泛化的过程奖励模型对于提供细粒度反馈至关重要，但现有时间价值函数通常无法泛化到训练域之外。本研究提出了TOPReward，一种新颖的、基于概率的时间价值函数，它利用预训练视频视觉-语言模型（VLMs）的潜在世界知识来估计机器人任务进度。与以往直接提示VLM输出进度值（易产生数值误表征）的方法不同，TOPReward直接从VLM的内部令牌逻辑中提取任务进度。在130多个真实世界任务和多个机器人平台的零样本评估中，TOPReward在Qwen3-VL上实现了0.947的平均价值-顺序相关性（VOC），显著优于最先进的GVL基线（在相同开源模型上相关性接近零）。此外，TOPReward可作为成功检测和奖励对齐行为克隆等下游应用的通用工具。|Ranjay Krishna Team|[2602.19313](http://arxiv.org/abs/2602.19313)|null|
|**2026-02-20**|**MALLVI: A Multi-Agent Framework for Integrated Generalized Robotics Manipulation**|现有的基于大语言模型（LLMs）的机器人操作任务规划方法常以开环方式运行，缺乏环境反馈，在动态环境中显得脆弱。为解决此问题，本文提出了MALLVi（Multi Agent Large Language and Vision）框架，一个实现闭环反馈驱动的机器人操作的多智能体系统。MALLVi协调专门的智能体（如Decomposer、Localizer、Thinker、Reflector）来管理感知、定位、推理和高级规划。在动作执行后，VLM会评估环境反馈并决定后续步骤，Reflector智能体支持有针对性的错误检测和恢复。实验结果表明，这种迭代闭环多智能体协调在模拟和真实世界场景中提高了零样本操作任务的泛化能力和成功率。|Babak Khalaj Team|[2602.16898](http://arxiv.org/abs/2602.16898)|null|
|**2026-02-20**|**CapNav: Benchmarking Vision Language Models on Capability-conditioned Indoor Navigation**|现有视觉-语言模型（VLMs）在视觉-语言导航（VLN）方面表现出色，但尚未充分考虑代理在真实世界导航中固有的移动能力限制。为此，本研究引入了CapNav基准，旨在评估VLM在给定代理特定物理和操作能力的情况下导航复杂室内空间的能力。CapNav定义了五种代表性代理，并提供了45个真实世界室内场景、473个导航任务和2365个问答对。实验结果显示，随着移动限制的收紧，VLM的导航性能急剧下降，即使最先进的模型也难以处理需要空间维度推理的障碍物，这凸显了未来VLM在能力感知导航和具身空间推理方面的改进空间。|Jon Froehlich Team|[2602.18424](http://arxiv.org/abs/2602.18424)|null|
|**2026-02-20**|**Zero-shot Interactive Perception**|在复杂的、部分可观察的场景中，机器人通过物理交互（交互式感知，IP）来提取隐藏信息并执行操作至关重要。针对现有方法可能未能充分利用多策略操作并有效引导交互的问题，本研究提出了Zero-Shot IP (ZS-IP) 框架，该框架将多策略操作（推、抓）与记忆驱动的视觉语言模型 (VLM) 相结合。ZS-IP 集成了增强观测模块（引入了新颖的推线）、记忆引导动作模块和机器人控制器。实验结果表明，ZS-IP 在7自由度Franka Panda机械臂上，尤其是在推动任务中，表现优于被动和基于视角的感知技术，并能保持非目标元素的完整性。|Amir Ghalamzan Team|[2602.18374](http://arxiv.org/abs/2602.18374)|**[link](https://openreview.net/forum?id=7MhpFcr5Nx)**|
|**2026-02-20**|**Simplifying Outcomes of Language Model Component Analyses with ELIA**|尽管机制可解释性为分析大型语言模型 (LLM) 提供了强大工具，但其复杂性限制了非专业人员的使用。为了解决这一挑战，本研究设计并评估了ELIA，一个交互式网页应用，它简化了语言模型组件分析结果，使其更易于理解。ELIA集成了归因分析、功能向量分析和电路追踪，并创新性地使用视觉语言模型自动生成自然语言解释。用户研究表明，用户明显偏爱交互式界面，且AI驱动的解释有效弥合了非专业用户的知识鸿沟，表明AI系统结合用户中心设计能够有效简化复杂模型分析。|Nils Feldhus Team|[2602.18262](http://arxiv.org/abs/2602.18262)|**[link](https://github.com/aaron0eidt/ELIA)**|
|**2026-02-20**|**FENCE: A Financial and Multimodal Jailbreak Detection Dataset**|越狱攻击对大型语言模型 (LLMs) 和视觉语言模型 (VLMs) 构成严重威胁，VLM因其多模态特性而具有更广阔的攻击面，尤其在金融领域，越狱检测资源稀缺。为弥补这一空白，本研究提出了FENCE，一个双语（韩语-英语）多模态数据集，用于训练和评估金融应用中的越狱检测器，其通过金融相关查询和图像威胁确保领域真实性。实验结果显示，商业和开源VLM均存在漏洞，GPT-4o有可测量的攻击成功率，而开源模型暴露程度更高。基于FENCE训练的基线检测器实现了99%的分布内准确性，并在外部基准上表现强劲，证实了数据集在训练可靠检测模型方面的有效性。|Youngjun Kwak Team|[2602.18154](http://arxiv.org/abs/2602.18154)|null|
|**2026-02-20**|**OODBench: Out-of-Distribution Benchmark for Large Vision-Language Models**|现有视觉-语言模型 (VLMs) 假设数据为独立同分布 (IID)，但在现实世界中，分布外 (OOD) 对象常出现并带来安全风险。鉴于当前缺乏全面评估VLM处理OOD数据能力的有效基准，本研究提出了OODBench，一种主要自动化的方法来构建新基准并评估VLM处理OOD数据的能力。OODBench包含40K实例级的OOD实例-类别对，并提出了一种可靠的自动化评估指标。实验结果表明，当前VLM在OODBench上的性能显著下降，即使在常见图像类别上也是如此。研究总结了为未来OOD数据获取和评估提供指导的发现和见解。|Jingrun Chen Team|[2602.18094](http://arxiv.org/abs/2602.18094)|null|
|**2026-02-20**|**UAOR: Uncertainty-aware Observation Reinjection for Vision-Language-Action Models**|视觉-语言-动作 (VLA) 模型在通用机器人操作方面潜力巨大，但现有方法常需额外观测信息或辅助模块以提高性能，这增加了数据收集和训练成本。受语言模型中前馈网络 (FFN) 可作为“键值记忆”的启发，本研究提出了不确定性感知观测重注入 (UAOR)，一个无需训练、即插即用的模块。UAOR通过注意力检索，在当前语言模型层表现出高不确定性时，将关键观测信息重新注入到下一层的前馈网络中。全面实验表明，UAOR以最小开销持续改进了多种VLA模型在模拟和真实世界任务中的表现，且无需额外观测线索或模块，使其成为现有VLA流水线的通用实用插件。|Liang Wang Team|[2602.18020](http://arxiv.org/abs/2602.18020)|null|
|**2026-02-19**|**Pushing the Frontier of Black-Box LVLM Attacks via Fine-Grained Detail Targeting**|大型视觉语言模型 (LVLMs) 的黑盒对抗性攻击因梯度缺失和复杂多模态边界而极具挑战性。现有最先进的基于迁移方法M-Attack在局部裁剪级匹配时导致高方差梯度，造成优化不稳定。为此，本研究将局部匹配重新表述为源变换和目标语义上的非对称期望，并提出了M-Attack-V2，一个对M-Attack进行梯度去噪的升级模块。M-Attack-V2通过多裁剪对齐 (MCA) 降低源侧梯度方差，并通过辅助目标对齐 (ATA) 优化目标流形。实验结果表明，M-Attack-V2显著提升了对前沿LVLMs的黑盒攻击成功率，例如，Claude-4.0从8%升至30%，Gemini-2.5-Pro从83%升至97%，GPT-5从98%升至100%，全面超越了此前的黑盒LVLM攻击方法。|Zhiqiang Shen Team|[2602.17645](http://arxiv.org/abs/2602.17645)|**[link](https://github.com/vila-lab/M-Attack-V2)**|
|**2026-02-19**|**Catastrophic Forgetting Resilient One-Shot Incremental Federated Learning**|现代大数据系统中的联邦学习在处理大规模、隐私敏感的增量数据流时，面临通信开销和灾难性遗忘的挑战。为此，本文提出了One-Shot Incremental Federated Learning (OSI-FL) 框架，该框架通过客户端VLM提取类别特定嵌入并在单轮通信中发送给服务器，服务器利用扩散模型合成新数据进行训练。为解决灾难性遗忘，OSI-FL引入了选择性样本保留（SSR）机制，基于样本损失保留最具信息量的样本。实验结果显示，OSI-FL在类增量和域增量场景下，在三个基准数据集上均优于现有基线方法。|Monowar Bhuyan Team|[2602.17625](http://arxiv.org/abs/2602.17625)|null|
|**2026-02-19**|**AI Gamestore: Scalable, Open-Ended Evaluation of Machine General Intelligence with Human Games**|为评估AI系统更接近人类的通用智能，本文指出传统基准测试的局限性，并提出通过AI玩和学习所有“人类游戏”来衡量通用智能。研究者定义了“人类游戏多重宇宙”作为评估空间，并介绍了AI GameStore平台，该平台利用LLM与人类协同生成新的代表性人类游戏。作为概念验证，团队基于热门榜单生成了100个游戏，并评估了七个视觉-语言模型（VLM）。结果表明，最佳模型在大多数游戏中的得分远低于人类平均水平，尤其在需要世界模型学习、记忆和规划的游戏中表现不佳。|Joshua B. Tenenbaum Team|[2602.17594](http://arxiv.org/abs/2602.17594)|null|
|**2026-02-19**|**LATA: Laplacian-Assisted Transductive Adaptation for Conformal Uncertainty in Medical VLMs**|医疗视觉-语言模型在零样本识别方面表现出色，但域偏移下可靠性不足，且传统保形预测方法在少样本不平衡场景下效率低、类别覆盖不均。为解决此问题，本文提出了	exttt{	extbf{LATA}} (Laplacian-Assisted Transductive Adaptation) 方法，这是一种无训练、无标签的精炼技术，通过在图像k-NN图上平滑零样本概率来优化预测集。此外，它引入了一种“失败感知”保形分数以提高效率和类别平衡。实验表明，	exttt{	extbf{LATA}}在三个医学VLM和九个下游任务上，在保持或收紧覆盖率的同时，显著减小了预测集大小和类别覆盖不均，并以更低的计算成本超越了现有转导基线。|Zongyuan Ge Team|[2602.17535](http://arxiv.org/abs/2602.17535)|null|
|**2026-02-19**|**Selective Training for Large Vision Language Models via Visual Information Gain**|大型视觉语言模型（LVLMs）常表现出语言偏见，即在缺乏视觉依据的情况下生成回答。为量化视觉信息对模型预测的贡献，本文引入了视觉信息增益（VIG），一个基于困惑度的指标，用于衡量视觉输入带来的预测不确定性降低。VIG支持细粒度的样本和token级别分析，能够突出视觉接地元素。基于此，研究者提出了一种VIG引导的选择性训练方案，优先训练高VIG的样本和token。该方法通过聚焦视觉信息丰富的元素，在显著减少监督的情况下，提高了模型的视觉接地能力并减轻了语言偏见。|Sangheum Hwang Team|[2602.17186](http://arxiv.org/abs/2602.17186)|null|
|**2026-02-19**|**Understanding the Fine-Grained Knowledge Capabilities of Vision-Language Models**|视觉-语言模型 (VLM) 在视觉问答基准上进步显著，但在需要细粒度视觉知识的传统图像分类基准上仍有不足。本研究在细粒度分类基准上测试了大量最新的VLM，并探究了细粒度知识与其他视觉基准之间脱节的潜在因素。通过一系列消融实验，研究发现，更好的LLM能均匀提升所有基准分数，而更好的视觉编码器则能显著提升细粒度分类性能。此外，预训练阶段对细粒度性能至关重要，特别是在预训练期间不冻结语言模型权重时。这些见解为增强VLM的细粒度视觉理解和以视觉为中心的能力提供了方向。|Ludwig Schmidt Team|[2602.17871](http://arxiv.org/abs/2602.17871)|null|
|**2026-02-19**|**Enabling Training-Free Text-Based Remote Sensing Segmentation**|视觉语言模型 (VLM) 和视觉基础模型 (VFM) 的进展为遥感图像的零样本文本引导分割提供了新机遇，但现有方法通常依赖额外可训练组件。本研究提出一种简单有效的方法，整合对比式和生成式VLM与Segment Anything Model (SAM)，实现了完全无需训练或轻量级LoRA调优的流水线。对比式方法使用CLIP作为SAM提议的掩码选择器，实现零样本开放词汇语义分割。生成式方法通过GPT-5或LoRA调优的Qwen-VL模型生成点击提示。在19个遥感基准数据集上的广泛实验表明，该方法在开放词汇、指代和基于推理的任务中表现出强大能力，其中LoRA调优的Qwen-VL模型效果最佳。|Djamila Aouada Team|[2602.17799](http://arxiv.org/abs/2602.17799)|null|
|**2026-02-19**|**CLUTCH: Contextualized Language model for Unlocking Text-Conditioned Hand motion modelling in the wild**|手部运动在日常生活中至关重要，但现有文本到手部动作生成方法依赖有限的摄影棚数据集，难以扩展到“野外”场景，且模型在动画保真度和文本-动作对齐方面表现不足。为解决此问题，本研究提出了“3D Hands in the Wild (3D-HIW)”数据集，包含32K 3D手部动作序列及对齐文本，并通过结合VLM和先进3D手部追踪器构建。在此基础上，引入了基于LLM的手部动画系统CLUTCH，其创新点包括：SHIFT（一种新颖的VQ-VAE架构用于手部动作分词）和几何精修阶段以微调LLM。实验证明，CLUTCH在文本到动作和动作到文本任务上均达到了最先进的性能，为可扩展的野外手部动作建模建立了首个基准。|Justus Thies Team|[2602.17770](http://arxiv.org/abs/2602.17770)|**[link](https://balamuruganthambiraja.github.io/CLUTCH/)**|
|**2026-02-18**|**Saliency-Aware Multi-Route Thinking: Revisiting Vision-Language Reasoning**|视觉-语言模型（VLMs）的推理能力受限于视觉输入一次性提供、文本推理易受早期视觉接地错误累积及朴素视觉引导粗糙等问题。为解决这些挑战，本文提出了“显著性感知原则”（SAP）选择方法，其作用于高层推理原则而非token级轨迹，能在噪声反馈下稳定控制离散生成，并允许模型在需要时重新参考视觉证据。此外，SAP支持多路径推理以探索多样化行为。SAP是模型无关且无需额外训练。实验结果表明，SAP在可比的token生成预算下，特别是在减少物体幻觉方面，实现了竞争性性能，并提供了比CoT风格长序列推理更稳定的推理和更低的响应延迟。|Jundong Li Team|[2602.16702](http://arxiv.org/abs/2602.16702)|null|
|**2026-02-18**|**A Contrastive Learning Framework Empowered by Attention-based Feature Adaptation for Street-View Image Classification**|街景图像属性分类是一项重要但计算量大的任务，而现有基于预训练视觉-语言模型（如CLIP）的适应方法多依赖全局图像嵌入，难以捕捉复杂街景中的细粒度局部属性。针对此问题，本文提出了CLIP-MHAdapter，这是对轻量级CLIP适应范式的一种改进。该方法通过在CLIP模型后附加一个配备多头自注意力机制的瓶颈MLP，使其能够作用于patch tokens，从而建模patch间的依赖关系。实验结果表明，CLIP-MHAdapter仅用约140万可训练参数，就在Global StreetScapes数据集的八项属性分类任务上实现了优异或具有竞争力的准确率，达到了新的SOTA，同时保持了较低的计算成本。|James Haworth Team|[2602.16590](http://arxiv.org/abs/2602.16590)|null|
|**2026-02-18**|**DressWild: Feed-Forward Pose-Agnostic Garment Sewing Pattern Generation from In-the-Wild Images**|现有服装图案生成方法在处理多样姿态和视角时面临挑战，且基于优化的方法计算成本高昂。本文针对需要可编辑、可分离、可模拟服装的缝纫图案生成，提出了一种名为DressWild的新型前馈流水线。该方法能够从单张“in-the-wild”图像重建物理一致的2D缝纫图案及对应的3D服装。DressWild利用视觉-语言模型（VLMs）规范姿态变化，并提取姿态感知和3D信息丰富的服装特征，通过Transformer编码器融合后预测缝纫图案参数。大量实验证明，该方法无需多视角输入或迭代优化，即可鲁棒地从真实图像中恢复多样的缝纫图案和3D服装，为服装模拟与动画提供了高效可扩展的方案。|Chenfanfu Jiang Team|[2602.16502](http://arxiv.org/abs/2602.16502)|null|
|**2026-02-18**|**Visual Self-Refine: A Pixel-Guided Paradigm for Accurate Chart Parsing**|针对大型视觉-语言模型（LVLMs）在图表解析等视觉感知密集任务中因视觉稠密导致数据遗漏、错位和幻觉等错误，本文提出了Visual Self-Refine（VSR）新范式。VSR受人类阅读复杂图表时使用“视觉锚点”的启发，使模型能生成像素级定位输出，并将其可视化反馈给自己进行错误修正。作者在图表解析领域实例化VSR为ChartVSR，将其解析过程分解为迭代修正像素级定位的“精炼阶段”和利用验证定位解析结构化数据的“解码阶段”。此外，还构建了高挑战性基准ChartP-Bench。这项工作展示了VSR作为通用视觉反馈机制在提升视觉中心任务准确性方面的潜力。|Dahua Lin Team|[2602.16455](http://arxiv.org/abs/2602.16455)|null|
|**2026-02-18**|**Designing Production-Scale OCR for India: Multilingual and Domain-Specific Systems**|针对印度OCR系统在语言多样性、文档异构性和部署限制方面的挑战，本文通过Chitrapathak系列研究了两种多语言OCR训练策略。第一种是结合通用视觉编码器与多语言语言模型进行端到端训练；第二种是微调现有OCR模型，即使其未针对目标语言训练。实验结果表明，第二种策略在准确性与延迟权衡方面表现更优，Chitrapathak-2在泰卢固语上达到SOTA，并实现3-6倍加速。此外，还提出了Parichay系列模型，专门用于印度9种政府文档的关键字段提取，达到了89.8%的精确匹配率和更快的推理速度。这些系统为印度语境下的生产级OCR流水线提供了实践指导。|Shubham Agarwal Team|[2602.16430](http://arxiv.org/abs/2602.16430)|null|
|**2026-02-18**|**Peeking Ahead of the Field Study: Exploring VLM Personas as Support Tools for Embodied Studies in HCI**|鉴于实地研究成本高昂、耗时且易出错，本文受快速原型启发，旨在探索VLM角色能否模拟实地研究结果。研究背景是尽管大型语言模型（LLMs）具有类人推理和语言能力，但自动驾驶车辆（AV）与行人交互需空间感知、情感共情和行为生成。为探究VLM角色模仿人类反应的程度，研究进行了平行实验：一项20人参与的真实街道穿越任务，以及一项20个VLM角色参与的视频研究。结果显示，VLM角色能模仿人类的反应模式（如平均穿越时间），但缺乏行为变异性和深度，它们在形成性研究、实地研究准备和人类数据增强方面展现了潜力。|Takeo Igarashi Team|[2602.16157](http://arxiv.org/abs/2602.16157)|null|
|**2026-02-18**|**Evaluating Demographic Misrepresentation in Image-to-Image Portrait Editing**|尽管文本到图像（T2I）生成中的人口偏见已广泛研究，但指令引导的图像到图像（I2I）编辑中与人口统计相关的失败模式仍未被充分探索。本文形式化了两种失败模式：软擦除（编辑被削弱或忽略）和刻板印象替换（引入非请求的刻板印象属性）。研究引入了一个受控基准，通过生成和编辑基于种族、性别和年龄的人像，并使用视觉-语言模型评分和人工评估来探究行为。分析表明，身份保留失败普遍存在，具有人口统计学上的不均衡性，并受隐含社会先验（如职业驱动的性别推断）的影响。研究还发现，提示级身份约束可在不更新模型的情况下显著减少少数群体的群体变化。这些发现确立了身份保留作为I2I编辑中一个核心且人口统计学上不均衡的失败模式，并推动了人口统计学鲁棒编辑系统的发展。|Jean Oh Team|[2602.16149](http://arxiv.org/abs/2602.16149)|null|
|**2026-02-18**|**IRIS: Intent Resolution via Inference-time Saccades for Open-Ended VQA in Large Vision-Language Models**|针对开放式视觉问答（VQA）中大型视觉-语言模型（VLMs）在处理歧义时的挑战，本文引入了IRIS（Intent Resolution via Inference-time Saccades）——一种无需训练的方法，利用实时眼动追踪数据解决歧义。通过包含500个独特图像-问题对的综合用户研究，结果表明，参与者开始口头提问时最近的注视点对于大型VLM的消歧最为信息丰富。该方法使歧义问题的响应准确性提高了一倍以上（从35.2%到77.2%），同时保持了无歧义查询的性能。研究还在各种最先进的VLM上验证了其效果。这项工作发布了新的基准数据集、实时交互协议和评估套件，强调了眼动数据在增强VQA准确性方面的潜力。|Miguel P. Eckstein Team|[2602.16138](http://arxiv.org/abs/2602.16138)|null|
|**2026-02-18**|**OmniCT: Towards a Unified Slice-Volume LVLM for Comprehensive CT Analysis**|针对CT影像诊断中现有大型视觉-语言模型（LVLMs）在切片级和体积级理解上的碎片化问题，即切片驱动模型缺乏空间一致性而体积驱动模型粒度粗糙，本文提出了OmniCT，一个强大的统一切片-体积LVLM。该模型通过空间一致性增强（SCE）引入体积一致性，器官级语义增强（OSE）明确对齐解剖区域，并构建了最大规模的切片-体积CT数据集和混合基准MedEval-CT。实验证明，OmniCT在多样化的临床任务中显著优于现有方法，同时满足微观细节敏感性和宏观空间推理需求，为跨模态医学影像理解建立了新范式。|Beng Chin Ooi Team|[2602.16110](http://arxiv.org/abs/2602.16110)|null|
|**2026-02-18**|**Narrow fine-tuning erodes safety alignment in vision-language agents**|终身多模态智能体在后训练过程中需适应新任务，但这与保持安全对齐存在冲突。本文研究发现，在狭窄领域的有害数据集上微调对齐的视觉-语言模型（VLM）会导致严重的突发性错位，且这种错位广泛泛化。实验在Gemma3-4B上进行，结果表明错位程度与LoRA秩单调递增，且多模态评估揭示的错位程度远高于纯文本评估。即使训练数据中只有10%的有害数据也会导致显著的对齐退化。几何分析显示有害行为占据低维子空间。虽然良性微调和基于激活的引导能减少错位，但均未能完全消除有害行为，强调了开发鲁棒持续学习框架的必要性。|Shivam Raval Team|[2602.16931](http://arxiv.org/abs/2602.16931)|null|
|**2026-02-18**|**DODO: Discrete OCR Diffusion Models**|光学字符识别（OCR）作为信息数字化的基础任务，现代视觉-语言模型（VLM）通常采用自回归解码，但这对长文档而言计算成本高且速度慢。本文指出OCR的确定性特征使其理论上适合通过扩散模型进行高效并行解码，但现有掩码扩散模型存在结构不稳定性，不适用于OCR的精确匹配要求。为克服此限制，本文提出了DODO，这是首个将块离散扩散应用于OCR的VLM。DODO通过将生成分解为块来缓解全局扩散的同步错误。实验结果表明，DODO在实现接近最先进准确率的同时，推理速度比自回归基线快3倍。|Niv Nayman Team|[2602.16872](http://arxiv.org/abs/2602.16872)|null|
|**2026-02-17**|**MedProbCLIP: Probabilistic Adaptation of Vision-Language Foundation Model for Reliable Radiograph-Report Retrieval**|鉴于视觉-语言基础模型在生物医学应用中因确定性嵌入而缺乏可靠性，本文提出了MedProbCLIP，一个用于胸部X射线和放射学报告表示学习及双向检索的概率视觉-语言学习框架。MedProbCLIP通过概率对比目标将图像和文本表示建模为高斯嵌入，从而明确捕捉不确定性和多对多对应关系，并通过变分信息瓶颈减轻过度自信预测。模型在训练中采用多视图X射线编码和多节报告编码以提供细粒度监督，但推理时仅需单张图像和报告。在MIMIC-CXR数据集上的评估显示，MedProbCLIP在检索和零样本分类方面均优于现有基线，并展现出卓越的校准性、可靠性和鲁棒性，突显了概率建模在提升放射学图像-文本检索系统可信度和安全性方面的价值。|Gongbo Liang Team|[2602.16019](http://arxiv.org/abs/2602.16019)|null|
|**2026-02-17**|**BTReport: A Framework for Brain Tumor Radiology Report Generation with Clinically Relevant Features**|鉴于神经肿瘤学领域因缺乏开放的配对图像-报告数据集而限制了放射学报告生成（RRG）的进展，本文引入了BTReport，一个开源的脑肿瘤RRG框架。该框架通过确定性提取成像特征来构建自然语言放射学报告。与依赖大型视觉-语言模型进行图像解释和报告撰写的方法不同，BTReport将RRG分解为确定性特征提取和报告生成两个步骤，仅使用大型语言模型进行句法结构化和叙述格式化，从而提高了报告的可解释性并减少了幻觉。研究表明，用于报告生成的特征能预测关键临床结果，且BTReport生成的报告与参考临床报告更紧密对齐。此外，本文还发布了伴随数据集BTReport-BraTS。|Mehmet Kurt Team|[2602.16006](http://arxiv.org/abs/2602.16006)|null|
|**2026-02-17**|**Can Vision-Language Models See Squares? Text-Recognition Mediates Spatial Reasoning Across Three Model Families**|针对视觉-语言模型（VLMs）在二进制网格中无法准确本地化缺乏文本标识的填充单元格这一基本限制，本研究通过实验进行了探究。研究生成了两种类型的15x15网格图像（文本符号和无网格线填充方块），并要求三个前沿VLM进行转录。实验结果显示，在文本符号条件下，VLM表现良好（高准确率和F1分数），但在填充方块条件下，所有模型的准确率和F1分数均大幅下降。这表明，VLM似乎拥有一个高保真的文本识别路径用于空间推理，其性能远优于其原生视觉路径。每个模型在处理非文本视觉元素时均表现出严重的空间定位退化，且各有不同的失败模式（如欠计数、过计数和模板幻觉）。|Yuval Levental Team|[2602.15950](http://arxiv.org/abs/2602.15950)|null|
|**2026-02-17**|**Visual Memory Injection Attacks for Multi-Turn Conversations**|鉴于大型生成式视觉-语言模型（LVLM）在长上下文多轮对话中安全性的研究不足，本论文提出了一种名为视觉记忆注入（VMI）的隐蔽攻击。该攻击通过操纵图像，使得LVLM在正常提示下行为正常，但在触发提示下输出预设的目标信息以操纵用户。实验证明，VMI攻击在多轮对话中依然有效，并在多个开源LVLM上得到验证，揭示了通过扰动图像进行大规模用户操纵的可行性，强调了提高LVLM对抗此类攻击的鲁棒性需求。|Matthias Hein Team|[2602.15927](http://arxiv.org/abs/2602.15927)|null|
|**2026-02-17**|**Concept-Enhanced Multimodal RAG: Towards Interpretable and Accurate Radiology Report Generation**|针对放射学报告生成（RRG）中视觉-语言模型（VLM）存在的解释性差和易产生“幻觉”的问题，本研究提出了概念增强多模态检索增强生成（CEMRAG）框架。该框架将视觉表示分解为可解释的临床概念，并将其与多模态RAG结合，通过丰富的上下文提示提升RRG的解释性和事实准确性。实验结果表明，CEMRAG在多个数据集和VLM架构下，在临床准确性和NLP指标上均优于现有基线，挑战了可解释性与性能之间的权衡假设，为医疗VLM提供了可信赖的AI辅助放射学途径。|Valerio Guarrasi Team|[2602.15650](http://arxiv.org/abs/2602.15650)|null|
|**2026-02-17**|**CARE Drive A Framework for Evaluating Reason-Responsiveness of Vision Language Models in Automated Driving**|针对自动驾驶中基础模型评估主要关注结果性能而忽视决策是否反映人类相关考量的问题，本研究提出了CARE Drive框架。该框架独立于模型，通过在受控上下文变化下比较基线和原因增强模型决策，评估人类原因对决策行为的因果影响。在一个骑车人超车场景中，结果显示明确的人类原因显著影响模型决策，提高了与专家推荐行为的一致性，证明了可以在不修改模型参数的情况下系统评估基础模型的原因响应性。|Arkady Zgonnikov Team|[2602.15645](http://arxiv.org/abs/2602.15645)|null|
|**2026-02-17**|**Req2Road: A GenAI Pipeline for SDV Test Artifact Generation and On-Vehicle Execution**|为解决软件定义汽车（SDV）功能测试中需求分散、规范复杂以及测试资产异构的挑战，本研究提出了一种自动化管道。该管道利用大型语言模型和视觉-语言模型提取信号和行为逻辑，自动生成Gherkin场景并转换为可运行的测试脚本，并通过VSS集成和检索增强生成（RAG）实现标准化。在儿童存在检测系统（CPDS）上的评估显示，89%的需求可转化为可执行场景，验证了该端到端需求-测试管道在仿真和在环测试环境中的可行性，但仍需人工审查。|Alois Knoll Team|[2602.15591](http://arxiv.org/abs/2602.15591)|null|
|**2026-02-17**|**VLM-DEWM: Dynamic External World Model for Verifiable and Resilient Vision-Language Planning in Manufacturing**|针对视觉-语言模型（VLM）在智能制造动态工作单元中面临的无状态操作和不透明推理挑战，本研究提出了VLM-DEWM认知架构。该架构通过一个持久且可查询的动态外部世界模型（DEWM），将VLM推理与世界状态管理解耦，并结构化每个VLM决策为可外部化的推理轨迹。实验结果表明，VLM-DEWM显著提高了状态跟踪准确性（从56%提升至93%）和恢复成功率（从低于5%提升至95%），并通过结构化内存减少了计算开销，为动态制造环境中的长周期机器人操作提供了可靠且弹性的解决方案。|Ning Ji Team|[2602.15549](http://arxiv.org/abs/2602.15549)|null|
|**2026-02-17**|**Selective Perception for Robot: Task-Aware Attention in Multimodal VLA**|针对机器人领域中视觉-语言-动作（VLA）模型静态融合多视角输入导致的计算冗余和噪声问题，本研究提出了一种动态信息融合框架。该框架引入轻量级自适应路由架构，实时分析文本提示和腕部摄像头观测，预测多摄像头视图的任务相关性，并有选择地向策略网络提供必要视觉特征。为路由器训练，本研究还开发了利用VLM的自动化标注管道。实验结果表明，该方法在真实世界机器人操纵场景中显著提高了推理效率和控制性能，验证了动态信息融合在资源受限实时机器人控制环境中的有效性。|Soo-Chul Lim Team|[2602.15543](http://arxiv.org/abs/2602.15543)|null|
|**2026-02-17**|**Semantic-Guided 3D Gaussian Splatting for Transient Object Removal**|为解决3D Gaussian Splatting (3DGS) 重建中瞬态物体导致的重影伪影问题，本研究提出了一种基于视觉-语言模型（VLM）的语义过滤框架。该方法通过计算渲染视图与干扰文本提示之间的CLIP相似度得分，并累积到每个高斯函数上，对超出校准阈值的高斯函数进行不透明度正则化和周期性修剪。实验结果表明，该方法在RobustNeRF基准测试中持续改善了重建质量，同时保持了最小内存开销和实时渲染性能，通过语义分类有效解决了视差模糊问题。|Priyesh Shukla Team|[2602.15516](http://arxiv.org/abs/2602.15516)|null|
|**2026-02-17**|**On the Out-of-Distribution Generalization of Reasoning in Multimodal LLMs for Simple Visual Planning Tasks**|本研究旨在深入探究大型语言模型和视觉-语言模型中思维链（CoT）推理方法的泛化能力，尤其是在简单规划任务上的表现。研究者提出了一个评估框架，对基于网格的导航任务中不同输入表示和CoT策略的模型进行了微调和系统评估。实验结果表明，CoT推理能提高分布内泛化能力，但对分布外（如更大地图）的泛化能力多数情况下仍非常有限；值得注意的是，结合多种文本格式的推理轨迹产生了最佳的分布外泛化效果，且纯文本模型表现优于基于图像输入的模型。|Francesco Croce Team|[2602.15460](http://arxiv.org/abs/2602.15460)|null|
|**2026-02-16**|**BPP: Long-Context Robot Imitation Learning by Focusing on Key History Frames**|在机器人任务中，传统策略因仅依赖当前观测而无法有效利用历史信息，导致在需要记忆的任务中泛化性差，尤其是在部署时容易受训练中虚假关联的影响。为解决此问题，本研究提出大图策略（BPP），通过视觉-语言模型识别并利用一组最小的关键帧作为历史观测条件。实验结果表明，BPP显著减少了训练与部署间的分布偏移，并在四项真实世界操作任务和三项仿真任务中，成功率比现有最佳方法提高了70%。|Aviral Kumar Team|[2602.15010](http://arxiv.org/abs/2602.15010)|null|
|**2026-02-16**|**ThermEval: A Structured Benchmark for Evaluation of Vision-Language Models on Thermal Imagery**|当前视觉语言模型（VLMs）在RGB图像上表现优异，但无法泛化至夜间监控、搜救等关键场景的热成像图像，且现有基准无法评估其对温度感知和推理能力。本研究引入ThermEval-B，一个包含5.5万个热视觉问答对的结构化基准，并整合了ThermEval-D数据集，首次提供带有语义身体部位标注的密集逐像素温度图。实验评估25个VLM后发现，模型在温度推理上普遍失败，在色图变换下性能下降，且容易依赖语言先验，提示或微调仅带来微弱提升，证实了热图像理解需专门评估。 |Nipun Batra Team|[2602.14989](http://arxiv.org/abs/2602.14989)|null|
|**2026-02-16**|**DM0: An Embodied-Native Vision-Language-Action Model towards Physical AI**|针对传统方法将互联网预训练模型适配到物理任务的局限性，本研究提出了DM0，一个具身原生（Embodied-Native）的视觉-语言-动作（VLA）框架，旨在为物理AI提供统一的具身操作和导航能力。该框架采用三阶段训练流水线：首先对VLM进行大规模统一预训练，整合网络文本、自动驾驶和具身交互日志数据，随后构建流匹配动作专家，并通过混合训练策略及具身空间支架策略实现高层推理与低层控制的协调。DM0在RoboChallenge基准测试中，于专业和通用设置下均取得了最先进的性能。|Tiancai Wang Team|[2602.14974](http://arxiv.org/abs/2602.14974)|**[link](https://github.com/Dexmal/dexbotic)**|
|**2026-02-16**|**MATEO: A Multimodal Benchmark for Temporal Reasoning and Planning in LVLMs**|AI智能体实现复杂目标需要精细规划，其中时间执行顺序（TEO）至关重要，但现有基础模型对TEO的理解研究不足，多限于线性近似或纯文本输入。为弥补这一空白，本研究引入MATEO基准，旨在评估和提升大型视觉语言模型（LVLMs）的时间推理能力。通过收集高质量多模态菜谱语料并利用众包构建TEO图谱，MATEO提供了丰富的标注。对六个主流LVLMs的评估揭示了语言上下文、多模态输入结构和微调策略对时间推理能力的关键影响，突显了当前LVLMs在该领域的局限性。|Giuseppe Riccardi Team|[2602.14589](http://arxiv.org/abs/2602.14589)|null|
|**2026-02-16**|**Replanning Human-Robot Collaborative Tasks with Vision-Language Models via Semantic and Physical Dual-Correction**|人机协作（HRC）在装配任务中面临人类指令模糊导致机器人行为不可靠的问题，现有基于VLM的方法虽能解释指令，但易产生幻觉推理和物理执行失败。为此，本研究提出了一个HRC框架，通过引入双重校正机制增强VLM的推理能力。该机制包含一个内部校正模型在执行前验证逻辑和可行性，以及一个外部校正模型通过事后反馈纠正物理失败。仿真和真实世界实验均表明，该框架显著提高了任务成功率，并能有效支持机器人根据人类指令进行交互式重规划。|Kensuke Harada Team|[2602.14551](http://arxiv.org/abs/2602.14551)|null|
|**2026-02-16**|**Error Patterns in Historical OCR: A Comparative Analysis of TrOCR and a Vision-Language Model**|18世纪印刷文本的OCR因降级打印、古体字和非标准化拼写而充满挑战，现有基于Transformer和VLM的OCR系统虽总体准确率高，但CER和WER等指标对学术使用可靠性洞察有限。本研究通过比较专用OCR Transformer（TrOCR）和通用VLM（Qwen）在历史英文文本线级识别上的表现，发现Qwen虽在总体准确率和鲁棒性方面占优，但存在选择性语言正则化和拼写规范化，可能改变历史原貌。TrOCR则能更好地保持拼写忠实度，但易出现级联错误。研究强调了架构归纳偏差对OCR错误结构的影响，以及在历史文献数字化中进行架构感知评估的必要性。|Mikko Tolonen Team|[2602.14524](http://arxiv.org/abs/2602.14524)|null|
|**2026-02-16**|**S2D: Selective Spectral Decay for Quantization-Friendly Conditioning of Neural Activations**|大规模Transformer模型中的激活异常值对模型量化构成严重挑战，导致量化精度显著下降，且随预训练规模的增加而加剧。本研究通过理论分析和实证观察，揭示了激活异常值与权重的主奇异值之间的直接联系。在此基础上，提出选择性谱衰减（S²D）方法，在微调阶段仅对最大奇异值对应的权重分量进行正则化。实验证明S²D显著减少了激活异常值，使得模型在W4A4量化下PTQ精度提升高达7%，结合QAT时提升4%，且泛化至下游任务和视觉-语言模型，提高了大规模模型的部署效率。|Deepak Gupta Team|[2602.14432](http://arxiv.org/abs/2602.14432)|null|
|**2026-02-16**|**Multi-Turn Adaptive Prompting Attack on Large Vision-Language Models**|针对多轮越狱攻击在大型视觉-语言模型（LVLMs）中因视觉输入过于恶意而易被防御的问题，本研究提出了MAPA（Multi-turn Adaptive Prompting Attack）方法。MAPA在每个回合中交替使用文本和视觉攻击动作以引发最恶意响应，并在跨回合中通过迭代式来回调整攻击轨迹，逐步放大响应的恶意性。这种双层设计使MAPA持续优于现有最先进方法，在对抗Llava-V1.6-Mistral-7B、Qwen2.5-VL-7B-Instruct、Llama-3.2-Vision-11B-Instruct和GPT-4o-mini等模型的最新基准测试中，攻击成功率提高了11-35%。|Yiliao Song Team|[2602.14399](http://arxiv.org/abs/2602.14399)|null|
|**2026-02-15**|**Moving Beyond Sparse Grounding with Complete Screen Parsing Supervision**|现代计算机使用智能体（CUA）需要对屏幕进行结构化感知才能可靠地理解指令和执行操作，但现有标注数据集稀疏且多样性不足，限制了其泛化能力，且实际部署需要高效率。本研究引入ScreenParse，一个用于完整屏幕解析的大规模数据集，包含771K个网页截图的密集标注。通过Webshot自动化流程及VLM辅助，ScreenParse提供了详尽的UI元素信息。基于此数据集，我们训练了紧凑型ScreenVLM，其在密集解析上显著优于更大规模的基础VLM，并在公共基准上展现了强大的迁移能力，证明了密集屏幕监督能为UI理解提供可迁移的结构先验。|Peter Staar Team|[2602.14276](http://arxiv.org/abs/2602.14276)|null|
|**2026-02-15**|**Dual-Signal Adaptive KV-Cache Optimization for Long-Form Video Understanding in Vision-Language Models**|视觉-语言模型（VLMs）在处理长视频时面临内存瓶颈，因KV缓存随序列长度线性增长，而现有驱逐策略先计算完整注意力矩阵再丢弃token，导致计算浪费。本研究提出Sali-Cache，一个先验优化框架，通过主动内存管理实现双信号自适应缓存。该方法结合光流分析的时间滤波器和显著性检测的空间滤波器，在注意力操作前智能管理内存。实验表明，Sali-Cache在LLaVA 1.6架构上实现了2.20倍的内存压缩比，同时保持100%的准确率，并在相同内存预算下能更长时间保留上下文特征，实现了在消费级硬件上高效处理长视频内容。|Priyesh Shukla Team|[2602.14236](http://arxiv.org/abs/2602.14236)|null|
|**2026-02-15**|**Index Light, Reason Deep: Deferred Visual Ingestion for Visual-Dense Document Question Answering**|当前多模态文档问答系统采用“供给侧摄取”策略，即在索引阶段全面生成视觉描述，导致成本高昂且不可靠。本文提出了“延迟视觉摄取（DVI）”框架，该框架采用“需求侧摄取”策略，仅在索引阶段进行轻量级元数据提取以实现页面定位，将视觉理解延迟到用户提问时，再将原始图像与特定问题发送给视觉语言模型进行分析。实验结果表明，DVI在零摄取视觉语言模型成本下，取得了与现有方法相当的整体准确率（46.7% vs 48.9%），在视觉必要查询上的有效率达50%，并实现了100%的页面定位率，有效将问答准确率问题转化为页面定位问题。|Tao Xu Team|[2602.14162](http://arxiv.org/abs/2602.14162)|null|
|**2026-02-15**|**Annotation-Efficient Vision-Language Model Adaptation to the Polish Language Using the LLaVA Framework**|现有视觉-语言模型（VLMs）多基于英语数据训练，限制了其在其他语言和文化背景下的应用。为解决此问题，本研究复现并调整了LLaVA-Next方法，通过全自动化流程翻译、过滤现有数据集并补充合成数据，构建了一套波兰语VLM。实验结果显示，该方法在波兰语改编的MMBench上相较于LLaVA-1.6-Vicuna-13B实现了9.5%的性能提升，并在生成性评估中，其生成的标题在语言正确性方面获得了更高评价，证明大规模自动化翻译结合轻量级过滤能有效为低资源语言引导高质量多模态模型。|Wojciech Kusa Team|[2602.14073](http://arxiv.org/abs/2602.14073)|null|
|**2026-02-15**|**MarsRetrieval: Benchmarking Vision-Language Models for Planetary-Scale Geospatial Retrieval on Mars**|行星科学中，现有深度学习基准多局限于监督视觉任务，不支持文本引导的地理空间发现。为此，本研究引入了MarsRetrieval，一个用于评估视觉-语言模型在火星地理空间发现能力的检索基准，包含图像-文本检索、地貌检索和全球地理定位等任务。研究提出统一的检索协议以评估多模态嵌入架构。实验结果表明MarsRetrieval极具挑战性，即使是强大的基础模型也难以捕捉领域特定的地貌区别，且领域特定微调对于行星环境中的可泛化地理空间发现至关重要。|Hongxin Wei Team|[2602.13961](http://arxiv.org/abs/2602.13961)|null|
|**2026-02-14**|**RMPL: Relation-aware Multi-task Progressive Learning with Stage-wise Training for Multimedia Event Extraction**|多媒体事件抽取（MEE）受标注数据缺乏限制，现有基准M2E2仅提供评估标注，导致直接监督训练困难，现有方法未能有效学习结构化事件表示。为解决这些局限，本文提出了RMPL（Relation-aware Multi-task Progressive Learning）框架，用于低资源条件下的MEE。RMPL通过阶段式训练，整合了来自单模态事件抽取和多媒体关系抽取的异构监督，先学习事件中心表示，再进行微调。实验结果表明，在M2E2基准上，结合多个视觉语言模型的RMPL在不同模态设置下均显示出持续的性能改进。|Yu Hong Team|[2602.13748](http://arxiv.org/abs/2602.13748)|null|
|**2026-02-14**|**Fine-tuned Vision Language Model for Localization of Parasitic Eggs in Microscopic Images**|土壤传播性蠕虫（STH）感染在全球范围广泛，但诊断专业知识有限，手动显微镜诊断耗时且易错。为实现自动化诊断，本文旨在利用经过微调的视觉语言模型（VLM），例如Microsoft Florence，来定位显微图像中的所有寄生虫卵。初步实验结果显示，该定位VLM的mIOU达到了0.94，优于其他目标检测方法，表明其有望成为自动化框架的核心组件，为智能寄生虫诊断提供可扩展的工程解决方案。|Nouar AlDahoul Team|[2602.13712](http://arxiv.org/abs/2602.13712)|null|
|**2026-02-14**|**LeafNet: A Large-Scale Dataset and Comprehensive Benchmark for Foundational Vision-Language Understanding of Plant Diseases**|基础模型和视觉-语言预训练在VLM领域取得显著进展，但在植物病理学等农业特定领域的应用受限于缺乏大规模、全面的多模态数据集和基准。为弥补此空白，本研究引入了LeafNet数据集和LeafBench视觉问答基准，涵盖97种病害的18.6万张图像和13,950个问答对。对12个先进VLM的基准测试揭示了其疾病理解能力的显著差异，二元分类准确率超90%，但细粒度识别低于65%。研究证实，多模态架构整合语言表示显著增强了诊断精度，凸显了LeafBench对VLM在植物病理学应用中方法学进展和评估的重要性。|Luyl-Da Quach Team|[2602.13662](http://arxiv.org/abs/2602.13662)|null|
|**2026-02-14**|**KorMedMCQA-V: A Multimodal Benchmark for Evaluating Vision-Language Models on the Korean Medical Licensing Examination**|当前视觉-语言模型（VLM）评估基准多为英语或通用领域，缺乏针对韩语医疗领域的多模态问答基准。为此，本研究引入了KorMedMCQA-V，一个韩语医学执照考试风格的多模态多项选择问答基准，包含1534个问题和2043张图像，涵盖多种临床模态，且约30%的问题需整合多图像证据。在统一零样本评估协议下，对50多个VLM的基准测试显示，最佳专有模型准确率达96.9%，最佳开源模型为83.7%，而最佳韩语专用模型仅为43.2%。研究还发现推理导向模型性能显著提升，医学领域专业化收益不一，所有模型在多图像问题上表现下降，且性能因成像模态而异。|Edward Choi Team|[2602.13650](http://arxiv.org/abs/2602.13650)|null|
|**2026-02-14**|**Towards Sparse Video Understanding and Reasoning**|现有视频问答（VQA）方法通常均匀采样视频帧，效率低下且未能有效捕捉关键信息，在多轮VQA中存在挑战。本研究提出了REVISE（Reasoning with Video Sparsity），一个多轮视频问答智能体，它选择少量信息帧，在多轮中维护摘要状态，并在有信心时提前停止。为微调开源模型，引入了EAGER（Evidence-Adjusted Gain for Efficient Reasoning）这一无标注奖励机制，包含置信度增益、摘要充分性和正确且提前停止三项。在多个VQA基准测试中，REVISE在提高准确率的同时，显著减少了帧数、轮次和提示token，展现了实用的稀疏视频推理能力。|Han Liu Team|[2602.13602](http://arxiv.org/abs/2602.13602)|null|
|**2026-02-14**|**AdaVBoost: Mitigating Hallucinations in LVLMs via Token-Level Adaptive Visual Attention Boosting**|大规模视觉-语言模型（LVLMs）存在幻觉问题，现有视觉注意力增强方法通过预定义缩放缓解，但固定缩放因子可能在不同生成步骤中表现出过弱或过强的局限性。为解决此问题，本文提出了AdaVBoost，一个token级别的自适应视觉注意力增强框架，旨在每个生成步骤动态确定注意力增强的程度。该框架引入视觉接地熵（VGE）来估计幻觉风险，并根据VGE对高风险token施加强视觉注意力增强，对低风险token施加弱增强。实验结果表明，AdaVBoost在多个LVLMs和幻觉基准测试中显著优于基线方法。|Tianyu Pang Team|[2602.13600](http://arxiv.org/abs/2602.13600)|null|
|**2026-02-14**|**OpAgent: Operator Agent for Web Navigation**|自主网络智能体在复杂且不稳定的真实网站环境中，面临现有SFT或离线RL方法因分布漂移而导致的性能局限。本文提出了一个强大的在线强化学习WebAgent，通过与非受限广域网站的直接迭代交互来优化策略。该方法包含分层多任务微调，建立了强大的VLM；开发了在线交互环境和RL管道，引入混合奖励机制缓解长期导航中的信用分配挑战；并提出了OpAgent模块化框架，整合规划器、接地器、反射器和总结器以实现错误恢复和自校正。实验结果显示，RL增强模型在WebArena上成功率达38.1%，而OpAgent框架进一步提升至71.6%，达到新的SOTA水平。|Peng Di Team|[2602.13559](http://arxiv.org/abs/2602.13559)|null|
|**2026-02-13**|**Steerable Vision-Language-Action Policies for Embodied Reasoning and Hierarchical Control**|预训练视觉语言模型(VLMs)拥有丰富的常识先验知识，但在机器人控制中有效落地仍面临挑战，现有分层方法中VLM对低层行为的引导受限于自然语言接口。为此，研究者提出了“可操控策略”(Steerable Policies)，通过在不同抽象级别（如子任务、动作、像素坐标）的丰富合成指令上训练视觉语言动作模型(VLA)，以提升低层可控性并释放VLM的预训练知识。实验结果表明，无论是通过学习型高层具身推理器还是即插即用VLM控制，Steerable Policies在真实的机器人操作实验中均优于现有基线，尤其在泛化和长任务方面表现出色。|Sergey Levine Team|[2602.13193](http://arxiv.org/abs/2602.13193)|null|
|**2026-02-13**|**Implicit-Scale 3D Reconstruction for Multi-Food Volume Estimation from Monocular Images**|现有膳食评估方法多依赖单图像分析或基于外观的推断，缺乏明确几何推理且对尺度模糊敏感，难以在真实用餐场景中准确估计食物份量。本研究提出了Implicit-Scale 3D Reconstruction from Monocular Multi-Food Images基准数据集，将食物份量估计重构为单目观测下的隐式尺度3D重建问题，通过餐盘、餐具等上下文线索而非显式度量来推断尺度，并着重于复杂的多食物场景。实验结果显示，几何重建方法相比强大的视觉语言基线，在准确性和鲁棒性上均有提升，最佳方法在体积估计上达到了0.21 MAPE，几何精度为5.7 L1 Chamfer Distance。|Jiangpeng He Team|[2602.13041](http://arxiv.org/abs/2602.13041)|**[link](https://www.kaggle.com/competitions/3d-reconstruction-from-monocular-multi-food-images/data)**|
|**2026-02-13**|**Training-Free Acceleration for Document Parsing Vision-Language Model with Hierarchical Speculative Decoding**|文档解析是多模态理解中的核心任务，但基于视觉语言模型(VLM)的端到端方法在处理长文档时，常因自回归生成长序列而导致显著的推理延迟。针对这一问题，本研究提出了一种免训练且高效的加速方法，借鉴推测解码的思想，使用轻量级文档解析流水线作为草稿模型预测未来批次token，并由更精确的VLM并行验证。此外，该方法还利用文档的布局结构将页面划分为独立区域进行并行解码。实验结果表明，该方法在通用OmniDocBench上为dots.ocr模型提供了2.42倍的无损加速，在长文档解析任务上加速高达4.89倍。|Lianwen Jin Team|[2602.12957](http://arxiv.org/abs/2602.12957)|null|
|**2026-02-13**|**HoRAMA: Holistic Reconstruction with Automated Material Assignment for Ray Tracing using NYURay**|下一代无线网络需要精确的特定站点确定性信道传播预测，而无线射线追踪(RT)依赖高精度3D环境模型和材料属性，手动建模耗时且传统视觉3D重建方法缺乏RT兼容性。为此，本研究提出了HoRAMA（Holistic Reconstruction with Automated Material Assignment）系统，该系统能利用智能手机捕获的RGB视频，结合MASt3R-SLAM的密集点云生成和视觉语言模型辅助的材料分配，自动生成RT兼容的3D模型。实验结果表明，HoRAMA的射线追踪预测在匹配多径分量功率预测方面与手动创建的3D模型基线表现相当（2.28 dB RMSE vs 2.18 dB），同时将3D重建时间从两个月缩短到16小时，显著提高了效率。|Theodore S. Rappaport Team|[2602.12942](http://arxiv.org/abs/2602.12942)|null|
|**2026-02-13**|**RoadscapesQA: A Multitask, Multimodal Dataset for Visual Question Answering on Indian Roads**|理解道路场景对自动驾驶至关重要，但现有数据集可能无法充分覆盖印度复杂多样的驾驶环境。本研究推出了Roadscapes，一个多任务多模态数据集，包含多达9,000张在印度不同驾驶环境中拍摄的图像，并附带手动验证的边界框。该数据集利用基于规则的启发式方法推断场景属性，并生成用于对象定位、推理和场景理解的问答对，涵盖了印度城市和乡村的多种昼夜场景。Roadscapes旨在推动非结构化环境中视觉场景理解的研究，并提供了使用视觉语言模型进行图像问答任务的初步基线。|Jyothikamalesh S Team|[2602.12877](http://arxiv.org/abs/2602.12877)|null|
|**2026-02-13**|**Thinking Like a Radiologist: A Dataset for Anatomy-Guided Interleaved Vision Language Reasoning in Chest X-ray Interpretation**|放射学诊断涉及视觉检查与语言推理的反复交织，但现有医学大型视觉语言模型(LVLMs)多依赖纯文本思维链推理，易产生幻觉，且现有伪视觉解决方案仍缺乏丰富的视觉细节。为此，本研究提出了MMRad-IVL-22K，这是首个专为胸部X射线解读中原生交织的视觉语言推理设计的大规模数据集，反映了放射科医生反复推理和视觉检查的工作流程。实验结果表明，多模态思维链引导的报告生成在临床准确性和报告质量方面显著优于纯文本思维链（RadGraph指标提高6%），证实高保真交织视觉语言证据是可靠医疗AI不可替代的组成部分。在MMRad-IVL-22K上微调的模型在推理一致性和报告质量方面也优于通用和医学专用LVLMs。|Wei Shen Team|[2602.12843](http://arxiv.org/abs/2602.12843)|null|
|**2026-02-13**|**X-SYS: A Reference Architecture for Interactive Explanation Systems**|可解释AI (XAI) 方法虽多，但将其部署为交互式系统仍面临挑战，因其需要兼顾算法与系统能力以维持解释可用性。本研究将可解释性视为一个信息系统问题，提出了X-SYS，一个交互式解释系统的参考架构。X-SYS围绕STAR（可伸缩性、可追溯性、响应性、适应性）四个质量属性，并指定了包含XUI服务、解释服务等五个组件的分解结构，将交互模式映射到系统能力以解耦用户界面和后端计算。通过SemanticLens系统实现X-SYS，展示了其如何通过契约化服务边界实现独立演进、通过离线/在线分离确保响应性以及通过持久状态管理支持可追溯性，为在操作约束下设计交互式解释系统提供了可重用蓝图和具体实例。|Sebastian Lapuschkin Team|[2602.12748](http://arxiv.org/abs/2602.12748)|null|
|**2026-02-13**|**SignScene: Visual Sign Grounding for Mapless Navigation**|导航标志能帮助人类在陌生环境中无地图导航，但机器人如何利用标志进行无地图导航是一个挑战，核心在于如何解释复杂多样的标志及其抽象语义内容，并将其与局部3D场景匹配。本研究将此形式化为“标志接地”(sign grounding)问题，即把标志上的语义指令映射到对应的场景元素和导航动作。研究者利用视觉语言模型(VLMs)的语义常识和推理能力，并提出了SignScene，一种以标志为中心的空-语义表示，旨在以利于VLM有效推理的形式呈现导航相关场景元素和标志信息。在包含九种环境类型、114个查询的数据集上，该方法实现了88%的接地准确率，显著优于基线，并成功使Spot机器人在真实世界中仅依赖标志进行无地图导航。|David Hsu Team|[2602.12686](http://arxiv.org/abs/2602.12686)|null|
|**2026-02-13**|**IndicFairFace: Balanced Indian Face Dataset for Auditing and Mitigating Geographical Bias in Vision-Language Models**|视觉语言模型(VLMs)常从训练数据中继承并放大社会偏见，印度群体尤其被错误代表，现有公平性数据集将印度视为单一类别，忽视了其内部地理多样性。为解决这一局限，本研究提出了IndicFairFace，一个包含14,400张图像的新颖且平衡的人脸数据集，旨在代表印度的地理多样性，图像伦理获取并在各邦和性别间均匀平衡。通过IndicFairFace，研究量化了基于CLIP的VLM中存在的印度国内地理偏见，并利用后验迭代零空间投影去偏方法成功减少了这种偏见。实验证明，该去偏方法对现有嵌入空间的影响很小，基准数据集上的检索准确率平均下降不到1.5%，确立了IndicFairFace作为研究印度背景下VLM地理偏见的第一个基准。|Jiechao Gao Team|[2602.12659](http://arxiv.org/abs/2602.12659)|null|
|**2026-02-13**|**PISHYAR: A Socially Intelligent Smart Cane for Indoor Social Navigation and Multimodal Human-Robot Interaction for Visually Impaired People**|现有智能助行设备多侧重物理导航，但缺乏社交智能和多模态人机交互能力。本研究提出了PISHYAR，一款结合社交感知导航和多模态人机交互的智能拐杖。该系统包含社交导航框架（集成RGB-D感知、对象检测、活动识别、路径规划和触觉反馈）和代理式多模态LLM-VLM交互框架（集成语音识别、VLM、LLM和文本转语音，并支持动态模式路由）。通过仿真、真实世界实验和用户研究，PISHYAR在避障和社会顺从性导航中表现可靠，整体准确率约80%，集体活动识别稳健。初步用户研究显示，视障用户对其交互框架的可用性、信任度和感知社交性给予高度评价，突显了PISHYAR作为多模态辅助移动设备在提供社交互动支持方面的潜力。|Alireza Taheri Team|[2602.12597](http://arxiv.org/abs/2602.12597)|null|
|**2026-02-13**|**On Robustness and Chain-of-Thought Consistency of RL-Finetuned VLMs**|在视觉语言模型（VLM）中，尽管强化学习（RL）微调能提升推理任务性能，但仍面临视觉基础薄弱、幻觉和过度依赖文本线索的问题。研究发现，简单的文本扰动（如误导性描述）会显著降低模型的鲁棒性和置信度，并揭示了RL微调中存在的准确性与忠实性之间的权衡。具体来说，微调提高了基准准确性，却可能损害推理链的可靠性和模型对上下文变化的鲁棒性。这些结果强调了仅凭准确性评估的局限性，并呼吁在训练和评估中同时关注正确性、鲁棒性以及视觉基础推理的忠实性。|Arnab Mondal Team|[2602.12506](http://arxiv.org/abs/2602.12506)|null|
|**2026-02-13**|**Layer-Specific Fine-Tuning for Improved Negation Handling in Medical Vision-Language Models**|由于视觉语言模型（VLM）在区分肯定和否定医疗陈述方面存在局限性，本研究引入了一个放射学诊断基准来系统评估VLM对极性的敏感性。为解决此问题，我们构建了一个包含结构化声明和属性级否定上下文临床否定数据集，并提出了一种名为否定感知选择性训练（NAST）的自适应方法。NAST利用因果追踪效应（CTE）根据各层对否定处理的因果贡献来调整梯度更新。实验结果表明，NAST在不损害通用视觉-语言对齐的情况下，显著提高了VLM对肯定和否定临床陈述的辨别能力，凸显了因果可解释性在安全关键医疗领域中进行有针对性模型适应的价值。|Rahmatollah Beheshti Team|[2602.12498](http://arxiv.org/abs/2602.12498)|null|
|**2026-02-12**|**Scaling Verification Can Be More Effective than Scaling Policy Learning for Vision-Language-Action Alignment**|针对视觉-语言-动作（VLA）模型在执行自然语言指令时存在的"意图-动作差距"问题，本研究探索了测试时验证方法。我们首先揭示了具身指令遵循的测试时标度定律，发现联合扩展复述指令和生成动作数量能更高效地恢复正确动作。为利用这些规律，我们提出了CoVer，一个对比验证器，并引入了“启动时计算”和分层验证推理管道。在部署时，该框架预先计算VLM生成的复述指令，生成动作候选项，然后使用验证器选择最优提示和动作块。实验结果表明，CoVer在SIMPLER基准上取得了显著提升（分布内22%，分布外13%），并在真实世界实验中进一步提升45%，在PolaRiS基准上任务进展提升14%，成功率提升9%。|Marco Pavone Team|[2602.12281](http://arxiv.org/abs/2602.12281)|null|
|**2026-02-12**|**ExStrucTiny: A Benchmark for Schema-Variable Structured Information Extraction from Document Images**|尽管通用视觉语言模型（VLM）在传统文档理解基准上表现良好，但它们在多样化文档类型和灵活模式下进行整体、细粒度结构化信息提取的能力仍未充分研究。现有数据集在实体本体、查询复杂度或文档类型上存在局限。为解决这些不足，本研究引入了ExStrucTiny，一个新的文档图像结构化信息提取（IE）基准数据集，它统一了关键实体提取、关系提取和视觉问答的特性。该数据集通过结合人工和合成并经过人工验证的样本构建，涵盖了更广泛的文档类型和提取场景。对开放和封闭式VLM在该基准上的分析揭示了模式适应、查询欠规范和答案定位等挑战，为未来改进通用模型在文档结构化信息提取方面的能力奠定了基础。|Manuela Veloso Team|[2602.12203](http://arxiv.org/abs/2602.12203)|null|
|**2026-02-12**|**3DGSNav: Enhancing Vision-Language Model Reasoning for Object Navigation via Active 3D Gaussian Splatting**|针对零样本对象导航（ZSON）中视觉语言模型（VLM）决策受低级感知准确性限制的问题，本研究提出了3DGSNav框架。3DGSNav利用3D高斯泼溅（3DGS）作为VLM的持久记忆来增强空间推理能力。通过主动感知，该框架逐步构建环境的3DGS表示，从而实现轨迹引导的、边界感知的第一人称视角的自由视点渲染。此外，研究设计了结构化视觉提示并结合思维链（CoT）提示来进一步提升VLM的推理能力。在导航过程中，实时对象检测器用于过滤潜在目标，而VLM驱动的主动视点切换则进行目标再验证，确保高效可靠的识别。在多个基准测试和真实世界四足机器人实验中，3DGSNav展现出鲁棒且具竞争力的性能。|Xinyi Yu Team|[2602.12159](http://arxiv.org/abs/2602.12159)|null|
|**2026-02-12**|**Affordance-Graphed Task Worlds: Self-Evolving Task Generation for Scalable Embodied Learning**|鉴于在真实世界中训练机器人策略成本高昂且现有生成模拟难以生成逻辑连贯的长周期任务并处理动态物理不确定性，本研究提出了Affordance-Graphed Task Worlds (AGT-World) 统一框架。该框架基于真实世界观测自主构建交互式模拟环境和机器人任务策略，通过将任务空间形式化为结构化图，实现复杂目标的精确分层分解，并引入了结合VLM推理和几何验证的自演化机制来优化策略。实验结果表明，该方法在成功率和泛化能力上显著优于现有方法，实现了提议、执行和修正的自我改进循环，以支持可扩展的机器人学习。|Changshui Zhang Team|[2602.12065](http://arxiv.org/abs/2602.12065)|null|
|**2026-02-12**|**Can Local Vision-Language Models improve Activity Recognition over Vision Transformers? -- Case Study on Newborn Resuscitation**|鉴于新生儿复苏活动准确记录的重要性及其在实践中的不足，以及现有方法在识别细粒度活动上的挑战，本研究探索了生成式AI（GenAI）方法，特别是结合大型语言模型（LLM）的局部视觉-语言模型（VLM），以改进新生儿复苏视频中的活动识别。研究将几种零样本VLM策略和微调VLM（包括LoRA）与分类头进行评估，并与监督式TimeSFormer基线进行比较。结果表明，小型VLM虽然容易产生幻觉，但经过LoRA微调后，F1分数可达0.91，显著超越TimeSFormer的0.70。|Øyvind Meinich-Bache Team|[2602.12002](http://arxiv.org/abs/2602.12002)|null|
|**2026-02-12**|**Benchmarking Vision-Language Models for French PDF-to-Markdown Conversion**|针对PDF到Markdown转换作为RAG管道关键步骤，以及现有基准多侧重英文或中文并可能过度惩罚无关格式差异的问题，本研究引入了一个专门针对法语文档、通过模型不一致采样挑选的挑战性新基准。该基准涵盖手写表格、复杂布局、密集表格和富含图形的页面，并采用单元测试式检查和类别特定归一化进行评估，以减少无关的呈现差异。实验结果显示，在15个模型中，最强的专有模型在手写和表格处理上表现出更高的鲁棒性，同时一些开源系统在标准打印布局上仍具竞争力。|Nicolas Mery Team|[2602.11960](http://arxiv.org/abs/2602.11960)|null|
|**2026-02-12**|**Are Two LLMs Better Than One? A Student-Teacher Dual-Head LLMs Architecture for Pharmaceutical Content Optimization**|针对大型语言模型（LLM）在制药等受监管领域内容创作中面临的科学准确性和合规性挑战，以及手动质量控制（QC）低效易错的问题，本研究引入了LRBTC，一个模块化的LLM和VLM驱动的QC架构。该架构涵盖语言、法规、品牌、技术和内容结构检查，并结合了师生双模型架构、人工干预（HITL）工作流和瀑布式规则过滤机制。实验结果表明，LRBTC在AIReg-Bench上取得了83.0%的F1和97.5%的召回率，相比Gemini 2.5 Pro，遗漏违规减少5倍；在CSpelling上平均准确率提高26.7%。误差分析揭示当前模型擅长检测拼写错误，但在复杂医学语法和标点错误识别方面仍有提升空间。|Anubhav Girdhar Team|[2602.11957](http://arxiv.org/abs/2602.11957)|null|
|**2026-02-12**|**LAMP: Implicit Language Map for Robot Navigation**|为解决现有视觉-语言模型（VLM）零样本导航方法在大环境中因显式存储语言向量而导致的内存需求过高和细粒度规划分辨率有限的问题，本研究提出了LAMP（Language Map）框架。该框架通过学习连续的、语言驱动的隐式神经语言场来编码语言特征，并结合稀疏图实现高效的粗略路径规划，进而通过梯度优化在学习场中细化目标姿态。该方法还采用贝叶斯框架建模嵌入不确定性以增强泛化能力，并通过图采样策略扩展到大环境。实验证明，LAMP在内存效率和细粒度目标到达精度方面均优于现有显式方法。|Sunwook Choi Team|[2602.11862](http://arxiv.org/abs/2602.11862)|**[link](https://lab-of-ai-and-robotics.github.io/LAMP/)**|
|**2026-02-12**|**JEPA-VLA: Video Predictive Embedding is Needed for VLA Models**|针对现有视觉-语言-动作（VLA）模型在机器人操作中存在的样本效率低和泛化能力有限的问题，本研究认为这与被忽视的预训练视觉表征在环境理解和策略先验方面知识不足有关。研究发现，视频上预训练的预测嵌入（特别是V-JEPA 2）能有效捕捉任务相关的时态动态并过滤不可预测的环境因素，从而弥补现有视觉表征的不足。基于此，论文提出了JEPA-VLA，一种将预测嵌入自适应集成到现有VLA中的简单而有效的方法。实验结果表明，JEPA-VLA在一系列基准和真实机器人任务中均取得了显著的性能提升。|Mingsheng Long Team|[2602.11832](http://arxiv.org/abs/2602.11832)|null|
|**2026-02-12**|**Revis: Sparse Latent Steering to Mitigate Object Hallucination in Large Vision-Language Models**|为解决大型视觉-语言模型（LVLMs）普遍存在的物体幻觉问题，该问题源于视觉特征和预训练文本表示在网络深层中的纠缠，本研究提出了REVIS，一个无需训练的框架。REVIS基于潜在空间几何原理，通过正交投影精确提取纯视觉信息向量，并采用校准策略，仅在视觉信息被抑制的精确深度进行稀疏干预，从而有效恢复视觉信息。经验评估结果显示，REVIS与最先进的基线相比，将目标幻觉率降低了约19%，同时保持了模型原有的通用推理能力。|Zhou Yang Team|[2602.11824](http://arxiv.org/abs/2602.11824)|null|
|**2026-02-12**|**Adaptive Debiasing Tsallis Entropy for Test-Time Adaptation**|现有主流的视觉-语言模型测试时自适应（TTA）方法依赖香农熵（SE）衡量预测不确定性，但由于预训练数据偏差，SE会产生有偏的不确定性估计。为解决此问题，研究发现并证明Tsallis熵（TE）通过引入非广延参数q，天然适合表征有偏分布。在此基础上，本文提出了自适应去偏Tsallis熵（ADTE），为每个类别定制类特定的q^l参数，该参数通过对持续传入的测试实例估计标签偏差进行归一化得到。实验结果表明，ADTE在ImageNet及其五种变体上超越了最先进的方法，并在10个跨域基准测试中取得了最高的平均性能，证实了其有效性。|Jianfeng Lu Team|[2602.11743](http://arxiv.org/abs/2602.11743)|null|
|**2026-02-12**|**Adapting Vision-Language Models for E-commerce Understanding at Scale**|电商产品理解需要文本、图像和结构化属性等多模态的强大理解能力。通用视觉-语言模型（VLMs）虽能提供泛化能力，但如何将其有效地适应电商数据中以属性为中心、多图像和噪声多的特性，同时不牺牲通用性能，是一个未被充分探索的问题。本文通过大规模实验研究，展示了对通用VLM进行有针对性的适配，可以在大幅提升电商任务性能的同时，保持其广泛的多模态能力。此外，本文还提出了一个新颖且全面的评估套件，涵盖了深度产品理解、严格指令遵循和动态属性提取等任务。|Shahram Khadivi Team|[2602.11733](http://arxiv.org/abs/2602.11733)|null|
|**2026-02-12**|**STVG-R1: Incentivizing Instance-Level Reasoning and Grounding in Videos via Reinforcement Learning**|在视觉-语言模型（VLMs）中，文本描述与视觉坐标之间的不匹配常导致幻觉问题，在时空视频定位（STVG）等密集预测任务中尤为突出。现有方法通常增强视觉-文本对齐或添加辅助解码器，但这会引入额外的可训练模块，导致高昂的标注和计算成本。本文提出了一种新颖的视觉提示范式，通过将每帧坐标预测重新表述为实例级识别问题，为每个对象分配唯一且时间一致的ID，并将其嵌入视频作为视觉提示。此外，研究引入了STVG-R1，这是首个用于STVG的强化学习框架，通过任务驱动奖励联合优化时间精度、空间一致性和结构化格式正则化。实验结果表明，STVG-R1在六个基准测试中表现出色，在HCSTVG-v2上m_IoU比基线Qwen2.5-VL-7B高出20.9%，达到SOTA，并展现出强大的零样本泛化能力。|Qing Li Team|[2602.11730](http://arxiv.org/abs/2602.11730)|null|
|**2026-02-12**|**ScalSelect: Scalable Training-Free Multimodal Data Selection for Efficient Visual Instruction Tuning**|大规模视觉指令微调（VIT）是提升视觉-语言模型（VLMs）在多模态任务中性能的关键范式，但其在大型数据集上的训练因数据冗余而计算昂贵且低效。现有的数据选择方法要么需要昂贵的训练或梯度计算，要么依赖代理模型或指令无关表示且复杂度高，限制了可扩展性。本文提出了ScalSelect，一种可扩展的无训练多模态数据选择方法，其复杂度与样本数量呈线性关系，无需外部模型或辅助数据集。ScalSelect通过提取指令令牌在目标VLM中最受关注的视觉特征来构建样本表示，然后识别其表示最能近似整个数据集主导子空间的样本，从而实现无需成对比较的可扩展重要性评分。广泛的实验表明，ScalSelect仅使用16%的数据即可达到全数据集训练97.5%以上的性能，在某些设置下甚至超越了全数据训练。|Kai Chen Team|[2602.11636](http://arxiv.org/abs/2602.11636)|**[link](https://github.com/ChangtiWu/ScalSelect}{ScalSelect})**|
|**2026-02-12**|**SkillRater: Untangling Capabilities in Multimodal Data**|传统数据筛选方法通常只给样本分配一个单一的质量分数，但这在模型需要学习多种不同能力时显得局限。本文认为质量应是多维的，每个维度对应模型需掌握的一种能力。为此，我们提出了SkillRater框架，将数据过滤分解为多个专业的评估器，每个评估器负责一种能力，并通过元学习在独立验证目标上进行训练。这些评估器的分数通过渐进式选择规则组合：在每个训练阶段，只要任一评估器将样本排名高于随时间收紧的阈值，该样本即被保留，以在早期保持多样性，后期聚焦高价值样本。在视觉语言模型上的验证表明，SkillRater在视觉理解、OCR和STEM推理能力上均显著优于未过滤基线，且学习到的评估器信号几乎正交，证实了多维度分解的有效性。|Akshat Shrivastava Team|[2602.11615](http://arxiv.org/abs/2602.11615)|null|
|**2026-02-12**|**Chatting with Images for Introspective Visual Thinking**|当前大型视觉-语言模型（LVLMs）通常依赖基于单次视觉编码的纯文本推理，这常导致细粒度视觉信息丢失，尤其是在处理跨区域或多图像的视觉语义或几何关系推理时。为解决这些挑战，本文提出了“与图像对话”这一新框架，将视觉操作重新定义为语言引导的特征调制。在该框架下，模型在富有表现力的语言提示指导下，动态地对多个图像区域进行联合再编码，从而实现语言推理与视觉状态更新之间更紧密的耦合。我们通过ViLaVT实例化了这一范式，它是一个配备动态视觉编码器的新型LVLM，并采用两阶段课程（监督微调和强化学习）进行训练。广泛的实验表明，ViLaVT在八个基准测试中取得了显著且一致的改进，尤其在复杂的多图像和基于视频的空间推理任务上表现突出。|Tieniu Tan Team|[2602.11073](http://arxiv.org/abs/2602.11073)|null|
|**2026-02-12**|**Self-Refining Vision Language Model for Robotic Failure Detection and Reasoning**|针对机器人系统故障推理面临的挑战，即真实世界故障的复杂性及丰富推理标签获取成本高昂，本研究提出了ARMOR模型。ARMOR将故障检测与自然语言推理视为一个多任务自完善过程，通过迭代预测和基于历史输出的条件推理进行训练。它利用大规模稀疏二元标签和少量丰富推理标注的异构监督，并通过离线和在线模仿学习进行优化。在推理阶段，ARMOR生成多条完善轨迹并利用自确定性指标选择最可靠的预测。实验结果显示，ARMOR在故障检测率上比现有方法提升了30%，在LLM模糊匹配分数衡量的推理能力上提升了100%，证明了其在异构监督下的鲁棒性及超越预定义故障模式的开放式推理能力。|Yesh Dattatreya Team|[2602.12405](http://arxiv.org/abs/2602.12405)|null|
|**2026-02-12**|**What does RL improve for Visual Reasoning? A Frankenstein-Style Analysis**|为了理解强化学习（RL）在视觉语言模型（VLM）视觉推理中相比监督微调（IN）的具体贡献，本研究提出了一种“弗兰肯斯坦式”分析框架。该框架通过因果探测定位功能、参数比较表征更新以及模型合并测试可迁移性。研究发现，RL主要通过在模型的中间到后期层诱导一致的推理时段转移来提升性能，并且这些中间到后期的优化对于RL的性能增益是可迁移和必要的。这些结果表明，RL对视觉推理的可靠贡献并非统一增强视觉感知，而是系统地细化了Transformer中间到后期层的计算，从而改善了视觉到推理的对齐和推理性能，揭示了仅通过基准评估来理解多模态推理改进的局限性。|Tianyi Zhou Team|[2602.12395](http://arxiv.org/abs/2602.12395)|null|
|**2026-02-12**|**Synthetic Image Detection with CLIP: Understanding and Assessing Predictive Cues**|鉴于现代生成模型能产生近乎真实的照片，合成图像检测（SID）的泛化能力及其在实际应用中的表现面临挑战，特别是在新生成模型面前。本研究旨在探究CLIP作为SID基础模型的有效性及其内在线索。为此，我们构建了SynthCLIC数据集以减少语义偏差，并利用可解释的线性分类器和文本概念模型分析CLIP特征。结果显示，CLIP检测器在GAN基准上表现优异，但在高质量扩散数据集SynthCLIC上性能略有下降，且跨不同生成器家族的泛化能力显著受限。研究发现，检测器主要依赖高层摄影属性而非明显的生成器伪影。这些发现强调了持续模型更新和更广泛训练暴露的必要性，并肯定了CLIP作为更通用、鲁棒SID方法的强大基础。|Michael Graber Team|[2602.12381](http://arxiv.org/abs/2602.12381)|null|
|**2026-02-12**|**ForeAct: Steering Your VLA with Efficient Visual Foresight Planning**|在开放世界环境中，视觉-语言-动作（VLA）模型将高级语言指令转换为具体可执行动作面临挑战。本研究提出了视觉预见规划器（ForeAct），一个通用高效的规划器，它通过想象未来观察和子任务描述来逐步指导VLA。ForeAct包含一个高效的预见图像生成模块，能从当前视觉输入和语言指令在0.33秒内预测高质量的未来观察，并结合一个视觉-语言模型生成子任务描述。最先进的VLA模型无需修改架构，只需通过增强视觉输入即可无缝集成ForeAct。预见生成器在超过100万个多任务、跨具身情景中进行预训练，学习了鲁棒的具身动力学。在包含11个真实世界任务的基准测试中，ForeAct取得了87.4%的平均成功率，比基线模型有显著提升。|Song Han Team|[2602.12322](http://arxiv.org/abs/2602.12322)|null|
|**2026-02-12**|**LatentAM: Real-Time, Large-Scale Latent Gaussian Attention Mapping via Online Dictionary Learning**|为解决开放词汇机器人感知中从流式RGB-D观测构建可扩展潜在特征地图的挑战，并克服传统VLM嵌入方法缺乏通用性和依赖预训练的问题，本研究提出了LatentAM框架。LatentAM是一种在线3D高斯泼溅（3DGS）映射框架，它采用模型无关且无需预训练的在线字典学习方法，实现了与不同VLM的即插即用集成。该方法将高斯基元与紧凑查询向量关联，通过带有可学习字典的注意力机制转换为近似VLM嵌入。字典在流式观测中高效初始化并在线优化，同时结合基于体素哈希的地图管理策略以实现大规模环境下的GPU内存有界使用。实验结果表明，LatentAM在特征重建保真度上显著优于现有方法，并在评估数据集上实现了接近实时的速度（12-35 FPS）。|Yulun Tian Team|[2602.12314](http://arxiv.org/abs/2602.12314)|null|
|**2026-02-11**|**Hierarchical Concept Embedding & Pursuit for Interpretable Image Classification**|可解释设计模型在计算机视觉中日益受到关注，因为它们能为其预测提供忠实的解释。在图像分类中，这类模型通常从图像中提取人类可解释的概念并用于分类。稀疏概念恢复方法利用视觉-语言模型的潜在空间将图像嵌入表示为概念嵌入的稀疏组合，但这类方法忽视了概念的层次结构，可能导致预测正确但解释与层次结构不一致。为解决此问题，本文提出了分层概念嵌入与追踪（HCEP）框架，它在潜在空间中诱导概念嵌入的层次结构，并利用分层稀疏编码来恢复图像中存在的概念。实验结果表明，HCEP在概念精度和召回率上均优于基线，同时保持了有竞争力的分类准确率，并且在样本数量有限时展现出卓越的分类和概念恢复性能，证明了将层次结构融入稀疏编码能产生更可靠和可解释的图像分类模型。|René Vidal Team|[2602.11448](http://arxiv.org/abs/2602.11448)|null|
|**2026-02-11**|**Beyond VLM-Based Rewards: Diffusion-Native Latent Reward Modeling**|扩散模型和流匹配模型的偏好优化需要既具有判别鲁棒性又计算高效的奖励函数。视觉-语言模型（VLMs）已成为主要的奖励提供者，利用其丰富的多模态先验来指导对齐。然而，它们的计算和内存成本很高，并且通过像素空间奖励优化潜在扩散生成器会引入域不匹配，使对齐复杂化。本文提出了DiNa-LRM，一种扩散原生的潜在奖励模型，它直接在噪声扩散状态上构建偏好学习。DiNa-LRM引入了一种噪声校准的Thurstone似然，其不确定性依赖于扩散噪声，并利用预训练的潜在扩散骨干网络与时间步条件奖励头。实验结果显示，DiNa-LRM在图像对齐基准测试中显著优于现有基于扩散的奖励基线，并以极低的计算成本实现了与最先进VLM相当的性能，显著改善了偏好优化动态，实现了更快、更资源高效的模型对齐。|Wenhan Luo Team|[2602.11146](http://arxiv.org/abs/2602.11146)|**[link](https://github.com/HKUST-C4G/diffusion-rm)**|
|**2026-02-11**|**Active Zero: Self-Evolving Vision-Language Models through Active Environment Exploration**|自博弈已使大型语言模型（LLMs）通过自生成挑战实现自主提升。然而，现有视觉-语言模型（VLMs）的自博弈方法依赖与静态图像集的被动交互，导致对初始数据集的强依赖和学习效率低下。为解决这些局限性，本文提出了Active-Zero框架，将模型学习从被动交互转向主动探索视觉环境。Active-Zero采用三个共同进化的智能体：一个Searcher根据模型能力前沿从开放世界存储库检索图像；一个Questioner合成校准的推理任务；一个Solver通过准确性奖励进行优化。这种闭环机制实现了自我脚手架的自动课程学习。在Qwen2.5-VL-7B-Instruct的12个基准测试中，Active-Zero在推理任务上平均准确率达到53.97%（提升5.7%），在通用理解上达到59.77%（提升3.9%），持续优于现有自博弈基线，表明主动探索是可扩展和自适应自进化视觉-语言系统的关键要素。|Tat-Seng Chua Team|[2602.11241](http://arxiv.org/abs/2602.11241)|null|
|**2026-02-11**|**Safe mobility support system using crowd mapping and avoidance route planning using VLM**|自主移动机器人在动态、尤其是拥挤环境中安全有效导航仍面临挑战。本文提出了一种新颖的框架，该框架集成了视觉-语言模型（VLM）和高斯过程回归（GPR），用于生成动态人群密度图（“抽象图”），以实现自主机器人导航。我们的方法利用VLM识别抽象环境概念（如人群密度）的能力，并通过GPR以概率方式表示这些概念。在大学校园进行的真实世界试验结果表明，机器人成功地生成了避开静态障碍物和动态人群的路径，从而增强了导航的安全性和适应性。|Koichi Ozaki Team|[2602.10910](http://arxiv.org/abs/2602.10910)|null|

<p align=right>(<a href=#updated-on-20260225>back to top</a>)</p>

## VLA

|Publish Date|Title|Chinese Summary|Authors|PDF|Code|
|---|---|---|---|---|---|
|**2026-02-24**|**NoRD: A Data-Efficient Vision-Language-Action Model that Drives without Reasoning**|当前视觉-语言-动作（VLA）模型在自动驾驶中因高昂的数据收集和推理标注成本而受限。本研究提出
NoR
easoning for
D
riving（
NoRD），该模型在减少60%以上数据量且无推理标注的情况下，通过引入旨在缓解难度偏差的Dr. GRPO算法，解决了标准GRPO在小数据集上的性能瓶颈。实验证明，
NoRD在Waymo和NAVSIM基准测试上以更少的数据和无推理开销达到了与现有VLA模型相当的性能，从而实现了更高效的自动驾驶系统。|Wei Zhan Team|[2602.21172](http://arxiv.org/abs/2602.21172)|null|
|**2026-02-24**|**ActionReasoning: Robot Action Reasoning in 3D Space with LLM for Robotic Brick Stacking**|传统机器人系统缺乏泛化能力，而数据驱动的视觉-语言-动作（VLA）方法在处理物理世界的连续动作空间时面临挑战。为解决此问题，本研究提出了ActionReasoning框架，一个由大型语言模型（LLM）驱动的系统，它通过显式动作推理，利用LLM中编码的物理先验和真实世界知识，生成符合物理规律的机器人操作决策。通过在砖块堆叠任务上的实例化，实验表明该多智能体LLM框架能够实现稳定的砖块放置，并可将开发重心从低级编码转向高级工具调用和提示，展现了其在弥合机器人感知与执行之间鸿沟方面的广阔前景。|Brian Sheil Team|[2602.21161](http://arxiv.org/abs/2602.21161)|null|
|**2026-02-24**|**HALO: A Unified Vision-Language-Action Model for Embodied Multimodal Chain-of-Thought Reasoning**|视觉-语言-动作（VLA）模型在机器人操作中遇到长周期或分布外场景的挑战，因其缺乏多模态推理和世界演变预测机制。本研究提出了HALO，一个统一的VLA模型，通过具身多模态思维链（EM-CoT）推理实现文本任务推理、视觉子目标预测和增强动作预测。HALO采用Transformer混合架构，将不同推理任务解耦，并通过自动化流程合成EM-CoT训练数据。实验结果表明，HALO在模拟和真实环境中均表现优异，显著提升了任务成功率，并在未见环境随机化下展现出强大的泛化能力。|Song Guo Team|[2602.21157](http://arxiv.org/abs/2602.21157)|null|
|**2026-02-24**|**From Perception to Action: An Interactive Benchmark for Vision Reasoning**|理解物理结构对于具身智能体和长周期操作至关重要，但现有视觉-语言模型（VLM）评估多限于结构无关的单轮任务，无法有效评估模型在动态环境中推理物理约束的能力。为此，本研究引入了Causal Hierarchy of Actions and Interactions (CHAIN) 基准，这是一个交互式3D物理驱动平台，旨在评估模型理解、规划和执行基于物理约束的结构化动作序列的能力。对现有先进VLM和扩散模型的测试表明，它们在内化物理结构和因果约束方面仍存在显著不足，难以生成可靠的长周期计划并有效执行动作。|Roy Ka-Wei Lee Team|[2602.21015](http://arxiv.org/abs/2602.21015)|**[link](https://social-ai-studio.github.io/CHAIN/)**|
|**2026-02-24**|**Notes-to-Self: Scratchpad Augmented VLAs for Memory Dependent Manipulation Tasks**|许多灵巧操作任务本质上是非马尔可夫的，而当前视觉-语言-动作（VLA）模型多为“无状态”设计，难以处理依赖记忆的长周期任务。本研究探索了通过引入语言暂存器（language scratchpad）来赋予VLA模型空间和时间记忆的方法，使其能够记忆任务特定信息、跟踪计划和子目标进展。在ClevrSkills、MemoryBench以及真实世界抓取放置任务上的实验表明，语言暂存器显著提升了非循环和循环模型在这些记忆依赖型任务上的泛化能力。|Roland Memisevic Team|[2602.21013](http://arxiv.org/abs/2602.21013)|null|
|**2026-02-24**|**Toward an Agentic Infused Software Ecosystem**|为充分发挥AI智能体在软件开发中的潜力，需要重新构思整个软件生态系统。本文提出了一个“智能体注入软件生态系统（AISE）”的概念框架，该框架由AI智能体、编程语言和API（作为人机协作的通信载体），以及运行时环境（提供与外部世界交互的能力）这三大支柱构成。论文强调，为实现AISE的愿景，必须以协同和整体的方式推进这三个支柱的发展，以适应当前和未来的AI智能体以及与之协作的人类开发者。|Mark Marron Team|[2602.20979](http://arxiv.org/abs/2602.20979)|null|
|**2026-02-24**|**IG-RFT: An Interaction-Guided RL Framework for VLA Models in Long-Horizon Robotic Manipulation**|视觉-语言-动作（VLA）模型在应对真实世界中的长周期复杂任务时，面临分布偏移、高质量演示稀缺以及强化学习（RL）微调效率和稳定性等挑战。为此，本研究提出了IG-RFT系统，一个针对流式VLA模型的交互引导强化微调框架。该系统引入了交互引导优势加权回归（IG-AWR）算法来动态调节探索强度，并设计了一种整合轨迹级和子任务级奖励的混合密集奖励函数，构建了一个三阶段RL微调系统。真实世界实验表明，IG-RFT在四个挑战性任务上取得了显著更高的成功率，验证了其在现实世界机器人操作中VLA模型强化微调的有效性。|Huixu Dong Team|[2602.20715](http://arxiv.org/abs/2602.20715)|null|
|**2026-02-24**|**How Foundational Skills Influence VLM-based Embodied Agents:A Native Perspective**|现有基于视觉-语言模型（VLM）的具身智能体基准测试常采用非原生的高级指令或离散动作空间，且缺乏低级别和高级别的联合评估。为解决这些局限，本研究提出了NativeEmbodied，一个使用统一原生低级别动作空间的挑战性基准。该基准包含复杂场景下的高级任务和解耦出的四种基本具身技能的低级别任务，实现精细化评估。对现有VLM的实验揭示了其在多项基本技能上的缺陷，并证明这些瓶颈严重限制了高级任务的性能，为未来VLM驱动的具身智能体研究提供了关键洞察。|Tong Xu Team|[2602.20687](http://arxiv.org/abs/2602.20687)|null|
|**2026-02-24**|**Recursive Belief Vision Language Model**|当前视觉-语言-动作（VLA）模型在部分可观测下的长周期操作中表现不佳，主要因为缺乏持久的、动作条件的状态表示及有限的时空和物理推理能力。本研究引入了RB-VLA，一种以信念为中心的架构，通过自监督世界模型目标进行训练，能够维护一个紧凑的潜在状态来编码任务相关历史和动态。RB-VLA仅需一次VLM查询以获取高级意图，随后由信念模块跟踪任务进度并实现阶段感知控制。实验表明，RB-VLA在长周期基准测试中显著超越现有VLA，成功率大幅提高，推理延迟降低，并解决了内存随时间增长的问题，凸显了基于信念状态表示对长周期VLA策略的有效性。|Nirav Patel Team|[2602.20659](http://arxiv.org/abs/2602.20659)|null|
|**2026-02-24**|**Efficient and Explainable End-to-End Autonomous Driving via Masked Vision-Language-Action Diffusion**|大型语言模型（LLM）和视觉-语言模型（VLM）在端到端自动驾驶中潜力巨大，但面临推理延迟、动作精度和可解释性方面的挑战。本研究提出了MVLAD-AD（Masked Vision-Language-Action Diffusion for Autonomous Driving）框架，通过引入离散动作token化策略，从真实世界驾驶分布中构建紧凑的运动学可行路径点码本，并通过几何感知嵌入学习确保潜在空间中的嵌入近似物理几何度量。实验结果表明，MVLAD-AD在nuScenes等基准测试中表现出卓越的效率和规划精度，优于现有自回归和扩散基线，同时提供了高保真和可解释的推理。|Ziran Wang Team|[2602.20577](http://arxiv.org/abs/2602.20577)|null|
|**2026-02-24**|**An interactive enhanced driving dataset for autonomous driving**|为解决自动驾驶VLA模型在全自动化发展中面临的交互场景数据稀疏和多模态对齐不足问题，本研究提出了交互增强驾驶数据集（IEDD）。该数据集通过开发可扩展的管道，从自然驾驶数据中基于交互轨迹挖掘百万级交互片段，并设计指标量化交互过程。此外，还构建了IEDD-VQA数据集，生成语义动作与结构化语言严格对齐的合成鸟瞰图（BEV）视频。基准测试结果显示，该数据集在评估和微调自动驾驶模型的推理能力方面具有重要价值。|Lu Xiong Team|[2602.20575](http://arxiv.org/abs/2602.20575)|null|
|**2026-02-24**|**BFA++: Hierarchical Best-Feature-Aware Token Prune for Multi-View Vision Language Action Model**|针对VLA模型在机器人操作中因多视角输入导致视觉令牌数量激增，现有剪枝技术性能下降的问题，本文提出了BFA++，一个专为VLA模型设计的动态令牌剪枝框架。BFA++引入了两级重要性预测器引导的分层剪枝策略，通过帧内预测器突出任务相关区域以抑制空间噪声，帧间预测器识别关键摄像头视图以减少跨视图冗余。实验结果表明，BFA++在RoboTwin基准和真实任务上，使成功率提高了约10%，并分别实现了1.8倍和1.5倍的加速，证明了上下文敏感和任务感知的令牌剪枝在提高计算效率和操作精度方面的有效性。|Hua Chen Team|[2602.20566](http://arxiv.org/abs/2602.20566)|null|
|**2026-02-24**|**Inner Speech as Behavior Guides: Steerable Imitation of Diverse Behaviors for Human-AI coordination**|针对现有模仿学习方法难以捕捉人类行为多样性和非马尔可夫性，且缺乏推理时行为引导能力的问题，本文提出了MIMIC框架，旨在通过将语言作为行为意图的内部表征，实现更有效的人机协作。MIMIC创新性地利用视觉语言模型作为语言支架，训练一个条件变分自编码器以从观察中生成“内在言语”，随后扩散行为克隆策略结合观察和内在言语选择行动。实验证明，MIMIC显著提升了行为多样性和对人类演示的忠实度，并实现了无需额外演示训练的精细行为引导。|David C Parkes Team|[2602.20517](http://arxiv.org/abs/2602.20517)|null|
|**2026-02-24**|**ActionEngine: From Reactive to Programmatic GUI Agents via State Machine Memory**|针对现有GUI代理因逐步调用VLM而导致高成本、高延迟和低准确性的问题，本文提出了ActionEngine，一个无训练框架。该框架采用双代理架构：爬行代理通过离线探索构建GUI状态机记忆，执行代理利用此记忆合成可执行Python程序进行在线任务执行。为增强鲁棒性，执行失败会触发视觉重定位回退机制进行修复。实验结果显示，ActionEngine在WebArena基准测试中实现了95%的任务成功率，平均仅需一次LLM调用，同时成本降低11.8倍，端到端延迟降低2倍，显著提升了效率和准确性。|Suman Nath Team|[2602.20502](http://arxiv.org/abs/2602.20502)|null|
|**2026-02-23**|**NovaPlan: Zero-Shot Long-Horizon Manipulation via Closed-Loop Video Language Planning**|为解决机器人执行长周期任务时VLM和视频生成模型缺乏物理基础的问题，本文提出了NovaPlan，一个分层框架，将闭环VLM和视频规划与几何接地的机器人执行相结合，以实现零样本长周期操作。在高层，VLM规划器分解子目标并监控执行；在低层，系统从生成视频中提取任务相关的对象关键点和人手姿态作为运动学先验，并动态选择最优参考以稳定执行。实验表明，NovaPlan无需预先演示或训练，即可成功执行复杂组装任务并展现灵活的错误恢复能力。|George Konidaris Team|[2602.20119](http://arxiv.org/abs/2602.20119)|**[link](https://nova-plan.github.io/)**|
|**2026-02-23**|**Universal Pose Pretraining for Generalizable Vision-Language-Action Policies**|针对现有VLA模型因高层感知与稀疏、具身特定动作监督的纠缠导致的特征塌陷和训练效率低下问题，本文提出了Pose-VLA，一种解耦范式。该方法将VLA训练分为预训练和后训练两阶段：预训练阶段在统一相机中心空间中提取通用3D空间先验，后训练阶段高效对齐机器人特定动作空间。通过引入离散姿态令牌作为通用表示，Pose-VLA无缝整合3D空间基础与几何级轨迹。实验结果表明，Pose-VLA在RoboTwin 2.0和LIBERO上取得了最先进的成功率，并在真实世界实验中验证了其在仅少量演示下对不同对象的鲁棒泛化能力。|Yanwei Fu Team|[2602.19710](http://arxiv.org/abs/2602.19710)|null|
|**2026-02-23**|**QuantVLA: Scale-Calibrated Post-Training Quantization for Vision-Language-Action Models**|为应对VLA模型在实际部署中日益增长的计算和内存需求，特别是模型规模扩大时的挑战，本文提出了QuantVLA，一个无训练的后训练量化（PTQ）框架。QuantVLA首次将PTQ应用于VLA系统并成功量化了扩散Transformer动作头，它整合了选择性量化布局、注意力温度匹配和输出头平衡等三个尺度校准组件。实验结果表明，QuantVLA在LIBERO基准上超越了全精度基线，实现了约70%的内存节省和1.22倍的端到端推理延迟加速，为低位具身智能提供了实际可行的解决方案。|Mi Zhang Team|[2602.20309](http://arxiv.org/abs/2602.20309)|null|
|**2026-02-23**|**De-rendering, Reasoning, and Repairing Charts with Vision-Language Models**|针对数据可视化中存在的错误及其对受众的误导问题，以及现有规则校验器和通用LLM在提供有意义设计反馈上的不足，本文提出了一个结合图表逆渲染、自动化分析和迭代改进的框架。该系统能够从图像重建图表结构，利用视觉语言推理识别设计缺陷，并基于可视化研究原则提供具体的修改建议。在Chart2Code基准的1000张图表上进行评估，系统生成了10,452条设计建议，分类为10个连贯类别，证明了LLM驱动的推荐系统在提供结构化、基于原则的可视化设计反馈方面的巨大潜力。|Emmanuel Iarussi Team|[2602.20291](http://arxiv.org/abs/2602.20291)|null|
|**2026-02-23**|**UniLACT: Depth-Aware RGB Latent Action Learning for Vision-Language-Action Models**|针对现有VLA模型中基于RGB的潜在动作表示缺乏精确操作所需的3D几何结构问题，本文引入了UniLACT，一个基于Transformer的VLA模型，通过深度感知潜在预训练融入几何结构。为实现此目的，本文提出了UniLARN，一个统一的潜在动作学习框架，利用逆动力学和正向动力学目标学习RGB和深度的共享嵌入空间，并显式建模它们的跨模态交互。实验证明，UniLACT在模拟和真实世界环境中，无论是域内还是域外预训练，以及在已见或未见操作任务上，都持续优于基于RGB的潜在动作基线，验证了深度感知统一潜在动作表示的有效性。|Srijan Das Team|[2602.20231](http://arxiv.org/abs/2602.20231)|**[link](https://manishgovind.github.io/unilact-vla/)**|
|**2026-02-23**|**An Approach to Combining Video and Speech with Large Language Models in Human-Robot Interaction**|为解决人机交互中准确理解人类意图的核心挑战，本文提出了一个新颖的多模态HRI框架，结合了先进的视觉语言模型、语音处理和模糊逻辑，以实现对Dobot Magician机械臂的精确自适应控制。该系统集成Florence-2进行物体检测、Llama 3.1进行自然语言理解和Whisper进行语音识别，为用户提供了通过语音命令操纵物体的无缝直观界面，显著提高了命令解释和执行的可靠性。在消费级硬件上的实验评估显示，该系统命令执行准确率为75%，为未来更复杂、自然的人机协作提供了灵活可扩展的基础。|Zi Tian Team|[2602.20219](http://arxiv.org/abs/2602.20219)|null|
|**2026-02-22**|**Seeing Farther and Smarter: Value-Guided Multi-Path Reflection for VLM Policy Optimization**|为解决复杂长周期机器人操作中VLM反射规划效率低、精度差、延迟大等问题，本文提出一种新型测试时计算框架，将状态评估与动作生成解耦，以提供更直接、细粒度的监督信号。该方法显式建模动作计划优势，通过可扩展的Critic进行估计，并采用束搜索探索多条未来路径以聚合预期长期回报。此外，引入基于置信度的触发器，实现必要时才启动反射。实验结果显示，该方法在多阶段机器人操作任务中成功率提高24.6%，同时推理时间减少56.5%。|Dimitris N. Metaxas Team|[2602.19372](http://arxiv.org/abs/2602.19372)|null|
|**2026-02-22**|**MentalBlackboard: Evaluating Spatial Visualization via Mathematical Transformations**|为探究当前视觉-语言模型（VLMs）是否具备人类的空间可视化能力，本文开发了MentalBlackboard基准，涵盖纸张折叠和打孔的预测与规划任务。研究发现，模型在应用对称变换和处理旋转方面存在挑战，即便能正确预测展开步骤序列。规划任务进一步揭示了模型在分析对称关系和执行多阶段对称过程上的局限性，其中Claude Opus 4.1的规划准确率最高仅为10%。在不需要空间可视化的泛化任务中，最佳模型o3表现优异（71.6%），但在文本预测任务上准确率仅为25%。|Yezhou Yang Team|[2602.19357](http://arxiv.org/abs/2602.19357)|null|
|**2026-02-22**|**TOPReward: Token Probabilities as Hidden Zero-Shot Rewards for Robotics**|现有VLA模型在预训练方面取得进展，但在强化学习中受限于低样本效率和稀疏奖励，同时现有时间价值函数泛化性差。针对此问题，本文提出TOPReward，一种利用预训练视频VLM的潜在世界知识来估计机器人任务进度的新型概率时间价值函数。与直接提示VLM输出进度值易产生数值误解不同，TOPReward直接从VLM的内部token logits中提取任务进度。零样本评估结果显示，TOPReward在130多个真实世界任务和多个机器人平台上取得0.947的平均价值顺序相关性，显著优于现有基线，并能作为成功检测和奖励对齐行为克隆的通用工具。|Ranjay Krishna Team|[2602.19313](http://arxiv.org/abs/2602.19313)|null|
|**2026-02-22**|**The Price Is Not Right: Neuro-Symbolic Methods Outperform VLAs on Structured Long-Horizon Manipulation Tasks with Significantly Lower Energy Consumption**|为评估VLA模型在结构化、长周期操作任务中的有效性和效率，本研究对比了一个微调的开源VLA模型（π0）与一个结合PDDL符号规划和低层控制的神经符号架构。实验在模拟汉诺塔任务中进行，同时测量任务性能和能耗。结果表明，在3块汉诺塔任务中，神经符号模型成功率达95%，远超VLA模型的34%，并能泛化至4块变体（78%成功），而VLA模型失败。此外，VLA模型训练能耗比神经符号方法高出近两个数量级，凸显了端到端基础模型与结构化推理架构在可靠性、数据效率和能效方面的权衡。|Matthias Scheutz Team|[2602.19260](http://arxiv.org/abs/2602.19260)|null|
|**2026-02-22**|**Human-to-Robot Interaction: Learning from Video Demonstration for Robot Imitation**|针对示教学习（LfD）中视频字幕模型不聚焦任务相关对象、端到端架构泛化性差且需大量数据的挑战，本文提出一种“人到机器人”模仿学习流程。该流程解耦为两阶段：视频理解阶段，结合TSM与VLM提取动作并识别交互对象；机器人模仿阶段，采用TD3深度强化学习执行操作。实验在PyBullet仿真和真实世界机器人上进行，视频理解部分动作分类准确率达89.97%，BLEU-4分数显著超越基线；机器人操作部分，所有动作平均成功率87.5%，其中抓取任务100%成功，抓取放置操作成功率高达90%。|Xiem HoangVan Team|[2602.19184](http://arxiv.org/abs/2602.19184)|null|
|**2026-02-22**|**VIGiA: Instructional Video Guidance via Dialogue Reasoning and Retrieval**|针对现有模型难以理解和推理复杂、多步骤指导性视频动作计划的局限性，本文提出VIGiA，一个新颖的多模态对话模型。VIGiA旨在支持基于地面的、计划感知的对话，需推理视觉输入、指导计划和交错的用户交互。为此，它整合了多模态计划推理能力，能将查询与当前任务计划对齐并准确响应；以及基于计划的检索能力，能以文本或视觉表示检索相关计划步骤。在包含烹饪和DIY计划的指导性视频对话数据集上的实验表明，VIGiA在会话计划指导设置中的所有任务上均优于现有SOTA模型，在计划感知的视觉问答（VQA）上准确率超过90%。|João Maglhães Team|[2602.19146](http://arxiv.org/abs/2602.19146)|null|
|**2026-02-21**|**Global Commander and Local Operative: A Dual-Agent Framework for Scene Navigation**|为解决视觉与语言场景导航中，多智能体成本高昂或单智能体全局规划与局部感知过载导致推理退化和指令漂移的问题，本文引入DACo。DACo是一种规划-落地解耦架构，它通过全局指挥官负责高层战略规划，局部操作员负责自我中心观察和精细执行，从而减轻认知过载并提高长周期稳定性。该框架还整合了动态子目标规划和自适应重规划，以实现结构化和弹性的导航。DACo在R2R、REVERIE和R4R等基准上的零样本评估中，成功率比最佳基线分别提高4.9%、6.5%和5.4%，并能有效泛化到多种骨干模型。|Tat-Seng Chua Team|[2602.18941](http://arxiv.org/abs/2602.18941)|null|
|**2026-02-21**|**Habilis- $β$ : A Fast-Motion and Long-Lasting On-Device Vision-Language-Action Model**|为解决现有VLA评估无法捕捉实际操作所需的高速和持久能力问题，本文提出Habilis-β，一个用于实际部署的快速、持久的设备端VLA模型，并引入生产力-可靠性平面（PRP）指标来衡量其性能。Habilis-β通过大规模游戏数据上的无语言预训练获得鲁棒交互先验，通过循环任务演示上的后训练捕捉状态漂移，并结合相位自适应运动塑形、整流流蒸馏实现高频控制和无分类器指导来动态平衡指令遵循与交互先验。连续运行评估显示，Habilis-β在模拟和真实世界环境中均在PRP指标下表现出色，并在RoboTwin 2.0排行榜上取得了最高性能。|Theo Taeyeong Kim Team|[2602.18813](http://arxiv.org/abs/2602.18813)|null|
|**2026-02-21**|**TAG: Thinking with Action Unit Grounding for Facial Expression Recognition**|面部表情识别(FER)作为一项细粒度视觉理解任务，现有视觉-语言模型(VLMs)的推理常缺乏视觉依据，易产生幻觉且鲁棒性差。为此，本研究提出了TAG (Thinking with Action Unit Grounding) 框架，通过显式地将多模态推理约束于面部动作单元(AUs)支持，并通过AU-grounded推理轨迹的监督微调和AU感知奖励的强化学习进行训练。实验结果表明，TAG在RAF-DB、FERPlus和AffectNet数据集上持续超越现有VLM基线，并显著提高了视觉忠实度，证明了结构化、有根据的中间表示对于FER中可信多模态推理的重要性。|Wentao Zhang Team|[2602.18763](http://arxiv.org/abs/2602.18763)|null|
|**2026-02-21**|**MIRROR: Multimodal Iterative Reasoning via Reflection on Visual Regions**|在视觉-语言模型（VLMs）时代，处理模糊或复杂视觉输入时，现有模型常产生幻觉或逻辑错误，即使反思也可能脱离图像证据。为此，本研究提出了MIRROR框架，将视觉反思嵌入为核心机制，形成一个包括草稿、评论、基于区域的验证和修正的闭环过程，直至输出具备视觉依据。为训练该模型，构建了ReflectV数据集。实验结果表明，MIRROR框架提高了正确性并减少了视觉幻觉，强调了将反思训练为证据寻求和区域感知验证过程的价值。|Yunde Jia Team|[2602.18746](http://arxiv.org/abs/2602.18746)|null|
|**2026-02-21**|**RoboCurate: Harnessing Diversity with Action-Verified Neural Trajectory for Robot Learning**|视频生成模型产生的合成数据虽具潜力，但常因生成不完善导致动作质量不一致，且现有视觉-语言模型难以精确评估动作。本研究引入RoboCurate框架，通过在模拟器中回放预测动作并测量运动一致性，来评估和过滤合成机器人数据的动作质量。此外，该框架利用图像编辑和视频转换增强观测多样性。实验结果显示，RoboCurate生成的合成数据显著提高了机器人任务的成功率，在多个基准测试中均优于仅使用真实数据的表现。|Jinwoo Shin Team|[2602.18742](http://arxiv.org/abs/2602.18742)|**[link](https://seungkukim.github.io/robocurate/)**|
|**2026-02-20**|**MALLVI: A Multi-Agent Framework for Integrated Generalized Robotics Manipulation**|现有利用大语言模型（LLM）进行机器人操作任务规划的方法常依赖特定模型、微调或提示调优，且多以开环方式运行，缺乏鲁棒的环境反馈，导致在动态环境中脆弱。为解决此问题，本文提出了MALLVi，一个多智能体大语言与视觉框架，实现了闭环反馈驱动的机器人操作。MALLVi协调专门的智能体（如Decomposer、Localizer、Thinker、Reflector）来管理感知、定位、推理和高级规划，其中Reflector智能体支持有针对性的错误检测和恢复。模拟和真实世界实验结果表明，迭代闭环多智能体协调显著提高了零样本操作任务的泛化能力和成功率。|Babak Khalaj Team|[2602.16898](http://arxiv.org/abs/2602.16898)|null|
|**2026-02-20**|**How Fast Can I Run My VLA? Demystifying VLA Inference Performance with VLA-Perf**|视觉-语言-动作（VLA）模型在具身AI任务中展现出强大能力，但其在真实机器人上的实时推理性能受模型架构和推理系统复杂性限制。本研究提出了VLA-Perf分析性能模型，首次系统地研究了VLA推理性能，从模型设计和部署角度分析了模型扩展、架构选择、长上下文视频输入、异步推理、双系统管道、执行位置、硬件能力和网络性能等因素的影响。研究提炼出15个关键发现，为未来VLA模型和推理系统的设计提供了实用指导。|Christos Kozyrakis Team|[2602.18397](http://arxiv.org/abs/2602.18397)|null|
|**2026-02-20**|**Zero-shot Interactive Perception**|交互式感知（IP）对机器人解决复杂场景中的遮挡和歧义至关重要。本研究提出了零样本交互式感知（ZS-IP）框架，将多策略操作（推和抓取）与记忆驱动的视觉语言模型相结合，以指导机器人交互并解决语义查询。ZS-IP整合了增强观察模块（引入推线）、记忆引导动作模块和机器人控制器。实验结果表明，ZS-IP在多样场景中表现优异，尤其在推任务中显著超越了被动和基于视角的感知技术，并保持了非目标元素的完整性，证明了其在接触式动作中的有效性。|Amir Ghalamzan Team|[2602.18374](http://arxiv.org/abs/2602.18374)|**[link](https://openreview.net/forum?id=7MhpFcr5Nx)**|
|**2026-02-20**|**SimVLA: A Simple VLA Baseline for Robotic Manipulation**|视觉-语言-动作（VLA）模型在机器人操作中表现出色，但由于训练方案多样，难以厘清经验增益来源。本研究引入SimVLA，一个精简的VLA基线模型，通过严格解耦感知与控制、使用标准视觉-语言骨干和轻量级动作头，并标准化训练动态。实验结果表明，尽管SimVLA参数量仅为0.5B，但在标准模拟基准上无需机器人预训练即可超越数十亿参数模型，并在真实机器人任务中达到与pi0.5相当的性能，为VLA研究提供了稳健可复现的参考点。|Zhenguo Li Team|[2602.18224](http://arxiv.org/abs/2602.18224)|null|
|**2026-02-20**|**UAOR: Uncertainty-aware Observation Reinjection for Vision-Language-Action Models**|视觉-语言-动作（VLA）模型在机器人操作中具有巨大潜力，但现有方法常依赖额外观测线索或辅助模块，导致高成本和额外训练。受语言模型中前馈网络（FFN）作为“键值记忆”的启发，本研究提出了不确定性感知观测重注（UAOR），一个无需训练且即插即用的模块。该模块在语言模型层表现出高不确定性时，通过注意力检索将关键观测信息重新注入到下一层FFN中，以提高动作生成信心。实验结果表明，UAOR以最小开销持续改进了多种VLA模型在模拟和真实世界任务中的表现。|Liang Wang Team|[2602.18020](http://arxiv.org/abs/2602.18020)|null|
|**2026-02-20**|**ROCKET: Residual-Oriented Multi-Layer Alignment for Spatially-Aware Vision-Language-Action Models**|视觉-语言-动作（VLA）模型在机器人操作中缺乏3D空间理解能力，且现有表示对齐方法常仅在单层施加监督或导致梯度冲突。本研究提出了ROCKET，一个残差导向的多层表示对齐框架，通过共享投影仪将VLA骨干的多个层与强大的3D视觉基础模型的多个层进行层不变映射对齐，从而减少梯度冲突。实验结果表明，结合无需训练的层选择策略，ROCKET仅需约4%的计算预算即可在LIBERO上达到98.5%的SOTA成功率，并在多个基准和VLA模型上表现卓越。|Ang Li Team|[2602.17951](http://arxiv.org/abs/2602.17951)|null|
|**2026-02-20**|**VLANeXt: Recipes for Building Strong VLA Models**|视觉-语言-动作（VLA）模型利用强大的视觉和语言理解能力实现通用策略学习，但当前领域碎片化，难以确定关键设计选择。本研究在统一框架下重新审视VLA设计空间，系统剖析了基础组件、感知要素和动作建模维度的设计选择，并提炼出12个构建强大VLA模型的关键发现。研究成果是VLANeXt模型，在LIBERO和LIBERO-plus基准测试中超越了现有先进方法，并在真实世界实验中展现出强大的泛化能力，为社区提供了统一的代码库和共享基础。|Chen Change Loy Team|[2602.18532](http://arxiv.org/abs/2602.18532)|**[link](https://dravenalg.github.io/VLANeXt/)**|
|**2026-02-19**|**When Vision Overrides Language: Evaluating and Mitigating Counterfactual Failures in VLAs**|视觉-语言-动作（VLA）模型在实际应用中常因数据集偏差导致视觉捷径，未能忠实遵循语言指令，即出现“反事实失败”。为系统研究此问题，本文引入了LIBERO-CF，首个评估VLA语言遵循能力的反事实基准。为解决反事实失败，提出了一种简单有效的双分支推理方案：反事实动作指导（CAG）。CAG结合标准VLA策略与语言非条件的视觉-动作（VA）模块，通过反事实比较减少对视觉捷径的依赖。实验证明，CAG无需额外演示或模型修改，即可在多种VLA模型上实现即插即用和性能提升，例如在LIBERO-CF上显著提高了语言遵循准确率和任务成功率，并在真实世界中减少了反事实失败并提升了任务成功率。|Mingyu Ding Team|[2602.17659](http://arxiv.org/abs/2602.17659)|**[link](https://vla-va.github.io/)**|
|**2026-02-19**|**Web Verbs: Typed Abstractions for Reliable Task Composition on the Agentic Web**|随着网络向软件代理操作环境演进，利用大语言模型（LLM）进行目标导向任务变得可行，但当前多数网络代理依赖低级、脆弱且低效的操作原语。为解决这一问题，本文提出了“Web Verbs”，一套网络规模的、类型化、语义文档化的函数，通过统一接口暴露网站功能。这些Verbs作为稳定且可组合的单元，代理可以发现、选择并合成简洁的程序，从而统一了基于API和基于浏览器的范式，使LLM能够合成具有显式控制和数据流的可靠且可审计的工作流。概念验证和案例研究表明，Web Verbs实现了相比现有代理更简洁和鲁棒的执行，并展望了其标准化以实现网络规模的部署。|Suman Nath Team|[2602.17245](http://arxiv.org/abs/2602.17245)|null|
|**2026-02-19**|**CLUTCH: Contextualized Language model for Unlocking Text-Conditioned Hand motion modelling in the wild**|现有手部动作建模方法依赖于受限的录音棚数据集，难以捕捉“野外”复杂场景下的动作保真度和文本-动作对齐。为解决此问题，本研究首先构建了“3D Hands in the Wild”(3D-HIW) 数据集，包含32K 3D手部动作序列及对齐文本，通过结合VLM和SOTA 3D手部追踪器进行数据标注。在此基础上，提出了CLUTCH，一个基于LLM的手部动画系统，其核心创新包括SHIFT（一种部分模态分解的VQ-VAE架构，用于手部动作标记化）和几何精修阶段（通过重建损失微调LLM）。实验证明，CLUTCH在文本到动作和动作到文本任务上均达到了最先进的性能，并建立了野外手部动作建模的首个可扩展基准。|Justus Thies Team|[2602.17770](http://arxiv.org/abs/2602.17770)|**[link](https://balamuruganthambiraja.github.io/CLUTCH/)**|
|**2026-02-19**|**KPM-Bench: A Kinematic Parsing Motion Benchmark for Fine-grained Motion-centric Video Understanding**|视频字幕模型在描述精细动作细节和避免幻觉方面存在显著局限性，尤其是在以动作为主的视频中，精确描绘复杂动作和肢体动态是关键挑战。为弥补这一不足，本研究引入了一个自动化标注流程，通过结合基于运动学的计算与语言解析，实现了复杂人体运动的详细分解和描述，并基于此构建并发布了Kinematic Parsing Motion Benchmark (KPM-Bench) 数据集。此外，为系统解决幻觉问题，本文提出了语言基础的运动解析与提取（MoPE）算法，并引入了独立的精确幻觉评估指标。实验表明，将MoPE整合到GRPO后训练框架中能有效缓解幻觉问题，显著提高了以运动为中心的视频字幕模型的可靠性。|Meng Wang Team|[2602.17768](http://arxiv.org/abs/2602.17768)|null|
|**2026-02-19**|**Sketch2Feedback: Grammar-in-the-Loop Framework for Rubric-Aligned Feedback on Student STEM Diagrams**|在STEM教育中，为学生绘制的图表提供及时、准确的反馈是一个挑战，而大型多模态模型（LMMs）的幻觉倾向损害了其可信度。本研究提出了Sketch2Feedback，一个基于语法循环的框架，将问题分解为混合感知、符号图构建、约束检查和受约束的VLM反馈四个阶段，确保语言模型只报告经规则引擎验证的违规。实验结果显示，端到端LMMs虽然F1分数高但幻觉率极高，而语法管道能产生更具可操作性的反馈，证明了语法与端到端方法之间存在可利用的互补性。|Aayam Bansal Team|[2602.18520](http://arxiv.org/abs/2602.18520)|null|
|**2026-02-18**|**EgoScale: Scaling Dexterous Manipulation with Diverse Egocentric Human Data**|在灵巧操作领域，如何有效利用大规模人类行为数据仍是挑战。本研究提出了EgoScale框架，通过在一个比现有数据集大20倍的20,854小时以自我为中心的人类视频数据上训练Vision Language Action (VLA) 模型。研究发现人类数据规模与验证损失之间存在对数线性缩放定律，并提出两阶段迁移策略：大规模人类预训练后进行轻量级人机对齐中期训练。实验结果表明，该策略使22自由度机械手的平均成功率比无预训练基线提高了54%，并能有效迁移到其他自由度机器人，证实大规模人类运动数据为机器人提供了可复用、与形态无关的运动先验。|Linxi Fan Team|[2602.16710](http://arxiv.org/abs/2602.16710)|null|
|**2026-02-17**|**Learning to Retrieve Navigable Candidates for Efficient Vision-and-Language Navigation**|针对视觉与语言导航（VLN）中LLM导航效率低下的问题，即LLM需重复解释指令并处理冗余导航候选。本研究提出了一种检索增强框架，在不修改LLM的前提下提升效率和稳定性。该框架在任务层面通过指令级嵌入检索器提供任务先验，在步骤层面通过模仿学习的候选检索器剪枝不相关的导航方向。在R2R基准上的评估显示，该方法在成功率、Oracle成功率和SPL方面均有持续改进，证明检索增强决策支持能有效提升LLM基VLN的效率和稳定性。|Lina Yao Team|[2602.15724](http://arxiv.org/abs/2602.15724)|null|
|**2026-02-17**|**The Next Paradigm Is User-Centric Agent, Not Platform-Centric Service**|现代数字服务的平台中心化模式往往与用户真实需求不符，尽管LLMs等技术进步，但平台服务质量提升未必然转化为用户利益，反而优先考虑提供商目标。本文提出数字服务的未来应转向用户中心化代理，这类代理优先保护隐私、符合用户目标并赋予用户控制权。文章探讨了实现用户中心化智能的机遇与挑战，提出了一个实用的设备-云协同管道，并讨论了必要的治理和生态系统结构。|Enhong Chen Team|[2602.15682](http://arxiv.org/abs/2602.15682)|null|
|**2026-02-17**|**CARE Drive A Framework for Evaluating Reason-Responsiveness of Vision Language Models in Automated Driving**|基础模型在自动驾驶中的解释性评估多关注结果，而非决策是否反映了人类考量，可能导致安全关键领域的虚假信心。本研究提出了CARE Drive框架，一个模型无关的自动驾驶视觉语言模型原因响应性评估方法。该框架通过控制上下文变化，比较基线与原因增强模型的决策，以评估人类原因（如安全裕度、社会压力）对决策的因果影响，并采用提示校准和系统性上下文扰动两阶段评估。实验证明，明确的人类原因能显著影响模型决策，提高与专家行为的一致性，但不同上下文因素的响应性存在差异，表明基础模型的原因响应性可系统评估。|Arkady Zgonnikov Team|[2602.15645](http://arxiv.org/abs/2602.15645)|null|
|**2026-02-17**|**World Action Models are Zero-shot Policies**|现有Vision-Language-Action (VLA) 模型在语义泛化方面表现出色，但在新环境中泛化到未见过的物理动作时仍面临挑战。本研究提出了DreamZero，一个基于预训练视频扩散骨干的“世界动作模型 (WAM)”，它通过预测未来世界状态和动作来学习物理动力学，将视频作为密集表示。DreamZero通过联合建模视频和动作，能从异构机器人数据中有效学习技能，无需重复演示。实验结果显示，在真实机器人任务中，DreamZero在新任务和环境上的泛化能力比最先进的VLA模型提升超过2倍，并实现了14B视频扩散模型的实时闭环控制。此外，它还展示了仅需少量数据即可进行跨实体迁移和少样本实体适应的能力。|Joel Jang Team|[2602.15922](http://arxiv.org/abs/2602.15922)|**[link](https://dreamzero0.github.io/)**|
|**2026-02-17**|**VLM-DEWM: Dynamic External World Model for Verifiable and Resilient Vision-Language Planning in Manufacturing**|视觉语言模型（VLM）在智能制造高层规划中应用时面临无状态操作导致世界状态漂移和推理不透明导致故障难以诊断的问题。本文提出了VLM-DEWM认知架构，通过一个持久、可查询的动态外部世界模型（DEWM）将VLM推理与世界状态管理解耦。VLM决策被结构化为包含动作提议、世界信念和因果假设的可外部化推理轨迹，并在执行前与DEWM验证，故障发生时能通过状态差异分析实现有针对性恢复。实验证明，VLM-DEWM将状态跟踪准确率从56%提升至93%，恢复成功率从不足5%提升至95%，并显著降低了计算开销，展现了其在动态制造环境中实现长周期机器人操作的可靠性和弹性。|Ning Ji Team|[2602.15549](http://arxiv.org/abs/2602.15549)|null|
|**2026-02-17**|**Selective Perception for Robot: Task-Aware Attention in Multimodal VLA**|机器人领域中的Vision-Language-Action (VLA) 模型通常采用静态融合处理所有视觉输入，导致计算开销大且易受任务无关噪声干扰。本研究受人类主动感知启发，提出了一个动态信息融合框架，旨在提高VLA模型的效率和鲁棒性。该框架引入了轻量级自适应路由架构，实时分析文本提示和腕部摄像头观测，预测多摄像头视图的任务相关性，有条件地衰减低信息效用视图的计算，并选择性地提供关键视觉特征。为高效训练路由，还建立了自动化标注流程。实验结果表明，在真实机器人操作场景中，该方法显著提升了推理效率和控制性能，验证了动态信息融合在资源受限实时机器人控制环境中的有效性。|Soo-Chul Lim Team|[2602.15543](http://arxiv.org/abs/2602.15543)|null|
|**2026-02-17**|**One Agent to Guide Them All: Empowering MLLMs for Vision-and-Language Navigation via Explicit World Representation**|可导航智能体需同时理解高层语义指令和精确空间感知，现有以MLLMs为核心的导航智能体虽具泛化能力，但紧耦合设计限制了性能。本研究提出解耦设计，分离低层空间状态估计与高层语义规划，并引入交互式度量世界表示，维护丰富一致的信息供MLLMs推理决策。此外，通过反事实推理进一步激发MLLMs能力，并确保生成动作的物理有效性。模拟和真实环境实验表明，该方法在R2R-CE和RxR-CE基准上达到了零样本SOTA，并实现了对不同实体（如轮式机器人、无人机）的零样本模拟到真实迁移，验证了该解耦框架作为具身视觉与语言导航的鲁棒通用接口的有效性。|Qi Wu Team|[2602.15400](http://arxiv.org/abs/2602.15400)|null|
|**2026-02-17**|**ActionCodec: What Makes for Good Action Tokenizers**|VLA模型利用VLM的自回归范式展现出色的指令遵循和训练效率，但动作令牌化设计主要侧重重建保真度，忽视其对VLA优化的直接影响，导致缺乏对“优秀动作令牌化器”的理解。本文从VLA优化角度建立了动作令牌化的设计原则，基于信息论洞察，提出最大化时间令牌重叠、最小化词汇冗余、增强多模态互信息和令牌独立性等最佳实践。在此指导下，引入ActionCodec高性能动作令牌化器。实验结果显示，ActionCodec显著提升了训练效率和VLA性能，在LIBERO基准上，未经机器人预训练的SmolVLM2-2.2B结合ActionCodec成功率达95.5%，进一步增强后达到97.4%，刷新了无机器人预训练VLA模型的SOTA。|Jianye Hao Team|[2602.15397](http://arxiv.org/abs/2602.15397)|null|
|**2026-02-17**|**EAA: Automating materials characterization with vision language model agents**|针对复杂实验显微镜工作流程的自动化需求，本文提出了实验自动化代理（EAA），一个由视觉-语言模型驱动的代理系统。EAA集成了多模态推理、工具增强动作和长期记忆，支持自主程序和交互式用户引导测量。系统基于灵活的任务管理器架构，可实现从完全代理驱动到局部LLM查询的工作流程，并提供与模型上下文协议（MCP）双向兼容的工具生态系统。在高级光子源的成像光束线演示中，EAA成功实现了自动化区域板聚焦、自然语言描述的特征搜索和交互式数据采集，验证了其在提高光束线效率、减轻操作负担和降低用户专业知识门槛方面的潜力。|Mathew J. Cherukara Team|[2602.15294](http://arxiv.org/abs/2602.15294)|null|
|**2026-02-16**|**Beyond Imitation: Reinforcement Learning-Based Sim-Real Co-Training for VLA Models**|模拟提供了可扩展且低成本的方式来丰富视觉-语言-动作（VLA）训练，减少对昂贵真实机器人演示的依赖，但多数模拟-现实协同训练方法依赖于监督微调（SFT），将模拟视为静态演示源，未充分利用大规模闭环交互，从而限制了真实世界收益和泛化。针对此问题，本文提出了RL-based sim-real Co-training (RL-Co) 框架，该框架利用交互式模拟同时保留真实世界能力。其设计分为两阶段：首先，通过对真实和模拟演示的混合数据进行SFT来预热策略，然后通过在模拟中进行强化学习来微调策略，并添加辅助的真实世界数据监督损失以锚定策略并缓解灾难性遗忘。在四项真实世界桌面操作任务上，RL-Co在两种VLA架构（OpenVLA和π₀.₅）上均持续优于仅真实世界微调和基于SFT的协同训练，例如OpenVLA的真实世界成功率提升24%，π₀.₅提升20%，同时在未见任务变体上表现出更强的泛化能力和显著提升的真实世界数据效率。|Yu Wang Team|[2602.12628](http://arxiv.org/abs/2602.12628)|null|
|**2026-02-16**|**DM0: An Embodied-Native Vision-Language-Action Model towards Physical AI**|针对传统方法将物理接地视为微调后期的限制，本研究提出了DM0，一个为物理AI设计的具身原生视觉-语言-动作（VLA）框架，旨在通过学习异构数据源从一开始就统一具身操作和导航。该方法采用三阶段流程：预训练阶段对VLM进行大规模统一预训练，整合网页文本、自动驾驶场景和具身交互日志；中训练阶段构建流匹配动作专家并采用混合训练策略；后训练阶段引入具身空间支架策略构建空间思维链（CoT）推理。实验结果表明，DM0在RoboChallenge基准测试的Table30上，无论在专业还是通用设置中均达到了最先进的性能。|Tiancai Wang Team|[2602.14974](http://arxiv.org/abs/2602.14974)|**[link](https://github.com/Dexmal/dexbotic)**|
|**2026-02-16**|**DriveFine: Refining-Augmented Masked Diffusion VLA for Precise and Robust Driving**|针对自动驾驶中扩散式和基于Token的VLA规划器各自存在的模态对齐困难、训练效率低或累积因果错误等问题，本研究提出了DriveFine，一种结合了灵活解码与自我修正能力的掩码扩散VLA模型。该模型设计了一个即插即用的block-MoE，将细化专家无缝注入生成专家，通过明确的专家选择和梯度阻断实现专家解耦。此外，引入混合强化学习策略以鼓励细化专家探索并保持训练稳定性。广泛实验表明，DriveFine在NAVSIM v1、v2和Navhard基准测试中表现出强大的效能和鲁棒性。|Yan Wang Team|[2602.14577](http://arxiv.org/abs/2602.14577)|null|
|**2026-02-16**|**Replanning Human-Robot Collaborative Tasks with Vision-Language Models via Semantic and Physical Dual-Correction**|针对人机协作（HRC）中人类指令模糊性及VLM在物理可行性判断上的局限，本研究提出了一个增强VLM推理的双重校正HRC框架。该框架包含一个内部校正模型在动作执行前验证逻辑一致性和任务可行性，以及一个外部校正模型通过执行后反馈检测并纠正物理失败。模拟研究表明该方法显著提高了成功率，真实世界的人形机器人协作装配实验进一步验证了其在根据人类指令进行交互式重新规划方面的有效性。|Kensuke Harada Team|[2602.14551](http://arxiv.org/abs/2602.14551)|null|
|**2026-02-16**|**TikArt: Aperture-Guided Observation for Fine-Grained Visual Reasoning via Reinforcement Learning**|针对多模态大语言模型（MLLMs）在细粒度视觉推理中关键信息易丢失的问题，本研究引入了TikArt（Thinking Aperture），一种光圈引导的智能体。TikArt通过“思考-光圈-观察”循环，交替进行语言生成和两种光圈动作（Zoom和Segment），将局部视觉线索转化为持久的语言记忆。该方法基于Qwen3-VL-8B构建，并使用AGRPO强化学习算法优化推理策略。实验结果显示，TikArt在多项基准测试中均超越基线模型，并在高分辨率推理中展现出可解释的光圈轨迹。|Lei Zhao Team|[2602.14482](http://arxiv.org/abs/2602.14482)|null|
|**2026-02-16**|**Hierarchical Vision-Language Interaction for Facial Action Unit Detection**|为解决面部动作单元（AU）检测在有限标注数据下学习判别性和可泛化表示的挑战，本研究提出了分层视觉-语言交互AU理解（HiVA）方法。HiVA利用大型语言模型生成丰富的AU文本描述作为语义先验，并通过AU感知动态图模块捕获细粒度和整体视觉-语言关联。此外，它采用分层跨模态注意力架构，包含解耦双重交叉注意力（DDCA）和上下文双重交叉注意力（CDCA），以建立细粒度AU特定交互和建模全局AU间依赖。广泛实验表明HiVA持续超越SOTA方法，并能产生语义上有意义的激活模式。|Cuntai Guan Team|[2602.14425](http://arxiv.org/abs/2602.14425)|null|
|**2026-02-16**|**Multi-Turn Adaptive Prompting Attack on Large Vision-Language Models**|针对多轮越狱攻击在视觉-语言模型（LVLMs）上因视觉输入过于恶意而易被防御的问题，本研究提出了MAPA，一种多轮自适应提示攻击方法。MAPA在每轮中交替进行文本-视觉攻击动作以引发最恶意响应，并在跨轮次通过迭代细化逐步放大响应的恶意程度。这种两级设计使MAPA持续优于现有SOTA方法，在Llama-3.2-Vision-11B-Instruct、GPT-4o-mini等主流LVLMs上，攻击成功率提高了11-35%。|Yiliao Song Team|[2602.14399](http://arxiv.org/abs/2602.14399)|null|
|**2026-02-15**|**WoVR: World Models as Reliable Simulators for Post-Training VLA Policies with RL**|为解决强化学习（RL）在视觉-语言-动作（VLA）模型中因真实世界交互需求巨大而难以直接部署的问题，并克服现有世界模型在想象轨迹中存在的幻觉和长期误差积累，本研究提出了WoVR框架。WoVR通过可控的动作条件视频世界模型提高轨迹稳定性，通过关键帧初始化轨迹（Keyframe-Initialized Rollouts）减少有效误差深度，并通过世界模型-策略协同演化保持策略与模拟器对齐。实验结果表明，WoVR在LIBERO基准和真实机器人操作中显著提升了平均成功率，验证了在明确控制幻觉的情况下，学习到的世界模型可作为实用的RL模拟器。|Dongbin Zhao Team|[2602.13977](http://arxiv.org/abs/2602.13977)|null|
|**2026-02-14**|**Semantic-Contact Fields for Category-Level Generalizable Tactile Tool Manipulation**|通用工具操作需要语义规划和精确物理控制，但现有通用机器人策略缺乏高保真物理基础，而接触感知策略又往往实例特定，难以泛化。大规模真实世界触觉数据难以获取，且软传感器复杂动力学使零样本仿真到真实迁移充满挑战。针对此，本文提出了语义-接触场（SCFields），一种融合视觉语义和密集接触估计的统一3D表示。通过两阶段Sim-to-Real接触学习管道实现：首先在大规模模拟数据上预训练以学习通用接触物理，再通过少量真实数据和伪标签进行微调以对齐传感器特性，从而实现对未见工具的物理泛化。SCFields作为扩散策略的密集观测输入，在刮擦、蜡笔画和剥皮任务中表现出鲁棒的类别级泛化能力，显著优于纯视觉和原始触觉基线。|Yan Wu Team|[2602.13833](http://arxiv.org/abs/2602.13833)|null|
|**2026-02-14**|**MOTIF: Learning Action Motifs for Few-shot Cross-Embodiment Transfer**|视觉-语言-动作（VLA）模型在通用机器人学习方面取得进展，但由于运动学异质性和高昂的数据收集成本，跨具身（cross-embodiment）迁移仍具挑战。现有跨具身策略多依赖共享-私有架构，存在私有参数容量有限和缺乏明确适应机制的问题。为解决这些限制，本文提出了MOTIF框架，旨在通过解耦具身无关的时空模式（称为动作基序）实现高效的少样本跨具身迁移。MOTIF首先通过带有进度感知对齐和具身对抗约束的向量量化学习统一基序，确保时空和跨具身一致性；随后，设计轻量级预测器从实时输入中预测基序，并将其与机器人特定状态融合，指导流匹配策略生成新具身动作。在仿真和真实世界环境中的评估均验证了MOTIF的优越性，少样本迁移成功率显著提升。|Heng Tao Shen Team|[2602.13764](http://arxiv.org/abs/2602.13764)|null|
|**2026-02-14**|**HBVLA: Pushing 1-Bit Post-Training Quantization for Vision-Language-Action Models**|视觉-语言-动作（VLA）模型因其巨大的计算和内存开销，难以部署在资源受限的机器人和边缘平台。尽管权重二值化可提高效率，但现有方法无法弥合二值化与全精度权重之间的分布差距，导致长期闭环执行中量化误差累积并严重降低动作质量。针对此，本文提出了HBVLA，一个VLA定制的二值化框架。HBVLA首先利用策略感知的增强Hessian识别对动作生成关键的权重，然后对非显著权重进行稀疏正交变换以引入低熵中间状态，最后在Harr域中对所有权重进行组式1比特量化。实验结果表明，HBVLA在LIBERO和SimplerEnv上，量化模型分别保留了92.2%和93.6%的全精度性能，并显著优于现有最先进的二值化方法，在真实世界评估中也仅造成微小的成功率下降，展示了在严格硬件约束下的鲁棒部署能力。|Ivor Tsang Team|[2602.13710](http://arxiv.org/abs/2602.13710)|null|
|**2026-02-13**|**Steerable Vision-Language-Action Policies for Embodied Reasoning and Hierarchical Control**|预训练的视觉-语言模型（VLMs）虽能提供丰富的常识先验，但将其有效落地到机器人行为仍具挑战，现有分层方法通过自然语言指令连接VLM与VLA，限制了VLM对低层行为的引导。为增强VLM对低层行为的控制，本文引入“可控策略”（Steerable Policies），即在多抽象层次（如子任务、运动、像素坐标）的丰富合成指令上训练VLA模型。这使得VLM能够通过上下文学习引导这些策略，从而解锁其预训练知识并提升任务泛化能力。广泛的真实世界操作实验表明，与现有基于VLM的VLA和分层基线相比，新方法在泛化和长周期任务中表现更优。|Sergey Levine Team|[2602.13193](http://arxiv.org/abs/2602.13193)|null|
|**2026-02-13**|**UniManip: General-Purpose Zero-Shot Robotic Manipulation with Agentic Operational Graph**|通用机器人操作需要机器人无缝连接高层语义意图与低层物理交互，但现有方法在零样本泛化方面存在不足。针对此问题，本文提出了UniManip框架，其核心是双层代理操作图（AOG），旨在统一语义推理与物理接地。该框架通过高层代理层进行任务编排，低层场景层表示动态状态，实现抽象规划与几何约束的持续对齐，从而支持鲁棒的零样本执行。作为一个动态代理循环，UniManip能从非结构化感知中实例化以物体为中心的场景图，通过安全感知局部规划器参数化无碰撞轨迹，并利用结构化记忆自主诊断和从执行失败中恢复。广泛实验证明，该系统在未见过物体和任务上的零样本成功率分别比VLA和分层基线高22.5%和25.0%，并能直接零样本迁移到移动操作任务。|Ziwei Wang Team|[2602.13086](http://arxiv.org/abs/2602.13086)|**[link](https://henryhcliu.github.io/unimanip)**|
|**2026-02-13**|**Learning Native Continuation for Action Chunking Flow Policies**|动作分块能使视觉-语言-动作（VLA）模型实时运行，但简单的分块执行常导致块边界处的不连续性。现有的实时分块（RTC）方法虽能缓解此问题，但由于其在策略外部，会引起虚假的多模态切换和非内在平滑的轨迹。为此，本文提出了Legato，一种针对基于动作分块流的VLA策略的训练时序延续方法。Legato通过将去噪初始化为已知动作和噪声的调度形混合物，使模型接触部分动作信息，并重塑学习到的流动力学以确保训练和推理在每步引导下保持一致性，同时使用随机调度条件训练以支持不同推理延迟和实现可控平滑性。实验证明，Legato能生成更平滑的轨迹，减少执行时的虚假多模态切换，从而减少犹豫和缩短任务完成时间，在五项操作任务中均比RTC表现更优，轨迹平滑度和任务完成时间均提升约10%。|Yang Gao Team|[2602.12978](http://arxiv.org/abs/2602.12978)|**[link](https://lyfeng001.github.io/Legato/)**|
|**2026-02-13**|**ALOE: Action-Level Off-Policy Evaluation for Vision-Language-Action Model Post-Training**|在真实世界中通过在线强化学习（RL）改进大型视觉-语言-动作（VLA）系统时，价值函数估计是关键，但其通常从混合数据源中收集的轨迹片段进行，这本质上是一个离策略评估问题，而现有工作常采用保守的同策略估计，限制了学习效果。为解决这一问题，本文提出了ALOE（Action-Level Off-Policy Evaluation）框架，用于VLA的后期训练。ALOE采用基于分块的时间差分自举法来评估单个动作序列而非预测最终任务结果，这在稀疏奖励下能更好地将信用归因于关键动作分块，并支持稳定的策略改进。在三项真实世界操作任务上的评估表明，ALOE在不影响执行速度的前提下提高了学习效率，验证了离策略RL在VLA后期训练中可被可靠地重新引入。|Maoqing Yao Team|[2602.12691](http://arxiv.org/abs/2602.12691)|null|
|**2026-02-13**|**SignScene: Visual Sign Grounding for Mapless Navigation**|人类可利用导航标识在陌生环境中无需地图进行导航，本研究旨在使机器人也能利用标识实现开放世界中的无图导航。核心挑战在于解释标识：真实世界标识多样复杂，其抽象语义内容需与局部3D场景进行接地。本文将此形式化为标识接地问题，即把标识上的语义指令映射到对应的场景元素和导航动作。考虑到视觉-语言模型（VLMs）具备所需的语义常识和推理能力但对空间表示敏感，我们提出了SignScene，一种以标识为中心的空间-语义表示方法，它捕获导航相关的场景元素和标识信息，并以有利于VLM有效推理的形式呈现。在包含9种不同环境类型、114个查询的数据集上评估，SignScene达到了88%的接地准确率，显著优于基线方法，并能驱动Spot机器人在真实世界中仅依赖标识进行无图导航。|David Hsu Team|[2602.12686](http://arxiv.org/abs/2602.12686)|null|
|**2026-02-13**|**Xiaomi-Robotics-0: An Open-Sourced Vision-Language-Action Model with Real-Time Execution**|本文介绍了小米机器人0号（Xiaomi-Robotics-0），一个针对高性能、快速且平滑实时执行优化的先进视觉-语言-动作（VLA）模型。该方法的核心在于精心设计的训练配方和部署策略。模型首先在大规模跨实体机器人轨迹和视觉-语言数据上进行预训练，赋予其广泛且泛化的动作生成能力，同时避免灾难性遗忘底层VLM的视觉语义知识。在后期训练中，作者提出了多种技术用于异步执行训练，以解决真实机器人执行时的推理延迟问题。部署时，仔细对齐连续预测动作块的时间步，确保实时执行的连续性和无缝性。广泛的模拟基准测试和两个需要精确灵巧双臂操作的真实机器人任务评估表明，该方法在所有模拟基准上均达到了最先进的性能，并在真实机器人任务上使用消费级GPU实现了高成功率和吞吐量，代码和模型检查点已开源。|Quanyun Zhou Team|[2602.12684](http://arxiv.org/abs/2602.12684)|**[link](https://xiaomi-robotics-0.github.io)**|
|**2026-02-13**|**CRAFT: Adapting VLA Models to Contact-rich Manipulation via Force-aware Curriculum Fine-tuning**|视觉-语言-动作（VLA）模型在处理接触密集型操作任务时面临挑战，因为成功需要精确对齐、稳定接触和处理变形物体，而高熵视觉语言输入与低熵但关键的力信号之间存在不平衡，导致模型过度依赖感知并产生不稳定控制。针对此问题，本文引入CRAFT，一个力感知课程微调框架，该框架集成了一个变分信息瓶颈模块，以在早期训练中调节视觉和语言嵌入，鼓励模型优先处理力信号，然后逐步恢复完整的多模态信息。为实现力感知学习，作者设计了一个同源主从遥操作系统，用于收集各种接触密集型任务中同步的视觉、语言和力数据。真实世界实验表明，CRAFT持续提高了任务成功率，泛化到未见物体和新任务变体，并能有效适应不同VLA架构，从而实现鲁棒且可泛化的接触密集型操作。|Jingtao Sun Team|[2602.12532](http://arxiv.org/abs/2602.12532)|null|
|**2026-02-13**|**AsyncVLA: An Asynchronous VLA for Fast and Robust Navigation on the Edge**|当前机器人基础模型虽泛化能力强，但推理延迟高，导致在动态环境中不安全。针对此问题，本文提出了AsyncVLA异步控制框架，将语义推理与反应性执行解耦。该框架通过远程工作站上的大型基础模型提供高层指导，同时由轻量级板载Edge Adapter高频精细化动作，并通过端到端微调协议和轨迹重加权策略弥合异步流间的域间隙。在面对高达6秒通信延迟的真实视觉导航任务中，AsyncVLA的成功率比现有最佳基线高出40%，成功连接了大型模型的语义智能与边缘机器人所需的实时反应能力。|Sergey Levine Team|[2602.13476](http://arxiv.org/abs/2602.13476)|null|
|**2026-02-13**|**FlowHOI: Flow-based Semantics-Grounded Generation of Hand-Object Interactions for Dexterous Robot Manipulation**|现有视觉-语言-动作（VLA）模型在长周期、接触密集型任务中表现不佳，原因在于缺乏对手-物体交互（HOI）结构的明确表示。为解决此问题，本文提出FlowHOI，一个两阶段流匹配框架，可根据自我中心观察、语言指令和3D高斯泼溅场景重建生成语义明确、时间连贯的HOI序列。该方法将以几何为中心的抓取与以语义为中心的操控解耦，并利用3D场景令牌和运动-文本对齐损失进行语义接地，同时通过从大规模自我中心视频重建HOI轨迹的方法弥补高保真HOI监督的稀缺性。实验结果显示，FlowHOI在GRAB和HOT3D基准测试中实现了最高的动作识别准确率，物理模拟成功率比扩散基线高1.7倍，推理速度提升40倍，并成功在真实机器人上执行了灵巧操作任务。|Xingxing Zuo Team|[2602.13444](http://arxiv.org/abs/2602.13444)|**[link](https://huajian-zeng.github.io/projects/flowhoi/)**|
|**2026-02-12**|**Scaling Verification Can Be More Effective than Scaling Policy Learning for Vision-Language-Action Alignment**|通用机器人理解并执行自然语言指令是长期愿景，尽管视觉-语言-动作（VLA）模型已取得显著进展，但其生成动作仍可能与指令不符。为缩小“意图-动作差距”，本文研究了测试时验证方法。通过分析具身指令遵循的测试时缩放定律，发现联合缩放复述指令和生成动作的数量能更有效地增加测试时样本多样性。在此基础上，提出了CoVer，一种用于VLA对齐的对比验证器，该架构能随计算资源和数据的增加而良好扩展。进一步引入“启动时计算”和分层验证推理流程：在部署时预计算多样化的复述指令，为每条指令重复生成动作候选，然后使用验证器选择最优高层提示和低层动作块。实验表明，相比扩展策略预训练，CoVer在SIMPLER基准上实现了22%的分布内和13%的分布外增益，并在真实世界实验中进一步提高了45%，在PolaRiS基准上任务进度和成功率分别提升了14%和9%。|Marco Pavone Team|[2602.12281](http://arxiv.org/abs/2602.12281)|null|
|**2026-02-12**|**GigaBrain-0.5M*: a VLA That Learns From World Model-Based Reinforcement Learning**|针对现有视觉-语言-动作（VLA）模型在场景理解和未来预测方面的局限性，本研究提出GigaBrain-0.5M*，一个通过世界模型强化学习训练的VLA模型。该模型基于已在大量机器人操作数据上预训练的GigaBrain-0.5，并整合了RAMP（Reinforcement learning via world Model-conditioned Policy）以实现鲁棒的跨任务适应。实验结果表明，RAMP在RECAP基线之上取得了显著的性能提升，在洗衣折叠、箱子包装和咖啡制作等挑战性任务上提升约30%，并在真实部署中展示了可靠的长期执行能力，能够无故障完成复杂的操纵任务。|Zheng Zhu Team|[2602.12099](http://arxiv.org/abs/2602.12099)|**[link](https://gigabrain05m.github.io/)**|
|**2026-02-12**|**VLAW: Iterative Co-Improvement of Vision-Language-Action Policy and World Model**|为提升视觉-语言-动作（VLA）模型性能和可靠性，并解决真实世界数据收集成本高及现有世界模型物理保真度不足的问题，本研究提出一个迭代改进算法。该算法利用少量真实世界试错数据提高世界模型的保真度，然后世界模型生成补充合成数据以改进VLA模型。在真实机器人上的实验表明，该方法使先进VLA模型的成功率相较于基础策略绝对提升39.2%，并且通过生成的合成试错数据训练，又额外提升了11.6%。|Chelsea Finn Team|[2602.12063](http://arxiv.org/abs/2602.12063)|null|
|**2026-02-12**|**HoloBrain-0 Technical Report**|针对基础模型研究与可靠机器人真实部署之间的差距，本研究提出了HoloBrain-0，一个全面的视觉-语言-动作（VLA）框架。其核心是一个新颖的VLA架构，通过明确整合机器人具身先验（如多视角相机参数和运动学描述）来增强3D空间推理并支持多样化具身形态。该设计通过“预训练后微调”范式验证，在多个仿真基准和长时程真实世界操作任务中取得领先结果，且一个高效的0.2B参数变体能支持低延迟部署。研究还全面开源了HoloBrain生态系统，旨在加速研究和实际应用。|Zhizhong Su Team|[2602.12062](http://arxiv.org/abs/2602.12062)|null|
|**2026-02-12**|**When would Vision-Proprioception Policies Fail in Robotic Manipulation?**|针对视觉-本体感受策略在机器人运动过渡阶段视觉模态作用受限、策略倾向于利用本体感受信号导致视觉学习受抑制的问题，本研究提出了梯度调整与阶段引导（GAP）算法。该算法通过利用本体感受信息估计运动过渡阶段的概率，并自适应地调整本体感受梯度的幅值，以实现视觉和本体感受模态的动态协同。综合实验表明，GAP算法能够提升视觉-本体感受策略的鲁棒性和泛化性，适用于模拟和真实环境、单臂和双臂设置，并兼容多种VLA模型。|Di Hu Team|[2602.12032](http://arxiv.org/abs/2602.12032)|null|
|**2026-02-12**|**JEPA-VLA: Video Predictive Embedding is Needed for VLA Models**|针对现有视觉-语言-动作（VLA）模型存在的样本效率低和泛化能力有限问题，本研究发现其根源在于预训练视觉表示在环境理解和策略先验方面的知识不足。通过深入分析，研究指出在视频上预训练的预测嵌入，特别是V-JEPA 2，能更有效地捕捉任务相关时态动态并忽略不可预测因素，从而弥补了现有视觉表示的缺陷。在此基础上，提出了JEPA-VLA，一种将预测嵌入自适应整合到现有VLA中的方法。实验证明，JEPA-VLA在LIBERO、LIBERO-plus、RoboTwin2.0和真实机器人任务等一系列基准测试中均取得了显著性能提升。|Mingsheng Long Team|[2602.11832](http://arxiv.org/abs/2602.11832)|null|
|**2026-02-12**|**ABot-N0: Technical Report on the VLA Foundation Model for Versatile Embodied Navigation**|针对具身导航任务碎片化的问题，本研究引入了ABot-N0，一个统一的视觉-语言-动作（VLA）基础模型，旨在实现点目标、物体目标、指令遵循、兴趣点目标和人员跟踪五大核心任务的“大一统”。该模型采用分层“大脑-动作”架构，利用大型语言模型进行语义推理，并结合流匹配专家生成精确轨迹。为支持大规模学习，研究构建了ABot-N0数据引擎，整理了海量专家轨迹和推理样本。实验结果表明，ABot-N0在7个基准测试中取得了最先进的性能，并能通过集成的Agentic导航系统实现动态真实世界环境中的鲁棒、长时程任务执行。|Mu Xu Team|[2602.11598](http://arxiv.org/abs/2602.11598)|**[link](https://amap-cvlab.github.io/ABot-Navigation/ABot-N0/)**|
|**2026-02-12**|**Scaling World Model for Hierarchical Manipulation Policies**|针对视觉-语言-动作（VLA）模型在域外（OOD）设置下泛化能力不足的问题，本研究引入了一个分层VLA框架VISTA。VISTA利用大规模预训练世界模型的泛化能力实现鲁棒且可泛化的视觉子目标任务分解，其中高层世界模型规划任务分解为带有目标图像的子任务序列，低层VLA策略遵循文本和视觉指导生成动作。这些合成的目标图像为低层策略提供了视觉和物理上的详细指导，使其能够泛化到未见过的物体和新场景。实验结果表明，在世界模型生成的指导下，VISTA在大量OOD场景中显著提升了VLA性能，在novel场景中性能从14%提高到69%。|Xinghang Li Team|[2602.10983](http://arxiv.org/abs/2602.10983)|null|
|**2026-02-12**|**ForeAct: Steering Your VLA with Efficient Visual Foresight Planning**|视觉-语言-动作（VLA）模型将高级语言指令转换为具体可执行动作，在开放世界环境中尤具挑战性。本文提出了Visual Foresight Planning (ForeAct)，这是一种通用高效的规划器，通过想象的未来观察和子任务描述逐步指导VLA。该规划器包含一个高效的预测图像生成模块（在0.33秒内预测高质量未来观察）和一个视觉-语言模型，后者负责推理任务并生成子任务描述。重要的是，ForeAct能够无缝集成到现有VLA中，只需扩充视觉输入而无需修改架构。经过百万级跨具身任务的预训练，预测生成器学习了鲁棒的具身动力学。在包含11项多样化、多步骤真实世界任务的基准测试中，ForeAct实现了87.4%的平均成功率，比基线 $π_0$提高了40.9%，比带有文本子任务指导的$π_0$ 提高了30.3%。|Song Han Team|[2602.12322](http://arxiv.org/abs/2602.12322)|null|
|**2026-02-11**|**H-WM: Robotic Task and Motion Planning Guided by Hierarchical World Model**|针对现有世界模型在长时程机器人规划中累积误差的问题，以及传统符号逻辑世界模型缺乏视觉感知接地的局限性，本研究提出分层世界模型（H-WM）。H-WM在一个统一的双层框架内联合预测逻辑和视觉状态转换，将符号推理的鲁棒性与视觉观察的感知基础相结合。为训练H-WM，研究引入了一个对齐机器人运动与符号状态、动作和视觉观察的机器人数据集。实验证明，分层输出为长时程任务提供了稳定一致的中间指导，有效减轻了误差积累，并在VLA控制策略上展示了该方法的有效性和通用性。|Yingxue Zhang Team|[2602.11291](http://arxiv.org/abs/2602.11291)|null|
|**2026-02-11**|**RISE: Self-Improving Robot Policy with Compositional World Model**|针对视觉-语言-动作（VLA）模型在接触密集和动态操作任务中易受执行偏差影响的脆弱性，以及物理世界中在线强化学习（RL）的限制，本研究提出了RISE，一个通过想象进行机器人强化学习的可扩展框架。其核心是一个组合世界模型，该模型能预测多视角未来并通过进度价值模型评估想象结果，从而为策略改进提供信息丰富的优势。这些组件被整合到一个闭环自改进流水线中，在想象空间中持续生成试错并更新策略。在三个真实世界任务中，RISE相对于现有技术取得了显著性能提升，如在动态砖块分类、背包包装和盒子关闭任务中，绝对性能分别提升超过35%、45%和35%。|Hongyang Li Team|[2602.11075](http://arxiv.org/abs/2602.11075)|**[link](https://opendrivelab.com/kai0-rl/)**|
|**2026-02-11**|**RADAR: Benchmarking Vision-Language-Action Generalization via Real-World Dynamics, Spatial-Physical Intelligence, and Autonomous Evaluation**|针对现有视觉-语言-动作（VLA）模型评估主要局限于仿真或高度受限的真实世界，导致现实差距大、泛化能力差的问题，本研究提出RADAR（Real-world Autonomous Dynamics And Reasoning）基准。RADAR旨在系统评估VLA在真实条件下的泛化能力，集成了物理动力学套件、专门测试空间推理和物理理解的任务，以及基于3D指标的全自主评估流程。通过RADAR对多个先进VLA模型进行审计，发现模型在适度物理动态下性能急剧下降，例如在传感器噪声下3D IoU从0.261下降到0.068，且空间推理能力有限，揭示了模型在真实世界条件下的严重脆弱性，强调了RADAR作为可靠泛化评估基准的必要性。|Guangrun Wang Team|[2602.10980](http://arxiv.org/abs/2602.10980)|null|
|**2026-02-11**|**From Representational Complementarity to Dual Systems: Synergizing VLM and Vision-Only Backbones for End-to-End Driving**|VLA驾驶将语言功能引入端到端规划，但其核心变化除准确性-成本权衡外尚不明确。本文通过RecogDrive进行3-RQ分析，比较VLM和纯视觉骨干网络在相同规划器下的表现。研究发现VLM能引入独特子空间，导致在长尾场景下行为差异，VLM更激进而ViT更保守。基于此，本文提出HybridDriveVLA，通过学习评分器融合两者的轨迹，将PDMS提升至92.10。进一步，DualDriveVLA实现了快慢策略，仅在低置信度时调用VLM，在15%场景中调用VLM即可达到91.00 PDMS，并使吞吐量提高3.2倍。|Yan Wang Team|[2602.10719](http://arxiv.org/abs/2602.10719)|null|
|**2026-02-11**|**Say, Dream, and Act: Learning Video World Models for Instruction-Driven Robot Manipulation**|机器人操作需要预测环境对动作的响应，但现有系统常缺乏此能力，导致效率低下。鉴于VLMs无法明确预测未来状态且现有世界模型存在预测短期或空间不一致帧的问题，本文提出了一种快速且预测性的视频条件动作框架。该方法首先适配鲁棒的视频生成模型以确保未来预测的可靠性，接着通过对抗蒸馏实现快速视频生成，最后训练一个动作模型利用生成视频和真实观测纠正空间错误。实验结果表明，该方法生成的视频预测在时间连贯性和空间准确性方面表现优异，显著提升了具身一致性、空间指代能力和任务完成度。|Yanwei Fu Team|[2602.10717](http://arxiv.org/abs/2602.10717)|null|
|**2026-02-11**|**AugVLA-3D: Depth-Driven Feature Augmentation for Vision-Language-Action Models**|当前VLA模型主要依赖2D图像训练，限制了其在复杂3D环境中的空间理解和动作落地。为解决此问题，本文提出了一个将深度估计集成到VLA模型中的新框架，以丰富3D特征表示。该方法利用VGGT从RGB输入中提取3D几何线索，并引入“动作助手”模块，通过动作先验约束3D表示以确保与下游控制任务的一致性。实验结果表明，所提出的方法不仅增强了几何模糊场景中的感知，还显著提高了动作预测准确性，强调了深度驱动数据增强对弥合2D观测与3D决策差距的潜力。|F. Richard Yu Team|[2602.10698](http://arxiv.org/abs/2602.10698)|null|
|**2026-02-11**|**Improving Medical Visual Reinforcement Fine-Tuning via Perception and Reasoning Augmentation**|强化微调（RFT）在语言模型后训练中展现潜力，但在跨模态、以视觉为中心的医学影像领域应用不足，而该领域需要强大的视觉感知和结构化推理。为此，本文提出了VRFT-Aug，一个专为医学领域设计的视觉强化微调框架。VRFT-Aug通过引入先验知识注入、感知驱动策略优化、医学知情奖励塑造和行为模仿等训练策略来增强感知和推理。实验结果表明，该方法在多个医学数据集上持续优于标准监督微调和RFT基线，并提供了可推广到其他医学图像任务的实用训练启发。|Qicheng Lao Team|[2602.10619](http://arxiv.org/abs/2602.10619)|null|
|**2026-02-11**|**LAP: Language-Action Pre-Training Enables Zero-shot Cross-Embodiment Transfer**|机器人学长期目标是实现零样本部署的通用策略，但现有VLA模型与训练实体紧密耦合。针对此问题，本文提出了语言-动作预训练（LAP），一种将低级机器人动作直接用自然语言表示的方法，使动作监督与预训练VLM的输入-输出分布对齐，无需定制化设计或昂贵标注。基于LAP，本文提出了LAP-3B，实现了在未见机器人上的显著零样本迁移，平均成功率超过50%，比现有VLA模型提升约2倍。此外，LAP还支持高效适应、有利扩展，并通过统一的语言-动作格式在协同训练中获得额外收益。|Anirudha Majumdar Team|[2602.10556](http://arxiv.org/abs/2602.10556)|**[link](https://lap-vla.github.io)**|
|**2026-02-11**|**Found-RL: foundation model-enhanced reinforcement learning for autonomous driving**|强化学习在自动驾驶中面临样本效率低和语义可解释性不足问题，而基础模型（特别是VLM）虽能提供丰富知识，但高推理延迟阻碍其在RL训练中的实时部署。为此，本文提出了Found-RL平台，通过异步批量推理框架将VLM推理与仿真循环解耦，解决了延迟瓶颈。该平台引入了价值边际正则化和优势加权动作指导等监督机制，将VLM建议有效提炼到RL策略中。此外，通过条件对比动作对齐解决CLIP的动态盲区问题，实现密集奖励塑造。Found-RL使得轻量级RL模型在保持实时推理（500 FPS）的同时，达到接近十亿参数VLM的性能。|Sikai Chen Team|[2602.10458](http://arxiv.org/abs/2602.10458)|null|
|**2026-02-10**|**Hardware Co-Design Scaling Laws via Roofline Modelling for On-Device LLMs**|VLA模型在资源受限设备部署中面临LLM骨干网络选择挑战，需平衡准确性与推理延迟及硬件效率。本文提出了硬件协同设计法则，将模型训练损失建模为架构超参数的函数，并通过屋脊线模型表征推理延迟，从而建立了准确性-延迟的直接对应关系。通过对1,942个候选架构进行经验评估和训练，拟合出架构与训练损失的缩放定律。将该定律与延迟建模结合，本文确定了硬件协同设计LLM的帕累托前沿，并将架构搜索公式化为精度与性能的联合优化。结果显示，该方法将架构选择时间从数月缩短到数天，并在相同延迟下，其协同设计的架构在WikiText-2上的困惑度降低了19.42%。|Cheng Deng Team|[2602.10377](http://arxiv.org/abs/2602.10377)|null|
|**2026-02-10**|**ST4VLA: Spatially Guided Training for Vision-Language-Action Models**|大型VLM虽擅长多模态理解，但在具身任务中将指令转化为低级运动动作时表现不足。为此，本文引入了ST4VLA，一个双系统VLA框架，通过空间引导训练使动作学习与VLM中的空间先验对齐。ST4VLA包含两阶段：首先是空间基础预训练，利用大规模和机器人数据通过点、框、轨迹预测为VLM注入可迁移先验；其次是空间引导动作后训练，通过空间提示促使模型生成更丰富的空间先验以指导动作生成。实验表明，ST4VLA显著提升了Google Robot和WidowX Robot的性能，在SimplerEnv上创造了SOTA，并展现出对未见物体、转述指令的更强泛化能力及在真实世界中对扰动的鲁棒性。|Jiangmiao Pang Team|[2602.10109](http://arxiv.org/abs/2602.10109)|null|
|**2026-02-10**|**EgoHumanoid: Unlocking In-the-Wild Loco-Manipulation with Robot-Free Egocentric Demonstration**|人形机器人运动-操作任务面临数据需求高和具身差距挑战。针对此，本文提出了EgoHumanoid框架，首次利用大量以自我为中心的人类演示数据与有限机器人数据共同训练视觉-语言-动作策略，使人形机器人在多样真实环境中执行运动-操作。为弥合人类与机器人间的形态和视点差异，本文引入了包含硬件设计、数据处理的系统对齐流程，并开发了便携式人类数据收集系统。其核心是视点对齐和动作对齐，分别减少视觉域差异和将人类动作映射到机器人动作空间。真实世界实验表明，整合人类演示数据显著优于仅用机器人数据的基线（51%），尤其是在未见环境中。|Li Chen Team|[2602.10106](http://arxiv.org/abs/2602.10106)|**[link](https://opendrivelab.com/EgoHumanoid)**|

<p align=right>(<a href=#updated-on-20260225>back to top</a>)</p>

## Humanoid

|Publish Date|Title|Chinese Summary|Authors|PDF|Code|
|---|---|---|---|---|---|
|**2026-02-24**|**Learning Humanoid End-Effector Control for Open-Vocabulary Visual Loco-Manipulation**|人形机器人在野外进行视觉-运动操作面临末端执行器（EE）控制精度和场景理解泛化性的挑战，而现有模仿学习方法因数据集限制泛化能力不足。本研究提出HERO范式，将大型视觉模型的开放词汇理解能力与模拟训练的强大控制性能相结合。通过设计一个精确的残差感知EE跟踪策略，结合逆运动学、学习神经网络正向模型、目标调整和重新规划。实验证明，该策略将EE跟踪误差降低3.2倍，使系统能在多种真实环境中可靠操作日常物品，验证了其在模拟和真实世界中的有效性。|Saurabh Gupta Team|[2602.16705](http://arxiv.org/abs/2602.16705)|**[link](https://hero-humanoid.github.io/)**|
|**2026-02-24**|**PMG: Parameterized Motion Generator for Human-like Locomotion Control**|现有参考引导的人形机器人控制方法难以适应高级指令接口，且对数据集和校准敏感。本文提出参数化运动生成器（PMG），它基于人类运动结构分析，仅使用紧凑的参数化运动数据和高维控制指令合成参考轨迹。该方法结合模仿学习和基于优化的sim-to-real电机参数识别模块，在人形机器人ZERITH Z1上得到验证。结果表明，PMG能够生成自然、类人的运动，精确响应高维控制输入（包括VR遥操作），并实现了高效、可验证的sim-to-real迁移，为自然可部署的人形机器人控制提供了实用路径。|Houde Liu Team|[2602.12656](http://arxiv.org/abs/2602.12656)|**[link](https://pmg-icra26.github.io/)**|
|**2026-02-24**|**Task-oriented grasping for dexterous robots using postural synergies and reinforcement learning**|当前人形机器人面向任务的抓取方法缺乏端到端解决方案，难以同时考虑下游任务约束和多个物体抓取。为解决此问题，本研究提出一种基于强化学习的抓取方法，优先考虑智能体抓取后的意图。通过从ContactPose数据集提取人类抓取偏好，并利用变分自编码器（VAE）训练手部协同模型模仿人类抓取，进而训练智能体以适应特定任务。实验结果表明，该方法结合了人类抓取行为的数据驱动洞察和强化学习的探索，能够开发出情境感知操作的人形机器人，以促进人机协作。|Plinio Moreno Team|[2602.20915](http://arxiv.org/abs/2602.20915)|null|
|**2026-02-23**|**Humanoid Hanoi: Investigating Shared Whole-Body Control for Skill-Based Box Rearrangement**|人形机器人长时间的箱体重排任务仍具挑战，简单共享全身控制器可能因状态和指令分布变化而降低鲁棒性。本文提出一种基于技能的框架，通过共享的、任务无关的全身控制器（WBC）执行所有技能，提供一致的闭环接口。为解决鲁棒性问题，采用数据聚合程序，通过域随机化下的闭环技能执行来增强共享WBC的训练。在Humanoid Hanoi基准任务上的仿真和物理机器人实验表明，该方法实现了长时间全自主重排，并量化了共享WBC方法相对于非共享基线的优势。|Alan Fern Team|[2602.13850](http://arxiv.org/abs/2602.13850)|**[link](https://osudrl.github.io/Humanoid_Hanoi/)**|
|**2026-02-23**|**Generalizing from References using a Multi-Task Reference and Goal-Driven RL Framework**|从人类动作学习灵巧人形行为存在参考追踪策略脆弱和纯任务驱动强化学习牺牲动作质量的弊端。本研究提出一个统一的多任务强化学习框架，将参考动作作为行为塑造的先验。该框架通过联合训练一个单一的目标条件策略，使其在参考引导的模仿任务和目标条件泛化任务之间实现平衡。实验证明，该方法使策略获得了结构化的类人运动技能，并能适应新目标和初始条件，成功应用于具有挑战性的跑酷场景，实现控制器泛化和长程行为生成。|Farbod Farshidian Team|[2602.20375](http://arxiv.org/abs/2602.20375)|null|
|**2026-02-23**|**Energy-Based Injury Protection Database: Including Shearing Contact Thresholds for Hand and Finger Using Porcine Surrogates**|人形机器人与人交互中的碰撞不可避免，现有能量限制控制方法依赖的EN ISO 10218-2:2025数据集未能覆盖刃口或尖锐物体碰撞。本研究扩展了现有数据集，纳入了无约束碰撞中的剪切接触场景，以评估碰撞角度对伤害结果的影响。通过重新评估猪替代物数据，建立了涵盖多种几何形状和接触类型的能量阈值，形成了首个能量基伤害保护数据库。实验结果表明，剪切接触下伤害少于垂直接触，所建立的数据库有助于开发更安全的能量限制控制器。|Sami Haddadin Team|[2602.20362](http://arxiv.org/abs/2602.20362)|null|
|**2026-02-22**|**Impact-Robust Posture Optimization for Aerial Manipulation**|机器人冲击时状态和指令峰值较大，影响安全性和鲁棒性。本文提出一种新方法，通过刚性冲击模型建立配置依赖的度量，量化冲击前后速度变化，以识别能显著减少冲击峰值的姿态。该问题被表述为基于梯度的运动任务，并嵌入到任务空间逆动力学全身控制器中。实验结果表明，该方法将冲击后机器人配置峰值降低高达51%，成功避免了执行器饱和，并证明运动学冗余对冲击鲁棒性至关重要，能使状态峰值降低达45%。|Antonio Franchi Team|[2602.13762](http://arxiv.org/abs/2602.13762)|null|
|**2026-02-21**|**Habilis- $β$ : A Fast-Motion and Long-Lasting On-Device Vision-Language-Action Model**|当前视觉-语言-动作（VLA）模型评估未能捕捉实际部署所需的高速和长时运行能力。本研究引入“生产力-可靠性平面（PRP）”指标，通过每小时任务数（TPH）和平均干预时间（MTBI）进行连续运行评估。为此，提出了Habilis-β模型，它结合了大规模游戏数据的无语言预训练和循环任务演示的后训练，并利用ESPADA、整流流蒸馏和无分类器引导（CFG）等技术。实验结果显示，Habilis-β在模拟和真实物流任务中均显著超越基线，并在RoboTwin 2.0排行榜上取得最佳性能，验证了其在复杂操作场景下的高效性和鲁棒性。|Theo Taeyeong Kim Team|[2602.18813](http://arxiv.org/abs/2602.18813)|null|
|**2026-02-21**|**RoboCurate: Harnessing Diversity with Action-Verified Neural Trajectory for Robot Learning**|视频生成模型产生的合成数据在机器人学习中面临动作质量不一致的问题，且现有视觉-语言模型难以精确评估物理动作。本研究提出RoboCurate合成机器人数据生成框架，通过在模拟器中回放预测动作并衡量运动一致性来评估和过滤标注动作质量。此外，该方法还通过图像到图像编辑和动作保留的视频到视频传输来增强观测多样性。实验结果表明，RoboCurate生成的数据在GR-1 Tabletop、DexMimicGen和ALLEX人形灵巧操作等任务中，成功率相比仅使用真实数据有显著提升。|Jinwoo Shin Team|[2602.18742](http://arxiv.org/abs/2602.18742)|**[link](https://seungkukim.github.io/robocurate/)**|
|**2026-02-20**|**CLOT: Closed-Loop Global Motion Tracking for Whole-Body Humanoid Teleoperation**|长周期全身人形机器人遥操作面临全局姿态漂移累积问题，现有方法常忽略全局反馈。本文提出CLOT系统，通过高频定位反馈实现闭环全局运动跟踪，在长周期内同步操作员和机器人姿态，实现无漂移模仿。为避免直接奖励导致的激进修正，引入数据驱动的随机化策略，解耦观测轨迹和奖励评估，实现平滑稳定的全局修正，并通过对抗性运动先验抑制不自然行为。仿真和真实实验验证了该策略在高动态运动、高精度跟踪和sim-to-real鲁棒性方面的卓越表现。|Yichao Yan Team|[2602.15060](http://arxiv.org/abs/2602.15060)|null|
|**2026-02-18**|**VIGOR: Visual Goal-In-Context Inference for Unified Humanoid Fall Safety**|人形机器人在杂乱环境中跌倒时面临高能冲击和复杂接触，现有跌倒恢复方法碎片化且泛化性受限。本研究提出一种统一的跌倒安全方法，涵盖跌倒恢复所有阶段，基于人类跌倒姿态的可迁移性和整合感知-运动表示的洞察。通过在平坦和复杂地形上使用稀疏人类演示训练特权教师模型，并将其提炼为仅依赖自我中心深度和本体感受的学生模型。实验结果表明，该方法在模拟和真实Unitree G1人形机器人上实现了多样化非平坦环境下的鲁棒零样本跌倒安全，无需真实世界微调。|Stella X. Yu Team|[2602.16511](http://arxiv.org/abs/2602.16511)|null|
|**2026-02-17**|**Perceptive Humanoid Parkour: Chaining Dynamic Human Skills via Motion Matching**|人形机器人动态运动仍难以匹敌人类敏捷性，尤其在复杂跑酷中缺乏类人表达、长程技能组合与感知决策。本研究提出“感知人形跑酷（PHP）”模块化框架，首先通过特征空间匹配组合原子人类技能成长程运动轨迹，然后训练运动追踪强化学习专家策略，并将其提炼成基于深度感知的多技能学生策略。这种结合感知和技能组合的方法使机器人能自主、上下文感知地决策。在Unitree G1人形机器人上的真实实验验证了框架的有效性，成功展示了动态跑酷技能和对障碍物的闭环适应能力。|C. Karen Liu Team|[2602.15827](http://arxiv.org/abs/2602.15827)|null|
|**2026-02-17**|**MeshMimic: Geometry-Aware Humanoid Motion Learning through 3D Scene Reconstruction**|当前人形机器人运动控制高度依赖昂贵的动作捕捉数据，但这些数据常缺乏几何上下文，导致运动与场景解耦并产生物理不一致。本研究提出MeshMimic框架，通过3D场景重建和具身智能，使机器人直接从视频中学习耦合的“运动-地形”交互。该框架利用3D视觉模型重建人类轨迹及地形，并引入基于运动学一致性的优化算法提取高质量运动数据，结合接触不变重定向方法转移人-环境交互特征。实验结果表明，MeshMimic在多样化地形上实现了鲁棒且动态的性能，证明了使用消费级单目传感器实现复杂物理交互训练的可行性。|Yijie Guo Team|[2602.15733](http://arxiv.org/abs/2602.15733)|null|
|**2026-02-16**|**Replanning Human-Robot Collaborative Tasks with Vision-Language Models via Semantic and Physical Dual-Correction**|人机协作中人类指令常含歧义，导致机器人难以生成可行行为，且视觉-语言模型（VLMs）存在幻觉推理和物理失败预测能力不足的问题。本研究提出一种增强型HRC框架，通过双重校正机制弥补VLM的不足。内部校正模型在执行前验证逻辑一致性和任务可行性，外部校正模型通过执行后反馈纠正物理故障。模拟消融研究和真实世界协作装配任务实验均证明，该框架能提高任务成功率，并支持机器人根据人类指令进行交互式重新规划，验证了其实用性。|Kensuke Harada Team|[2602.14551](http://arxiv.org/abs/2602.14551)|null|
|**2026-02-16**|**AdaptManip: Learning Adaptive Whole-Body Object Lifting and Delivery with Online Recurrent State Estimation**|现有模仿学习方法在人形机器人导航、抓取和递送中对干扰敏感且需要人类演示。本文提出AdaptManip框架，通过强化学习训练鲁棒的全身定位-操作策略，无需人类演示。该框架包含实时对象状态估计器、全身移动与操作控制策略以及基于LiDAR的全局定位器。实验结果显示，AdaptManip在适应性和成功率上显著优于基线方法，尤其在遮挡下仍能准确操作，并在真实人形机器人上实现了完全自主的导航、抓取和递送。|Sehoon Ha Team|[2602.14363](http://arxiv.org/abs/2602.14363)|**[link](https://morganbyrd03.github.io/adaptmanip/)**|
|**2026-02-15**|**WIMLE: Uncertainty-Aware World Models with IMLE for Sample-Efficient Continuous Control**|基于模型的强化学习（MBRL）常因模型误差累积、单峰世界模型和预测过自信而表现不佳。本文引入WIMLE方法，通过将隐式最大似然估计（IMLE）扩展到MBRL框架，学习随机、多峰世界模型，并通过集成和潜在采样估计预测不确定性。训练时，WIMLE根据置信度加权合成转换以稳定学习。在多项连续控制任务中，WIMLE展现了卓越的样本效率，并在多个任务上优于现有基线，特别在Humanoid-run任务中样本效率提高超50%，证实了IMLE驱动的多模态和不确定性感知加权对稳定MBRL的价值。|Ke Li Team|[2602.14351](http://arxiv.org/abs/2602.14351)|**[link](https://openreview.net/forum?id=mzLOnTb3WH)**|
|**2026-02-15**|**ProAct: A Dual-System Framework for Proactive Embodied Social Agents**|具身社交代理在实现需要累积上下文和意图推理的主动社交行为时，面临实时交互的低延迟挑战。本文提出ProAct双系统框架，通过解耦低延迟的“行为系统”和执行长周期社会推理的“认知系统”来解决这一冲突。框架引入流匹配模型，利用ControlNet支持异步意图注入，实现反应式和主动式手势的无缝转换。在物理人形机器人上的用户研究表明，ProAct在感知到的主动性、社交存在感和整体参与度方面显著优于反应式系统。|Libin Liu Team|[2602.14048](http://arxiv.org/abs/2602.14048)|**[link](https://proactrobot.github.io/)**|
|**2026-02-14**|**A Kung Fu Athlete Bot That Can Do It All Day: Highly Dynamic, Balance-Challenging Motion Dataset and Autonomous Fall-Resilient Tracking**|当前人形机器人运动跟踪系统在处理高度动态行为时存在差距，且缺乏统一的跌倒恢复策略。本文构建了KungFuAthlete数据集，包含专业运动员的高动态武术动作，其运动强度远超现有数据集。在此基础上，提出一种新颖的训练范式，使单个策略能联合学习高动态运动跟踪和跌倒恢复，将敏捷执行和稳定性统一在一个框架内。该框架扩展了机器人的能力，使其能够在高动态场景中实现更鲁棒和自主的性能。|Xuesong Li Team|[2602.13656](http://arxiv.org/abs/2602.13656)|null|
|**2026-02-13**|**Adding internal audio sensing to internal vision enables human-like in-hand fabric recognition with soft robotic fingertips**|人类区分织物触感涉及空间-时间力模式和纹理诱导振动的整合，但机器人触觉传感器通常难以同时实现高空间和时间分辨率。本文提出一个系统，通过集成Minsight视觉触觉传感器（测量形变和力）和Minsound振动传感器（捕捉振动）来感知这两种信息。机器人模仿人类评估织物的方式进行主动探索，并通过基于Transformer的方法对20种常见织物进行分类。结果显示，音频传感器对分类性能具有高效用，分类精度最高达97%，并能泛化学习织物弹性、厚度和粗糙度的通用表示。|Katherine J. Kuchenbecker Team|[2602.12918](http://arxiv.org/abs/2602.12918)|null|
|**2026-02-12**|**General Humanoid Whole-Body Control via Pretraining and Fast Adaptation**|通用人形机器人全身控制器面临运动多样性、快速适应和高动态平衡等挑战。本文提出FAST框架，旨在实现快速适应和稳定运动跟踪。FAST引入Parseval-Guided残差策略适应，通过正交性和KL约束学习轻量级delta动作策略，实现对分布外运动的高效适应并减轻灾难性遗忘。同时，提出质心感知控制，整合质心相关观测和目标以增强平衡。在仿真和真实世界的广泛实验表明，FAST在鲁棒性、适应效率和泛化性方面持续优于现有先进基线方法。|Zongqing Lu Team|[2602.11929](http://arxiv.org/abs/2602.11929)|null|
|**2026-02-12**|**HAIC: Humanoid Agile Object Interaction Control via Dynamics-Aware World Model**|为解决人形机器人在非结构化环境中与非完全受控物体进行交互的挑战，尤其是在存在独立动力学、非完整约束、耦合力和遮挡的情况下，本研究提出了HAIC统一框架。其核心贡献在于一个仅通过本体感知历史估计高阶物体状态的动力学预测器，这些预测被投射到静态几何先验上形成动态占用图，使策略能推断盲点中的碰撞边界。HAIC还利用不对称微调，确保在分布变化下鲁棒的状态估计。在人形机器人上的实验表明，HAIC通过主动补偿惯性扰动，在敏捷任务（如滑板和推拉推车）中实现了高成功率，并能预测多个物体动力学，成功执行多物体长时程任务。|Renjing Xu Team|[2602.11758](http://arxiv.org/abs/2602.11758)|**[link](https://haic-humanoid.github.io/)**|
|**2026-02-12**|**Future Mining: Learning for Safety and Security**|针对采矿业向AI驱动的物理信息系统转型中，面临恶劣环境、网络物理威胁及传感器能量限制导致的感知与安全挑战，本研究提出了一个统一的智能安全与安保架构愿景。该架构整合了多模态感知、安全联邦学习、强化学习、DTN通信和能源感知传感。文章介绍了矿工定位、多模态态势感知、后门攻击监控、TrustFed LFD和IoT设备健康监控等五个核心模块，旨在共同解决矿工定位、危险理解、联邦鲁棒性和预测性维护等问题，以构建一个在对抗条件下能保持操作连续性的弹性、可信智能采矿系统。|Sanjay Madria Team|[2602.11472](http://arxiv.org/abs/2602.11472)|null|
|**2026-02-12**|**Humanoid Manipulation Interface: Humanoid Whole-Body Manipulation from Robot-Free Demonstrations**|当前人形机器人全身操作方法受限于硬件物流和复杂奖励工程，导致自主技能有限且通常仅限于受控环境。为解决这些问题，本文提出了Humanoid Manipulation Interface (HuMI)，一个便携高效的框架，用于在各种环境中学习多样化的全身操作任务。HuMI通过便携硬件捕捉丰富的全身运动，实现无机器人数据收集，并利用分层学习流程将人类运动转化为灵巧且可行的人形技能。广泛实验表明，HuMI的数据收集效率比遥操作提高3倍，并在未知环境中取得了70%的成功率，有效提升了人形机器人的泛化操作能力。|Yang Gao Team|[2602.06643](http://arxiv.org/abs/2602.06643)|**[link](https://humanoid-manipulation-interface.github.io)**|
|**2026-02-11**|**ExtremControl: Low-Latency Humanoid Teleoperation with Direct Extremity Control**|针对现有低延迟人形机器人遥操作系统因复杂运动重定向和仅位置PD控制导致的显著延迟问题，本研究提出了ExtremControl框架。该框架通过直接操作人形机器人末端执行器的SE(3)姿态来避免全身重定向，利用笛卡尔空间映射直接将人类运动转换为机器人链节目标，并在底层引入速度前馈控制以支持高响应行为。仿真和真实世界实验验证了其有效性。基于ExtremControl实现的遥操作系统可支持光学动作捕捉和VR运动跟踪，实现了低至50ms的端到端延迟，从而能够进行乒乓球平衡、杂耍和实时回击等高响应行为，显著超越了现有工作中200ms的延迟限制。|Chuang Gan Team|[2602.11321](http://arxiv.org/abs/2602.11321)|**[link](https://owenowl.github.io/extremcontrol)**|
|**2026-02-11**|**APEX: Learning Adaptive High-Platform Traversal for Humanoid Robots**|针对深度强化学习在类人机器人高平台跨越中易收敛于高冲击、不安全的跳跃式方案的不足，本研究提出了APEX系统，实现了感知型、基于攀爬的高平台穿越。APEX结合多种地形条件行为，并引入了广义棘轮进度奖励机制来学习接触密集型、目标导向的操纵。通过对LiDAR生成的高程图进行数据增强和过滤，减少了sim-to-real感知差距。最终将六种技能提炼成单一策略。实验结果表明，在Unitree G1机器人上实现了对0.8米平台（腿长约114%）的零样本sim-to-real穿越，并展现出鲁棒的适应性和平滑稳定的多技能转换。|Ding Zhao Team|[2602.11143](http://arxiv.org/abs/2602.11143)|**[link](https://apex-humanoid.github.io/)**|
|**2026-02-11**|**Towards Learning a Generalizable 3D Scene Representation from 2D Observations**|为解决现有神经辐射场方法难以直接应用于机器人操作且需要场景特定微调的问题，本研究提出了一种可泛化的神经辐射场方法。该模型在全局工作空间坐标系中构建占用表示，使其能直接应用于机器人操作，并集成了灵活的源视图，能泛化到未见过的物体排列，无需场景特定微调。在类人机器人上进行的验证实验表明，该模型在40个真实场景上训练后实现了26毫米的重建误差，包括被遮挡区域，证明了其推断完整3D占用信息的能力超越了传统立体视觉方法。|Stefan Wermter Team|[2602.10943](http://arxiv.org/abs/2602.10943)|null|
|**2026-02-11**|**MOSAIC: Bridging the Sim-to-Real Gap in Generalist Humanoid Motion Tracking and Teleoperation with Rapid Residual Adaptation**|鉴于通用人形运动追踪器在模拟中表现优异，但在实际硬件持续遥操作时易受接口和动力学误差影响，本文提出了开源全栈系统MOSAIC。该系统首先通过强化学习在多源运动库上训练面向遥操作的通用运动追踪器，采用自适应重采样和强调世界坐标系运动一致性的奖励。为弥合模拟到真实世界的接口差距，MOSAIC通过快速残差适应，使用少量接口特定数据训练一个接口特定策略，并通过加性残差模块将其蒸馏到通用追踪器中，优于传统微调方法。实验结果（包括系统消融、分布外基准测试和真实机器人实验）证明，MOSAIC在实际延迟和噪声下能实现稳健的离线运动回放和在线长周期遥操作。|Alois Knoll Team|[2602.08594](http://arxiv.org/abs/2602.08594)|null|
|**2026-02-11**|**MotionWeaver: Holistic 4D-Anchored Framework for Multi-Humanoid Image Animation**|鉴于现有角色图像动画方法难以泛化到涉及多样人形形式、复杂交互和频繁遮挡的多人场景，本文提出了MotionWeaver框架。该框架引入统一的运动表示，提取与身份无关的运动并明确绑定到角色，以泛化到多样人形并扩展到多人场景。同时，提出了整体4D锚定范式，构建共享4D空间融合运动与视频潜空间，并通过分层4D级别监督强化交互和遮挡处理。为支持此研究，构建了46小时多人视频数据集和300视频基准。定量和定性实验结果表明，MotionWeaver在自建基准上达到SOTA，并能有效泛化至复杂多人场景。|Weizhan Zhang Team|[2602.13326](http://arxiv.org/abs/2602.13326)|null|
|**2026-02-10**|**EgoHumanoid: Unlocking In-the-Wild Loco-Manipulation with Robot-Free Egocentric Demonstration**|针对人形机器人动作操作对数据需求高，而现有方法未能充分利用人类演示数据，且存在人机体现差异的问题，本文提出了EgoHumanoid框架。该框架首次利用大量自我中心人类演示和少量机器人数据共同训练视觉-语言-动作策略，使人形机器人能在多样真实世界环境中执行动作操作。通过硬件设计到数据处理的系统对齐流水线，包括视图对齐和动作对齐，成功弥合了人机之间的形态和视角差异。广泛的真实世界实验表明，整合无机器人自我中心数据相比仅机器人基线性能显著提升51%，尤其是在未见过的环境中，且分析揭示了行为有效迁移及扩展人类数据的潜力。|Li Chen Team|[2602.10106](http://arxiv.org/abs/2602.10106)|**[link](https://opendrivelab.com/EgoHumanoid)**|
|**2026-02-10**|**Humanoid Factors: Design Principles for AI Humanoids in Human Worlds**|随着人形机器人开始与人类共享空间，传统人因工程需要扩展，不仅考虑人类因素，也要考虑人形机器人因素。当前人形机器人带来了人类行为、沟通和社会存在的期望，重塑了可用性、信任和安全。本文引入“人形机器人因素”框架，围绕物理、认知、社会和伦理四大支柱，指导人形机器人开发，使其能有效与人类共存和协作，并表征了人机能力间的重叠与差异。通过评估真实人形机器人控制算法，该框架揭示了传统机器人任务指标如何忽视关键人类认知和交互原则，为设计、评估和管理持续人机共存提供了基础性框架。|Lixiao Huang Team|[2602.10069](http://arxiv.org/abs/2602.10069)|null|
|**2026-02-10**|**TeleGate: Whole-Body Humanoid Teleoperation via Gated Expert Selection with Motion Prior**|针对人形机器人实时全身遥操作中，现有方法通过知识蒸馏将多专家策略整合，常导致高动态运动性能下降的挑战，本文提出了TeleGate统一遥操作框架。其核心思想是训练一个轻量级门控网络，根据本体感知状态和参考轨迹实时动态激活领域特定专家策略，从而保留其完整能力，避免知识蒸馏的性能损失。此外，引入基于VAE的运动先验模块，从历史观测中提取未来运动意图，实现预期控制。在模拟和Unitree G1机器人上的实验表明，TeleGate仅需2.5小时训练数据，即在跑步、跌倒恢复和跳跃等多样动态运动中实现了高精度实时遥操作，显著优于基线方法。|Rongyun Cao Team|[2602.09628](http://arxiv.org/abs/2602.09628)|null|
|**2026-02-09**|**Characteristics, Management, and Utilization of Muscles in Musculoskeletal Humanoids: Empirical Study on Kengoro and Musashi**|当前肌肉骨骼人形机器人研究中，对其生物仿生结构固有的多样属性及其管理利用方式缺乏统一讨论。本研究基于作者团队开发的Kengoro和Musashi机器人，将肌肉骨骼结构特征归纳为冗余性、独立性、各向异性、可变力臂和非线性弹性五大属性。文章进一步探讨了这些属性组合所带来的优势与劣势，并重点讨论了身体图式学习、反射控制、肌肉分组及身体图式适应等机制。最后，研究阐述了通过集成系统实现运动的实践，并展望了未来的研究挑战。|Masayuki Inaba Team|[2602.08518](http://arxiv.org/abs/2602.08518)|null|
|**2026-02-09**|**Learning Human-Like Badminton Skills for Humanoid Robots**|人形机器人实现羽毛球等高强度运动的类人表现面临巨大挑战，尤其是在运动学模仿与功能性、物理感知击打之间难以兼顾自然风格。为解决此问题，本文提出了Imitation-to-Interaction渐进式强化学习框架，旨在使机器人从“模仿者”进化为“击球手”。该方法通过人类数据建立运动先验，蒸馏到模型化状态表示中，并利用对抗性先验稳定动力学，同时引入流形扩展策略以应对稀疏的专家演示。实验结果显示，该框架在仿真中掌握了多样羽毛球技能，并首次实现了类人羽毛球技能从仿真到真实机器人的零样本迁移，展示了物理世界中的优雅和精准打击。|Peng Lu Team|[2602.08370](http://arxiv.org/abs/2602.08370)|null|
|**2026-02-07**|**VividFace: Real-Time and Realistic Facial Expression Shadowing for Humanoid Robots**|人形机器人面部表情实时模仿对于实现逼真、情感丰富的人机交互至关重要，但现有方法常因离线推理和细节捕捉不足而难以同时达到实时性和逼真性。为解决这些局限，本文提出了VividFace，一个实时且逼真的人形机器人面部表情阴影系统。该系统通过优化模仿框架X2CNet++，并引入特征适应训练策略，显著增强了表情表现力；同时，通过视频流兼容推理管线和基于异步I/O的工作流，实现了高效的实时模仿。广泛的真实世界演示验证了VividFace在0.05秒内模仿人类表情并生成生动人形面部的实用能力，且能泛化至多种面部配置。|Yang Zhang Team|[2602.07506](http://arxiv.org/abs/2602.07506)|null|
|**2026-02-07**|**TextOp: Real-time Interactive Text-Driven Humanoid Robot Motion Generation and Control**|现有的人形机器人全身控制器在灵活性和自主性方面存在局限，难以实现实时和交互式驱动。为解决这一问题，本文提出了TextOp，一个实时文本驱动的人形运动生成与控制框架，支持流式语言指令和即时修改。TextOp采用两级架构：高级运动扩散模型根据文本生成短时域轨迹，低级运动跟踪策略则在机器人上执行这些轨迹。广泛的真实机器人实验和离线评估表明，TextOp实现了即时响应、平滑全身运动和精确控制，在舞蹈、跳跃等复杂行为中展现出自由形式的意图表达和流畅过渡。|Xuelong Li Team|[2602.07439](http://arxiv.org/abs/2602.07439)|**[link](https://text-op.github.io/)**|
|**2026-02-07**|**Bridging Speech, Emotion, and Motion: a VLM-based Multimodal Edge-deployable Framework for Humanoid Robots**|为解决多数人形机器人缺乏协调的语音、面部表情和手势，以及在设备上自主运行的需求，本文提出了SeM²，一个基于视觉语言模型的框架。SeM²通过多模态感知模块捕捉用户上下文，结合思维链推理规划响应，并利用语义序列对齐机制确保言语内容与物理表达的精确时间协调，从而实现情感一致的多模态交互。研究实现了云端及边缘部署版本，其中边缘版本通过知识蒸馏高效运行。综合评估显示，SeM²在自然度、情感清晰度和模态一致性方面显著优于单模态基线，推动了社交表达性人形机器人在多样现实环境中的应用。|Miao Li Team|[2602.07434](http://arxiv.org/abs/2602.07434)|null|
|**2026-02-06**|**Cerebellar-Inspired Residual Control for Fault Recovery: From Inference-Time Adaptation to Structural Consolidation**|针对机器人策略在真实世界部署中常遇到的训练后故障，且不便重新训练的问题，本文提出了一种推理时、受小脑启发的残差控制框架。该框架通过在线校正动作增强冻结的强化学习策略，无需修改基础策略参数即可实现故障恢复。它实例化了小脑核心原理，如高维模式分离、并行残差路径和局部误差驱动可塑性，并通过保守的元适应调节残差权限。实验结果表明，在MuJoCo基准测试中，该框架在执行器、动力学和环境扰动下，对HalfCheetah-v5和Humanoid-v5在适度故障下性能显著提升，并在严重故障下表现出优雅的性能下降。|Amit Ranjan Trivedi Team|[2602.07227](http://arxiv.org/abs/2602.07227)|null|
|**2026-02-06**|**DynaRetarget: Dynamically-Feasible Retargeting using Sampling-Based Trajectory Optimization**|将人类运动重定向到人形机器人控制策略并确保其动态可行性是一项挑战。本文介绍了DynaRetarget，一个将人类运动重定向到人形控制策略的完整流程。其核心是新颖的基于采样的轨迹优化（SBTO）框架，该框架能将不完善的运动学轨迹细化为动态可行的运动，并通过逐步推进优化范围来处理长时域任务。DynaRetarget在重定向数百个人形-物体演示中取得了比现有技术更高的成功率，并能泛化到不同物体属性的场景，为生成大规模人形局部操作轨迹合成数据集提供了可能。|Majid Khadiv Team|[2602.06827](http://arxiv.org/abs/2602.06827)|null|
|**2026-02-06**|**ECO: Energy-Constrained Optimization with Reinforcement Learning for Humanoid Walking**|为解决仿人机器人节能稳定运动中现有方法需大量超参数调优且易导致次优策略的问题，本研究提出了ECO（Energy-Constrained Optimization）约束强化学习框架。该框架将能量指标明确定义为不等式约束，通过拉格朗日法强制执行能量消耗和参考运动约束，从而实现稳定、对称且节能的行走。ECO在仿真和真实机器人（BRUCE）上的实验结果表明，在保持鲁棒行走性能的同时，其能耗显著低于MPC、标准RL及其他SOTA约束RL方法，实现了仿人机器人节能运动的实质性进步。|Yao Su Team|[2602.06445](http://arxiv.org/abs/2602.06445)|null|
|**2026-02-06**|**Now You See That: Learning End-to-End Humanoid Locomotion from Raw Pixels**|为解决基于视觉的仿人机器人鲁棒运动中模拟到真实差距产生的感知噪声和多样地形下学习目标冲突的挑战，本研究提出了一个端到端的视觉驱动运动框架。为实现鲁棒的sim-to-real迁移，该框架开发了高保真深度传感器模拟，并提出了视觉感知行为蒸馏方法。为适应多样地形，引入了地形特定奖励塑形，并结合多评论家和多判别器学习。在配备不同立体深度摄像头的两个仿人机器人平台上，该策略展现出在多样环境中的鲁棒性能，能处理极端挑战和精细任务，包括双向长期楼梯遍历。|Zongwu Xie Team|[2602.06382](http://arxiv.org/abs/2602.06382)|null|

<p align=right>(<a href=#updated-on-20260225>back to top</a>)</p>
