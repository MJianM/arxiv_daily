## Updated on 2026.02.19

## Categories

- [Manipulation](#manipulation)
- [World Model](#world-model)
- [VLM](#vlm)
- [VLA](#vla)
- [Humanoid](#humanoid)

## Manipulation

|Publish Date|Title|Chinese Summary|Authors|PDF|Code|
|---|---|---|---|---|---|
|**2026-02-18**|**BPP: Long-Context Robot Imitation Learning by Focusing on Key History Frames**|许多机器人任务需要关注过去观测历史，但现有策略常因虚假相关性导致泛化失败，问题源于训练期间对历史空间覆盖的限制。为解决此问题，本研究提出大图策略（Big Picture Policies, BPP），通过视觉-语言模型检测到的一组最小有意义关键帧进行条件化。通过将多样化轨迹映射到紧凑的任务相关事件集，BPP显著减少了训练与部署间的分布偏移，同时不牺牲表达能力。在四项真实世界和三项模拟任务上的评估显示，BPP在真实世界评估中成功率比最佳对比方法高70%，证实了其有效性。|Aviral Kumar Team|[2602.15010](http://arxiv.org/abs/2602.15010)|null|
|**2026-02-18**|**Learning Humanoid End-Effector Control for Open-Vocabulary Visual Loco-Manipulation**|针对人形机器人对野外任意物体的视觉定位-操作任务中末端执行器控制精度和场景泛化理解能力受限的问题，本研究提出HERO范式。该方法融合大型视觉模型的泛化能力与模拟训练的控制性能，设计了一种残差感知末端执行器跟踪策略，结合逆运动学、神经前向模型、目标调整和重新规划，将跟踪误差降低3.2倍。在此基础上，构建模块化系统，并利用开放词汇大型视觉模型实现强大的视觉泛化。实验结果表明，该系统能在多种真实世界环境中可靠操作不同高度表面的各种日常物体，并通过模拟和真实世界测试验证了其有效性。|Saurabh Gupta Team|[2602.16705](http://arxiv.org/abs/2602.16705)|**[link](https://hero-humanoid.github.io/)**|
|**2026-02-18**|**VIGOR: Visual Goal-In-Context Inference for Unified Humanoid Fall Safety**|鉴于人形机器人在复杂环境中跌倒恢复的挑战（如高能量冲击、复杂接触、视角变化），且现有方法通常将跌倒安全割裂处理或缺乏视觉信息，本研究提出一种统一的跌倒安全方法。该方法基于人类跌倒姿态的可迁移性和感知-运动表征的整合，通过在平坦和模拟复杂地形上使用稀疏人类演示训练特权教师模型，并将其提炼为仅依赖自我中心深度和本体感受的学生模型。学生模型通过匹配结合目标姿态和局部地形的上下文潜在表征来学习反应。模拟和真实Unitree G1人形机器人上的实验结果表明，该方法在多样化的非平坦环境中实现了鲁棒的零样本跌倒安全，无需真实世界微调。|Stella X. Yu Team|[2602.16511](http://arxiv.org/abs/2602.16511)|null|
|**2026-02-18**|**RoboGene: Boosting VLA Pre-training via Diversity-Driven Agentic Framework for Real-World Task Generation**|为解决通用机器人操作因缺乏多样化真实世界交互数据而受阻，以及现有任务策划方法不可扩展或易产生物理上不可行指令的问题，本研究引入RoboGene框架。该框架是一个代理系统，旨在自动化生成单臂、双臂和移动机器人的多样化、物理合理操作任务。它集成了多样性驱动采样、自我反思机制和人机循环精炼三大核心组件。通过大规模定量分析和真实世界实验，结果表明RoboGene显著优于现有基础模型，且经其预训练的VLA模型展现出更高的成功率和更强的泛化能力，凸显了高质量任务生成的重要性。|Jian Tang Team|[2602.16444](http://arxiv.org/abs/2602.16444)|null|
|**2026-02-17**|**Learning to Retrieve Navigable Candidates for Efficient Vision-and-Language Navigation**|针对基于LLM的视觉-语言导航（VLN）在未知环境中决策效率低下和稳定性不足的问题，本研究提出了一个检索增强框架，旨在提高效率和稳定性，且无需修改或微调LLM。该方法在情景和步骤两个层面引入检索：情景级嵌入检索器提供任务先验上下文示例，步进级候选检索器在LLM推理前剪枝不相关方向以减少动作模糊性和提示复杂性。这两个模块轻量、模块化且独立训练。在R2R基准上的实验结果显示，该方法在成功率、Oracle成功率和SPL方面均有所提升，消融研究进一步证实了各检索模块的互补效益，表明检索增强是提升LLM-based VLN的有效策略。|Lina Yao Team|[2602.15724](http://arxiv.org/abs/2602.15724)|null|
|**2026-02-17**|**Selective Perception for Robot: Task-Aware Attention in Multimodal VLA**|针对机器人VLA模型中静态融合多视图视觉输入导致的计算开销和噪声问题，本研究提出一种动态信息融合框架，以提高VLA模型的效率和鲁棒性。该方法引入轻量级自适应路由架构，实时分析文本提示和腕部摄像头输入，预测多视图与任务的相关性，并有条件地衰减低信息效用视图的计算。为高效获取训练数据，建立了VLM驱动的自动化标注流程。真实世界机器人操作实验结果表明，所提方法在推理效率和控制性能上均显著优于现有VLA模型，验证了动态信息融合在资源受限实时机器人控制环境中的有效性。|Soo-Chul Lim Team|[2602.15543](http://arxiv.org/abs/2602.15543)|null|
|**2026-02-17**|**Feasibility-aware Imitation Learning from Observation with Multimodal Feedback**|模仿学习框架通过演示者手持界面学习机器人控制策略时，常因演示与机器人物理特性差异而面临数据不含机器人动作及演示动作对机器人不可行的问题。为解决这些局限，本研究提出可行性感知从观察中行为克隆（FABCO）方法。FABCO结合从观察中行为克隆和可行性估计，利用机器人动力学模型评估演示动作的可复现性，并将估计的可行性用于多模态反馈和可行性感知策略学习，以改进演示并学习鲁棒策略。在两项任务中与15名参与者的实验结果表明，FABCO将模仿学习性能提高了3.2倍以上。|Takamitsu Matsubara Team|[2602.15351](http://arxiv.org/abs/2602.15351)|null|
|**2026-02-16**|**PhyScensis: Physics-Augmented LLM Agents for Complex Physical Scene Arrangement**|针对自动生成交互式3D环境时，现有工作常忽略物体间物理关系（如接触、支撑）导致场景不逼真复杂，以及高密度、复杂支撑和准确物理建模的挑战，本研究提出了PhyScensis框架。该框架是一个基于LLM代理和物理引擎的系统，用于生成高复杂度的物理合理场景配置。它由LLM代理提出资产、求解器实现谓词和反馈机制组成。通过概率编程和互补启发式方法，框架保持对文本描述和数值参数的强大可控性。实验结果表明，该方法在场景复杂度、视觉质量和物理精度方面优于现有方法，为机器人操作提供了统一的复杂物理场景布局生成管道。|Chuang Gan Team|[2602.14968](http://arxiv.org/abs/2602.14968)|null|
|**2026-02-16**|**Affordance Transfer Across Object Instances via Semantically Anchored Functional Map**|传统示教学习（LfD）收集物理演示数据效率低下且难以扩展，而从人类视频中学习则面临在几何差异大的物体实例间泛化交互的挑战。本研究提出语义锚定功能图（SemFM）框架，旨在从单一视觉演示中跨物体传输功能属性。该方法从图像重建的粗糙网格开始，识别物体间语义对应的功能区域，选择互斥语义锚点，并利用功能图在表面传播约束以获得稠密、语义一致的对应关系。实验表明，SemFM能在合成物体类别和真实机器人操作任务中以适度计算成本实现精确的功能属性传输，适用于实际机器人感知-行动流程。|Weiming Zhi Team|[2602.14874](http://arxiv.org/abs/2602.14874)|null|
|**2026-02-16**|**DriveFine: Refining-Augmented Masked Diffusion VLA for Precise and Robust Driving**|针对自动驾驶VLA模型中，基于扩散的规划器存在模态对齐困难、训练低效和泛化受限，而基于令牌的规划器面临累积因果误差和不可逆解码的问题，本研究提出DriveFine。这是一种结合灵活解码和自校正能力的掩码扩散VLA模型。其核心是一个新颖的即插即用block-MoE，通过无缝注入细化专家于生成专家之上，并在推理时显式选择、训练时梯度阻断实现专家解耦。此外，设计混合强化学习策略以鼓励细化专家探索并维持训练稳定性。在NAVSIM v1/v2和Navhard基准上进行的大量实验证明，DriveFine展现出强大的有效性和鲁棒性。|Yan Wang Team|[2602.14577](http://arxiv.org/abs/2602.14577)|null|
|**2026-02-16**|**RoboSolver: A Multi-Agent Large Language Model Framework for Solving Robotic Arm Problems**|为解决机器人操纵器问题的自动化分析与求解，本研究提出了一种基于LLM和VLM的智能多智能体框架。该框架能够接受文本和视觉输入，并自动执行正逆运动学、速度/加速度计算、3D仿真以及仿真环境中的运动控制。实验结果表明，与原始模型相比，该框架在文本输入下结合GPT-4o在正运动学计算中准确率高达0.97，视觉输入下结合GPT-4o和Gemini 2.5 Pro VLM准确率达0.93，并且在涵盖多种机器人任务的综合测试中取得了0.97的准确率，显著优于现有基线。|Alireza Taheri Team|[2602.14438](http://arxiv.org/abs/2602.14438)|null|
|**2026-02-16**|**A Soft Wrist with Anisotropic and Selectable Stiffness for Robust Robot Learning in Contact-rich Manipulation**|针对非结构化环境中接触丰富的操作任务对机器人学习的鲁棒性挑战，以及现有软末端执行器变形受限或驱动复杂的问题，本研究引入了CLAW（Compliant Leaf-spring Anisotropic soft Wrist）软腕机构。CLAW采用简单的板簧和旋转关节设计，实现了大的6自由度变形和可调的各向异性刚度。通过模仿学习的实验评估，CLAW在插销任务中取得了76%的成功率，显著优于Fin Ray和刚性夹具，并在高精度装配等任务中展现了处理复杂接触场景的潜力。|Masashi Hamaya Team|[2602.14434](http://arxiv.org/abs/2602.14434)|null|
|**2026-02-16**|**AdaptManip: Learning Adaptive Whole-Body Object Lifting and Delivery with Online Recurrent State Estimation**|为实现类人机器人自主完成集成导航、物体举升和递送任务，克服传统模仿学习方法对干扰的脆弱性，本研究提出了AdaptManip框架。该框架通过强化学习训练鲁棒的运动操作策略，包含实时对象状态估计器、全身基础策略和基于LiDAR的全局定位器。所有组件均在仿真中训练并零样本部署到真实硬件。实验证明，AdaptManip在适应性和成功率上显著优于基线方法，并且即使在遮挡下，准确的对象状态估计也能提高操作性能，成功在真实世界中实现了类人机器人的全自主任务执行。|Sehoon Ha Team|[2602.14363](http://arxiv.org/abs/2602.14363)|**[link](https://morganbyrd03.github.io/adaptmanip/)**|
|**2026-02-15**|**GRAIL: Goal Recognition Alignment through Imitation Learning**|针对现有目标识别方法依赖最优策略表示而难以准确捕捉智能体真实（可能次优）行为的问题，本研究提出了通过模仿学习进行目标识别对齐（GRAIL）方法。该方法利用模仿学习和逆强化学习，直接从演示轨迹中为每个候选目标学习策略，并通过单次前向传播进行目标识别。实验结果表明，GRAIL在存在系统偏差、次优和嘈杂行为的情况下，F1分数相比基线方法有显著提升（最高超过0.5），同时在完全最优设置下也保持竞争力，有助于构建更鲁棒的智能体目标解释模型。|Reuth Mirsky Team|[2602.14252](http://arxiv.org/abs/2602.14252)|null|
|**2026-02-15**|**Learning Part-Aware Dense 3D Feature Field for Generalizable Articulated Object Manipulation**|为解决铰接物体操作中跨对象泛化的挑战，尤其是在3D空间中理解功能部件的难度，本研究提出了Part-Aware 3D Feature Field (PA3FF)。PA3FF是一种新型的、具有部件感知能力的密集3D特征，通过大规模标注数据集和对比学习训练，能从点云中预测连续3D特征场。在此基础上，结合模仿学习提出了Part-Aware Diffusion Policy (PADP)。实验证明，PA3FF在模拟和真实世界操作任务中性能优于多种2D和3D表示，并能泛化到对应学习和分割等下游任务，为机器人操作提供了通用的基础特征。|Hao Dong Team|[2602.14193](http://arxiv.org/abs/2602.14193)|**[link](https://pa3ff.github.io)**|
|**2026-02-15**|**RoboAug: One Annotation to Hundreds of Scenes via Region-Contrastive Data Augmentation for Robotic Manipulation**|针对机器人学习在多样化未见场景中泛化能力不足，且现有方法过度依赖大规模预训练或完美目标检测的挑战，本研究提出了RoboAug生成式数据增强框架。该框架仅需单张图像的边界框标注，利用预训练生成模型进行语义数据增强，并整合即插即用的区域对比损失以聚焦任务相关区域。在UR-5e、AgileX和Tien Kung 2.0三台机器人上超过3.5万次实验表明，RoboAug在未见场景下的泛化能力显著超越现有基线，成功率分别从低基线提高至0.47、0.60和0.67，有效提升了真实世界操作任务的成功率。|Jian Tang Team|[2602.14032](http://arxiv.org/abs/2602.14032)|null|
|**2026-02-15**|**WoVR: World Models as Reliable Simulators for Post-Training VLA Policies with RL**|为克服强化学习在机器人VLA模型中因大量真实世界交互需求而难以直接部署，以及世界模型模拟器在长周期想象轨迹中易产生幻觉和误差累积的问题，本研究提出了WoVR框架。WoVR通过可控的动作条件视频世界模型提高轨迹稳定性，利用关键帧初始化轨迹减少有效误差深度，并通过世界模型-策略协同演化保持对齐。实验结果显示，WoVR在LIBERO基准和真实机器人操作任务中实现了稳定的长周期想象轨迹和有效的策略优化，平均成功率在LIBERO上提升了29.3个百分点，在真实机器人上提升了30.0个百分点，证明了在有效控制幻觉的情况下，学习到的世界模型可作为实用的强化学习模拟器。|Dongbin Zhao Team|[2602.13977](http://arxiv.org/abs/2602.13977)|null|
|**2026-02-14**|**Enabling Option Learning in Sparse Rewards with Hindsight Experience Replay**|针对分层强化学习框架（如MOC）在稀疏奖励多目标环境中表现不佳，特别是在对象操作任务中难以发现与对象的交互策略问题，本研究首先提出了MOC-HER，将回溯经验回放（HER）集成到MOC中。在此基础上，为更有效处理对象操作任务，进一步引入了Dual Objectives Hindsight Experience Replay (2HER)，通过同时生成对象最终状态目标和智能体效应器位置目标，奖励智能体与对象的交互和任务完成。实验结果表明，MOC-2HER在机器人操作环境中的成功率高达90%，远高于MOC和MOC-HER的不足11%，验证了双目标重标记策略的有效性。|Gabriel de Oliveira Ramos Team|[2602.13865](http://arxiv.org/abs/2602.13865)|null|
|**2026-02-14**|**Mean Flow Policy with Instantaneous Velocity Constraint for One-step Action Generation**|针对流基策略在强化学习中表达能力与计算负担的权衡问题，本研究提出了一种新的生成式策略函数——平均速度策略（MVP）。MVP通过建模平均速度场实现最快的一步动作生成，并引入瞬时速度约束（IVC）以确保高表达能力。理论上证明IVC作为关键边界条件可提高学习精度和策略表达力。实验结果表明，MVP在Robomimic和OGBench的多个机器人操作任务中取得了最先进的成功率，并在训练和推理速度上显著优于现有流基策略基线。|Shengbo Eben Li Team|[2602.13810](http://arxiv.org/abs/2602.13810)|null|
|**2026-02-14**|**Gaussian Sequences with Multi-Scale Dynamics for 4D Reconstruction from Monocular Casual Videos**|为解决从单目日常视频中进行四维（4D）动态场景重建的ill-posed问题，本研究基于真实世界动态的多尺度规律性，设计了多尺度动态机制以分解复杂运动场。在此基础上，提出了具有多尺度动态的高斯序列，通过多级运动组合构建动态3D高斯表示，显著减轻了重建歧义并促进物理合理性。同时，结合视觉基础模型的多模态先验提供补充监督，进一步约束解空间并提高重建保真度。实验证明，该方法在动态新视角合成任务中，在基准和真实世界操作数据集上均显著优于现有方法，实现了从单目视频中准确且全局一致的4D重建。|Lei Sun Team|[2602.13806](http://arxiv.org/abs/2602.13806)|null|
|**2026-02-14**|**MOTIF: Learning Action Motifs for Few-shot Cross-Embodiment Transfer**|尽管视觉-语言-动作（VLA）模型推动了通用机器人学习，但由于运动学异构性以及收集足够真实世界示范数据进行微调的高成本，跨具身（cross-embodiment）迁移仍然充满挑战。现有跨具身策略通常依赖共享-私有架构，其私有参数容量有限且缺乏明确的适应机制。为解决这些局限性，本文提出了MOTIF框架，旨在实现高效的小样本跨具身迁移，它将具身无关的时空模式（称为动作基序）与异构动作数据解耦。具体而言，MOTIF首先通过带有进度感知对齐和具身对抗约束的矢量量化学习统一的基序，以确保时间和跨具身一致性。然后，设计一个轻量级预测器从实时输入预测这些基序，并将其与机器人特定状态融合，以指导流匹配策略在新的具身上生成动作。模拟和真实世界环境的评估均验证了MOTIF的优越性，在小样本迁移场景中显著优于强基线，模拟中提升6.5%，真实世界中提升43.7%。|Heng Tao Shen Team|[2602.13764](http://arxiv.org/abs/2602.13764)|null|
|**2026-02-14**|**HybridFlow: A Two-Step Generative Policy for Robotic Manipulation**|现有的机器人操作策略受推理延迟限制，缺乏足够的实时环境交互能力。尽管流匹配等更快的生成方法正逐步取代扩散方法，但其精度仍难以满足机器人操作的严格要求。本文关注MeanFlow作为流匹配的单步变体虽然快速但在动作生成精度上的不足。为平衡推理速度和生成质量，本文提出了HybridFlow，这是一种具有2-NFE（函数评估次数）的三阶段方法，包括MeanFlow模式下的全局跳转、用于分布对齐的ReNoise以及ReFlow模式下的局部细化。该方法利用MeanFlow单步生成的快速优势，同时以最少的生成步骤确保动作精度。真实世界实验表明，HybridFlow在成功率上比16步扩散策略高出15-25%，并将推理时间从152毫秒缩短到19毫秒（8倍加速，约52赫兹）；在未见颜色OOD抓取和可变形物体折叠任务上分别达到了70.0%和66.3%的成功率。这些结果表明HybridFlow是一种实用的低延迟方法，能增强机器人操作策略的真实世界交互能力。|Yide Liu Team|[2602.13718](http://arxiv.org/abs/2602.13718)|null|
|**2026-02-14**|**Symmetry-Aware Fusion of Vision and Tactile Sensing via Bilateral Force Priors for Robotic Manipulation**|机器人插入任务需要精密的、富接触的交互，仅凭视觉难以解决。尽管触觉反馈具有直观价值，但现有研究表明，朴素的视觉-触觉融合往往未能持续提供改进。为解决此问题，本文提出了一种用于视觉-触觉融合的跨模态Transformer（CMT），它通过结构化的自注意力与交叉注意力机制整合腕部摄像头观测和触觉信号。为稳定触觉嵌入，本文进一步引入了物理信息正则化，鼓励双边力平衡，反映了人类运动控制的原理。在TacSL基准上的实验表明，带有对称正则化的CMT实现了96.59%的插入成功率，超越了朴素和门控融合基线，并与“腕部+接触力”的优越配置（96.09%）非常接近。这些结果突出表明：触觉感知对于精确对齐不可或缺，以及经过物理信息正则化强化的原则性多模态融合，能够充分发挥视觉和触觉的互补优势，在现实感知条件下接近最优性能。|Tao Yu Team|[2602.13689](http://arxiv.org/abs/2602.13689)|null|
|**2026-02-14**|**Hierarchical Audio-Visual-Proprioceptive Fusion for Precise Robotic Manipulation**|现有机器人操作方法主要依赖视觉和本体感受，在部分可观测的真实世界环境中难以推断接触相关的交互状态。而声学线索能自然编码丰富的接触动态，但在多模态融合中却未被充分利用，且大多数融合方法错误地假设模态作用均一。为实现基于声学信息的精确机器人操作，本文提出一种分层表示融合框架，逐步整合音频、视觉和本体感受。该方法首先将视觉和本体感受表示条件化于声学线索，然后明确建模高阶跨模态交互以捕捉模态间的互补依赖。融合后的表示被扩散策略用于直接从多模态观测生成连续机器人动作。在真实世界机器人操作任务（如倒液体和开柜门）上的广泛实验表明，该方法持续优于现有最先进的多模态融合框架，尤其是在声学线索提供视觉无法轻易获得的任务相关信息时。此外，通过互信息分析解释了音频线索在机器人操作中的作用。|Peng Liu Team|[2602.13640](http://arxiv.org/abs/2602.13640)|null|
|**2026-02-13**|**Imitating What Works: Simulation-Filtered Modular Policy Learning from Human Videos**|通过观看人类视频学习操作技能，有望为机器人学习提供大规模数据的新来源。然而，人类视频在学习抓取后动作方面提供了强信号，但在学习抓取行为方面用处较小，特别是对于没有类似人手的机器人而言，任意稳定的抓取通常不兼容任务。为解决这一挑战，本文提出了Perceive-Simulate-Imitate (PSI) 框架，用于使用经过模拟中成对抓取-轨迹过滤处理的人类视频运动数据训练模块化操作策略。这一模拟步骤通过抓取适用性标签扩展了轨迹数据，从而能够监督学习面向任务的抓取能力。真实世界实验表明，该框架可以无需任何机器人数据高效学习精确操作技能，与简单使用抓取生成器相比，性能显著更鲁棒。|Wei-Chiu Ma Team|[2602.13197](http://arxiv.org/abs/2602.13197)|null|
|**2026-02-13**|**UniManip: General-Purpose Zero-Shot Robotic Manipulation with Agentic Operational Graph**|针对现有机器人操作方法在零样本泛化方面的不足，即端到端VLA模型缺乏精度而传统规划器语义刚性问题，本文提出了UniManip框架。该框架基于双层Agentic Operational Graph (AOG)，通过高层Agentic层进行任务编排和低层Scene层表示动态状态，实现语义推理与物理接地的统一，并以动态智能体循环方式主动实例化场景图、规划无碰撞轨迹并自主恢复失败。实验结果表明，UniManip在未见对象和任务上展现出鲁棒的零样本能力，成功率显著高于现有VLA和分层基线，且支持从固定基座到移动操作的零样本迁移。|Ziwei Wang Team|[2602.13086](http://arxiv.org/abs/2602.13086)|**[link](https://henryhcliu.github.io/unimanip)**|
|**2026-02-13**|**How Swarms Differ: Challenges in Collective Behaviour Comparison**|针对群体行为分析中数值特征集通常缺乏通用性且难以定量衡量行为相似性的问题，本研究深入探讨了特征集对集体行为的影响。我们从现有群体机器人学工作中筛选出特征集和相似性度量，并评估了它们在特定行为背景外的鲁棒性。研究发现，特征集和相似性度量的相互作用决定了区分相似行为群体的有效性，并提出了一种基于自组织图的方法来识别特征空间中行为难以区分的区域。|Jonas Kuckling Team|[2602.13016](http://arxiv.org/abs/2602.13016)|null|
|**2026-02-13**|**SafeFlowMPC: Predictive and Safe Trajectory Planning for Robot Manipulators with Learning-based Policies**|面对机器人日益融入日常生活对灵活性和实时反应能力的需求，以及学习方法缺乏安全保证和优化方法泛化能力不足的挑战，本文提出了SafeFlowMPC框架。该框架结合了流匹配与在线优化，旨在融合学习和优化方法的优势，并通过次优模型预测控制公式，实时确保操作安全性。在KUKA 7自由度机械臂上的真实世界实验（包括抓取和人机交接任务）中，SafeFlowMPC展现了强大的性能。|Andreas Kugi Team|[2602.12794](http://arxiv.org/abs/2602.12794)|null|
|**2026-02-13**|**Scaling Single Human Demonstrations for Imitation Learning using Generative Foundational Models**|鉴于模仿学习中收集机器人演示数据的困难以及人类演示到机器人转移的挑战，本文提出了Real2Gen框架，仅通过单个人类演示来训练操作策略。Real2Gen从人类演示中提取关键信息并传输到模拟环境，利用可编程专家智能体生成无限量的训练数据来学习流匹配策略。实验结果表明，Real2Gen平均成功率提高了26.6%，并且由于训练数据的丰富性，训练出的策略具有更强的泛化能力，实现了纯模拟训练策略的零样本真实世界部署。|Abhinav Valada Team|[2602.12734](http://arxiv.org/abs/2602.12734)|null|
|**2026-02-13**|**$\mathcal{X}$-KD: General Experiential Knowledge Distillation for Large Language Models**|针对大语言模型知识蒸馏（KD）中，现有方法常忽视教师模型原始学习环境的问题，本文提出了Experiential Knowledge Distillation ($\mathcal{X}$-KD) 框架。受经验学习理论和逆强化学习启发，$\mathcal{X}$-KD采用Approximated Variational Reward Imitation Learning (AVRIL) 框架，联合建模教师的原始奖励函数并执行策略蒸馏，使学生模型能在教师的原始学习环境中学习。实验证明，$\mathcal{X}$ -KD在抽象摘要、机器翻译和算术推理任务上均优于基线方法，并实现了更好的性能-多样性权衡和数据效率。|Yuyu Yuan Team|[2602.12674](http://arxiv.org/abs/2602.12674)|null|
|**2026-02-13**|**PMG: Parameterized Motion Generator for Human-like Locomotion Control**|为解决人形机器人运动中，现有全身体参考引导方法对高级命令接口适应性差、对数据和校准敏感等实际挑战，本文提出了Parameterized Motion Generator (PMG)。PMG是一种基于人类运动结构分析的实时运动生成器，通过紧凑的参数化运动数据和高维控制命令合成参考轨迹，并结合模仿学习流水线和仿真到现实电机参数识别模块。实验证明，该集成系统能生成自然、类人运动，精确响应高维控制输入（如VR远程操作），并实现高效、可验证的仿真到现实迁移。|Houde Liu Team|[2602.12656](http://arxiv.org/abs/2602.12656)|null|
|**2026-02-13**|**Real-to-Sim for Highly Cluttered Environments via Physics-Consistent Inter-Object Reasoning**|为解决从单视图观测重建物理有效3D场景时，现有方法常忽略物理约束导致无效状态，进而影响下游模拟可靠性的问题，本文提出了一种新颖的物理约束Real-to-Sim管道。该管道能够从单视图RGB-D数据重建物理一致的3D场景，其核心是一个可微分优化管道，通过接触图建模空间依赖，并利用可微分刚体模拟联合优化物体姿态和物理属性。实验结果表明，重建场景具有高物理保真度，能忠实复现真实世界接触动力学，从而实现稳定可靠的接触密集型操作。|Jun Ma Team|[2602.12633](http://arxiv.org/abs/2602.12633)|**[link](https://physics-constrained-real2sim.github.io)**|
|**2026-02-13**|**FlowHOI: Flow-based Semantics-Grounded Generation of Hand-Object Interactions for Dexterous Robot Manipulation**|近期视觉-语言-动作（VLA）模型能够生成看似合理的末端执行器运动，但在长程、富接触的任务中常常失败，因为缺乏对手-物体交互（HOI）结构的显式表示。为解决此问题，本文提出了FlowHOI，一个两阶段流匹配框架，它能根据第一人称观测、语言指令和3D高斯飞溅（3DGS）场景重建，生成语义接地、时间连贯的HOI序列，包括手部姿态、物体姿态和手-物体接触状态。该框架将以几何为中心的抓取与以语义为中心的操作解耦，后者通过紧凑的3D场景令牌进行条件化，并采用运动-文本对齐损失来语义化生成的交互。为解决高保真HOI监督数据稀缺的问题，本文引入了一个重建流水线，从大规模第一人称视频中恢复对齐的手-物体轨迹和网格，为鲁棒生成提供了HOI先验。FlowHOI在GRAB和HOT3D基准上实现了最高的动作识别精度和比最强扩散基线高1.7倍的物理模拟成功率，同时推理速度提升了40倍。此外，通过将生成的HOI表示重定向到真实机器人执行流程，本文在四个灵巧操作任务上验证了真实机器人执行的可行性。|Xingxing Zuo Team|[2602.13444](http://arxiv.org/abs/2602.13444)|**[link](https://huajian-zeng.github.io/projects/flowhoi/)**|
|**2026-02-12**|**FAIL: Flow Matching Adversarial Imitation Learning for Image Generation**|鉴于流匹配模型后训练与模仿学习的数学等同性，以及监督微调无法纠正策略漂移而偏好优化成本高昂的问题，本文提出了Flow Matching Adversarial Imitation Learning (FAIL) 框架。该框架通过对抗训练最小化策略与专家之间的散度，无需明确奖励或成对比较，并推导出了FAIL-PD和FAIL-PG两种算法。实验证明，FAIL在仅使用少量演示数据的情况下，能在提示遵循和美学基准上取得竞争性性能，并能有效泛化至离散图像和视频生成，同时作为鲁棒正则化器减轻奖励欺骗。|Weidi Xie Team|[2602.12155](http://arxiv.org/abs/2602.12155)|null|
|**2026-02-12**|**GigaBrain-0.5M*: a VLA That Learns From World Model-Based Reinforcement Learning**|针对传统VLA模型在场景理解和未来预测上的局限性，本研究提出了GigaBrain-0.5M*，一个基于世界模型强化学习的VLA模型。该模型在预训练的GigaBrain-0.5基础上，通过RAMP（Reinforcement leArning via world Model-conditioned Policy）进一步整合了世界模型强化学习，以实现鲁棒的跨任务适应性。实验结果表明，RAMP在洗衣折叠、箱子包装和意式浓缩咖啡制作等复杂任务中，相较于RECAP基线性能提升了约30%，并且在实际部署中展示了可靠的长期执行能力，能够无故障完成复杂操作任务。|Zheng Zhu Team|[2602.12099](http://arxiv.org/abs/2602.12099)|**[link](https://gigabrain05m.github.io/)**|
|**2026-02-12**|**Affordance-Graphed Task Worlds: Self-Evolving Task Generation for Scalable Embodied Learning**|鉴于在真实世界中训练机器人策略成本高昂且难以扩展，而现有生成模拟方法难以生成逻辑连贯的长时任务并处理动态物理不确定性，本研究提出了Affordance-Graphed Task Worlds (AGT-World) 框架。该框架能够根据真实世界观测自主构建交互式模拟环境和相应的机器人任务策略，通过将任务空间形式化为结构化图实现复杂目标的精确分层分解，并引入结合视觉-语言模型推理和几何验证的混合反馈自进化机制来完善策略。广泛实验证明，AGT-World在成功率和泛化能力上显著优于现有方法，实现了提议、执行和修正的自改进循环，以支持可扩展的机器人学习。|Changshui Zhang Team|[2602.12065](http://arxiv.org/abs/2602.12065)|null|
|**2026-02-12**|**HoloBrain-0 Technical Report**|为了弥合基础模型研究与可靠的真实世界机器人部署之间的差距，本研究引入了HoloBrain-0，一个全面的视觉-语言-动作 (VLA) 框架。该框架的核心是一个新颖的VLA架构，明确融入了机器人本体先验信息（如多视图相机参数和运动学描述），以增强3D空间推理并支持多样化的本体。通过“预训练-后训练”范式进行验证，该系统在RoboTwin 2.0、LIBERO和GenieSim等模拟基准测试中取得了最先进的成果，并在长时程真实世界操作任务中表现出色。值得注意的是，其高效的0.2B参数变体能与大得多的基线媲美，并支持低延迟的设备部署。为加速研究和实际应用，HoloBrain生态系统已完全开源。|Zhizhong Su Team|[2602.12062](http://arxiv.org/abs/2602.12062)|null|
|**2026-02-12**|**When would Vision-Proprioception Policies Fail in Robotic Manipulation?**|针对视觉-本体感受策略在复杂任务中泛化能力不一致的问题，本研究发现，在机器人运动转换的子阶段，视觉模态的作用有限，策略倾向于更简洁的本体感受信号，抑制了视觉学习。为此，我们提出了梯度调整与阶段引导 (GAP) 算法，通过利用本体感受估计运动转换阶段的概率，并据此自适应地调节本体感受梯度的幅度，从而实现视觉与本体感受的动态协作。综合实验表明，GAP算法在模拟和真实世界环境、单臂和双臂设置以及不同模型类型中均适用，并能形成鲁棒且可泛化的视觉-本体感受策略。|Di Hu Team|[2602.12032](http://arxiv.org/abs/2602.12032)|null|
|**2026-02-12**|**Accelerating Robotic Reinforcement Learning with Agent Guidance**|强化学习在机器人操作中面临严重的样本效率问题，而现有人机协作 (HIL) 方法虽能加速训练，但受限于可扩展性、操作员疲劳和不一致的人类专业知识。为解决此问题，本研究提出了Agent-guided Policy Search (AGPS) 框架，通过多模态智能体取代人工监督者，实现训练流程自动化。其核心思想是将智能体视为语义世界模型，注入内在价值先验来结构化物理探索，并利用可执行工具通过纠正性路点和空间约束提供精确指导。实验证明，AGPS在样本效率方面优于HIL方法，从而实现了无劳动力的可扩展机器人学习路径。|Yaodong Yang Team|[2602.11978](http://arxiv.org/abs/2602.11978)|null|
|**2026-02-12**|**Robot-DIFT: Distilling Diffusion Features for Geometrically Consistent Visuomotor Control**|本研究认为机器人操作泛化性的瓶颈在于当前视觉骨干网络与闭环控制物理需求之间的结构性不匹配，尤其在于现有模型缺乏精细的几何敏感性。鉴于生成扩散模型内在地编码了几何依赖性，但其随机性和延迟阻碍了直接应用，我们提出了Robot-DIFT框架。该框架通过流形蒸馏 (Manifold Distillation) 将冻结的扩散教师模型蒸馏到确定性空间-语义特征金字塔网络 (S2-FPN) 中，从而在保持生成模型丰富几何先验的同时，确保了时间稳定性、实时执行和抗漂移鲁棒性。在DROID数据集上的预训练结果表明，Robot-DIFT在几何一致性和控制性能上均优于领先的判别式基线，验证了视觉学习方式对机器人行为能力的关键影响。|Georgia Chalvatzaki Team|[2602.11934](http://arxiv.org/abs/2602.11934)|null|
|**2026-02-12**|**JEPA-VLA: Video Predictive Embedding is Needed for VLA Models**|现有VLA模型在机器人操作中仍面临样本效率低和泛化能力有限的问题，本研究认为这与预训练视觉表示在环境理解和策略先验方面知识不足有关。通过深入分析，我们发现现有VLA中常用的视觉表示未能有效捕获关键任务相关信息及诱导有效策略先验，而通过视频预训练的预测嵌入，特别是V-JEPA 2，能够灵活地处理不可预测因素并编码任务相关的时间动态。基于此，我们提出了JEPA-VLA，一种将预测嵌入自适应集成到现有VLA的简单有效方法。实验证明，JEPA-VLA在LIBERO、LIBERO-plus、RoboTwin2.0和真实机器人任务等基准测试中均取得了显著的性能提升。|Mingsheng Long Team|[2602.11832](http://arxiv.org/abs/2602.11832)|null|
|**2026-02-12**|**Clutt3R-Seg: Sparse-view 3D Instance Segmentation for Language-grounded Grasping in Cluttered Scenes**|针对杂乱环境中3D实例分割在遮挡、有限视角和噪声掩码下的挑战，本研究提出了Clutt3R-Seg，一种用于语言引导抓取的零样本鲁棒3D实例分割流水线。该方法的核心思想是引入语义线索的分层实例树，通过跨视图分组和条件替换，将噪声掩码作为信息线索，从而抑制过分割和欠分割，产生视图一致的掩码和鲁棒的3D实例。为应对多阶段任务中的场景变化，该方法还引入了一致性感知更新机制。在合成和真实世界数据集及真实机器人上的验证表明，Clutt3R-Seg在杂乱和稀疏视图场景中持续优于现有最先进基线，尤其在重度杂乱序列中表现出超过2.2倍的性能提升。|Ayoung Kim Team|[2602.11660](http://arxiv.org/abs/2602.11660)|null|
|**2026-02-12**|**ViTaS: Visual Tactile Soft Fusion Contrastive Learning for Visuomotor Learning**|针对现有机器人操作中视觉与触觉信息融合方法在遮挡场景下效果不佳、未能充分利用两种模态互补性且集成机制多为直接拼接的问题，本研究提出了ViTaS框架。该框架旨在结合视觉和触觉信息指导智能体行为，并引入了软融合对比学习（Soft Fusion Contrastive Learning）以及一个CVAE模块，以更好地利用视觉-触觉表示中的对齐和互补性。在12个模拟环境和3个真实世界环境中的实验验证表明，ViTaS显著优于现有基线，证明了其在利用多模态信息方面的有效性。|Huazhe Xu Team|[2602.11643](http://arxiv.org/abs/2602.11643)|null|
|**2026-02-12**|**EasyMimic: A Low-Cost Framework for Robot Imitation Learning from Human Videos**|鉴于大规模真实世界数据采集成本高昂阻碍了机器人模仿学习，尤其对低成本家用机器人而言，本研究提出了EasyMimic框架。该框架是一个低成本、可复制的解决方案，使机器人能通过标准RGB相机捕获的人类视频演示快速学习操作策略。其方法首先从视频中提取3D手部轨迹，并利用动作对齐模块将其映射到低成本机器人的夹持器控制空间；为弥合人机领域差距，引入了简单的手部视觉增强策略，并通过协同训练方法在处理过的人类数据和少量机器人数据上微调模型。实验证明，EasyMimic在LeRobot平台上在多种操作任务中取得了高性能，显著减少了对昂贵机器人数据采集的依赖，为智能机器人进入家庭提供了实用途径。|Qin Jin Team|[2602.11464](http://arxiv.org/abs/2602.11464)|null|
|**2026-02-12**|**Scaling World Model for Hierarchical Manipulation Policies**|视觉-语言-动作（VLA）模型在通用机器人操作中虽有前景，但在有限真实机器人数据下，其在分布外（OOD）场景中的泛化能力仍脆弱。本研究提出VISTA，一个分层视觉-语言-动作框架，利用大规模预训练世界模型的泛化能力实现鲁棒且可泛化的视觉子目标任务分解。该框架将世界模型作为高层规划器来生成带有目标图像的子任务序列，VLA作为低层执行器遵循指导生成动作。实验结果表明，与原始文本目标相比，合成的目标图像提供了更具视觉和物理细节的指导，使低层策略能泛化到新物体和场景，将VLA在OOD场景中的性能从14%提升至69%。|Xinghang Li Team|[2602.10983](http://arxiv.org/abs/2602.10983)|null|
|**2026-02-12**|**Self-Refining Vision Language Model for Robotic Failure Detection and Reasoning**|针对机器人故障推理中，真实世界故障的复杂性及丰富推理标签获取成本高昂的问题，本文提出了ARMOR框架。该框架将故障检测和推理建模为一个多任务自细化过程，模型通过迭代预测检测结果和自然语言推理，并从大规模稀疏二元标签和少量丰富推理标注的异构监督中学习。实验结果表明，ARMOR在故障检测率上比现有方法提升高达30%，在LLM模糊匹配分数测量的推理能力上提升高达100%，展现了对异构监督和开放式推理的鲁棒性。|Yesh Dattatreya Team|[2602.12405](http://arxiv.org/abs/2602.12405)|null|
|**2026-02-11**|**Human Preference Modeling Using Visual Motion Prediction Improves Robot Skill Learning from Egocentric Human Video**|当前从第一视角人类视频中学习机器人的方法，在度量视觉状态长期价值时存在假设限制，且在跨实体和环境时需要迁移学习到的价值函数。本研究提出一种新方法，通过预测后续图像中跟踪点的运动来建模人类偏好，并将奖励函数定义为机器人行为中预测与观察到的物体运动的一致性。随后，利用改进的Soft Actor Critic (SAC) 算法（通过10次机器人演示初始化）来估计此奖励的价值函数，并在机器人上优化策略。结果显示，该方法在真实机器人上有效，并且在模拟和真实机器人上的多项任务中，其学习到的策略与现有工作相当或超越。|Christopher G. Atkeson Team|[2602.11393](http://arxiv.org/abs/2602.11393)|null|
|**2026-02-11**|**MolmoSpaces: A Large-Scale Open Ecosystem for Robot Navigation and Manipulation**|机器人大规模部署要求对日常情境具备鲁棒性，但现有基准测试缺乏足够多样化的场景。本研究引入了MolmoSpaces，一个支持机器人策略大规模基准测试的开放生态系统，包含超过23万个多样化室内环境和13万个丰富注释的物体资产，且与模拟器无关。同时设计了MolmoSpaces-Bench基准套件。实验证明，MolmoSpaces-Bench具有强大的模拟到真实关联性，验证了新型零样本策略的优越性，并揭示了对提示措辞、初始关节位置和相机遮挡的关键敏感性，为机器人学习研究提供了可扩展的基础。|Ranjay Krishna Team|[2602.11337](http://arxiv.org/abs/2602.11337)|null|
|**2026-02-11**|**YOR: Your Own Mobile Manipulator for Generalizable Robotics**|随着机器人学习的发展，低成本机器人平台日益增多，但移动操作的最佳外形仍然是待解问题。本研究推出YOR，一个开源、低成本的移动操纵器，集成了全向基座、伸缩式垂直升降器和带抓手的双臂，以实现全身移动和操作。其设计强调模块化、易于组装和经济性（物料清单成本低于1万美元）。YOR在需要协调全身控制、双臂操作和自主导航的任务中展现出其能力，以远低于现有平台的成本提供了具有竞争力的移动操作研究功能。|Zichen Jeff Cui Team|[2602.11150](http://arxiv.org/abs/2602.11150)|null|
|**2026-02-11**|**ABot-M0: VLA Foundation Model for Robotic Manipulation with Action Manifold Learning**|在异构硬件上构建通用具身智能体是机器人学的一大挑战，面临数据碎片化、表示不一致和训练目标不匹配等问题。本研究提出了ABot-M0框架，它通过建立系统的数据整理管道，同时优化模型架构和训练策略，将异构原始数据转换为统一、高效的表示。通过构建大规模UniACT数据集，统一预训练提高了跨平台和任务的知识迁移和泛化能力。为提升动作预测效率和稳定性，引入了动作流形学习（AML）。此外，框架通过双流机制实现模块化感知，整合VLM语义与几何先验和多视角输入。实验结果表明各组件独立且具有累加效益。|Mu Xu Team|[2602.11236](http://arxiv.org/abs/2602.11236)|**[link](https://amap-cvlab.github.io/ABot-Manipulation/)**|
|**2026-02-11**|**OSIL: Learning Offline Safe Imitation Policies with Safety Inferred from Non-preferred Trajectories**|离线安全模仿学习面临在缺乏每时间步安全成本或奖励信息的演示数据中学习安全和奖励最大化策略的难题。本研究针对此问题提出了一种新颖的离线安全模仿学习算法OSIL，它通过非首选轨迹推断安全性。OSIL将安全策略学习表述为约束马尔可夫决策过程（CMDP），并通过推导奖励最大化目标的下界并学习一个估计非首选行为可能性的成本模型来重新构建CMDP问题，从而无需明确的安全成本和奖励标注。实验证明，OSIL能学习到更安全的策略，满足成本约束而不降低奖励性能，优于现有基线方法。|Balaraman Ravindran Team|[2602.11018](http://arxiv.org/abs/2602.11018)|null|
|**2026-02-11**|**Towards Learning a Generalizable 3D Scene Representation from 2D Observations**|现有方法在相机坐标系中构建表示，限制了其在机器人操作中的直接应用。本研究引入了一种可泛化的神经辐射场方法，用于根据以机器人自我为中心的观察来预测3D工作空间占用。该模型在全局工作空间框架中构建占用表示，使其直接适用于机器人操作，并能集成灵活的源视图，无需场景特定微调即可泛化到未见物体布局。在人形机器人上的实验证明，该模型在40个真实场景上训练后，实现了26毫米的重建误差（包括遮挡区域），验证了其推断完整3D占用（超越传统立体视觉方法）的能力。|Stefan Wermter Team|[2602.10943](http://arxiv.org/abs/2602.10943)|null|
|**2026-02-11**|**Semi-Supervised Cross-Domain Imitation Learning**|跨域模仿学习（CDIL）通过领域知识迁移加速策略学习，但在专家数据收集成本高昂时尤为重要。现有CDIL方法或依赖监督（不稳定），或无监督（不稳健）。本研究提出了半监督CDIL（SS-CDIL）设置及其首个具有理论依据的算法。该方法仅使用少量目标专家演示和未标记的不完善轨迹等离线数据。为解决域差异，提出一种新颖的跨域损失函数来学习域间状态-动作映射，并设计自适应权重函数以平衡源域和目标域知识。实验表明，该方法在MuJoCo和Robosuite上始终优于基线，实现了最小监督下稳定且数据高效的策略学习。|Ping-Chun Hsieh Team|[2602.10793](http://arxiv.org/abs/2602.10793)|null|
|**2026-02-11**|**Say, Dream, and Act: Learning Video World Models for Instruction-Driven Robot Manipulation**|机器人操作通常缺乏预测环境响应动作的能力，导致错误和低效，且现有视觉-语言模型（VLM）和世界模型在预测未来状态或生成空间一致帧方面存在局限。本研究提出一种用于快速且可预测的视频条件动作框架。该方法首先选择并适应一个鲁棒的视频生成模型以确保可靠的未来预测，然后应用对抗蒸馏进行快速、少步骤的视频生成，最后训练一个动作模型，利用生成的视频和真实观察来纠正空间误差。大量实验表明，该方法生成的时间连贯、空间准确的视频预测直接支持精确操作，在具身一致性、空间指代能力和任务完成度方面显著优于现有基线。|Yanwei Fu Team|[2602.10717](http://arxiv.org/abs/2602.10717)|null|

<p align=right>(<a href=#updated-on-20260219>back to top</a>)</p>

## World Model

|Publish Date|Title|Chinese Summary|Authors|PDF|Code|
|---|---|---|---|---|---|
|**2026-02-18**|**Parameter-free representations outperform single-cell foundation models on downstream benchmarks**|在单细胞RNA测序(scRNA-seq)领域，基于Transformer的基础模型在下游任务中表现卓越，但本文质疑是否必须依赖计算密集型深度学习方法。研究者通过精心归一化和线性方法构建的简单、可解释流程，在多个基准测试中达到了与先进基础模型相当甚至超越的性能，尤其是在涉及训练数据外的新细胞类型和生物体任务中。这表明细胞身份的生物学特性可由单细胞基因表达数据的简单线性表示捕获，并强调了严格基准测试的重要性。|Pankaj Mehta Team|[2602.16696](http://arxiv.org/abs/2602.16696)|null|
|**2026-02-18**|**Are Object-Centric Representations Better At Compositional Generalization?**|组合泛化能力对机器智能至关重要，但以对象为中心(OC)的表示在复杂视觉环境中支持泛化的证据有限。研究者通过跨三个视觉世界的视觉问答基准，系统评估了有无OC偏见的视觉编码器在对象属性新组合上的泛化能力。结果显示，OC方法在更困难的泛化场景中表现更优，且样本效率更高，而原始密集表示仅在较简单或数据充足的情况下才能超越。这表明当数据集规模、训练数据多样性或计算资源受限时，OC表示能提供更强的组合泛化能力。|Andrea Dittadi Team|[2602.16689](http://arxiv.org/abs/2602.16689)|null|
|**2026-02-18**|**Scaling Open Discrete Audio Foundation Models with Interleaved Semantic, Acoustic, and Text Tokens**|当前音频语言模型多以文本为先，限制了通用音频建模。本文提出并系统研究了原生音频基础模型，通过大规模的下一令牌预测，联合建模语义内容、声学细节和文本，以支持通用音频生成和跨模态能力。研究者系统探索了数据源、文本混合比例等设计选择，并首次对离散音频模型进行了缩放定律研究。基于这些发现，他们训练了SODA模型套件，并在保留说话者声音的语音-语音翻译等任务中展示了其作为灵活骨干网络的有效性。|Diyi Yang Team|[2602.16687](http://arxiv.org/abs/2602.16687)|null|
|**2026-02-18**|**Retrieval-Augmented Foundation Models for Matched Molecular Pair Transformations to Recapitulate Medicinal Chemistry Intuition**|针对药物化学中类似物设计缺乏高效、可控的分子编辑方法，现有机器学习方法存在局限性的问题，本文提出了一种基于大规模匹配分子对转换（MMPTs）的变异到变异（variable-to-variable）类似物生成基础模型。该模型通过提示机制实现用户对转换模式的精确控制，并引入MMPT-RAG框架利用外部参考类似物进行上下文指导。实验证明，该方法在通用化学语料库和专利数据集上显著提升了生成多样性、新颖性和可控性，能在实际发现场景中恢复出真实的类似物结构。|Liang Zhao Team|[2602.16684](http://arxiv.org/abs/2602.16684)|null|
|**2026-02-18**|**Learning Situated Awareness in the Real World**|多模态基础模型（MFMs）现有基准多侧重环境中心空间关系，忽视了以观察者为中心的、需推理智能体视角和动作的情境感知。为填补此空白，研究者引入了SAW-Bench，一个基于Ray-Ban Meta智能眼镜录制真实世界视频的新型基准，用于评估自我中心情境感知。该基准包含786个视频和2071个问答对，通过六项感知任务探测模型的观察者中心理解。评估显示，即使是最佳MFM，与人类仍有显著差距，模型常未能推断出连贯的相机几何导致空间推理错误，突显了超越被动观察、理解以观察者为中心的物理动态的重要性。|Xin Eric Wang Team|[2602.16682](http://arxiv.org/abs/2602.16682)|null|
|**2026-02-18**|**VETime: Vision Enhanced Zero-Shot Time Series Anomaly Detection**|时间序列异常检测（TSAD）需要识别点异常和上下文异常，但现有基础模型在精细点定位和全局上下文理解之间存在权衡。为解决这一困境，本文提出了VETime，首个通过细粒度视觉-时间对齐和动态融合统一时间与视觉模态的TSAD框架。VETime引入可逆图像转换和补丁级时间对齐模块，以建立共享的视觉-时间轴并保留细节。此外，它设计了异常窗口对比学习和任务自适应多模态融合机制。大量实验表明，VETime在零样本场景中显著优于先进模型，实现了更高的定位精度和更低的计算开销。|Chen Zhang Team|[2602.16681](http://arxiv.org/abs/2602.16681)|null|
|**2026-02-18**|**Learning to unfold cloth: Scaling up world models to deformable object manipulation**|机器人布料操作因其复杂的物理特性而极具挑战性，需要通用策略以适应不同布料特性。本文提出一种改进的DreamerV2强化学习架构应用于空中布料操作，通过引入表面法线作为输入，并优化回放缓冲区及数据增强程序，增强了机器人使用的世界模型来应对物理复杂性。在仿真和物理机器人的零样本部署实验中，该方法成功实现了多种布料的空中展开，证明了所提出架构在泛化性能上的显著优势。|Subramanian Ramamoorthy Team|[2602.16675](http://arxiv.org/abs/2602.16675)|null|
|**2026-02-18**|**A Systematic Evaluation of Sample-Level Tokenization Strategies for MEG Foundation Models**|神经影像数据的基础模型需将连续神经时间序列数据“令牌化”，但不同令牌化策略的影响尚不明确。本文对应用于脑磁图（MEG）数据的基于Transformer的大型神经影像模型（LNMs）的样本级令牌化策略进行了系统评估。通过比较可学习（引入基于自编码器的新方法）和不可学习令牌器在信号重建保真度、基础建模性能及下游任务上的表现，研究发现在多个MEG数据集上，两者均实现了高重建精度和大致相当的性能，表明简单的固定样本级令牌化策略足以支持神经基础模型的开发。|Mark W. Woolrich Team|[2602.16626](http://arxiv.org/abs/2602.16626)|null|
|**2026-02-18**|**Why Thinking Hurts? Diagnosing and Rectifying the Reasoning Shift in Foundation Recommender Models**|将思维链（CoT）推理整合到语义ID推荐基础模型中，常导致推荐性能下降，究其原因在于“通用子空间”中冗长推理的文本惯性，使得模型忽视关键语义ID。为解决此问题，本文提出一个训练无关的“推理时子空间对齐”框架。该方法通过压缩推理链和应用偏差减去的对比解码，有效缓解了无根据的文本漂移。实验证明，此框架能有效校准推理过程，使基础模型在利用推理能力的同时，不牺牲基于ID的推荐准确性。|Enhong Chen Team|[2602.16587](http://arxiv.org/abs/2602.16587)|null|
|**2026-02-18**|**Arc2Morph: Identity-Preserving Facial Morphing with Arc2Face**|人脸融合攻击对电子身份文档中的人脸识别系统构成严峻威胁，尤其是在护照注册过程中缺乏实时监督采集时。本文提出一种基于身份条件化人脸基础模型Arc2Face的新型人脸融合技术，能够从紧凑的身份表示合成逼真的人脸图像。通过在多个大规模数据集上与现有先进方法进行比较，实验结果表明，所提出的深度学习方法在融合攻击潜力方面达到了与传统上最具挑战性的基于地标技术相当的水平，证实了其在融合生成过程中有效保留和管理身份信息的能力。|Davide Maltoni Team|[2602.16569](http://arxiv.org/abs/2602.16569)|null|
|**2026-02-18**|**MMA: Multimodal Memory Agent**|长时程多模态智能体在依赖外部记忆时，常因检索到过时、低可信或冲突信息而产生过度自信的错误。针对此问题，本文提出了多模态记忆智能体（MMA），该智能体结合来源可信度、时间衰减和冲突感知网络共识，动态评估每个记忆项的可靠性，并利用此信号加权证据或在支持不足时弃权。同时，引入了MMA-Bench基准来研究信念动态。实验结果表明，在FEVER数据集上，MMA在保持基线准确率的同时，将方差降低了35.2%；在LoCoMo上，提高了操作准确性并减少了错误答案；在MMA-Bench上，视觉模式下MMA的准确率显著优于基线，并揭示了RAG智能体中潜在的“视觉安慰剂效应”。|Hao Tang Team|[2602.16493](http://arxiv.org/abs/2602.16493)|null|
|**2026-02-18**|**RoboGene: Boosting VLA Pre-training via Diversity-Driven Agentic Framework for Real-World Task Generation**|通用机器人操作受限于稀缺且成本高昂的真实世界交互数据，且现有任务策划方法不可扩展或易产生不可行指令。为解决这一问题，本文提出了RoboGene框架，旨在自动化生成多样化且物理上可行的单臂、双臂和移动机器人操作任务。RoboGene包含多样性驱动采样、自反思机制以强制物理约束以及人机协作精炼等核心组件。实验结果表明，RoboGene在定量分析和大规模真实世界实验中显著优于现有基础模型，且使用RoboGene预训练的VLA模型展现出更高的成功率和泛化能力，强调了高质量任务生成的重要性。|Jian Tang Team|[2602.16444](http://arxiv.org/abs/2602.16444)|null|
|**2026-02-18**|**Automated Histopathology Report Generation via Pyramidal Feature Extraction and the UNI Foundation Model**|从千兆像素级的组织病理学全玻片图像（WSIs）生成精确的诊断文本面临巨大挑战。本文提出了一个分层视觉语言框架，将一个冻结的病理学基础模型与Transformer解码器相结合进行报告生成。该方法通过多分辨率金字塔补丁选择和背景伪影去除技术处理WSI，并利用UNI Vision Transformer提取特征，随后由Transformer解码器生成文本，并使用BioGPT进行分词。为提高可靠性，该框架还引入了一个基于检索的验证步骤，通过比较生成报告与参考语料库来修正报告内容。|Serkan Sokmen Team|[2602.16422](http://arxiv.org/abs/2602.16422)|null|
|**2026-02-18**|**CADEvolve: Creating Realistic CAD via Program Evolution**|计算机辅助设计（CAD）的AI自动化受限于缺乏复杂操作和设计意图的数据集，导致现有方法难以生成工业级程序。为此，本文提出了CADEvolve，一个基于进化的管道和数据集，它从简单基元开始，通过VLM引导的编辑和验证，逐步生成工业级复杂性的CAD程序。该方法生成了8k个复杂零件作为可执行的CadQuery参数化生成器，并经过处理和增强后，形成了一个包含130万个脚本的统一数据集。实验结果表明，在CADEvolve上微调的VLM在DeepCAD、Fusion 360和MCB基准测试的Image2CAD任务上均达到了最先进的性能。|Dmitrii Zhemchuzhnikov Team|[2602.16317](http://arxiv.org/abs/2602.16317)|null|
|**2026-02-18**|**AFFMAE: Scalable and Efficient Vision Pretraining for Desktop Graphics Cards**|自监督预训练虽提高了计算机视觉效率，但高分辨率训练仍需服务器级基础设施，限制了基础模型的开发。传统MAE与分层架构结合时面临密集网格和掩码感知设计挑战。本文提出了AFFMAE，一个基于自适应、非网格token合并的掩码友好型分层预训练框架，通过丢弃被掩码的token并仅对可见token执行动态合并，消除了密集网格假设并保持了分层可扩展性。实验结果表明，在相同参数量下，AFFMAE在高分辨率电子显微镜分割任务上匹配了ViT-MAE的性能，同时将FLOPs减少高达7倍，内存使用减半，并在单个RTX 5090上实现了更快的训练。|Behzad Najafian Team|[2602.16249](http://arxiv.org/abs/2602.16249)|null|
|**2026-02-18**|**EasyControlEdge: A Foundation-Model Fine-Tuning for Edge Detection**|在实际边缘检测中，清晰度和数据效率至关重要，但用有限数据生成清晰边缘图极具挑战，且图像生成基础模型在边缘检测领域的潜力尚未充分挖掘。本文提出了EasyControlEdge，旨在将图像生成基础模型适应于边缘检测任务，以实现高清晰度和数据高效性。该方法通过引入边缘导向目标和高效像素空间损失来专门化基础模型，并在推理时利用基于无条件动力学的引导，实现通过引导尺度控制边缘密度。实验结果显示，EasyControlEdge在多个基准测试中持续优于现有方法，特别是在无后处理清晰度评估和有限训练数据条件下表现突出。|Tadahiro Taniguchi Team|[2602.16238](http://arxiv.org/abs/2602.16238)|null|
|**2026-02-18**|**Factored Latent Action World Models**|从无动作视频中学习潜在动作是构建可控世界模型的强大范式，但现有方法通常依赖单一动力学模型控制整个场景，难以应对多实体复杂环境。为解决此问题，本文提出了因子化潜在动作模型（FLAM），该框架将场景分解为独立因子，每个因子推断其自身潜在动作并预测下一时刻状态。这种因子化结构能够更准确地建模复杂的多实体动力学。实验结果表明，FLAM在模拟和真实世界多实体数据集上的预测准确性和表示质量均优于现有方法，改善了视频生成质量，并促进了下游策略学习。|Peter Stone Team|[2602.16229](http://arxiv.org/abs/2602.16229)|null|
|**2026-02-18**|**World Model Failure Classification and Anomaly Detection for Autonomous Inspection**|自主检查机器人可降低工业现场监测的成本与风险，但遮挡、视角受限等问题使准确读数充满挑战。本文提出了一个混合框架，结合监督式故障分类与异常检测，将检查任务分类为成功、已知故障或异常情况。该方法以带有压缩视频输入的世界模型为骨干，并通过共形预测阈值确定的两个决策函数在人类观察者之前进行分类。在办公室和工业现场仪表检查上的实验结果显示，该框架在区分成功、故障和OOD情况方面准确率超过90%，且分类发生时间早于人类观察者，展现了其在自主检查中实现鲁棒、预见性故障检测的潜力。|Shayegan Omidshafiei Team|[2602.16182](http://arxiv.org/abs/2602.16182)|null|
|**2026-02-17**|**MedProbCLIP: Probabilistic Adaptation of Vision-Language Foundation Model for Reliable Radiograph-Report Retrieval**|视觉-语言基础模型在多模态理解方面潜力巨大，但其确定性嵌入难以满足高风险生物医学应用对可靠性的要求。本文提出了MedProbCLIP，一个用于胸部X射线和放射学报告表示学习及双向检索的概率视觉-语言学习框架。MedProbCLIP通过概率对比目标将图像和文本表示建模为高斯嵌入，显式捕获不确定性和多对多对应关系，并利用变分信息瓶颈减轻过度自信预测。实验结果表明，在MIMIC-CXR数据集上，MedProbCLIP在检索和零样本分类方面均优于现有基线，并展现出卓越的校准性、风险-覆盖行为、选择性检索可靠性以及对临床相关损坏的鲁棒性，提升了放射学图像-文本检索系统的可信度和安全性。|Gongbo Liang Team|[2602.16019](http://arxiv.org/abs/2602.16019)|null|
|**2026-02-17**|**Neural Scaling Laws for Boosted Jet Tagging**|大型语言模型（LLMs）的成功表明计算规模扩展是性能提升的关键，然而高能物理（HEP）领域最先进模型的训练计算量远低于工业界。针对该背景，本文研究了使用公共JetClass数据集进行增压射流分类的神经网络扩展定律。研究推导了计算最优的扩展定律，并识别出一个可通过增加计算持续接近的有效性能极限。研究结果表明，增加计算能可靠地推动性能接近渐近极限，且更具表达能力的低级特征可以提高性能极限并在固定数据集大小下改善结果，同时量化了数据重复对有效数据集大小的增益影响。|Lukas Heinrich Team|[2602.15781](http://arxiv.org/abs/2602.15781)|null|
|**2026-02-16**|**EditCtrl: Disentangled Local and Global Control for Real-Time Generative Video Editing**|背景：高保真生成式视频编辑依赖预训练视频基础模型，但其计算成本高昂，即使是局部编辑也需处理整个视频上下文。方法：本文提出EditCtrl，一个高效的视频修复控制框架。它引入了新颖的局部视频上下文模块，仅对掩码标记进行操作，使计算成本与编辑区域大小成正比。同时，一个轻量级的时间全局上下文嵌入器确保视频整体上下文的一致性。结果：EditCtrl比现有先进方法计算效率高10倍，并提升了编辑质量。该方法还支持多区域文本提示编辑和自回归内容传播。|Caleb Leak Team|[2602.15031](http://arxiv.org/abs/2602.15031)|**[link](https://yehonathanlitman.github.io/edit_ctrl)**|
|**2026-02-16**|**Generalization from Low- to Moderate-Resolution Spectra with Neural Networks for Stellar Parameter Estimation: A Case Study with DESI**|背景：在恒星光谱分析中，跨巡天泛化能力（特别是从低分辨率到中分辨率光谱的迁移）是一个关键挑战。方法：本文研究了使用预训练多层感知机（MLPs）解决此问题，以LAMOST低分辨率光谱到DESI中分辨率光谱的迁移为例。作者在LAMOST低分辨率光谱或其嵌入上预训练MLPs，并在DESI光谱上进行微调，比较了直接在光谱上训练的MLPs与基于Transformer模型嵌入训练的MLPs，并评估了不同的微调策略。结果：预训练在LAMOST低分辨率光谱上的MLPs表现出色，即便不微调也能获得良好性能，适度微调可进一步提升。研究表明，简单预训练MLPs能提供有竞争力的跨巡天泛化能力，但光谱基础模型在跨巡天恒星参数估计中的作用仍需深入探索。|Viska Wei Team|[2602.15021](http://arxiv.org/abs/2602.15021)|null|
|**2026-02-16**|**Cold-Start Personalization via Training-Free Priors from Structured World Models**|背景：冷启动个性化（即在无用户历史数据时推断用户偏好）是一个挑战，因为用户只关心少数偏好维度，且关键维度因人而异。现有强化学习方法在多轮交互中难以有效利用偏好数据的分因子结构。方法：本文提出Pep（Preference Elicitation with Priors）框架，将冷启动偏好获取分解为离线结构学习和在线贝叶斯推理。Pep离线从完整用户档案中学习偏好相关性的结构化世界模型，然后在线进行免训练的贝叶斯推理，以选择信息丰富的提问并预测完整的偏好档案。结果：Pep在生成响应与用户偏好的一致性方面达到80.8%，远高于强化学习的68.5%，且交互次数减少3-5倍。它仅用约1万参数就实现此效果，而强化学习需80亿参数，突显了利用偏好数据分因子结构的重要性。|Asli Celikyilmaz Team|[2602.15012](http://arxiv.org/abs/2602.15012)|null|
|**2026-02-16**|**PDE foundation models are skillful AI weather emulators for the Martian atmosphere**|背景：为火星大气层构建熟练的预测天气模拟器，面临训练数据和计算资源不足的挑战。方法：本文展示了如何将预训练在多源偏微分方程数值解上的AI基础模型（Poseidon PDE基础模型）适配并微调，以构建火星大气的预测天气模拟器。研究扩展了Poseidon模型从二维到三维的方法，同时保留了预训练信息，并探讨了在稀疏初始条件下的模型性能。结果：通过预训练与模型扩展的结合，模型在独立验证年份的性能提升了34.4%。这表明偏微分方程基础模型不仅能近似其他偏微分方程的解，还能作为解决实际世界复杂交互问题的锚定模型，尤其是在训练数据或计算预算有限的情况下。|Juan Bernabe-Moreno Team|[2602.15004](http://arxiv.org/abs/2602.15004)|null|
|**2026-02-16**|**Use What You Know: Causal Foundation Models with Partial Graphs**|背景：传统的因果量估计依赖于为特定假设定制的估计器。新兴的因果基础模型（CFMs）提供统一方法，但目前无法融入领域知识，导致预测次优。方法：本文提出将因果信息（如完整因果图或部分祖先信息）条件化到CFMs中的方法。研究系统评估了不同的条件化策略，发现将可学习偏差注入注意力机制是利用完整和部分因果信息最有效的方法。结果：通过条件化，通用CFM的性能可以与针对特定因果结构训练的专用模型相媲美。这一方法克服了构建一体化因果基础模型的核心障碍，使其能够以数据驱动的方式回答因果查询，同时有效利用任何程度的领域专业知识。|Bernhard Schölkopf Team|[2602.14972](http://arxiv.org/abs/2602.14972)|null|
|**2026-02-16**|**Model Context Protocol (MCP) Tool Descriptions Are Smelly! Towards Improving AI Agent Efficiency with Augmented MCP Tool Descriptions**|背景：基于基础模型（FM）的智能体通过工具描述与外部系统交互，但这些自然语言描述中的缺陷可能误导FM，其普遍性和影响尚不明确。方法：本文对103个MCP服务器上的856个工具进行了大规模实证研究。通过识别工具描述的六个组成部分，开发了评分标准并据此形式化了“工具描述异味”。利用FM-based扫描器进行操作化评估，并增强描述以评估其对智能体性能的影响。结果：97.1%的工具描述至少含有一种“异味”，其中56%未能清晰阐明目的。尽管增强所有组件的描述可使任务成功率中位数提升5.85个百分点，但执行步骤也增加了67.46%，并导致性能下降，揭示了性能与成本之间的权衡。|Ahmed E. Hassan Team|[2602.14878](http://arxiv.org/abs/2602.14878)|null|
|**2026-02-16**|**World Models for Policy Refinement in StarCraft II**|背景：尽管大型语言模型（LLMs）展现出强大的推理能力，但现有基于LLM的《星际争霸II》（SC2）智能体主要关注策略优化，缺乏可学习的、动作条件化的转移模型来辅助决策。方法：本文提出StarWM，这是首个在部分可观测环境下预测SC2未来观测的世界模型。为学习SC2的混合动态，作者引入了一种将观测分解为五个语义模块的结构化文本表示，并构建了首个用于SC2动态预测的指令微调数据集SC2-Dynamics-50k。StarWM被集成到“生成-模拟-细化”决策循环中，形成StarWM-Agent。结果：StarWM在资源预测准确性等指标上比零样本基线提升近60%。在线评估中，StarWM-Agent对不同难度级别（Hard、Harder、VeryHard）的胜率分别提升了30%、15%和30%，同时改善了宏观管理稳定性和战术风险评估。|Bo Xu Team|[2602.14857](http://arxiv.org/abs/2602.14857)|null|
|**2026-02-16**|**SAILS: Segment Anything with Incrementally Learned Semantics for Task-Invariant and Training-Free Continual Learning**|背景：持续学习在类增量语义分割（CISS）中面临重复训练、高计算成本和灾难性遗忘的限制，制约了其实际应用。方法：本文提出SAILS（Segment Anything with Incrementally Learned Semantics），一个免训练的CISS框架，它将CISS解耦为两个阶段：首先利用Segment Anything Model (SAM)进行零样本区域提取，然后通过固定特征空间中的原型进行语义关联。SAILS通过选择性类内聚类，为每个类生成多个原型以更好地建模类内变异性。结果：SAILS无需增量训练，但在标准CISS数据集上通常超越了现有的基于训练的方法，特别是在遗忘问题严重的长期和挑战性任务序列中。SAILS完全消除了遗忘，保持了任务不变的一致性能，并展现出正向反向迁移。|René Schuster Team|[2602.14767](http://arxiv.org/abs/2602.14767)|null|
|**2026-02-16**|**Depth Completion as Parameter-Efficient Test-Time Adaptation**|背景：现有深度补全方法通常通过训练任务特定编码器来利用辅助输入，但易过拟合且泛化性差。而3D基础模型可提供更强的几何先验。方法：本文提出CAPA，一个参数高效的测试时优化框架，用于利用稀疏几何线索对预训练3D基础模型进行深度补全。CAPA冻结基础模型骨干，仅通过参数高效微调（如LoRA或VPT）更新少量参数，并利用推理时稀疏观测直接计算梯度进行指导。对于视频，CAPA引入序列级参数共享以利用时间相关性并强制多帧一致性。结果：CAPA与任何基于ViT的基础模型兼容，并在室内外数据集的各种条件下取得了最先进的结果。它有效地将基础模型的几何先验与场景特定测量相结合，修正了畸变。|Shengyu Huang Team|[2602.14751](http://arxiv.org/abs/2602.14751)|null|
|**2026-02-16**|**WebWorld: A Large-Scale World Model for Web Agent Training**|背景：网页智能体需要大量轨迹以实现泛化，但真实世界训练受网络延迟、速率限制和安全风险制约。现有模拟器局限于封闭环境。方法：本文推出WebWorld系列，首个大规模训练的开放网络模拟器。它利用可扩展数据管道在超过100万次开放网络交互中进行训练，支持推理、多格式数据和超过30步的长周期模拟。结果：WebWorld在WebWorld-Bench上实现了与Gemini-3-Pro相当的模拟性能。在WebWorld合成轨迹上训练的Qwen3-14B在WebArena上性能提升9.2%，达到与GPT-4o相当的水平。WebWorld作为世界模型，在推理时搜索方面超越了GPT-5。此外，它还展现出跨领域泛化能力，为构建世界模型提供了可复现的方法。|Zuozhu Liu Team|[2602.14721](http://arxiv.org/abs/2602.14721)|null|
|**2026-02-16**|**Arbor: A Framework for Reliable Navigation of Critical Conversation Flows**|针对大型语言模型在医疗分诊等高风险领域难以遵循结构化工作流程的问题，以及单一提示方法在长提示下易导致指令依从性下降的挑战，本文提出了Arbor框架。该框架将决策树导航分解为节点级任务，通过基于DAG的编排机制动态检索和评估转换，并将响应生成解耦。实验结果表明，与单一提示基线相比，Arbor在真实临床分诊对话中将平均轮次准确率提高了29.4个百分点，同时显著降低了延迟和成本，证明了架构分解能有效提升模型性能并降低对模型固有能力的依赖。|Luís Ungaro Team|[2602.14643](http://arxiv.org/abs/2602.14643)|null|
|**2026-02-16**|**Tabular Foundation Models Can Learn Association Rules**|传统的关联规则挖掘（ARM）方法存在规则爆炸和可扩展性差的问题，而现有神经方法在低数据量下性能不佳，但表格基础模型（TFMs）为解决这些局限性提供了基础。为此，本文提出了一个模型无关的关联规则学习框架，能够利用TFMs从任何条件概率模型中提取关联规则，并实例化了TabProbe。实验结果显示，TabProbe利用TFMs作为条件概率估计器，无需频繁项集挖掘即可生成简洁、高质量的关联规则，在低数据设置下仍保持强大的预测性能和鲁棒性。|Victoria Degeler Team|[2602.14622](http://arxiv.org/abs/2602.14622)|null|
|**2026-02-16**|**MATEO: A Multimodal Benchmark for Temporal Reasoning and Planning in LVLMs**|AI智能体在实现复杂目标时需要规划，但现有研究对基础模型时间执行顺序（TEO）的理解有限，多局限于线性近似或纯文本输入。为解决此问题，本文引入了MATEO（MultimodAl Temporal Execution Order）基准，旨在评估和提升大型视觉语言模型（LVLMs）的时间推理能力。通过收集高质量多模态食谱语料库及相应的TEO图注释，作者使用MATEO评估了六个最先进的LVLM，考察了不同模型规模、语言上下文、多模态输入结构和微调策略对时间推理能力的影响。|Giuseppe Riccardi Team|[2602.14589](http://arxiv.org/abs/2602.14589)|null|
|**2026-02-16**|**MedVAR: Towards Scalable and Efficient Medical Image Generation via Next-scale Autoregressive Prediction**|医学图像生成在数据增强和隐私保护中至关重要，但现有方法在架构效率、多器官数据和原则性评估方面存在不足。为此，本文提出了MedVAR，首个基于自回归的医学基础模型，采用“下一尺度预测”范式，实现快速可扩展的医学图像合成。MedVAR以粗到精的方式生成图像，并构建了一个包含约44万张CT和MRI图像的协调数据集。综合实验表明，MedVAR在图像保真度、多样性和可扩展性方面均达到最先进水平，为未来的医学生成基础模型提供了 promising 的架构方向。|Yueming Jin Team|[2602.14512](http://arxiv.org/abs/2602.14512)|null|
|**2026-02-16**|**Covariance-Aware Transformers for Quadratic Programming and Decision Making**|针对Transformer在涉及协方差矩阵的决策问题中的应用潜力，本文首先证明线性注意力机制可通过模拟梯度下降求解无约束二次规划（QP），并扩展至求解L1惩罚/约束QP。在此基础上，本文提出了Time2Decide，一种通过显式输入协方差矩阵来增强时间序列基础模型（TSFM）的通用方法。实验结果表明，Time2Decide在经典的投资组合优化问题上，其性能普遍优于基础TSFM模型，并在特定条件下甚至超越了传统的“预测-优化”流程，证明Transformer通过显式利用二阶统计量能有效解决复杂决策问题。|Samet Oymak Team|[2602.14506](http://arxiv.org/abs/2602.14506)|null|
|**2026-02-16**|**A Soft Wrist with Anisotropic and Selectable Stiffness for Robust Robot Learning in Contact-rich Manipulation**|在非结构化环境中，机器人进行接触密集型操作任务时，现有软末端执行器因形变范围有限、缺乏定向刚度控制或系统复杂而面临挑战。本文介绍了一种名为CLAW（Compliant Leaf-spring Anisotropic soft Wrist）的新型软腕机构，它通过简单的板簧和锁定旋转关节设计，实现了大范围6自由度形变和可调的各向异性刚度，同时保持轻量和低成本。在模仿学习实验中，CLAW在插销任务中实现了76%的成功率，显著优于其他夹具，并在处理高精度装配和精细物体操作等接触密集型场景中表现出强大潜力，预示其能增强机器人学习的鲁棒性。|Masashi Hamaya Team|[2602.14434](http://arxiv.org/abs/2602.14434)|null|
|**2026-02-15**|**WIMLE: Uncertainty-Aware World Models with IMLE for Sample-Efficient Continuous Control**|基于模型的强化学习（MBRL）常因模型误差累积、世界模型处理多模态动力学不佳及预测过度自信而表现受限。本文提出了WIMLE，一种将隐式最大似然估计（IMLE）扩展到MBRL框架的方法，旨在学习随机、多模态的世界模型，并利用集成和潜在采样估计预测不确定性。WIMLE在训练中根据预测置信度加权合成转换，以稳定学习。在40个连续控制任务上的实验结果表明，WIMLE实现了卓越的样本效率和有竞争力的渐近性能，尤其在挑战性任务上显著提升了样本效率，凸显了IMLE基多模态和不确定性感知加权对稳定MBRL的价值。|Ke Li Team|[2602.14351](http://arxiv.org/abs/2602.14351)|**[link](https://openreview.net/forum?id=mzLOnTb3WH)**|
|**2026-02-15**|**Multi-Agent Debate: A Unified Agentic Framework for Tabular Anomaly Detection**|表格异常检测常依赖单一或静态集成检测器，但异构模型在分布漂移、缺失数据和稀有异常下常出现分歧。本文提出了MAD（Multi-Agent Debating）框架，将这种分歧作为核心信号，通过数学协调层解决。框架中每个代理是一个ML检测器，提供异常分数、置信度和证据，并由LLM评论员增强。协调器将消息转换为损失并更新代理影响力，生成最终异常分数和可审计的辩论轨迹。实验表明，MAD在各种表格异常基准测试上提高了鲁棒性，并提供了更清晰的模型分歧追踪。|Sheng Li Team|[2602.14251](http://arxiv.org/abs/2602.14251)|null|
|**2026-02-15**|**Towards Spatial Transcriptomics-driven Pathology Foundation Models**|空间转录组学（ST）能够提供超越组织学评估的分子景观，多模态基础模型也显示了形态分子耦合提升组织学表征的潜力。为整合局部分子信息到病理视觉编码器，本文提出了Spatial Expression-Aligned Learning (SEAL) 框架，作为一种参数高效的视觉-组学自监督微调方法，可应用于现有病理学基础模型。SEAL通过在涵盖14个器官的70多万个配对基因表达点-组织区域示例上进行训练，在38项幻灯片级和15项补丁级下游任务上，持续优于纯视觉和ST预测基线，并展示了强大的域泛化能力和基因到图像检索等跨模态能力，为病理学基础模型的ST引导微调提供了通用且实用的框架。|Faisal Mahmood Team|[2602.14177](http://arxiv.org/abs/2602.14177)|null|
|**2026-02-15**|**ARport: An Augmented Reality System for Markerless Image-Guided Port Placement in Robotic Surgery**|精确的端口放置是机器人辅助手术的关键步骤，但术前规划与术中执行之间存在差距。本文提出了ARport，一个增强现实（AR）系统，旨在自动将预规划的套管布局映射到患者体表，提供直观的术中空间指导。ARport在光学透视头戴式显示器（OST-HMD）上实现，无需外部传感器或标记，通过基础模型提取患者体表并进行无标记配准，实现术前解剖模型与患者体表的对齐，从而现场可视化套管布局。全尺寸人体模型实验表明，ARport能够准确叠加预规划的套管位置，实现虚拟规划与真实解剖之间的一致空间对应，为临床工作流程的无缝集成提供了高效且极简的解决方案。|Qi Dou Team|[2602.14153](http://arxiv.org/abs/2602.14153)|null|
|**2026-02-13**|**Imitating What Works: Simulation-Filtered Modular Policy Learning from Human Videos**|针对机器人通过观察人类视频学习抓取操作技能时，传统方法难以有效学习与任务兼容的抓取行为的问题，本研究提出了Perceive-Simulate-Imitate (PSI) 框架。该框架利用仿真中的抓取轨迹过滤技术，对人类视频数据进行处理，并生成带有抓取适用性标签的扩展轨迹数据，从而实现面向任务的抓取能力监督学习。真实世界实验表明，PSI无需任何机器人数据即可高效学习精确的操纵技能，并且相比简单使用抓取生成器的方法，性能显著提升，鲁棒性更强。|Wei-Chiu Ma Team|[2602.13197](http://arxiv.org/abs/2602.13197)|null|
|**2026-02-13**|**Order Matters in Retrosynthesis: Structure-aware Generation via Reaction-Center-Guided Discrete Flow Matching**|为解决现有免模板逆合成方法学习效率低和半模板方法泛化受限的问题，本研究提出了一种结构感知的免模板框架，核心在于利用原子排序信息。该方法将反应中心原子置于序列头部，通过位置归纳偏差编码化学反应的两阶段特性，并采用RetroDiT骨干网络与离散流匹配相结合。实验结果表明，该方法在USPTO-50k和USPTO-Full数据集上取得了SOTA性能，且在预测反应中心下，性能超越了使用更多数据训练的基础模型，并验证了结构先验的重要性。|Tianshu Yu Team|[2602.13136](http://arxiv.org/abs/2602.13136)|null|
|**2026-02-13**|**A Calibrated Memorization Index (MI) for Detecting Training Data Leakage in Generative MRI Models**|鉴于图像生成模型可能复制训练数据，尤其在医学图像生成中引发隐私问题，本研究提出了一种校准的逐样本度量方法来检测训练数据的记忆化和重复。该方法利用MRI基础模型提取图像特征，聚合多层白化最近邻相似度，并映射为有界的“过拟合/新颖性指数”（ONI）和“记忆化指数”（MI）分数。在多个MRI数据集上的实验结果表明，该度量能稳健检测重复数据，并提供一致的度量值，在样本级别实现了近乎完美的重复项检测。|Ibrahim Habli Team|[2602.13066](http://arxiv.org/abs/2602.13066)|null|
|**2026-02-13**|**INHerit-SG: Incremental Hierarchical Semantic Scene Graphs with RAG-Style Retrieval**|针对现有语义场景图在机器人导航中难以支持可解释的人类意图推理的问题，本研究提出了INHerit-SG框架。该框架将地图定义为RAG-ready的知识库，通过引入自然语言描述作为语义锚点对齐人类意图，并采用异步双进程架构和分层结构解耦几何分割与语义推理，通过事件触发机制保持地图长期一致性。实验在新建数据集和真实世界环境中进行，结果表明INHerit-SG在复杂查询上达到了最先进性能，并提高了检索成功率和可靠性，展现了其在下游导航任务中的可扩展性。|Yang Gao Team|[2602.12971](http://arxiv.org/abs/2602.12971)|null|
|**2026-02-13**|**Information-theoretic analysis of world models in optimal reward maximizers**|为量化最优行为对世界内部表示的需求，本研究考虑了一个具有n个状态和m个动作的受控马尔可夫过程，并假设转移动态存在均匀先验。研究证明，观察一个对任何非恒定奖励函数最优的确定性策略，可以精确地传达n log m比特关于环境的信息。具体来说，环境与最优策略之间的互信息为n log m比特。这些发现为实现最优性所需的“隐式世界模型”提供了精确的信息理论下限，适用于多种奖励最大化目标。|Alex Altair Team|[2602.12963](http://arxiv.org/abs/2602.12963)|null|
|**2026-02-13**|**Scaling Single Human Demonstrations for Imitation Learning using Generative Foundational Models**|鉴于机器人模仿学习中收集演示数据耗时且不易从人类演示直接迁移，本研究提出Real2Gen框架，旨在从单个“人类”演示中训练机器人操纵策略。该方法从人类演示中提取必要信息并传输至仿真环境，在仿真中利用可编程专家智能体生成无限数据来训练流匹配策略。实验结果显示，Real2Gen在三个真实世界任务上成功率平均提升26.6%，并且由于训练数据丰富多样，训练策略的泛化能力显著提高，纯仿真训练的策略还能零样本部署到真实世界。|Abhinav Valada Team|[2602.12734](http://arxiv.org/abs/2602.12734)|null|
|**2026-02-13**|**MedXIAOHE: A Comprehensive Recipe for Building Medical MLLMs**|为提升真实世界临床应用中的通用医学理解和推理能力，本研究提出了医疗视觉-语言基础模型MedXIAOHE。该模型采用实体感知持续预训练框架，组织异构医学语料以拓宽知识覆盖并减少长尾问题；通过强化学习和工具增强的代理训练，整合多样化医学推理模式以支持带可验证决策轨迹的多步骤诊断推理；并融合用户偏好规则、证据推理和低幻觉长文本报告生成，提高真实世界使用的可靠性。MedXIAOHE在多项医学基准测试中取得了最先进的性能，并超越了领先的闭源多模态系统。|Zhixiong Yang Team|[2602.12705](http://arxiv.org/abs/2602.12705)|null|
|**2026-02-13**|**RelBench v2: A Large-Scale Benchmark and Repository for Relational Data**|为推动关系深度学习（RDL）的发展，并应对日益增长的模型规模需求，本研究引入了RelBench v2，一个大规模、真实的关系数据库基准扩展。RelBench v2新增了四个大型数据集和“自动完成任务”，旨在直接推理关系表中缺失的属性值，并整合了外部基准和评估框架以实现统一的关系-时间评估。实验结果表明，RDL模型在自动完成、预测和推荐任务中始终优于单表基线，突出了显式建模关系结构的重要性。|Jure Leskovec Team|[2602.12606](http://arxiv.org/abs/2602.12606)|**[link](https://relbench.stanford.edu)**|
|**2026-02-13**|**The Constant Eye: Benchmarking and Bridging Appearance Robustness in Autonomous Driving**|针对自动驾驶算法在OOD条件下易受外观变化影响，且难以区分外观与结构场景变化导致规划器失效的问题，本研究建立了Navdream，一个高保真鲁棒性基准。该基准利用生成式像素对齐风格迁移，隔离外观变化对驾驶性能的影响。为弥合这一差距，研究提出了一种通用感知接口，利用冻结的视觉基础模型（DINOv3）提取外观不变特征作为规划器的稳定接口。实验表明，现有规划算法在OOD外观下性能显著下降，而该即插即用解决方案在各种规划范式中实现了卓越的零样本泛化，在极端外观变化下仍保持一致性能。|Yiyi Liao Team|[2602.12563](http://arxiv.org/abs/2602.12563)|null|
|**2026-02-13**|**Self-Supervised JEPA-based World Models for LiDAR Occupancy Completion and Forecasting**|鉴于自动驾驶需要世界模型来支持长期规划，且模型学习需具备自监督的可扩展性，本研究提出AD-LiST-JEPA，一个基于联合嵌入预测架构（JEPA）的自监督世界模型。该模型旨在利用JEPA框架从激光雷达数据预测未来时空演变。通过下游基于激光雷达的占用完成和预测（OCF）任务评估学习到的表示质量，概念验证实验表明，经过JEPA世界模型学习预训练后的编码器在OCF性能上有所提升，证明了该方法在感知和预测联合任务中的潜力。|Anna Choromanska Team|[2602.12540](http://arxiv.org/abs/2602.12540)|null|
|**2026-02-13**|**Multi-Agent Model-Based Reinforcement Learning with Joint State-Action Learned Embeddings**|在部分可观察和高度动态环境中，多智能体协调学习面临表示学习和数据效率挑战。为此，本文提出了一种新颖的基于模型的强化学习框架，该框架将联合状态-动作表示学习与想象式展开相结合。作者设计了一个使用变分自编码器训练的世界模型，并利用学习到的状态-动作嵌入（SALE）进行增强，将其注入到预测未来展开的想象模块和估计联合动作值函数的联合智能体网络中。在星际争霸II微管理、多智能体MuJoCo和基于级别的觅食挑战等基准测试中，该方法在有限真实环境交互下，通过将想象轨迹与基于SALE的动作值相结合，显著优于基线算法，验证了其在多智能体模型范式中学习联合状态-动作嵌入的有效性。|David Meger Team|[2602.12520](http://arxiv.org/abs/2602.12520)|null|
|**2026-02-12**|**The Observer Effect in World Models: Invasive Adaptation Corrupts Latent Physics**|判断神经网络模型是内化了物理定律还是仅利用统计捷径，尤其是在分布外（OOD）变化下，仍是一个难题。传统的适应性评估方法（如微调或高容量探针）可能改变被测量的表示，从而混淆自监督学习（SSL）期间的真实学习内容。为解决此问题，本文提出了一种非侵入性评估协议PhyIP，该协议基于线性表示假设，通过测试物理量能否从冻结表示中线性解码来评估。在流体动力学和轨道力学任务中，实验发现当SSL错误率较低时，潜在结构变得线性可访问，PhyIP在OOD测试中成功恢复了内能和牛顿反平方标度（ρ>0.90）。相比之下，基于适应性的评估可能使这种结构崩溃（ρ≈0.05）。这表明低容量探针能更准确地评估物理世界模型，而适应性评估可能掩盖潜在结构。|Barbara Hammer Team|[2602.12218](http://arxiv.org/abs/2602.12218)|null|
|**2026-02-12**|**LDA-1B: Scaling Latent Dynamics Action Model via Universal Embodied Data Ingestion**|当前机器人基础模型多依赖大规模行为克隆，忽视了异构具身数据中可迁移的动力学知识，而现有统一世界模型（UWM）因粗糙数据使用和碎片化数据集难以扩展。为此，本文提出了LDA-1B，一个通过通用具身数据摄取进行扩展的机器人基础模型，它联合学习动力学、策略和视觉预测，并为不同质量数据分配不同角色。为支持大规模训练，作者构建并标准化了EI-30k数据集（超过3万小时的人类和机器人轨迹）。通过在结构化的DINO潜在空间中进行预测，实现了异构数据的可扩展动力学学习，避免了冗余的像素空间外观建模，并采用多模态扩散Transformer处理异步视觉和动作流，实现了10亿参数规模的稳定训练。实验结果表明，LDA-1B在接触密集型、灵巧型和长程任务上分别比现有方法（如π0.5）提高了21%、48%和23%，并能通过利用30%通常有害且被丢弃的低质量轨迹，实现数据高效微调，性能提升10%。|He Wang Team|[2602.12215](http://arxiv.org/abs/2602.12215)|**[link](https://pku-epic.github.io/LDA)**|
|**2026-02-12**|**DreamID-Omni: Unified Framework for Controllable Human-Centric Audio-Video Generation**|尽管基础模型在音视频生成方面取得进展，但以人物为中心的多任务（如参考音视频生成、视频编辑、音频驱动动画）仍被孤立处理，且难以在单一框架内实现对多角色身份和音色的精确解耦控制。本文提出了DreamID-Omni，一个统一的可控人物中心音视频生成框架。作者设计了一个对称条件扩散Transformer，通过对称条件注入方案整合异构条件信号。为解决多人物场景中普遍存在的身份-音色绑定失败和说话人混淆问题，提出了双层解耦策略：在信号层面采用同步RoPE确保严格的注意力空间绑定，在语义层面采用结构化字幕建立显式属性-主题映射。此外，还设计了多任务渐进训练方案，利用弱约束生成先验来正则化强约束任务。大量实验证明，DreamID-Omni在视频、音频和音视频一致性方面均达到了全面的最先进性能，甚至超越了领先的商业模型。|Xiangwang Hou Team|[2602.12160](http://arxiv.org/abs/2602.12160)|**[link](https://guoxu1233.github.io/DreamID-Omni/)**|
|**2026-02-12**|**It's TIME: Towards the Next Generation of Time Series Forecasting Benchmarks**|时间序列基础模型（TSFMs）正在革新预测领域，但现有基准在数据构成、数据完整性、任务制定和分析视角上存在局限。为解决这些问题，本文提出了TIME，一个新一代任务中心基准，包含50个全新数据集和98个预测任务，专为严格的零样本TSFM评估而设计，避免数据泄露。通过整合大型语言模型和人类专业知识，建立了严格的人机协作基准构建流程，确保高数据完整性，并根据真实操作需求和变量可预测性重新定义任务。此外，作者提出了一种新颖的模式级评估视角，超越了基于静态元标签的传统数据集级评估，通过利用结构化时间序列特征表征内在时间属性，为模型能力提供了更具普适性的见解。对12个代表性TSFMs进行评估，并建立了一个多粒度排行榜，以促进深入分析和可视化检查。|Chenghao Liu Team|[2602.12147](http://arxiv.org/abs/2602.12147)|null|
|**2026-02-12**|**Commencing-Student Enrolment Forecasting Under Data Sparsity with Time Series Foundation Models**|许多大学面临日益增长的财政压力，亟需准确预测新生入学人数，然而高等教育入学预测通常数据稀疏，年度序列短且受报告变化和体制转变影响。流行的经典方法因短样本导致参数估计和模型选择不稳定，以及结构性中断导致外推能力下降而不可靠。近期，TSFMs在泄漏受限的协变量构建下，为年度、数据稀疏的机构预测提供了强大的零样本先验。本文在零样本设置下，对多种TSFM家族进行了基准测试，并测试了一组紧凑、防泄漏的协变量集。作者引入了“机构运营状况指数”（IOCI），这是一个从时间戳文件证据中提取的可转移的0-100区间状态协变量，并结合了具有稳定特征工程的Google Trends需求代理。使用严格对齐的回溯测试，结果表明，在没有机构特定训练的情况下，条件化TSFMs的表现与经典基准相当，具体表现差异因群体和模型而异。|Surangika Ranathunga Team|[2602.12120](http://arxiv.org/abs/2602.12120)|null|
|**2026-02-12**|**The Pensieve Paradigm: Stateful Language Models Mastering Their Own Context**|当前的大型语言模型（LLMs）虽有强大的数据库和检索系统，但缺乏主动管理自身状态和记忆的机制，被动接受手动提供的上下文，受限于固定窗口大小。本研究引入了StateLM，一种新型基础模型，它具备内部推理循环来管理自身状态。该模型配备了一系列记忆工具（如上下文剪枝、文档索引、笔记）并被训练主动管理这些工具，从而摆脱固定窗口的架构限制。实验结果表明，StateLM在所有模型规模的长文档问答任务中持续优于标准LLMs，在聊天记忆任务中绝对准确率提升10%至20%，在深度研究任务BrowseComp-Plus上更是达到52%的准确率，远超标准LLM的5%左右。这使得LLMs从被动预测器转变为状态感知代理，实现了状态化和可管理的推理过程。|Yan Wang Team|[2602.12108](http://arxiv.org/abs/2602.12108)|null|
|**2026-02-12**|**GigaBrain-0.5M*: a VLA That Learns From World Model-Based Reinforcement Learning**|直接从当前观察预测多步动作块的视觉-语言-动作（VLA）模型，因场景理解和未来预测能力受限而存在不足。相比之下，预训练于网络规模视频语料库的视频世界模型具有强大的时空推理和准确的未来预测能力，为增强VLA学习提供了基础。本研究提出了GigaBrain-0.5M*，一个通过世界模型驱动强化学习训练的VLA模型，其基于在超过1万小时机器人操作数据上预训练的GigaBrain-0.5。GigaBrain-0.5M*通过RAMP（通过世界模型条件策略进行强化学习）整合强化学习，以实现鲁棒的跨任务适应。实验结果表明，RAMP相对于RECAP基线取得了显著性能提升，在“叠衣服”、“装箱”和“制作浓缩咖啡”等挑战性任务上提升了约30%，并且在真实世界部署中展示了可靠的长时序复杂操作能力。|Zheng Zhu Team|[2602.12099](http://arxiv.org/abs/2602.12099)|**[link](https://gigabrain05m.github.io/)**|
|**2026-02-12**|**Affordance-Graphed Task Worlds: Self-Evolving Task Generation for Scalable Embodied Learning**|在现实世界中直接训练机器人策略成本高昂且难以扩展，而生成式仿真虽能合成大规模数据，但现有方法常难以生成逻辑连贯的长时序任务，且因开环执行难以应对动态物理不确定性。本研究提出了Affordance-Graphed Task Worlds (AGT-World)，一个统一框架，能够根据现实世界观察自主构建交互式仿真环境及相应的机器人任务策略。AGT-World通过将任务空间形式化为结构化图，实现复杂目标的精确分层分解为原子原语，并引入结合视觉-语言模型推理和几何验证的混合反馈自进化机制来自主优化策略。大量实验证明，AGT-World在成功率和泛化能力上显著优于现有方法，实现了提议、执行和纠错的自改进循环，从而促进可扩展的机器人学习。|Changshui Zhang Team|[2602.12065](http://arxiv.org/abs/2602.12065)|null|
|**2026-02-12**|**VLAW: Iterative Co-Improvement of Vision-Language-Action Policy and World Model**|视觉-语言-动作（VLA）模型的性能和可靠性可通过迭代在线交互提升，但现实世界策略回放数据收集昂贵。虽然学习到的模拟器（动作条件视频生成模型）有望生成额外回放数据，但现有世界模型缺乏物理保真度，难以精确建模接触密集物体操纵中的关键物理细节。本研究提出一种简单的迭代改进算法，利用真实世界回放数据提升世界模型的保真度，然后该世界模型再用于生成补充合成数据以改进VLA模型。在真实机器人实验中，该方法显著提升了最先进VLA模型在多个下游任务上的性能，相比基础策略，成功率绝对提升39.2%，相比使用生成合成回放数据进行训练，提升了11.6%。|Chelsea Finn Team|[2602.12063](http://arxiv.org/abs/2602.12063)|null|
|**2026-02-12**|**HoloBrain-0 Technical Report**|基础模型研究与可靠的真实世界机器人部署之间存在差距。为弥合此差距，本研究引入了HoloBrain-0，一个全面的视觉-语言-动作（VLA）框架。其核心是一个新颖的VLA架构，明确融入了机器人具身先验（如多视角相机参数和运动学描述），以增强3D空间推理并支持多样化的具身形态。该设计通过可扩展的“预训练-后训练”范式进行验证，在RoboTwin 2.0、LIBERO和GenieSim等仿真基准测试中取得了最先进的结果，并在具有挑战性的长时序真实世界操作任务中表现出色。值得一提的是，其高效的0.2B参数变体能与更大的基线模型相媲美，支持低延迟的设备端部署。为加速研究，HoloBrain生态系统已完全开源，包括预训练VLA基础模型、后训练检查点和全栈VLA基础设施RoboOrchard。|Zhizhong Su Team|[2602.12062](http://arxiv.org/abs/2602.12062)|null|
|**2026-02-12**|**FedGRPO: Privately Optimizing Foundation Models with Group-Relative Rewards from Domain Client**|针对联邦基础模型(FedFMs)中现有知识迁移方法存在的训练成本高、通信开销大及隐私风险问题，本研究将其重构为强化学习式评估过程。提出了FedGRPO框架，通过构建轻量级置信图进行能力专家选择，并利用"组相对"概念将问题与解决方案打包为候选策略，仅聚合客户端返回的标量奖励信号，而非直接交换数据或模型更新。实验结果表明，FedGRPO在保证隐私和降低通信开销的同时，实现了优于传统FedFMs基线的下游任务精度和通信效率。|Yuxing Han Team|[2602.12014](http://arxiv.org/abs/2602.12014)|null|
|**2026-02-12**|**Accelerating Robotic Reinforcement Learning with Agent Guidance**|强化学习(RL)在机器人操作技能学习中面临样本效率低下，而现有的人在环(HIL)方法受限于可扩展性与人类监督的低效性。本研究提出了Agent-guided Policy Search (AGPS)框架，用多模态智能体取代人类监督者，将智能体视为语义世界模型，注入内在价值先验来指导物理探索。通过可执行工具，智能体提供精确的纠正航点和空间约束来修剪探索。实验结果表明，AGPS在精细插入和变形物体操作任务中，样本效率优于HIL方法，从而实现了自动化监督，为可扩展的机器人学习提供了新途径。|Yaodong Yang Team|[2602.11978](http://arxiv.org/abs/2602.11978)|null|
|**2026-02-12**|**Where Bits Matter in World Model Planning: A Paired Mixed-Bit Study for Efficient Spatial Reasoning**|为实现高效空间推理，本研究探讨了低比特规划行为在有限精度预算下，是否主要由总比特位宽决定，或由比特位在模型模块间的分配方式决定。通过在DINO-WM上进行混合位评估，观察到8比特和6比特设置接近FP16，3比特设置性能崩溃，而4比特设置对位宽分配敏感。在4比特过渡区，保留编码器精度可提升规划性能。这些发现表明，模块感知、预算感知的量化策略对于高效空间推理具有重要研究意义。|Vaishak Menon Team|[2602.11882](http://arxiv.org/abs/2602.11882)|null|
|**2026-02-12**|**PuYun-LDM: A Latent Diffusion Model for High-Resolution Ensemble Weather Forecasts**|潜在扩散模型(LDMs)在 <=0.25° 的高分辨率集合天气预报中存在扩散性受限问题，且现有频率方法因缺乏对变量间频谱异质性的考虑而导致正则化强度不均。本研究提出了PuYun-LDM模型，结合3D掩码自编码器(3D-MAE)编码天气状态演化特征作为扩散模型的额外条件，并采用变量感知掩码频率建模(VA-MFM)策略自适应选择阈值。实验证明，PuYun-LDM增强了潜在扩散性，在短时效预报中表现优于ENS，长时效预报中与ENS相当，并能在短时间内高效生成全球预报。|Bin Wang Team|[2602.11807](http://arxiv.org/abs/2602.11807)|null|
|**2026-02-12**|**HAIC: Humanoid Agile Object Interaction Control via Dynamics-Aware World Model**|类人机器人在处理非结构化环境中复杂全身任务时，现有方法难以应对具有独立动力学和非完整约束的欠驱动物体，且缺乏外部状态估计。本研究提出了HAIC统一框架，通过仅利用本体感受历史数据估计高阶物体状态的动力学预测器，并将预测投影至静态几何先验形成空间接地动态占用图，使策略能在盲区推断碰撞边界和接触可供性。通过非对称微调，世界模型持续适应学生策略。实验表明，HAIC在敏捷任务和多物体长时序任务中均取得了高成功率，有效应对惯性扰动并预测多物体动力学。|Renjing Xu Team|[2602.11758](http://arxiv.org/abs/2602.11758)|**[link](https://haic-humanoid.github.io/)**|
|**2026-02-12**|**ABot-N0: Technical Report on the VLA Foundation Model for Versatile Embodied Navigation**|具身导航领域长期受限于任务特定的架构。本研究引入了ABot-N0，一个统一的视觉-语言-动作(VLA)基础模型，旨在实现点目标、物体目标、指令跟随、POI目标和人物跟随五项核心导航任务的“大一统”。模型采用分层“大脑-行动”架构，将基于大型语言模型的认知大脑与基于流匹配的动作专家结合。为支持大规模学习，研究团队开发了ABot-N0数据引擎， curating 16.9M专家轨迹和5.0M推理样本。ABot-N0在7个基准测试中取得了新的SOTA性能，并整合规划器与分层拓扑记忆，实现动态真实世界环境中的鲁棒长时序任务。|Mu Xu Team|[2602.11598](http://arxiv.org/abs/2602.11598)|**[link](https://amap-cvlab.github.io/ABot-Navigation/ABot-N0/)**|
|**2026-02-12**|**Brain4FMs: A Benchmark of Foundation Models for Electrical Brain Signal**|脑基础模型(BFMs)在神经科学领域快速发展，但缺乏统一的方法理解和标准化评估框架。本研究旨在填补这一空白，通过自监督学习(SSL)分类法组织BFMs，并从数据集角度总结常见下游任务，整理临床和以人为中心神经技术应用的代表性公共数据集。在此基础上，推出了Brain4FMs开放评估平台，集成了15个代表性BFM和18个公共数据集。该平台支持标准化比较和分析预训练数据、SSL策略和架构对泛化和下游性能的影响，以指导更准确、可迁移的BFMs开发。|Yang Yang Team|[2602.11558](http://arxiv.org/abs/2602.11558)|null|
|**2026-02-12**|**TS-Memory: Plug-and-Play Memory for Time Series Foundation Models**|时间序列基础模型(TSFMs)在分布偏移下的下游领域适应面临挑战，现有参数化适应易导致灾难性遗忘，而非参数化检索则引入高推理延迟。本研究提出了参数化记忆蒸馏方法并实现为TS-Memory，一个轻量级记忆适配器。TS-Memory分两阶段训练：首先构建离线、无信息泄露的kNN教师模型合成置信度感知的量化目标；其次通过置信度门控监督将检索诱导的分布校正蒸馏到记忆适配器中。实验表明，TS-Memory在多种TSFMs和基准测试上持续改进点预测和概率预测性能，且效率与冻结骨干网络相当，实现了无检索部署。|Yuxuan Liang Team|[2602.11550](http://arxiv.org/abs/2602.11550)|null|
|**2026-02-12**|**Budget-Constrained Agentic Large Language Models: Intention-Based Planning for Costly Tool Use**|在预算约束下，大型语言模型调用外部工具解决多步骤任务的工具增强型智能体面临巨大状态-动作空间、高结果方差和过高探索成本导致直接规划难以处理的问题。本研究提出了INTENT，一个推理时规划框架，利用意图感知的层次世界模型来预测未来的工具使用和风险校准成本，并在线指导决策。实验证明，INTENT在成本增强型StableToolBench上严格遵循硬预算可行性，显著提高了任务成功率，并在工具价格和预算动态变化下表现出鲁棒性。|Qi Qi Team|[2602.11541](http://arxiv.org/abs/2602.11541)|null|
|**2026-02-12**|**Vascular anatomy-aware self-supervised pre-training for X-ray angiogram analysis**|X射线血管造影图像分析受限于带标注数据稀缺，大规模自监督学习(SSL)在该领域潜力未被充分挖掘。本研究引入了VasoMIM框架，通过解剖引导的掩码策略强制模型学习血管语义，并利用解剖一致性损失保留血管结构一致性，从而增强学习表征的判别力。同时，策展了迄今最大的X射线血管造影预训练数据集XA-170K。VasoMIM在多个下游任务中展现出卓越的可迁移性和领先的性能，凸显了其作为X射线血管造影分析基础模型的巨大潜力。|Zeng-Guang Hou Team|[2602.11536](http://arxiv.org/abs/2602.11536)|null|
|**2026-02-12**|**Semantic-aware Adversarial Fine-tuning for CLIP**|当前研究表明，通过对抗性微调CLIP图像编码器可增强其零样本分类的对抗鲁棒性，但生成对抗样本（AEs）时仅依赖图像与单一手动模板的余弦相似度，不足以衡量图文对的语义相似性，导致微调后的模型鲁棒性不足。为解决此问题，本文提出了一种语义集成攻击方法，通过最小化原始图像与一组精炼文本描述（由基础模型生成并去除了幻觉）之间的平均相似度来生成语义感知的AEs。在此基础上，作者提出了语义感知对抗微调（SAFT）框架。实验结果表明，SAFT在16个数据集上的零样本对抗鲁棒性方面显著优于现有方法，实现了实质性提升。|Feng Liu Team|[2602.12461](http://arxiv.org/abs/2602.12461)|null|
|**2026-02-12**|**Stabilizing Native Low-Rank LLM Pretraining**|基础模型日益增长的参数量带来了巨大的计算和内存挑战，而低秩分解是降低成本的潜在途径，但从头开始仅使用低秩权重训练模型且性能匹配全秩模型仍缺乏稳定的方法。本文研究表明，无需先验方法的“全秩”辅助指导，大型语言模型（LLMs）可以从头开始仅使用低秩分解权重训练所有非嵌入矩阵。作者发现权重矩阵更新中谱范数（最大奇异值）的失控增长是导致原生低秩训练不稳定和损失尖峰的主要因素，并提出Spectron方法：通过正交化进行谱重归一化，根据因子当前的谱范数动态限制所得权重更新。实验证明，Spectron实现了稳定、端到端的低秩训练，开销可忽略不计，并为原生低秩Transformer建立了计算最优的缩放定律，展示了可预测的幂律行为和相对于全秩模型改进的推理效率。|Eugene Belilovsky Team|[2602.12429](http://arxiv.org/abs/2602.12429)|null|
|**2026-02-12**|**Policy4OOD: A Knowledge-Guided World Model for Policy Intervention Simulation against the Opioid Overdose Crisis**|阿片类药物危机是美国严重的公共卫生问题，但由于政策互动复杂且系统动态，评估干预措施极具挑战。本文提出Policy4OOD，一个知识引导的时空世界模型，旨在整合预测、反事实推理和优化三种关键能力来有效评估阿片类政策。该模型通过策略知识图谱、州级空间依赖性及社会经济时间序列的联合编码，构建一个策略条件化的Transformer来预测阿片类药物相关结果。训练完成后，世界模型可作为模拟器，通过前向传播进行预测，通过替换历史策略编码进行反事实分析，并通过蒙特卡洛树搜索进行策略优化。实验结果表明，空间依赖性和结构化策略知识显著提高了预测准确性，验证了该模型在数据驱动的公共卫生决策支持中的潜力。|Yanfang Ye Team|[2602.12373](http://arxiv.org/abs/2602.12373)|null|
|**2026-02-12**|**Free Lunch in Medical Image Foundation Model Pre-training via Randomized Synthesis and Disentanglement**|医学图像基础模型（MIFMs）在临床任务中展现巨大潜力，但其发展受限于大规模标注数据集的稀缺、异质性和高成本。本文提出RaSD（Randomized Synthesis and Disentanglement），一个可扩展的框架，可完全利用合成数据预训练MIFMs。RaSD通过随机高斯分布模拟解剖结构和外观变异，使模型接触足够的多尺度结构和外观扰动，从而迫使其依赖不变和任务相关的解剖线索而非数据集特有纹理，实现鲁棒和可迁移的表示学习。在120万3D体和960万2D图像上进行预训练后，RaSD模型在6种成像模态、48个数据集和56个下游任务中，持续优于从零开始训练的模型，在17个任务上取得了最佳性能，并在大多数其他任务上与使用大型真实数据集预训练的模型表现相当。这些结果证明了仅合成数据即可驱动鲁棒表示学习的能力，为医学AI领域带来了范式转变。|Hao Chen Team|[2602.12317](http://arxiv.org/abs/2602.12317)|null|

<p align=right>(<a href=#updated-on-20260219>back to top</a>)</p>

## VLM

|Publish Date|Title|Chinese Summary|Authors|PDF|Code|
|---|---|---|---|---|---|
|**2026-02-18**|**Saliency-Aware Multi-Route Thinking: Revisiting Vision-Language Reasoning**|现有视觉-语言模型（VLMs）在推理时计算扩展面临挑战，主要问题在于视觉输入通常只在生成开始时提供一次，导致推理趋向于文本主导，并可能累积早期视觉理解错误。针对此问题，本文提出了显著性感知原则（Saliency-Aware Principle, SAP）选择方法。SAP通过操作高层次推理原则而非令牌级轨迹，实现了在噪声反馈下对离散生成的稳定控制，并允许在需要时重新参考视觉证据。此外，SAP支持多路径推理。实验结果表明，SAP在相似的令牌生成预算下，特别是在减少目标幻觉方面，取得了具有竞争力的性能，并展现出比传统CoT式长序列推理更稳定的推理和更低的响应延迟。|Jundong Li Team|[2602.16702](http://arxiv.org/abs/2602.16702)|null|
|**2026-02-18**|**A Contrastive Learning Framework Empowered by Attention-based Feature Adaptation for Street-View Image Classification**|街景图像属性分类是自动驾驶、城市分析等领域的关键任务，但现有方法计算量大。预训练视觉-语言模型（如CLIP）提供的全局图像嵌入难以捕捉复杂街景中细粒度的局部属性。为解决此问题，本文提出了CLIP-MHAdapter，一种轻量级CLIP适配范式，它在补丁令牌上附加一个带有多头自注意力机制的瓶颈多层感知机（MLP），以建模补丁间的依赖关系。CLIP-MHAdapter拥有约140万个可训练参数，在Global StreetScapes数据集的八项属性分类任务上实现了卓越或具有竞争力的准确性，取得了新的SOTA结果，同时保持了较低的计算成本。|James Haworth Team|[2602.16590](http://arxiv.org/abs/2602.16590)|null|
|**2026-02-18**|**DressWild: Feed-Forward Pose-Agnostic Garment Sewing Pattern Generation from In-the-Wild Images**|当前的服装版型生成方法在处理多样姿态和视角时表现不佳，或计算成本高昂且难以扩展，尤其难以满足可编辑、可分离和可仿真的需求。为此，本文提出了DressWild，一个新颖的前馈流水线，能够从单张野外图像重建物理一致的2D缝纫版型和相应的3D服装。该方法利用视觉-语言模型（VLMs）在图像层面规范姿态变化，然后提取姿态感知和3D感知的服装特征，并通过基于Transformer的编码器进行融合，最终预测可直接用于物理仿真、纹理合成和多层虚拟试穿的缝纫版型参数。大量实验证明，该方法能稳定地从野外图像中恢复多样化的缝纫版型和3D服装，无需多视角输入或迭代优化，为逼真的服装仿真和动画提供了高效且可扩展的解决方案。|Chenfanfu Jiang Team|[2602.16502](http://arxiv.org/abs/2602.16502)|null|
|**2026-02-18**|**Visual Self-Refine: A Pixel-Guided Paradigm for Accurate Chart Parsing**|尽管大型视觉-语言模型（LVLMs）在文本推理和自我修正方面表现出色，但这些优势在以视觉感知为中心的复杂任务（如图表解析）中效果甚微，现有模型常在处理视觉密集图表时出现数据遗漏、错位和幻觉。受人类阅读复杂图表时使用手指作为“视觉锚点”以确保准确性的启发，本文提出了视觉自我修正（Visual Self-Refine, VSR）新范式。VSR使模型能够生成像素级定位输出，并将其可视化反馈给自身以直观地检查和修正潜在的视觉感知错误。本文通过ChartVSR模型在图表解析领域实例化了VSR范式，该模型将解析过程分解为修正阶段（迭代使用视觉反馈确保数据点像素级定位的准确性）和解码阶段（使用验证后的定位作为视觉锚点解析最终结构化数据）。为解决现有基准的局限性，本文还构建了新的挑战性图表解析基准ChartP-Bench。这项工作强调了VSR作为通用视觉反馈机制的潜力，为提升一系列视觉中心任务的准确性提供了新方向。|Dahua Lin Team|[2602.16455](http://arxiv.org/abs/2602.16455)|null|
|**2026-02-18**|**Designing Production-Scale OCR for India: Multilingual and Domain-Specific Systems**|为印度设计光学字符识别（OCR）系统面临语言多样性、文档异构性和部署限制等多重挑战。本文通过Chitrapathak系列研究了两种利用视觉-语言模型构建多语言OCR系统的训练策略：一是主流的多模态方法，将通用视觉编码器与强大的多语言语言模型配对并端到端训练；二是探索微调一个现有但未针对目标语言训练的OCR模型。此外，本文还提出了Parichay系列，一个专为9种印度政府文件设计用于提取结构化关键字段的独立OCR模型。广泛评估表明，第二种微调策略在精度-延迟权衡方面表现更优，Chitrapathak-2比其前身提速3-6倍，并在泰卢固语上达到SOTA，在其他语言上排名第二。Parichay以更快的推理速度达到了89.8%的精确匹配分数。这些系统共同实现了SOTA性能，并为在印度构建生产级OCR流水线提供了实用指导。|Shubham Agarwal Team|[2602.16430](http://arxiv.org/abs/2602.16430)|null|
|**2026-02-18**|**Peeking Ahead of the Field Study: Exploring VLM Personas as Support Tools for Embodied Studies in HCI**|实地研究虽不可替代但成本高昂、耗时且易出错。受快速原型开发的启发，本文提出了一种利用视觉-语言模型（VLM）角色模拟实地结果的快速低成本评估方法。鉴于自动驾驶车辆（AV）与行人交互需要空间意识、情感共情和行为生成，本文探讨了VLM角色在多大程度上能模仿实地研究中的人类反应。研究通过一项包含20名参与者的真实世界研究和一项使用20个VLM角色的视频研究并行进行街头穿越任务，并比较了它们的反应。结果显示，VLM角色能模仿人类的反应模式（例如，平均穿越时间分别为5.25秒和5.07秒），但在行为变异性和深度上有所欠缺。该方法在形成性研究、实地研究准备和人类数据增强方面展现了潜力。|Takeo Igarashi Team|[2602.16157](http://arxiv.org/abs/2602.16157)|null|
|**2026-02-18**|**Evaluating Demographic Misrepresentation in Image-to-Image Portrait Editing**|文本到图像（T2I）生成中的人口学偏见已被广泛研究，但指令引导的图像到图像（I2I）编辑中与人口学相关的失败模式仍未被充分探索。本文考察了在开源I2I编辑器中，相同的编辑指令是否会导致不同人口学特征主题产生系统性差异的输出。研究将两种失败模式形式化：软擦除（Soft Erasure），即编辑在输出图像中被静默削弱或忽略；刻板印象替换（Stereotype Replacement），即编辑引入了未请求的、符合刻板印象的属性。本文建立了一个受控基准，通过生成和编辑基于种族、性别和年龄条件的肖像，并使用诊断提示集进行探测，并通过视觉-语言模型（VLM）评分和人工评估来评估多个编辑器。分析表明，身份保留失败普遍存在、人口学分布不均，并受隐含社会先验（包括职业驱动的性别推断）的影响。最后，研究证明在不更新模型的情况下，提示级别的身份约束可以显著减少少数群体的图像变化，同时多数群体的肖像基本保持不变，揭示了当前编辑器中不对称的身份先验。研究结果确立了身份保留作为I2I编辑中一个核心且人口学分布不均的失败模式，并为开发人口学鲁棒的编辑系统提供了动力。|Jean Oh Team|[2602.16149](http://arxiv.org/abs/2602.16149)|null|
|**2026-02-18**|**IRIS: Intent Resolution via Inference-time Saccades for Open-Ended VQA in Large Vision-Language Models**|在开放式视觉问答（VQA）中，解决歧义对于大型视觉-语言模型（LVLMs）而言是一个挑战。本文引入了IRIS（Intent Resolution via Inference-time Saccades），一种新颖的免训练方法，通过实时眼动追踪数据来解决VQA中的歧义。研究通过一项包含500对独特图像-问题的用户研究表明，在参与者开始口头提问时最近的注视点对于LVLMs的消歧最为有效，使歧义问题的响应准确率提高了一倍以上（从35.2%增至77.2%），同时保持了非歧义问题的性能。该方法在多种先进LVLMs上进行了评估，结果显示在整合凝视数据后，歧义图像-问题对的性能得到了一致提升，无论模型架构差异如何。本文还发布了一个新的基准数据集，用于利用眼动数据进行消歧VQA，一个新颖的实时交互协议和一套评估套件。|Miguel P. Eckstein Team|[2602.16138](http://arxiv.org/abs/2602.16138)|null|
|**2026-02-18**|**OmniCT: Towards a Unified Slice-Volume LVLM for Comprehensive CT Analysis**|计算机断层扫描（CT）是广泛应用的诊断成像方式，但现有大型视觉-语言模型（LVLMs）在CT切片和体素理解上存在割裂，切片驱动模型泛化性强但缺乏跨切片空间一致性，而体素驱动模型虽捕获体素语义但粒度粗且与切片输入兼容性差。为解决这种统一建模范式的缺失，本文提出了OmniCT，一个强大的CT场景统一切片-体素LVLM。其贡献包括：(i) 空间一致性增强（SCE）：通过体素切片组合和三轴位置嵌入引入体素一致性，并采用MoE混合投影实现高效切片-体素适应。(ii) 器官级语义增强（OSE）：通过分割和ROI定位明确对齐解剖区域，强调病灶和器官级语义。(iii) MedEval-CT：最大的切片-体素CT数据集和混合基准，集成了全面的统一评估指标。OmniCT在多样化的临床任务中始终以显著优势超越现有方法，同时满足微观细节敏感性和宏观空间推理能力，为跨模态医学影像理解建立了新范式。|Beng Chin Ooi Team|[2602.16110](http://arxiv.org/abs/2602.16110)|null|
|**2026-02-17**|**MedProbCLIP: Probabilistic Adaptation of Vision-Language Foundation Model for Reliable Radiograph-Report Retrieval**|视觉-语言基础模型在多模态理解方面潜力巨大，但其确定性嵌入在生物医学等高风险应用中缺乏所需可靠性，尤其难以捕获胸部X射线与放射学报告之间的不确定性和多对多对应关系。本文提出了MedProbCLIP，一个用于胸部X射线和放射学报告表示学习及双向检索的概率视觉-语言学习框架。MedProbCLIP通过概率对比目标将图像和文本表示建模为高斯嵌入，显式捕获不确定性和多对多对应关系。变分信息瓶颈缓解了过度自信的预测，并且MedProbCLIP在训练过程中采用多视图X射线编码和多节报告编码以提供临床对齐的细粒度监督，但在推理时仅需单张X射线和单份报告。在MIMIC-CXR数据集上的评估表明，MedProbCLIP在检索和零样本分类方面均优于确定性和概率基线模型（包括CLIP、CXR-CLIP和PCME++）。除了准确性，MedProbCLIP还展示了卓越的校准性、风险覆盖行为、选择性检索可靠性以及对临床相关损坏的鲁棒性，突出了概率视觉-语言建模在提升放射学图像-文本检索系统可信度和安全性方面的价值。|Gongbo Liang Team|[2602.16019](http://arxiv.org/abs/2602.16019)|null|
|**2026-02-17**|**BTReport: A Framework for Brain Tumor Radiology Report Generation with Clinically Relevant Features**|针对脑肿瘤放射学报告生成（RRG）领域缺乏开放图像-报告配对数据集的挑战，本研究提出了BTReport框架。该方法通过确定性提取影像特征进行图像分析，并仅利用大型语言模型（LLM）进行句法结构和叙事格式化，从而将图像解释与报告生成解耦。实验结果表明，BTReport生成的报告具有高可解释性，并能有效预测临床结果，且与现有基线相比，其报告与参考临床报告更吻合。|Mehmet Kurt Team|[2602.16006](http://arxiv.org/abs/2602.16006)|null|
|**2026-02-17**|**Can Vision-Language Models See Squares? Text-Recognition Mediates Spatial Reasoning Across Three Model Families**|针对视觉-语言模型（VLM）在准确识别缺乏文本标识的非文本视觉元素时存在局限性，本研究通过实验揭示了这一根本性问题。研究者生成了包含文本符号和填充方块两种图像类型的网格，并要求前沿VLM进行转录。结果显示，模型在文本符号条件下表现良好，但在填充方块条件下性能显著下降，表明VLM的文本识别路径在空间推理方面远优于其原生视觉路径，且对非文本视觉元素的空间定位能力严重不足。|Yuval Levental Team|[2602.15950](http://arxiv.org/abs/2602.15950)|null|
|**2026-02-17**|**Visual Memory Injection Attacks for Multi-Turn Conversations**|鉴于大型生成式视觉-语言模型（LVLM）在长上下文多轮对话中安全性的研究不足，本论文提出了一种名为视觉记忆注入（VMI）的隐蔽攻击。该攻击通过操纵图像，使得LVLM在正常提示下行为正常，但在触发提示下输出预设的目标信息以操纵用户。实验证明，VMI攻击在多轮对话中依然有效，并在多个开源LVLM上得到验证，揭示了通过扰动图像进行大规模用户操纵的可行性，强调了提高LVLM对抗此类攻击的鲁棒性需求。|Matthias Hein Team|[2602.15927](http://arxiv.org/abs/2602.15927)|null|
|**2026-02-17**|**Concept-Enhanced Multimodal RAG: Towards Interpretable and Accurate Radiology Report Generation**|针对放射学报告生成（RRG）中视觉-语言模型（VLM）存在的解释性差和易产生“幻觉”的问题，本研究提出了概念增强多模态检索增强生成（CEMRAG）框架。该框架将视觉表示分解为可解释的临床概念，并将其与多模态RAG结合，通过丰富的上下文提示提升RRG的解释性和事实准确性。实验结果表明，CEMRAG在多个数据集和VLM架构下，在临床准确性和NLP指标上均优于现有基线，挑战了可解释性与性能之间的权衡假设，为医疗VLM提供了可信赖的AI辅助放射学途径。|Valerio Guarrasi Team|[2602.15650](http://arxiv.org/abs/2602.15650)|null|
|**2026-02-17**|**CARE Drive A Framework for Evaluating Reason-Responsiveness of Vision Language Models in Automated Driving**|针对自动驾驶中基础模型评估主要关注结果性能而忽视决策是否反映人类相关考量的问题，本研究提出了CARE Drive框架。该框架独立于模型，通过在受控上下文变化下比较基线和原因增强模型决策，评估人类原因对决策行为的因果影响。在一个骑车人超车场景中，结果显示明确的人类原因显著影响模型决策，提高了与专家推荐行为的一致性，证明了可以在不修改模型参数的情况下系统评估基础模型的原因响应性。|Arkady Zgonnikov Team|[2602.15645](http://arxiv.org/abs/2602.15645)|null|
|**2026-02-17**|**Req2Road: A GenAI Pipeline for SDV Test Artifact Generation and On-Vehicle Execution**|为解决软件定义汽车（SDV）功能测试中需求分散、规范复杂以及测试资产异构的挑战，本研究提出了一种自动化管道。该管道利用大型语言模型和视觉-语言模型提取信号和行为逻辑，自动生成Gherkin场景并转换为可运行的测试脚本，并通过VSS集成和检索增强生成（RAG）实现标准化。在儿童存在检测系统（CPDS）上的评估显示，89%的需求可转化为可执行场景，验证了该端到端需求-测试管道在仿真和在环测试环境中的可行性，但仍需人工审查。|Alois Knoll Team|[2602.15591](http://arxiv.org/abs/2602.15591)|null|
|**2026-02-17**|**VLM-DEWM: Dynamic External World Model for Verifiable and Resilient Vision-Language Planning in Manufacturing**|针对视觉-语言模型（VLM）在智能制造动态工作单元中面临的无状态操作和不透明推理挑战，本研究提出了VLM-DEWM认知架构。该架构通过一个持久且可查询的动态外部世界模型（DEWM），将VLM推理与世界状态管理解耦，并结构化每个VLM决策为可外部化的推理轨迹。实验结果表明，VLM-DEWM显著提高了状态跟踪准确性（从56%提升至93%）和恢复成功率（从低于5%提升至95%），并通过结构化内存减少了计算开销，为动态制造环境中的长周期机器人操作提供了可靠且弹性的解决方案。|Ning Ji Team|[2602.15549](http://arxiv.org/abs/2602.15549)|null|
|**2026-02-17**|**Selective Perception for Robot: Task-Aware Attention in Multimodal VLA**|针对机器人领域中视觉-语言-动作（VLA）模型静态融合多视角输入导致的计算冗余和噪声问题，本研究提出了一种动态信息融合框架。该框架引入轻量级自适应路由架构，实时分析文本提示和腕部摄像头观测，预测多摄像头视图的任务相关性，并有选择地向策略网络提供必要视觉特征。为路由器训练，本研究还开发了利用VLM的自动化标注管道。实验结果表明，该方法在真实世界机器人操纵场景中显著提高了推理效率和控制性能，验证了动态信息融合在资源受限实时机器人控制环境中的有效性。|Soo-Chul Lim Team|[2602.15543](http://arxiv.org/abs/2602.15543)|null|
|**2026-02-17**|**Semantic-Guided 3D Gaussian Splatting for Transient Object Removal**|为解决3D Gaussian Splatting (3DGS) 重建中瞬态物体导致的重影伪影问题，本研究提出了一种基于视觉-语言模型（VLM）的语义过滤框架。该方法通过计算渲染视图与干扰文本提示之间的CLIP相似度得分，并累积到每个高斯函数上，对超出校准阈值的高斯函数进行不透明度正则化和周期性修剪。实验结果表明，该方法在RobustNeRF基准测试中持续改善了重建质量，同时保持了最小内存开销和实时渲染性能，通过语义分类有效解决了视差模糊问题。|Priyesh Shukla Team|[2602.15516](http://arxiv.org/abs/2602.15516)|null|
|**2026-02-17**|**On the Out-of-Distribution Generalization of Reasoning in Multimodal LLMs for Simple Visual Planning Tasks**|本研究旨在深入探究大型语言模型和视觉-语言模型中思维链（CoT）推理方法的泛化能力，尤其是在简单规划任务上的表现。研究者提出了一个评估框架，对基于网格的导航任务中不同输入表示和CoT策略的模型进行了微调和系统评估。实验结果表明，CoT推理能提高分布内泛化能力，但对分布外（如更大地图）的泛化能力多数情况下仍非常有限；值得注意的是，结合多种文本格式的推理轨迹产生了最佳的分布外泛化效果，且纯文本模型表现优于基于图像输入的模型。|Francesco Croce Team|[2602.15460](http://arxiv.org/abs/2602.15460)|null|
|**2026-02-16**|**BPP: Long-Context Robot Imitation Learning by Focusing on Key History Frames**|在机器人任务中，传统策略因仅依赖当前观测而无法有效利用历史信息，导致在需要记忆的任务中泛化性差，尤其是在部署时容易受训练中虚假关联的影响。为解决此问题，本研究提出大图策略（BPP），通过视觉-语言模型识别并利用一组最小的关键帧作为历史观测条件。实验结果表明，BPP显著减少了训练与部署间的分布偏移，并在四项真实世界操作任务和三项仿真任务中，成功率比现有最佳方法提高了70%。|Aviral Kumar Team|[2602.15010](http://arxiv.org/abs/2602.15010)|null|
|**2026-02-16**|**ThermEval: A Structured Benchmark for Evaluation of Vision-Language Models on Thermal Imagery**|当前视觉语言模型（VLMs）在RGB图像上表现优异，但无法泛化至夜间监控、搜救等关键场景的热成像图像，且现有基准无法评估其对温度感知和推理能力。本研究引入ThermEval-B，一个包含5.5万个热视觉问答对的结构化基准，并整合了ThermEval-D数据集，首次提供带有语义身体部位标注的密集逐像素温度图。实验评估25个VLM后发现，模型在温度推理上普遍失败，在色图变换下性能下降，且容易依赖语言先验，提示或微调仅带来微弱提升，证实了热图像理解需专门评估。 |Nipun Batra Team|[2602.14989](http://arxiv.org/abs/2602.14989)|null|
|**2026-02-16**|**DM0: An Embodied-Native Vision-Language-Action Model towards Physical AI**|针对传统方法将互联网预训练模型适配到物理任务的局限性，本研究提出了DM0，一个具身原生（Embodied-Native）的视觉-语言-动作（VLA）框架，旨在为物理AI提供统一的具身操作和导航能力。该框架采用三阶段训练流水线：首先对VLM进行大规模统一预训练，整合网络文本、自动驾驶和具身交互日志数据，随后构建流匹配动作专家，并通过混合训练策略及具身空间支架策略实现高层推理与低层控制的协调。DM0在RoboChallenge基准测试中，于专业和通用设置下均取得了最先进的性能。|Tiancai Wang Team|[2602.14974](http://arxiv.org/abs/2602.14974)|**[link](https://github.com/Dexmal/dexbotic)**|
|**2026-02-16**|**MATEO: A Multimodal Benchmark for Temporal Reasoning and Planning in LVLMs**|AI智能体实现复杂目标需要精细规划，其中时间执行顺序（TEO）至关重要，但现有基础模型对TEO的理解研究不足，多限于线性近似或纯文本输入。为弥补这一空白，本研究引入MATEO基准，旨在评估和提升大型视觉语言模型（LVLMs）的时间推理能力。通过收集高质量多模态菜谱语料并利用众包构建TEO图谱，MATEO提供了丰富的标注。对六个主流LVLMs的评估揭示了语言上下文、多模态输入结构和微调策略对时间推理能力的关键影响，突显了当前LVLMs在该领域的局限性。|Giuseppe Riccardi Team|[2602.14589](http://arxiv.org/abs/2602.14589)|null|
|**2026-02-16**|**Replanning Human-Robot Collaborative Tasks with Vision-Language Models via Semantic and Physical Dual-Correction**|人机协作（HRC）在装配任务中面临人类指令模糊导致机器人行为不可靠的问题，现有基于VLM的方法虽能解释指令，但易产生幻觉推理和物理执行失败。为此，本研究提出了一个HRC框架，通过引入双重校正机制增强VLM的推理能力。该机制包含一个内部校正模型在执行前验证逻辑和可行性，以及一个外部校正模型通过事后反馈纠正物理失败。仿真和真实世界实验均表明，该框架显著提高了任务成功率，并能有效支持机器人根据人类指令进行交互式重规划。|Kensuke Harada Team|[2602.14551](http://arxiv.org/abs/2602.14551)|null|
|**2026-02-16**|**Error Patterns in Historical OCR: A Comparative Analysis of TrOCR and a Vision-Language Model**|18世纪印刷文本的OCR因降级打印、古体字和非标准化拼写而充满挑战，现有基于Transformer和VLM的OCR系统虽总体准确率高，但CER和WER等指标对学术使用可靠性洞察有限。本研究通过比较专用OCR Transformer（TrOCR）和通用VLM（Qwen）在历史英文文本线级识别上的表现，发现Qwen虽在总体准确率和鲁棒性方面占优，但存在选择性语言正则化和拼写规范化，可能改变历史原貌。TrOCR则能更好地保持拼写忠实度，但易出现级联错误。研究强调了架构归纳偏差对OCR错误结构的影响，以及在历史文献数字化中进行架构感知评估的必要性。|Mikko Tolonen Team|[2602.14524](http://arxiv.org/abs/2602.14524)|null|
|**2026-02-16**|**S2D: Selective Spectral Decay for Quantization-Friendly Conditioning of Neural Activations**|大规模Transformer模型中的激活异常值对模型量化构成严重挑战，导致量化精度显著下降，且随预训练规模的增加而加剧。本研究通过理论分析和实证观察，揭示了激活异常值与权重的主奇异值之间的直接联系。在此基础上，提出选择性谱衰减（S²D）方法，在微调阶段仅对最大奇异值对应的权重分量进行正则化。实验证明S²D显著减少了激活异常值，使得模型在W4A4量化下PTQ精度提升高达7%，结合QAT时提升4%，且泛化至下游任务和视觉-语言模型，提高了大规模模型的部署效率。|Deepak Gupta Team|[2602.14432](http://arxiv.org/abs/2602.14432)|null|
|**2026-02-16**|**Multi-Turn Adaptive Prompting Attack on Large Vision-Language Models**|针对多轮越狱攻击在大型视觉-语言模型（LVLMs）中因视觉输入过于恶意而易被防御的问题，本研究提出了MAPA（Multi-turn Adaptive Prompting Attack）方法。MAPA在每个回合中交替使用文本和视觉攻击动作以引发最恶意响应，并在跨回合中通过迭代式来回调整攻击轨迹，逐步放大响应的恶意性。这种双层设计使MAPA持续优于现有最先进方法，在对抗Llava-V1.6-Mistral-7B、Qwen2.5-VL-7B-Instruct、Llama-3.2-Vision-11B-Instruct和GPT-4o-mini等模型的最新基准测试中，攻击成功率提高了11-35%。|Yiliao Song Team|[2602.14399](http://arxiv.org/abs/2602.14399)|null|
|**2026-02-15**|**Moving Beyond Sparse Grounding with Complete Screen Parsing Supervision**|现代计算机使用智能体（CUA）需要对屏幕进行结构化感知才能可靠地理解指令和执行操作，但现有标注数据集稀疏且多样性不足，限制了其泛化能力，且实际部署需要高效率。本研究引入ScreenParse，一个用于完整屏幕解析的大规模数据集，包含771K个网页截图的密集标注。通过Webshot自动化流程及VLM辅助，ScreenParse提供了详尽的UI元素信息。基于此数据集，我们训练了紧凑型ScreenVLM，其在密集解析上显著优于更大规模的基础VLM，并在公共基准上展现了强大的迁移能力，证明了密集屏幕监督能为UI理解提供可迁移的结构先验。|Peter Staar Team|[2602.14276](http://arxiv.org/abs/2602.14276)|null|
|**2026-02-15**|**Dual-Signal Adaptive KV-Cache Optimization for Long-Form Video Understanding in Vision-Language Models**|视觉-语言模型（VLMs）在处理长视频时面临内存瓶颈，因KV缓存随序列长度线性增长，而现有驱逐策略先计算完整注意力矩阵再丢弃token，导致计算浪费。本研究提出Sali-Cache，一个先验优化框架，通过主动内存管理实现双信号自适应缓存。该方法结合光流分析的时间滤波器和显著性检测的空间滤波器，在注意力操作前智能管理内存。实验表明，Sali-Cache在LLaVA 1.6架构上实现了2.20倍的内存压缩比，同时保持100%的准确率，并在相同内存预算下能更长时间保留上下文特征，实现了在消费级硬件上高效处理长视频内容。|Priyesh Shukla Team|[2602.14236](http://arxiv.org/abs/2602.14236)|null|
|**2026-02-15**|**Index Light, Reason Deep: Deferred Visual Ingestion for Visual-Dense Document Question Answering**|当前多模态文档问答系统采用“供给侧摄取”策略，即在索引阶段全面生成视觉描述，导致成本高昂且不可靠。本文提出了“延迟视觉摄取（DVI）”框架，该框架采用“需求侧摄取”策略，仅在索引阶段进行轻量级元数据提取以实现页面定位，将视觉理解延迟到用户提问时，再将原始图像与特定问题发送给视觉语言模型进行分析。实验结果表明，DVI在零摄取视觉语言模型成本下，取得了与现有方法相当的整体准确率（46.7% vs 48.9%），在视觉必要查询上的有效率达50%，并实现了100%的页面定位率，有效将问答准确率问题转化为页面定位问题。|Tao Xu Team|[2602.14162](http://arxiv.org/abs/2602.14162)|null|
|**2026-02-15**|**Annotation-Efficient Vision-Language Model Adaptation to the Polish Language Using the LLaVA Framework**|现有视觉-语言模型（VLMs）多基于英语数据训练，限制了其在其他语言和文化背景下的应用。为解决此问题，本研究复现并调整了LLaVA-Next方法，通过全自动化流程翻译、过滤现有数据集并补充合成数据，构建了一套波兰语VLM。实验结果显示，该方法在波兰语改编的MMBench上相较于LLaVA-1.6-Vicuna-13B实现了9.5%的性能提升，并在生成性评估中，其生成的标题在语言正确性方面获得了更高评价，证明大规模自动化翻译结合轻量级过滤能有效为低资源语言引导高质量多模态模型。|Wojciech Kusa Team|[2602.14073](http://arxiv.org/abs/2602.14073)|null|
|**2026-02-15**|**MarsRetrieval: Benchmarking Vision-Language Models for Planetary-Scale Geospatial Retrieval on Mars**|行星科学中，现有深度学习基准多局限于监督视觉任务，不支持文本引导的地理空间发现。为此，本研究引入了MarsRetrieval，一个用于评估视觉-语言模型在火星地理空间发现能力的检索基准，包含图像-文本检索、地貌检索和全球地理定位等任务。研究提出统一的检索协议以评估多模态嵌入架构。实验结果表明MarsRetrieval极具挑战性，即使是强大的基础模型也难以捕捉领域特定的地貌区别，且领域特定微调对于行星环境中的可泛化地理空间发现至关重要。|Hongxin Wei Team|[2602.13961](http://arxiv.org/abs/2602.13961)|null|
|**2026-02-14**|**RMPL: Relation-aware Multi-task Progressive Learning with Stage-wise Training for Multimedia Event Extraction**|多媒体事件抽取（MEE）受标注数据缺乏限制，现有基准M2E2仅提供评估标注，导致直接监督训练困难，现有方法未能有效学习结构化事件表示。为解决这些局限，本文提出了RMPL（Relation-aware Multi-task Progressive Learning）框架，用于低资源条件下的MEE。RMPL通过阶段式训练，整合了来自单模态事件抽取和多媒体关系抽取的异构监督，先学习事件中心表示，再进行微调。实验结果表明，在M2E2基准上，结合多个视觉语言模型的RMPL在不同模态设置下均显示出持续的性能改进。|Yu Hong Team|[2602.13748](http://arxiv.org/abs/2602.13748)|null|
|**2026-02-14**|**Fine-tuned Vision Language Model for Localization of Parasitic Eggs in Microscopic Images**|土壤传播性蠕虫（STH）感染在全球范围广泛，但诊断专业知识有限，手动显微镜诊断耗时且易错。为实现自动化诊断，本文旨在利用经过微调的视觉语言模型（VLM），例如Microsoft Florence，来定位显微图像中的所有寄生虫卵。初步实验结果显示，该定位VLM的mIOU达到了0.94，优于其他目标检测方法，表明其有望成为自动化框架的核心组件，为智能寄生虫诊断提供可扩展的工程解决方案。|Nouar AlDahoul Team|[2602.13712](http://arxiv.org/abs/2602.13712)|null|
|**2026-02-14**|**LeafNet: A Large-Scale Dataset and Comprehensive Benchmark for Foundational Vision-Language Understanding of Plant Diseases**|基础模型和视觉-语言预训练在VLM领域取得显著进展，但在植物病理学等农业特定领域的应用受限于缺乏大规模、全面的多模态数据集和基准。为弥补此空白，本研究引入了LeafNet数据集和LeafBench视觉问答基准，涵盖97种病害的18.6万张图像和13,950个问答对。对12个先进VLM的基准测试揭示了其疾病理解能力的显著差异，二元分类准确率超90%，但细粒度识别低于65%。研究证实，多模态架构整合语言表示显著增强了诊断精度，凸显了LeafBench对VLM在植物病理学应用中方法学进展和评估的重要性。|Luyl-Da Quach Team|[2602.13662](http://arxiv.org/abs/2602.13662)|null|
|**2026-02-14**|**KorMedMCQA-V: A Multimodal Benchmark for Evaluating Vision-Language Models on the Korean Medical Licensing Examination**|当前视觉-语言模型（VLM）评估基准多为英语或通用领域，缺乏针对韩语医疗领域的多模态问答基准。为此，本研究引入了KorMedMCQA-V，一个韩语医学执照考试风格的多模态多项选择问答基准，包含1534个问题和2043张图像，涵盖多种临床模态，且约30%的问题需整合多图像证据。在统一零样本评估协议下，对50多个VLM的基准测试显示，最佳专有模型准确率达96.9%，最佳开源模型为83.7%，而最佳韩语专用模型仅为43.2%。研究还发现推理导向模型性能显著提升，医学领域专业化收益不一，所有模型在多图像问题上表现下降，且性能因成像模态而异。|Edward Choi Team|[2602.13650](http://arxiv.org/abs/2602.13650)|null|
|**2026-02-14**|**Towards Sparse Video Understanding and Reasoning**|现有视频问答（VQA）方法通常均匀采样视频帧，效率低下且未能有效捕捉关键信息，在多轮VQA中存在挑战。本研究提出了REVISE（Reasoning with Video Sparsity），一个多轮视频问答智能体，它选择少量信息帧，在多轮中维护摘要状态，并在有信心时提前停止。为微调开源模型，引入了EAGER（Evidence-Adjusted Gain for Efficient Reasoning）这一无标注奖励机制，包含置信度增益、摘要充分性和正确且提前停止三项。在多个VQA基准测试中，REVISE在提高准确率的同时，显著减少了帧数、轮次和提示token，展现了实用的稀疏视频推理能力。|Han Liu Team|[2602.13602](http://arxiv.org/abs/2602.13602)|null|
|**2026-02-14**|**AdaVBoost: Mitigating Hallucinations in LVLMs via Token-Level Adaptive Visual Attention Boosting**|大规模视觉-语言模型（LVLMs）存在幻觉问题，现有视觉注意力增强方法通过预定义缩放缓解，但固定缩放因子可能在不同生成步骤中表现出过弱或过强的局限性。为解决此问题，本文提出了AdaVBoost，一个token级别的自适应视觉注意力增强框架，旨在每个生成步骤动态确定注意力增强的程度。该框架引入视觉接地熵（VGE）来估计幻觉风险，并根据VGE对高风险token施加强视觉注意力增强，对低风险token施加弱增强。实验结果表明，AdaVBoost在多个LVLMs和幻觉基准测试中显著优于基线方法。|Tianyu Pang Team|[2602.13600](http://arxiv.org/abs/2602.13600)|null|
|**2026-02-14**|**OpAgent: Operator Agent for Web Navigation**|自主网络智能体在复杂且不稳定的真实网站环境中，面临现有SFT或离线RL方法因分布漂移而导致的性能局限。本文提出了一个强大的在线强化学习WebAgent，通过与非受限广域网站的直接迭代交互来优化策略。该方法包含分层多任务微调，建立了强大的VLM；开发了在线交互环境和RL管道，引入混合奖励机制缓解长期导航中的信用分配挑战；并提出了OpAgent模块化框架，整合规划器、接地器、反射器和总结器以实现错误恢复和自校正。实验结果显示，RL增强模型在WebArena上成功率达38.1%，而OpAgent框架进一步提升至71.6%，达到新的SOTA水平。|Peng Di Team|[2602.13559](http://arxiv.org/abs/2602.13559)|null|
|**2026-02-13**|**Steerable Vision-Language-Action Policies for Embodied Reasoning and Hierarchical Control**|预训练视觉语言模型(VLMs)拥有丰富的常识先验知识，但在机器人控制中有效落地仍面临挑战，现有分层方法中VLM对低层行为的引导受限于自然语言接口。为此，研究者提出了“可操控策略”(Steerable Policies)，通过在不同抽象级别（如子任务、动作、像素坐标）的丰富合成指令上训练视觉语言动作模型(VLA)，以提升低层可控性并释放VLM的预训练知识。实验结果表明，无论是通过学习型高层具身推理器还是即插即用VLM控制，Steerable Policies在真实的机器人操作实验中均优于现有基线，尤其在泛化和长任务方面表现出色。|Sergey Levine Team|[2602.13193](http://arxiv.org/abs/2602.13193)|null|
|**2026-02-13**|**Implicit-Scale 3D Reconstruction for Multi-Food Volume Estimation from Monocular Images**|现有膳食评估方法多依赖单图像分析或基于外观的推断，缺乏明确几何推理且对尺度模糊敏感，难以在真实用餐场景中准确估计食物份量。本研究提出了Implicit-Scale 3D Reconstruction from Monocular Multi-Food Images基准数据集，将食物份量估计重构为单目观测下的隐式尺度3D重建问题，通过餐盘、餐具等上下文线索而非显式度量来推断尺度，并着重于复杂的多食物场景。实验结果显示，几何重建方法相比强大的视觉语言基线，在准确性和鲁棒性上均有提升，最佳方法在体积估计上达到了0.21 MAPE，几何精度为5.7 L1 Chamfer Distance。|Jiangpeng He Team|[2602.13041](http://arxiv.org/abs/2602.13041)|**[link](https://www.kaggle.com/competitions/3d-reconstruction-from-monocular-multi-food-images/data)**|
|**2026-02-13**|**Training-Free Acceleration for Document Parsing Vision-Language Model with Hierarchical Speculative Decoding**|文档解析是多模态理解中的核心任务，但基于视觉语言模型(VLM)的端到端方法在处理长文档时，常因自回归生成长序列而导致显著的推理延迟。针对这一问题，本研究提出了一种免训练且高效的加速方法，借鉴推测解码的思想，使用轻量级文档解析流水线作为草稿模型预测未来批次token，并由更精确的VLM并行验证。此外，该方法还利用文档的布局结构将页面划分为独立区域进行并行解码。实验结果表明，该方法在通用OmniDocBench上为dots.ocr模型提供了2.42倍的无损加速，在长文档解析任务上加速高达4.89倍。|Lianwen Jin Team|[2602.12957](http://arxiv.org/abs/2602.12957)|null|
|**2026-02-13**|**HoRAMA: Holistic Reconstruction with Automated Material Assignment for Ray Tracing using NYURay**|下一代无线网络需要精确的特定站点确定性信道传播预测，而无线射线追踪(RT)依赖高精度3D环境模型和材料属性，手动建模耗时且传统视觉3D重建方法缺乏RT兼容性。为此，本研究提出了HoRAMA（Holistic Reconstruction with Automated Material Assignment）系统，该系统能利用智能手机捕获的RGB视频，结合MASt3R-SLAM的密集点云生成和视觉语言模型辅助的材料分配，自动生成RT兼容的3D模型。实验结果表明，HoRAMA的射线追踪预测在匹配多径分量功率预测方面与手动创建的3D模型基线表现相当（2.28 dB RMSE vs 2.18 dB），同时将3D重建时间从两个月缩短到16小时，显著提高了效率。|Theodore S. Rappaport Team|[2602.12942](http://arxiv.org/abs/2602.12942)|null|
|**2026-02-13**|**RoadscapesQA: A Multitask, Multimodal Dataset for Visual Question Answering on Indian Roads**|理解道路场景对自动驾驶至关重要，但现有数据集可能无法充分覆盖印度复杂多样的驾驶环境。本研究推出了Roadscapes，一个多任务多模态数据集，包含多达9,000张在印度不同驾驶环境中拍摄的图像，并附带手动验证的边界框。该数据集利用基于规则的启发式方法推断场景属性，并生成用于对象定位、推理和场景理解的问答对，涵盖了印度城市和乡村的多种昼夜场景。Roadscapes旨在推动非结构化环境中视觉场景理解的研究，并提供了使用视觉语言模型进行图像问答任务的初步基线。|Jyothikamalesh S Team|[2602.12877](http://arxiv.org/abs/2602.12877)|null|
|**2026-02-13**|**Thinking Like a Radiologist: A Dataset for Anatomy-Guided Interleaved Vision Language Reasoning in Chest X-ray Interpretation**|放射学诊断涉及视觉检查与语言推理的反复交织，但现有医学大型视觉语言模型(LVLMs)多依赖纯文本思维链推理，易产生幻觉，且现有伪视觉解决方案仍缺乏丰富的视觉细节。为此，本研究提出了MMRad-IVL-22K，这是首个专为胸部X射线解读中原生交织的视觉语言推理设计的大规模数据集，反映了放射科医生反复推理和视觉检查的工作流程。实验结果表明，多模态思维链引导的报告生成在临床准确性和报告质量方面显著优于纯文本思维链（RadGraph指标提高6%），证实高保真交织视觉语言证据是可靠医疗AI不可替代的组成部分。在MMRad-IVL-22K上微调的模型在推理一致性和报告质量方面也优于通用和医学专用LVLMs。|Wei Shen Team|[2602.12843](http://arxiv.org/abs/2602.12843)|null|
|**2026-02-13**|**X-SYS: A Reference Architecture for Interactive Explanation Systems**|可解释AI (XAI) 方法虽多，但将其部署为交互式系统仍面临挑战，因其需要兼顾算法与系统能力以维持解释可用性。本研究将可解释性视为一个信息系统问题，提出了X-SYS，一个交互式解释系统的参考架构。X-SYS围绕STAR（可伸缩性、可追溯性、响应性、适应性）四个质量属性，并指定了包含XUI服务、解释服务等五个组件的分解结构，将交互模式映射到系统能力以解耦用户界面和后端计算。通过SemanticLens系统实现X-SYS，展示了其如何通过契约化服务边界实现独立演进、通过离线/在线分离确保响应性以及通过持久状态管理支持可追溯性，为在操作约束下设计交互式解释系统提供了可重用蓝图和具体实例。|Sebastian Lapuschkin Team|[2602.12748](http://arxiv.org/abs/2602.12748)|null|
|**2026-02-13**|**SignScene: Visual Sign Grounding for Mapless Navigation**|导航标志能帮助人类在陌生环境中无地图导航，但机器人如何利用标志进行无地图导航是一个挑战，核心在于如何解释复杂多样的标志及其抽象语义内容，并将其与局部3D场景匹配。本研究将此形式化为“标志接地”(sign grounding)问题，即把标志上的语义指令映射到对应的场景元素和导航动作。研究者利用视觉语言模型(VLMs)的语义常识和推理能力，并提出了SignScene，一种以标志为中心的空-语义表示，旨在以利于VLM有效推理的形式呈现导航相关场景元素和标志信息。在包含九种环境类型、114个查询的数据集上，该方法实现了88%的接地准确率，显著优于基线，并成功使Spot机器人在真实世界中仅依赖标志进行无地图导航。|David Hsu Team|[2602.12686](http://arxiv.org/abs/2602.12686)|null|
|**2026-02-13**|**IndicFairFace: Balanced Indian Face Dataset for Auditing and Mitigating Geographical Bias in Vision-Language Models**|视觉语言模型(VLMs)常从训练数据中继承并放大社会偏见，印度群体尤其被错误代表，现有公平性数据集将印度视为单一类别，忽视了其内部地理多样性。为解决这一局限，本研究提出了IndicFairFace，一个包含14,400张图像的新颖且平衡的人脸数据集，旨在代表印度的地理多样性，图像伦理获取并在各邦和性别间均匀平衡。通过IndicFairFace，研究量化了基于CLIP的VLM中存在的印度国内地理偏见，并利用后验迭代零空间投影去偏方法成功减少了这种偏见。实验证明，该去偏方法对现有嵌入空间的影响很小，基准数据集上的检索准确率平均下降不到1.5%，确立了IndicFairFace作为研究印度背景下VLM地理偏见的第一个基准。|Jiechao Gao Team|[2602.12659](http://arxiv.org/abs/2602.12659)|null|
|**2026-02-13**|**PISHYAR: A Socially Intelligent Smart Cane for Indoor Social Navigation and Multimodal Human-Robot Interaction for Visually Impaired People**|现有智能助行设备多侧重物理导航，但缺乏社交智能和多模态人机交互能力。本研究提出了PISHYAR，一款结合社交感知导航和多模态人机交互的智能拐杖。该系统包含社交导航框架（集成RGB-D感知、对象检测、活动识别、路径规划和触觉反馈）和代理式多模态LLM-VLM交互框架（集成语音识别、VLM、LLM和文本转语音，并支持动态模式路由）。通过仿真、真实世界实验和用户研究，PISHYAR在避障和社会顺从性导航中表现可靠，整体准确率约80%，集体活动识别稳健。初步用户研究显示，视障用户对其交互框架的可用性、信任度和感知社交性给予高度评价，突显了PISHYAR作为多模态辅助移动设备在提供社交互动支持方面的潜力。|Alireza Taheri Team|[2602.12597](http://arxiv.org/abs/2602.12597)|null|
|**2026-02-13**|**On Robustness and Chain-of-Thought Consistency of RL-Finetuned VLMs**|在视觉语言模型（VLM）中，尽管强化学习（RL）微调能提升推理任务性能，但仍面临视觉基础薄弱、幻觉和过度依赖文本线索的问题。研究发现，简单的文本扰动（如误导性描述）会显著降低模型的鲁棒性和置信度，并揭示了RL微调中存在的准确性与忠实性之间的权衡。具体来说，微调提高了基准准确性，却可能损害推理链的可靠性和模型对上下文变化的鲁棒性。这些结果强调了仅凭准确性评估的局限性，并呼吁在训练和评估中同时关注正确性、鲁棒性以及视觉基础推理的忠实性。|Arnab Mondal Team|[2602.12506](http://arxiv.org/abs/2602.12506)|null|
|**2026-02-13**|**Layer-Specific Fine-Tuning for Improved Negation Handling in Medical Vision-Language Models**|由于视觉语言模型（VLM）在区分肯定和否定医疗陈述方面存在局限性，本研究引入了一个放射学诊断基准来系统评估VLM对极性的敏感性。为解决此问题，我们构建了一个包含结构化声明和属性级否定上下文临床否定数据集，并提出了一种名为否定感知选择性训练（NAST）的自适应方法。NAST利用因果追踪效应（CTE）根据各层对否定处理的因果贡献来调整梯度更新。实验结果表明，NAST在不损害通用视觉-语言对齐的情况下，显著提高了VLM对肯定和否定临床陈述的辨别能力，凸显了因果可解释性在安全关键医疗领域中进行有针对性模型适应的价值。|Rahmatollah Beheshti Team|[2602.12498](http://arxiv.org/abs/2602.12498)|null|
|**2026-02-12**|**Scaling Verification Can Be More Effective than Scaling Policy Learning for Vision-Language-Action Alignment**|针对视觉-语言-动作（VLA）模型在执行自然语言指令时存在的"意图-动作差距"问题，本研究探索了测试时验证方法。我们首先揭示了具身指令遵循的测试时标度定律，发现联合扩展复述指令和生成动作数量能更高效地恢复正确动作。为利用这些规律，我们提出了CoVer，一个对比验证器，并引入了“启动时计算”和分层验证推理管道。在部署时，该框架预先计算VLM生成的复述指令，生成动作候选项，然后使用验证器选择最优提示和动作块。实验结果表明，CoVer在SIMPLER基准上取得了显著提升（分布内22%，分布外13%），并在真实世界实验中进一步提升45%，在PolaRiS基准上任务进展提升14%，成功率提升9%。|Marco Pavone Team|[2602.12281](http://arxiv.org/abs/2602.12281)|null|
|**2026-02-12**|**ExStrucTiny: A Benchmark for Schema-Variable Structured Information Extraction from Document Images**|尽管通用视觉语言模型（VLM）在传统文档理解基准上表现良好，但它们在多样化文档类型和灵活模式下进行整体、细粒度结构化信息提取的能力仍未充分研究。现有数据集在实体本体、查询复杂度或文档类型上存在局限。为解决这些不足，本研究引入了ExStrucTiny，一个新的文档图像结构化信息提取（IE）基准数据集，它统一了关键实体提取、关系提取和视觉问答的特性。该数据集通过结合人工和合成并经过人工验证的样本构建，涵盖了更广泛的文档类型和提取场景。对开放和封闭式VLM在该基准上的分析揭示了模式适应、查询欠规范和答案定位等挑战，为未来改进通用模型在文档结构化信息提取方面的能力奠定了基础。|Manuela Veloso Team|[2602.12203](http://arxiv.org/abs/2602.12203)|null|
|**2026-02-12**|**3DGSNav: Enhancing Vision-Language Model Reasoning for Object Navigation via Active 3D Gaussian Splatting**|针对零样本对象导航（ZSON）中视觉语言模型（VLM）决策受低级感知准确性限制的问题，本研究提出了3DGSNav框架。3DGSNav利用3D高斯泼溅（3DGS）作为VLM的持久记忆来增强空间推理能力。通过主动感知，该框架逐步构建环境的3DGS表示，从而实现轨迹引导的、边界感知的第一人称视角的自由视点渲染。此外，研究设计了结构化视觉提示并结合思维链（CoT）提示来进一步提升VLM的推理能力。在导航过程中，实时对象检测器用于过滤潜在目标，而VLM驱动的主动视点切换则进行目标再验证，确保高效可靠的识别。在多个基准测试和真实世界四足机器人实验中，3DGSNav展现出鲁棒且具竞争力的性能。|Xinyi Yu Team|[2602.12159](http://arxiv.org/abs/2602.12159)|null|
|**2026-02-12**|**Affordance-Graphed Task Worlds: Self-Evolving Task Generation for Scalable Embodied Learning**|鉴于在真实世界中训练机器人策略成本高昂且现有生成模拟难以生成逻辑连贯的长周期任务并处理动态物理不确定性，本研究提出了Affordance-Graphed Task Worlds (AGT-World) 统一框架。该框架基于真实世界观测自主构建交互式模拟环境和机器人任务策略，通过将任务空间形式化为结构化图，实现复杂目标的精确分层分解，并引入了结合VLM推理和几何验证的自演化机制来优化策略。实验结果表明，该方法在成功率和泛化能力上显著优于现有方法，实现了提议、执行和修正的自我改进循环，以支持可扩展的机器人学习。|Changshui Zhang Team|[2602.12065](http://arxiv.org/abs/2602.12065)|null|
|**2026-02-12**|**Can Local Vision-Language Models improve Activity Recognition over Vision Transformers? -- Case Study on Newborn Resuscitation**|鉴于新生儿复苏活动准确记录的重要性及其在实践中的不足，以及现有方法在识别细粒度活动上的挑战，本研究探索了生成式AI（GenAI）方法，特别是结合大型语言模型（LLM）的局部视觉-语言模型（VLM），以改进新生儿复苏视频中的活动识别。研究将几种零样本VLM策略和微调VLM（包括LoRA）与分类头进行评估，并与监督式TimeSFormer基线进行比较。结果表明，小型VLM虽然容易产生幻觉，但经过LoRA微调后，F1分数可达0.91，显著超越TimeSFormer的0.70。|Øyvind Meinich-Bache Team|[2602.12002](http://arxiv.org/abs/2602.12002)|null|
|**2026-02-12**|**Benchmarking Vision-Language Models for French PDF-to-Markdown Conversion**|针对PDF到Markdown转换作为RAG管道关键步骤，以及现有基准多侧重英文或中文并可能过度惩罚无关格式差异的问题，本研究引入了一个专门针对法语文档、通过模型不一致采样挑选的挑战性新基准。该基准涵盖手写表格、复杂布局、密集表格和富含图形的页面，并采用单元测试式检查和类别特定归一化进行评估，以减少无关的呈现差异。实验结果显示，在15个模型中，最强的专有模型在手写和表格处理上表现出更高的鲁棒性，同时一些开源系统在标准打印布局上仍具竞争力。|Nicolas Mery Team|[2602.11960](http://arxiv.org/abs/2602.11960)|null|
|**2026-02-12**|**Are Two LLMs Better Than One? A Student-Teacher Dual-Head LLMs Architecture for Pharmaceutical Content Optimization**|针对大型语言模型（LLM）在制药等受监管领域内容创作中面临的科学准确性和合规性挑战，以及手动质量控制（QC）低效易错的问题，本研究引入了LRBTC，一个模块化的LLM和VLM驱动的QC架构。该架构涵盖语言、法规、品牌、技术和内容结构检查，并结合了师生双模型架构、人工干预（HITL）工作流和瀑布式规则过滤机制。实验结果表明，LRBTC在AIReg-Bench上取得了83.0%的F1和97.5%的召回率，相比Gemini 2.5 Pro，遗漏违规减少5倍；在CSpelling上平均准确率提高26.7%。误差分析揭示当前模型擅长检测拼写错误，但在复杂医学语法和标点错误识别方面仍有提升空间。|Anubhav Girdhar Team|[2602.11957](http://arxiv.org/abs/2602.11957)|null|
|**2026-02-12**|**LAMP: Implicit Language Map for Robot Navigation**|为解决现有视觉-语言模型（VLM）零样本导航方法在大环境中因显式存储语言向量而导致的内存需求过高和细粒度规划分辨率有限的问题，本研究提出了LAMP（Language Map）框架。该框架通过学习连续的、语言驱动的隐式神经语言场来编码语言特征，并结合稀疏图实现高效的粗略路径规划，进而通过梯度优化在学习场中细化目标姿态。该方法还采用贝叶斯框架建模嵌入不确定性以增强泛化能力，并通过图采样策略扩展到大环境。实验证明，LAMP在内存效率和细粒度目标到达精度方面均优于现有显式方法。|Sunwook Choi Team|[2602.11862](http://arxiv.org/abs/2602.11862)|**[link](https://lab-of-ai-and-robotics.github.io/LAMP/)**|
|**2026-02-12**|**JEPA-VLA: Video Predictive Embedding is Needed for VLA Models**|针对现有视觉-语言-动作（VLA）模型在机器人操作中存在的样本效率低和泛化能力有限的问题，本研究认为这与被忽视的预训练视觉表征在环境理解和策略先验方面知识不足有关。研究发现，视频上预训练的预测嵌入（特别是V-JEPA 2）能有效捕捉任务相关的时态动态并过滤不可预测的环境因素，从而弥补现有视觉表征的不足。基于此，论文提出了JEPA-VLA，一种将预测嵌入自适应集成到现有VLA中的简单而有效的方法。实验结果表明，JEPA-VLA在一系列基准和真实机器人任务中均取得了显著的性能提升。|Mingsheng Long Team|[2602.11832](http://arxiv.org/abs/2602.11832)|null|
|**2026-02-12**|**Revis: Sparse Latent Steering to Mitigate Object Hallucination in Large Vision-Language Models**|为解决大型视觉-语言模型（LVLMs）普遍存在的物体幻觉问题，该问题源于视觉特征和预训练文本表示在网络深层中的纠缠，本研究提出了REVIS，一个无需训练的框架。REVIS基于潜在空间几何原理，通过正交投影精确提取纯视觉信息向量，并采用校准策略，仅在视觉信息被抑制的精确深度进行稀疏干预，从而有效恢复视觉信息。经验评估结果显示，REVIS与最先进的基线相比，将目标幻觉率降低了约19%，同时保持了模型原有的通用推理能力。|Zhou Yang Team|[2602.11824](http://arxiv.org/abs/2602.11824)|null|
|**2026-02-12**|**Adaptive Debiasing Tsallis Entropy for Test-Time Adaptation**|现有主流的视觉-语言模型测试时自适应（TTA）方法依赖香农熵（SE）衡量预测不确定性，但由于预训练数据偏差，SE会产生有偏的不确定性估计。为解决此问题，研究发现并证明Tsallis熵（TE）通过引入非广延参数q，天然适合表征有偏分布。在此基础上，本文提出了自适应去偏Tsallis熵（ADTE），为每个类别定制类特定的q^l参数，该参数通过对持续传入的测试实例估计标签偏差进行归一化得到。实验结果表明，ADTE在ImageNet及其五种变体上超越了最先进的方法，并在10个跨域基准测试中取得了最高的平均性能，证实了其有效性。|Jianfeng Lu Team|[2602.11743](http://arxiv.org/abs/2602.11743)|null|
|**2026-02-12**|**Adapting Vision-Language Models for E-commerce Understanding at Scale**|电商产品理解需要文本、图像和结构化属性等多模态的强大理解能力。通用视觉-语言模型（VLMs）虽能提供泛化能力，但如何将其有效地适应电商数据中以属性为中心、多图像和噪声多的特性，同时不牺牲通用性能，是一个未被充分探索的问题。本文通过大规模实验研究，展示了对通用VLM进行有针对性的适配，可以在大幅提升电商任务性能的同时，保持其广泛的多模态能力。此外，本文还提出了一个新颖且全面的评估套件，涵盖了深度产品理解、严格指令遵循和动态属性提取等任务。|Shahram Khadivi Team|[2602.11733](http://arxiv.org/abs/2602.11733)|null|
|**2026-02-12**|**STVG-R1: Incentivizing Instance-Level Reasoning and Grounding in Videos via Reinforcement Learning**|在视觉-语言模型（VLMs）中，文本描述与视觉坐标之间的不匹配常导致幻觉问题，在时空视频定位（STVG）等密集预测任务中尤为突出。现有方法通常增强视觉-文本对齐或添加辅助解码器，但这会引入额外的可训练模块，导致高昂的标注和计算成本。本文提出了一种新颖的视觉提示范式，通过将每帧坐标预测重新表述为实例级识别问题，为每个对象分配唯一且时间一致的ID，并将其嵌入视频作为视觉提示。此外，研究引入了STVG-R1，这是首个用于STVG的强化学习框架，通过任务驱动奖励联合优化时间精度、空间一致性和结构化格式正则化。实验结果表明，STVG-R1在六个基准测试中表现出色，在HCSTVG-v2上m_IoU比基线Qwen2.5-VL-7B高出20.9%，达到SOTA，并展现出强大的零样本泛化能力。|Qing Li Team|[2602.11730](http://arxiv.org/abs/2602.11730)|null|
|**2026-02-12**|**ScalSelect: Scalable Training-Free Multimodal Data Selection for Efficient Visual Instruction Tuning**|大规模视觉指令微调（VIT）是提升视觉-语言模型（VLMs）在多模态任务中性能的关键范式，但其在大型数据集上的训练因数据冗余而计算昂贵且低效。现有的数据选择方法要么需要昂贵的训练或梯度计算，要么依赖代理模型或指令无关表示且复杂度高，限制了可扩展性。本文提出了ScalSelect，一种可扩展的无训练多模态数据选择方法，其复杂度与样本数量呈线性关系，无需外部模型或辅助数据集。ScalSelect通过提取指令令牌在目标VLM中最受关注的视觉特征来构建样本表示，然后识别其表示最能近似整个数据集主导子空间的样本，从而实现无需成对比较的可扩展重要性评分。广泛的实验表明，ScalSelect仅使用16%的数据即可达到全数据集训练97.5%以上的性能，在某些设置下甚至超越了全数据训练。|Kai Chen Team|[2602.11636](http://arxiv.org/abs/2602.11636)|**[link](https://github.com/ChangtiWu/ScalSelect}{ScalSelect})**|
|**2026-02-12**|**SkillRater: Untangling Capabilities in Multimodal Data**|传统数据筛选方法通常只给样本分配一个单一的质量分数，但这在模型需要学习多种不同能力时显得局限。本文认为质量应是多维的，每个维度对应模型需掌握的一种能力。为此，我们提出了SkillRater框架，将数据过滤分解为多个专业的评估器，每个评估器负责一种能力，并通过元学习在独立验证目标上进行训练。这些评估器的分数通过渐进式选择规则组合：在每个训练阶段，只要任一评估器将样本排名高于随时间收紧的阈值，该样本即被保留，以在早期保持多样性，后期聚焦高价值样本。在视觉语言模型上的验证表明，SkillRater在视觉理解、OCR和STEM推理能力上均显著优于未过滤基线，且学习到的评估器信号几乎正交，证实了多维度分解的有效性。|Akshat Shrivastava Team|[2602.11615](http://arxiv.org/abs/2602.11615)|null|
|**2026-02-12**|**Chatting with Images for Introspective Visual Thinking**|当前大型视觉-语言模型（LVLMs）通常依赖基于单次视觉编码的纯文本推理，这常导致细粒度视觉信息丢失，尤其是在处理跨区域或多图像的视觉语义或几何关系推理时。为解决这些挑战，本文提出了“与图像对话”这一新框架，将视觉操作重新定义为语言引导的特征调制。在该框架下，模型在富有表现力的语言提示指导下，动态地对多个图像区域进行联合再编码，从而实现语言推理与视觉状态更新之间更紧密的耦合。我们通过ViLaVT实例化了这一范式，它是一个配备动态视觉编码器的新型LVLM，并采用两阶段课程（监督微调和强化学习）进行训练。广泛的实验表明，ViLaVT在八个基准测试中取得了显著且一致的改进，尤其在复杂的多图像和基于视频的空间推理任务上表现突出。|Tieniu Tan Team|[2602.11073](http://arxiv.org/abs/2602.11073)|null|
|**2026-02-12**|**Self-Refining Vision Language Model for Robotic Failure Detection and Reasoning**|针对机器人系统故障推理面临的挑战，即真实世界故障的复杂性及丰富推理标签获取成本高昂，本研究提出了ARMOR模型。ARMOR将故障检测与自然语言推理视为一个多任务自完善过程，通过迭代预测和基于历史输出的条件推理进行训练。它利用大规模稀疏二元标签和少量丰富推理标注的异构监督，并通过离线和在线模仿学习进行优化。在推理阶段，ARMOR生成多条完善轨迹并利用自确定性指标选择最可靠的预测。实验结果显示，ARMOR在故障检测率上比现有方法提升了30%，在LLM模糊匹配分数衡量的推理能力上提升了100%，证明了其在异构监督下的鲁棒性及超越预定义故障模式的开放式推理能力。|Yesh Dattatreya Team|[2602.12405](http://arxiv.org/abs/2602.12405)|null|
|**2026-02-12**|**What does RL improve for Visual Reasoning? A Frankenstein-Style Analysis**|为了理解强化学习（RL）在视觉语言模型（VLM）视觉推理中相比监督微调（IN）的具体贡献，本研究提出了一种“弗兰肯斯坦式”分析框架。该框架通过因果探测定位功能、参数比较表征更新以及模型合并测试可迁移性。研究发现，RL主要通过在模型的中间到后期层诱导一致的推理时段转移来提升性能，并且这些中间到后期的优化对于RL的性能增益是可迁移和必要的。这些结果表明，RL对视觉推理的可靠贡献并非统一增强视觉感知，而是系统地细化了Transformer中间到后期层的计算，从而改善了视觉到推理的对齐和推理性能，揭示了仅通过基准评估来理解多模态推理改进的局限性。|Tianyi Zhou Team|[2602.12395](http://arxiv.org/abs/2602.12395)|null|
|**2026-02-12**|**Synthetic Image Detection with CLIP: Understanding and Assessing Predictive Cues**|鉴于现代生成模型能产生近乎真实的照片，合成图像检测（SID）的泛化能力及其在实际应用中的表现面临挑战，特别是在新生成模型面前。本研究旨在探究CLIP作为SID基础模型的有效性及其内在线索。为此，我们构建了SynthCLIC数据集以减少语义偏差，并利用可解释的线性分类器和文本概念模型分析CLIP特征。结果显示，CLIP检测器在GAN基准上表现优异，但在高质量扩散数据集SynthCLIC上性能略有下降，且跨不同生成器家族的泛化能力显著受限。研究发现，检测器主要依赖高层摄影属性而非明显的生成器伪影。这些发现强调了持续模型更新和更广泛训练暴露的必要性，并肯定了CLIP作为更通用、鲁棒SID方法的强大基础。|Michael Graber Team|[2602.12381](http://arxiv.org/abs/2602.12381)|null|
|**2026-02-12**|**ForeAct: Steering Your VLA with Efficient Visual Foresight Planning**|在开放世界环境中，视觉-语言-动作（VLA）模型将高级语言指令转换为具体可执行动作面临挑战。本研究提出了视觉预见规划器（ForeAct），一个通用高效的规划器，它通过想象未来观察和子任务描述来逐步指导VLA。ForeAct包含一个高效的预见图像生成模块，能从当前视觉输入和语言指令在0.33秒内预测高质量的未来观察，并结合一个视觉-语言模型生成子任务描述。最先进的VLA模型无需修改架构，只需通过增强视觉输入即可无缝集成ForeAct。预见生成器在超过100万个多任务、跨具身情景中进行预训练，学习了鲁棒的具身动力学。在包含11个真实世界任务的基准测试中，ForeAct取得了87.4%的平均成功率，比基线模型有显著提升。|Song Han Team|[2602.12322](http://arxiv.org/abs/2602.12322)|null|
|**2026-02-12**|**LatentAM: Real-Time, Large-Scale Latent Gaussian Attention Mapping via Online Dictionary Learning**|为解决开放词汇机器人感知中从流式RGB-D观测构建可扩展潜在特征地图的挑战，并克服传统VLM嵌入方法缺乏通用性和依赖预训练的问题，本研究提出了LatentAM框架。LatentAM是一种在线3D高斯泼溅（3DGS）映射框架，它采用模型无关且无需预训练的在线字典学习方法，实现了与不同VLM的即插即用集成。该方法将高斯基元与紧凑查询向量关联，通过带有可学习字典的注意力机制转换为近似VLM嵌入。字典在流式观测中高效初始化并在线优化，同时结合基于体素哈希的地图管理策略以实现大规模环境下的GPU内存有界使用。实验结果表明，LatentAM在特征重建保真度上显著优于现有方法，并在评估数据集上实现了接近实时的速度（12-35 FPS）。|Yulun Tian Team|[2602.12314](http://arxiv.org/abs/2602.12314)|null|
|**2026-02-11**|**Hierarchical Concept Embedding & Pursuit for Interpretable Image Classification**|可解释设计模型在计算机视觉中日益受到关注，因为它们能为其预测提供忠实的解释。在图像分类中，这类模型通常从图像中提取人类可解释的概念并用于分类。稀疏概念恢复方法利用视觉-语言模型的潜在空间将图像嵌入表示为概念嵌入的稀疏组合，但这类方法忽视了概念的层次结构，可能导致预测正确但解释与层次结构不一致。为解决此问题，本文提出了分层概念嵌入与追踪（HCEP）框架，它在潜在空间中诱导概念嵌入的层次结构，并利用分层稀疏编码来恢复图像中存在的概念。实验结果表明，HCEP在概念精度和召回率上均优于基线，同时保持了有竞争力的分类准确率，并且在样本数量有限时展现出卓越的分类和概念恢复性能，证明了将层次结构融入稀疏编码能产生更可靠和可解释的图像分类模型。|René Vidal Team|[2602.11448](http://arxiv.org/abs/2602.11448)|null|
|**2026-02-11**|**Beyond VLM-Based Rewards: Diffusion-Native Latent Reward Modeling**|扩散模型和流匹配模型的偏好优化需要既具有判别鲁棒性又计算高效的奖励函数。视觉-语言模型（VLMs）已成为主要的奖励提供者，利用其丰富的多模态先验来指导对齐。然而，它们的计算和内存成本很高，并且通过像素空间奖励优化潜在扩散生成器会引入域不匹配，使对齐复杂化。本文提出了DiNa-LRM，一种扩散原生的潜在奖励模型，它直接在噪声扩散状态上构建偏好学习。DiNa-LRM引入了一种噪声校准的Thurstone似然，其不确定性依赖于扩散噪声，并利用预训练的潜在扩散骨干网络与时间步条件奖励头。实验结果显示，DiNa-LRM在图像对齐基准测试中显著优于现有基于扩散的奖励基线，并以极低的计算成本实现了与最先进VLM相当的性能，显著改善了偏好优化动态，实现了更快、更资源高效的模型对齐。|Wenhan Luo Team|[2602.11146](http://arxiv.org/abs/2602.11146)|**[link](https://github.com/HKUST-C4G/diffusion-rm)**|
|**2026-02-11**|**Active Zero: Self-Evolving Vision-Language Models through Active Environment Exploration**|自博弈已使大型语言模型（LLMs）通过自生成挑战实现自主提升。然而，现有视觉-语言模型（VLMs）的自博弈方法依赖与静态图像集的被动交互，导致对初始数据集的强依赖和学习效率低下。为解决这些局限性，本文提出了Active-Zero框架，将模型学习从被动交互转向主动探索视觉环境。Active-Zero采用三个共同进化的智能体：一个Searcher根据模型能力前沿从开放世界存储库检索图像；一个Questioner合成校准的推理任务；一个Solver通过准确性奖励进行优化。这种闭环机制实现了自我脚手架的自动课程学习。在Qwen2.5-VL-7B-Instruct的12个基准测试中，Active-Zero在推理任务上平均准确率达到53.97%（提升5.7%），在通用理解上达到59.77%（提升3.9%），持续优于现有自博弈基线，表明主动探索是可扩展和自适应自进化视觉-语言系统的关键要素。|Tat-Seng Chua Team|[2602.11241](http://arxiv.org/abs/2602.11241)|null|
|**2026-02-11**|**Safe mobility support system using crowd mapping and avoidance route planning using VLM**|自主移动机器人在动态、尤其是拥挤环境中安全有效导航仍面临挑战。本文提出了一种新颖的框架，该框架集成了视觉-语言模型（VLM）和高斯过程回归（GPR），用于生成动态人群密度图（“抽象图”），以实现自主机器人导航。我们的方法利用VLM识别抽象环境概念（如人群密度）的能力，并通过GPR以概率方式表示这些概念。在大学校园进行的真实世界试验结果表明，机器人成功地生成了避开静态障碍物和动态人群的路径，从而增强了导航的安全性和适应性。|Koichi Ozaki Team|[2602.10910](http://arxiv.org/abs/2602.10910)|null|

<p align=right>(<a href=#updated-on-20260219>back to top</a>)</p>

## VLA

|Publish Date|Title|Chinese Summary|Authors|PDF|Code|
|---|---|---|---|---|---|
|**2026-02-18**|**EgoScale: Scaling Dexterous Manipulation with Diverse Egocentric Human Data**|现有研究未能有效利用大规模人类数据进行高自由度灵巧操作。本文提出了EgoScale框架，基于超过20,854小时的以自我为中心的人类视频数据训练视觉-语言-动作(VLA)模型，并发现人类数据规模与验证损失之间存在对数线性缩放定律。通过两阶段迁移方法：大规模人类预训练和轻量级人机对齐中训练，该策略使22自由度机械手的平均成功率比无预训练基线提高了54%，并能有效迁移到低自由度机械手，证明了大规模人类运动作为可重用、与具身无关运动先验的潜力。|Linxi Fan Team|[2602.16710](http://arxiv.org/abs/2602.16710)|null|
|**2026-02-17**|**Learning to Retrieve Navigable Candidates for Efficient Vision-and-Language Navigation**|基于大语言模型（LLM）的视觉-语言导航(VLN)常因重复解释指令和处理冗余候选项而效率低下。为此，本文提出了一个检索增强框架，在不修改LLM的情况下，通过在回合层面利用指令级嵌入检索器选择相似轨迹作为上下文示例，并在步骤层面通过模仿学习的候选检索器剪枝不相关的导航方向。实验结果显示，该方法在R2R基准测试中显著提升了成功率、Oracle成功率和SPL，且检索模块对全局指导和步进决策效率贡献互补，验证了检索增强决策支持对LLM-based VLN的有效性。|Lina Yao Team|[2602.15724](http://arxiv.org/abs/2602.15724)|null|
|**2026-02-17**|**The Next Paradigm Is User-Centric Agent, Not Platform-Centric Service**|当前数字服务大多遵循平台中心模式，优化平台指标而非用户真实需求，导致平台利益与用户利益冲突。本文主张将数字服务的未来转向用户中心智能代理，这类代理优先保护隐私、与用户目标对齐并赋予用户控制权。文章探讨了实现这一愿景的机遇与挑战，提出了设备-云管道的实现方案，并讨论了其采用所需的治理和生态系统结构，以期推动数字服务向用户福祉导向转变。|Enhong Chen Team|[2602.15682](http://arxiv.org/abs/2602.15682)|null|
|**2026-02-17**|**CARE Drive A Framework for Evaluating Reason-Responsiveness of Vision Language Models in Automated Driving**|自动驾驶中，基础模型（如VLM）的现有评估方法侧重结果表现，未能判断其决策是否反映人类考虑，可能导致虚假信任。针对此问题，本文提出了CARE Drive框架，一个模型无关的自动驾驶VLM理性响应能力评估方法。该框架通过受控的上下文变化，比较基线与理由增强的模型决策，评估人类理由对决策行为的因果影响。实验结果表明，在骑车人超车场景中，显式人类理由显著影响模型决策，提高了与专家行为的一致性，证明了在不修改模型参数的前提下，可以系统地评估基础模型的理性响应能力。|Arkady Zgonnikov Team|[2602.15645](http://arxiv.org/abs/2602.15645)|null|
|**2026-02-17**|**World Action Models are Zero-shot Policies**|现有视觉-语言-动作(VLA)模型在语义泛化方面表现突出，但在新环境中对未见物理运动的泛化能力不足。本文提出了DreamZero，一个基于预训练视频扩散骨干的世界动作模型(WAM)，它通过预测未来世界状态和动作来学习物理动力学，利用视频作为密集的世界演变表示。与现有VLA模型相比，DreamZero在真实机器人实验中对新任务和环境的泛化能力提高了2倍以上，并实现了14B视频扩散模型的实时闭环控制。此外，它还展示了跨具身迁移能力，仅需少量视频或游戏数据即可实现对新任务和新具身的有效适应。|Joel Jang Team|[2602.15922](http://arxiv.org/abs/2602.15922)|**[link](https://dreamzero0.github.io/)**|
|**2026-02-17**|**VLM-DEWM: Dynamic External World Model for Verifiable and Resilient Vision-Language Planning in Manufacturing**|视觉-语言模型（VLM）在智能制造中的高级规划应用面临无状态操作导致的世界状态漂移和不透明推理造成的故障诊断困难。本文提出了VLM-DEWM认知架构，通过一个持久且可查询的动态外部世界模型（DEWM）将VLM推理与世界状态管理解耦。每个VLM决策形成一个可外部化的推理轨迹，并在执行前通过DEWM验证，故障发生时可进行有针对性的恢复。实验结果显示，VLM-DEWM将状态跟踪精度从56%提高到93%，恢复成功率从低于5%提高到95%，并显著降低了计算开销，展现了其在动态制造环境中实现长周期机器人操作的可靠性和弹性。|Ning Ji Team|[2602.15549](http://arxiv.org/abs/2602.15549)|null|
|**2026-02-17**|**Selective Perception for Robot: Task-Aware Attention in Multimodal VLA**|机器人VLA模型中静态融合所有多视图输入会带来不必要的计算开销和背景噪声。受人类主动感知启发，本文提出一种动态信息融合框架，引入轻量级自适应路由架构。该架构实时分析文本提示和腕部摄像机观测，预测多摄像机视图的任务相关性，并有条件地衰减低信息效用视图的计算，选择性地向策略网络提供必要视觉特征，从而实现计算效率与任务相关性成比例。此外，通过VLM实现了自动化标注流程。实验结果表明，该方法在真实机器人操作中显著提高了推理效率和控制性能，验证了动态信息融合在资源受限的实时机器人控制环境中的有效性和实用性。|Soo-Chul Lim Team|[2602.15543](http://arxiv.org/abs/2602.15543)|null|
|**2026-02-17**|**One Agent to Guide Them All: Empowering MLLMs for Vision-and-Language Navigation via Explicit World Representation**|现有以多模态大语言模型（MLLM）为中心的导航代理通常采用紧耦合设计，限制了其在理解高级语义指令和精确空间感知方面的性能。为此，本文提出一种解耦设计，将低级空间状态估计与高级语义规划分离，并引入交互式度量世界表示以维护丰富一致的信息，供MLLM进行交互和推理。此外，引入反事实推理以激发MLLM能力，并确保生成动作的物理有效性。该方法在R2R-CE和RxR-CE基准测试中建立了零样本SOTA，并通过零样本模拟到真实世界的迁移，验证了其作为具身视觉-语言导航的稳健、领域不变接口的通用性。|Qi Wu Team|[2602.15400](http://arxiv.org/abs/2602.15400)|null|
|**2026-02-17**|**ActionCodec: What Makes for Good Action Tokenizers**|VLA模型中动作标记化设计主要关注重建保真度，但其对VLA优化的直接影响未被充分探讨。本文从VLA优化的角度建立了动作标记器的设计原则，包括最大化时间标记重叠、最小化词汇冗余、增强多模态互信息和标记独立性。在此指导下，提出了高性能动作标记器ActionCodec。实验结果表明，ActionCodec显著提高了LIBERO等基准测试中的训练效率和VLA性能，使SmolVLM2-2.2B模型在无机器人预训练情况下成功率达到95.5%，刷新了SOTA，为开发更有效的动作标记器提供了明确路线图。|Jianye Hao Team|[2602.15397](http://arxiv.org/abs/2602.15397)|null|
|**2026-02-17**|**EAA: Automating materials characterization with vision language model agents**|复杂的实验显微镜工作流需要自动化以提高效率和降低操作门槛。本文提出了实验自动化代理（EAA），一个由视觉-语言模型驱动的代理系统，旨在自动化复杂的实验显微镜工作流。EAA集成了多模态推理、工具增强行动和可选的长期记忆，支持自主程序和交互式用户引导测量，并构建在灵活的任务管理器架构上。在Advanced Photon Source的成像光束线演示中，EAA成功实现了自动区域板聚焦、自然语言描述的特征搜索和交互式数据采集，验证了视觉智能体在提高光束线效率、减轻操作负担和降低用户专业门槛方面的潜力。|Mathew J. Cherukara Team|[2602.15294](http://arxiv.org/abs/2602.15294)|null|
|**2026-02-16**|**Beyond Imitation: Reinforcement Learning-Based Sim-Real Co-Training for VLA Models**|模拟提供了可扩展且低成本的方式来丰富视觉-语言-动作（VLA）训练，减少对昂贵真实机器人演示的依赖，但多数模拟-现实协同训练方法依赖于监督微调（SFT），将模拟视为静态演示源，未充分利用大规模闭环交互，从而限制了真实世界收益和泛化。针对此问题，本文提出了RL-based sim-real Co-training (RL-Co) 框架，该框架利用交互式模拟同时保留真实世界能力。其设计分为两阶段：首先，通过对真实和模拟演示的混合数据进行SFT来预热策略，然后通过在模拟中进行强化学习来微调策略，并添加辅助的真实世界数据监督损失以锚定策略并缓解灾难性遗忘。在四项真实世界桌面操作任务上，RL-Co在两种VLA架构（OpenVLA和π₀.₅）上均持续优于仅真实世界微调和基于SFT的协同训练，例如OpenVLA的真实世界成功率提升24%，π₀.₅提升20%，同时在未见任务变体上表现出更强的泛化能力和显著提升的真实世界数据效率。|Yu Wang Team|[2602.12628](http://arxiv.org/abs/2602.12628)|null|
|**2026-02-16**|**DM0: An Embodied-Native Vision-Language-Action Model towards Physical AI**|传统方法将预训练模型适应到物理任务中时，物理具身常被视为后续微调。针对此问题，本文提出了DM0，一个专为物理AI设计的本体原生视觉-语言-动作（VLA）框架，旨在通过整合网络文本、自动驾驶场景和具身交互日志等异构数据源，统一学习具身操作和导航。其方法分三阶段：大规模VLM预训练以获取语义知识和物理先验；构建流匹配动作专家；以及采用混合训练策略（具身数据不反传梯度到VLM以保持泛化，非具身数据VLM可训练）并引入具身空间支架策略来约束动作空间。实验结果表明，DM0在RoboChallenge基准测试的Table30上，于专家和通用设置中均达到了最先进的性能。|Tiancai Wang Team|[2602.14974](http://arxiv.org/abs/2602.14974)|**[link](https://github.com/Dexmal/dexbotic)**|
|**2026-02-16**|**DriveFine: Refining-Augmented Masked Diffusion VLA for Precise and Robust Driving**|自动驾驶中的视觉-语言-动作（VLA）模型普遍采用生成式规划器，但扩散模型存在模态对齐、训练效率和泛化能力问题，而令牌模型则面临累积因果错误和不可逆解码的困境。为结合两者的优势并弥补不足，本文提出了DriveFine，一个掩码扩散VLA模型，其特点是结合了灵活解码与自校正能力。核心在于设计了一种即插即用的块MoE，将细化专家与生成专家解耦，通过推理时专家选择和训练时梯度阻断，保留了预训练权重的基础能力；此外，还设计了混合强化学习策略来平衡细化专家的探索与训练稳定性。大量实验证明，DriveFine在NAVSIM v1, v2和Navhard基准测试中展现出强大的有效性和鲁棒性。|Yan Wang Team|[2602.14577](http://arxiv.org/abs/2602.14577)|null|
|**2026-02-16**|**Replanning Human-Robot Collaborative Tasks with Vision-Language Models via Semantic and Physical Dual-Correction**|人机协作（HRC）在装配任务中面临人类指令模糊不清、难以生成可行机器人行为的问题，现有基于视觉-语言模型（VLM）的方法常出现幻觉推理且无法预测物理执行失败。为解决这些挑战，本文提出了一个HRC框架，通过引入双重校正机制来增强VLM推理能力。该机制包含一个内部校正模型，在动作执行前验证逻辑一致性和任务可行性；以及一个外部校正模型，通过执行后反馈检测并纠正物理故障。仿真消融研究显示该方法提高了成功率，真实世界中的协作装配实验也证实了其在根据人类指令进行交互式重新规划方面的有效性。|Kensuke Harada Team|[2602.14551](http://arxiv.org/abs/2602.14551)|null|
|**2026-02-16**|**TikArt: Aperture-Guided Observation for Fine-Grained Visual Reasoning via Reinforcement Learning**|多模态大型语言模型（MLLMs）在细粒度视觉推理中常因关键信息（如微小物体、杂乱区域或细微标记）在全局图像编码中丢失而受限。为解决此问题，本文引入了TikArt (Thinking Aperture)，一个以光圈为导向的智能体，将多步视觉-语言推理建模为对感兴趣区域的决策过程。TikArt遵循“思考-光圈-观察”循环，交替进行语言生成和两种光圈动作（Zoom用于矩形裁剪，Segment调用SAM2获取掩码裁剪），并将局部视觉线索转化为语言记忆。该模型基于Qwen3-VL-8B，利用AGRPO优化推理策略，采用两阶段课程学习。实验结果表明，TikArt在多个细粒度视觉推理基准测试中持续超越了基线模型，并为高分辨率推理提供了可解释的光圈轨迹。|Lei Zhao Team|[2602.14482](http://arxiv.org/abs/2602.14482)|null|
|**2026-02-16**|**Hierarchical Vision-Language Interaction for Facial Action Unit Detection**|面部动作单元（AU）检测在标注数据有限的情况下，难以学习判别性强且泛化性好的AU表征。针对此挑战，本文提出了分层视觉-语言交互AU理解（HiVA）方法，利用文本AU描述作为语义先验来指导和增强AU检测。HiVA通过大型语言模型生成多样化的AU描述以强化语言表征学习，并引入AU感知动态图模块捕获细粒度和整体视觉-语言关联，促进AU特异性视觉表征学习。这些特征通过解耦双重交叉注意力（DDCA）和上下文双重交叉注意力（CDCA）组成的层次跨模态注意力架构进行整合。实验证明HiVA持续超越了现有先进方法，且定性分析揭示其能产生语义上有意义的激活模式，表明其在学习鲁棒且可解释的跨模态对应关系方面的有效性。|Cuntai Guan Team|[2602.14425](http://arxiv.org/abs/2602.14425)|null|
|**2026-02-16**|**Multi-Turn Adaptive Prompting Attack on Large Vision-Language Models**|多轮越狱攻击对文本大型语言模型有效，但将其应用于大型视觉-语言模型（LVLMs）时，直接添加视觉输入易触发安全防御机制，导致响应保守。为解决这一问题，本文提出了MAPA（multi-turn adaptive prompting attack），一种多轮自适应提示攻击方法。MAPA在每一轮中交替使用文本-视觉攻击动作以激发最恶意的响应，并在跨轮次通过迭代细化调整攻击轨迹，逐步增强响应的恶意性。这种双层设计使得MAPA在多款LVLMs（如LLaVA-V1.6-Mistral-7B, Qwen2.5-VL-7B-Instruct, Llama-3.2-Vision-11B-Instruct和GPT-4o-mini）的最新基准测试中，攻击成功率比现有最先进方法提高了11-35%。|Yiliao Song Team|[2602.14399](http://arxiv.org/abs/2602.14399)|null|
|**2026-02-15**|**WoVR: World Models as Reliable Simulators for Post-Training VLA Policies with RL**|强化学习（RL）有望为视觉-语言-动作（VLA）模型带来超越模仿学习的能力，但其对大量真实世界交互的需求阻碍了在物理机器人上的直接部署。现有利用学习到的世界模型作为模拟器进行策略优化的方法，常因闭环想象推演中的幻觉和长期误差积累而导致优化信号失真。为解决此问题，本文提出了WoVR，一个可靠的基于世界模型的强化学习框架，用于VLA策略的后训练。WoVR通过可控的动作条件视频世界模型提高推演稳定性，通过关键帧初始化推演减少有效误差深度，并通过世界模型-策略协同演化保持策略与模拟器对齐。实验结果显示，WoVR在LIBERO基准和真实机器人操作中显著提高了成功率，证明了在显式控制幻觉的情况下，学习到的世界模型可作为实用的强化学习模拟器。|Dongbin Zhao Team|[2602.13977](http://arxiv.org/abs/2602.13977)|null|
|**2026-02-14**|**Semantic-Contact Fields for Category-Level Generalizable Tactile Tool Manipulation**|通用工具操作需要语义规划和精确物理控制，但现有通用机器人策略缺乏高保真物理基础，而接触感知策略又往往实例特定，难以泛化。大规模真实世界触觉数据难以获取，且软传感器复杂动力学使零样本仿真到真实迁移充满挑战。针对此，本文提出了语义-接触场（SCFields），一种融合视觉语义和密集接触估计的统一3D表示。通过两阶段Sim-to-Real接触学习管道实现：首先在大规模模拟数据上预训练以学习通用接触物理，再通过少量真实数据和伪标签进行微调以对齐传感器特性，从而实现对未见工具的物理泛化。SCFields作为扩散策略的密集观测输入，在刮擦、蜡笔画和剥皮任务中表现出鲁棒的类别级泛化能力，显著优于纯视觉和原始触觉基线。|Yan Wu Team|[2602.13833](http://arxiv.org/abs/2602.13833)|null|
|**2026-02-14**|**MOTIF: Learning Action Motifs for Few-shot Cross-Embodiment Transfer**|视觉-语言-动作（VLA）模型在通用机器人学习方面取得进展，但由于运动学异质性和高昂的数据收集成本，跨具身（cross-embodiment）迁移仍具挑战。现有跨具身策略多依赖共享-私有架构，存在私有参数容量有限和缺乏明确适应机制的问题。为解决这些限制，本文提出了MOTIF框架，旨在通过解耦具身无关的时空模式（称为动作基序）实现高效的少样本跨具身迁移。MOTIF首先通过带有进度感知对齐和具身对抗约束的向量量化学习统一基序，确保时空和跨具身一致性；随后，设计轻量级预测器从实时输入中预测基序，并将其与机器人特定状态融合，指导流匹配策略生成新具身动作。在仿真和真实世界环境中的评估均验证了MOTIF的优越性，少样本迁移成功率显著提升。|Heng Tao Shen Team|[2602.13764](http://arxiv.org/abs/2602.13764)|null|
|**2026-02-14**|**HBVLA: Pushing 1-Bit Post-Training Quantization for Vision-Language-Action Models**|视觉-语言-动作（VLA）模型因其巨大的计算和内存开销，难以部署在资源受限的机器人和边缘平台。尽管权重二值化可提高效率，但现有方法无法弥合二值化与全精度权重之间的分布差距，导致长期闭环执行中量化误差累积并严重降低动作质量。针对此，本文提出了HBVLA，一个VLA定制的二值化框架。HBVLA首先利用策略感知的增强Hessian识别对动作生成关键的权重，然后对非显著权重进行稀疏正交变换以引入低熵中间状态，最后在Harr域中对所有权重进行组式1比特量化。实验结果表明，HBVLA在LIBERO和SimplerEnv上，量化模型分别保留了92.2%和93.6%的全精度性能，并显著优于现有最先进的二值化方法，在真实世界评估中也仅造成微小的成功率下降，展示了在严格硬件约束下的鲁棒部署能力。|Ivor Tsang Team|[2602.13710](http://arxiv.org/abs/2602.13710)|null|
|**2026-02-13**|**Steerable Vision-Language-Action Policies for Embodied Reasoning and Hierarchical Control**|预训练的视觉-语言模型（VLMs）虽能提供丰富的常识先验，但将其有效落地到机器人行为仍具挑战，现有分层方法通过自然语言指令连接VLM与VLA，限制了VLM对低层行为的引导。为增强VLM对低层行为的控制，本文引入“可控策略”（Steerable Policies），即在多抽象层次（如子任务、运动、像素坐标）的丰富合成指令上训练VLA模型。这使得VLM能够通过上下文学习引导这些策略，从而解锁其预训练知识并提升任务泛化能力。广泛的真实世界操作实验表明，与现有基于VLM的VLA和分层基线相比，新方法在泛化和长周期任务中表现更优。|Sergey Levine Team|[2602.13193](http://arxiv.org/abs/2602.13193)|null|
|**2026-02-13**|**UniManip: General-Purpose Zero-Shot Robotic Manipulation with Agentic Operational Graph**|通用机器人操作需要机器人无缝连接高层语义意图与低层物理交互，但现有方法在零样本泛化方面存在不足。针对此问题，本文提出了UniManip框架，其核心是双层代理操作图（AOG），旨在统一语义推理与物理接地。该框架通过高层代理层进行任务编排，低层场景层表示动态状态，实现抽象规划与几何约束的持续对齐，从而支持鲁棒的零样本执行。作为一个动态代理循环，UniManip能从非结构化感知中实例化以物体为中心的场景图，通过安全感知局部规划器参数化无碰撞轨迹，并利用结构化记忆自主诊断和从执行失败中恢复。广泛实验证明，该系统在未见过物体和任务上的零样本成功率分别比VLA和分层基线高22.5%和25.0%，并能直接零样本迁移到移动操作任务。|Ziwei Wang Team|[2602.13086](http://arxiv.org/abs/2602.13086)|**[link](https://henryhcliu.github.io/unimanip)**|
|**2026-02-13**|**Learning Native Continuation for Action Chunking Flow Policies**|动作分块能使视觉-语言-动作（VLA）模型实时运行，但简单的分块执行常导致块边界处的不连续性。现有的实时分块（RTC）方法虽能缓解此问题，但由于其在策略外部，会引起虚假的多模态切换和非内在平滑的轨迹。为此，本文提出了Legato，一种针对基于动作分块流的VLA策略的训练时序延续方法。Legato通过将去噪初始化为已知动作和噪声的调度形混合物，使模型接触部分动作信息，并重塑学习到的流动力学以确保训练和推理在每步引导下保持一致性，同时使用随机调度条件训练以支持不同推理延迟和实现可控平滑性。实验证明，Legato能生成更平滑的轨迹，减少执行时的虚假多模态切换，从而减少犹豫和缩短任务完成时间，在五项操作任务中均比RTC表现更优，轨迹平滑度和任务完成时间均提升约10%。|Yang Gao Team|[2602.12978](http://arxiv.org/abs/2602.12978)|**[link](https://lyfeng001.github.io/Legato/)**|
|**2026-02-13**|**ALOE: Action-Level Off-Policy Evaluation for Vision-Language-Action Model Post-Training**|在真实世界中通过在线强化学习（RL）改进大型视觉-语言-动作（VLA）系统时，价值函数估计是关键，但其通常从混合数据源中收集的轨迹片段进行，这本质上是一个离策略评估问题，而现有工作常采用保守的同策略估计，限制了学习效果。为解决这一问题，本文提出了ALOE（Action-Level Off-Policy Evaluation）框架，用于VLA的后期训练。ALOE采用基于分块的时间差分自举法来评估单个动作序列而非预测最终任务结果，这在稀疏奖励下能更好地将信用归因于关键动作分块，并支持稳定的策略改进。在三项真实世界操作任务上的评估表明，ALOE在不影响执行速度的前提下提高了学习效率，验证了离策略RL在VLA后期训练中可被可靠地重新引入。|Maoqing Yao Team|[2602.12691](http://arxiv.org/abs/2602.12691)|null|
|**2026-02-13**|**SignScene: Visual Sign Grounding for Mapless Navigation**|人类可利用导航标识在陌生环境中无需地图进行导航，本研究旨在使机器人也能利用标识实现开放世界中的无图导航。核心挑战在于解释标识：真实世界标识多样复杂，其抽象语义内容需与局部3D场景进行接地。本文将此形式化为标识接地问题，即把标识上的语义指令映射到对应的场景元素和导航动作。考虑到视觉-语言模型（VLMs）具备所需的语义常识和推理能力但对空间表示敏感，我们提出了SignScene，一种以标识为中心的空间-语义表示方法，它捕获导航相关的场景元素和标识信息，并以有利于VLM有效推理的形式呈现。在包含9种不同环境类型、114个查询的数据集上评估，SignScene达到了88%的接地准确率，显著优于基线方法，并能驱动Spot机器人在真实世界中仅依赖标识进行无图导航。|David Hsu Team|[2602.12686](http://arxiv.org/abs/2602.12686)|null|
|**2026-02-13**|**Xiaomi-Robotics-0: An Open-Sourced Vision-Language-Action Model with Real-Time Execution**|本文介绍了小米机器人0号（Xiaomi-Robotics-0），一个针对高性能、快速且平滑实时执行优化的先进视觉-语言-动作（VLA）模型。该方法的核心在于精心设计的训练配方和部署策略。模型首先在大规模跨实体机器人轨迹和视觉-语言数据上进行预训练，赋予其广泛且泛化的动作生成能力，同时避免灾难性遗忘底层VLM的视觉语义知识。在后期训练中，作者提出了多种技术用于异步执行训练，以解决真实机器人执行时的推理延迟问题。部署时，仔细对齐连续预测动作块的时间步，确保实时执行的连续性和无缝性。广泛的模拟基准测试和两个需要精确灵巧双臂操作的真实机器人任务评估表明，该方法在所有模拟基准上均达到了最先进的性能，并在真实机器人任务上使用消费级GPU实现了高成功率和吞吐量，代码和模型检查点已开源。|Quanyun Zhou Team|[2602.12684](http://arxiv.org/abs/2602.12684)|**[link](https://xiaomi-robotics-0.github.io)**|
|**2026-02-13**|**CRAFT: Adapting VLA Models to Contact-rich Manipulation via Force-aware Curriculum Fine-tuning**|视觉-语言-动作（VLA）模型在处理接触密集型操作任务时面临挑战，因为成功需要精确对齐、稳定接触和处理变形物体，而高熵视觉语言输入与低熵但关键的力信号之间存在不平衡，导致模型过度依赖感知并产生不稳定控制。针对此问题，本文引入CRAFT，一个力感知课程微调框架，该框架集成了一个变分信息瓶颈模块，以在早期训练中调节视觉和语言嵌入，鼓励模型优先处理力信号，然后逐步恢复完整的多模态信息。为实现力感知学习，作者设计了一个同源主从遥操作系统，用于收集各种接触密集型任务中同步的视觉、语言和力数据。真实世界实验表明，CRAFT持续提高了任务成功率，泛化到未见物体和新任务变体，并能有效适应不同VLA架构，从而实现鲁棒且可泛化的接触密集型操作。|Jingtao Sun Team|[2602.12532](http://arxiv.org/abs/2602.12532)|null|
|**2026-02-13**|**AsyncVLA: An Asynchronous VLA for Fast and Robust Navigation on the Edge**|当前机器人基础模型虽泛化能力强，但推理延迟高，导致在动态环境中不安全。针对此问题，本文提出了AsyncVLA异步控制框架，将语义推理与反应性执行解耦。该框架通过远程工作站上的大型基础模型提供高层指导，同时由轻量级板载Edge Adapter高频精细化动作，并通过端到端微调协议和轨迹重加权策略弥合异步流间的域间隙。在面对高达6秒通信延迟的真实视觉导航任务中，AsyncVLA的成功率比现有最佳基线高出40%，成功连接了大型模型的语义智能与边缘机器人所需的实时反应能力。|Sergey Levine Team|[2602.13476](http://arxiv.org/abs/2602.13476)|null|
|**2026-02-13**|**FlowHOI: Flow-based Semantics-Grounded Generation of Hand-Object Interactions for Dexterous Robot Manipulation**|现有视觉-语言-动作（VLA）模型在长周期、接触密集型任务中表现不佳，原因在于缺乏对手-物体交互（HOI）结构的明确表示。为解决此问题，本文提出FlowHOI，一个两阶段流匹配框架，可根据自我中心观察、语言指令和3D高斯泼溅场景重建生成语义明确、时间连贯的HOI序列。该方法将以几何为中心的抓取与以语义为中心的操控解耦，并利用3D场景令牌和运动-文本对齐损失进行语义接地，同时通过从大规模自我中心视频重建HOI轨迹的方法弥补高保真HOI监督的稀缺性。实验结果显示，FlowHOI在GRAB和HOT3D基准测试中实现了最高的动作识别准确率，物理模拟成功率比扩散基线高1.7倍，推理速度提升40倍，并成功在真实机器人上执行了灵巧操作任务。|Xingxing Zuo Team|[2602.13444](http://arxiv.org/abs/2602.13444)|**[link](https://huajian-zeng.github.io/projects/flowhoi/)**|
|**2026-02-12**|**Scaling Verification Can Be More Effective than Scaling Policy Learning for Vision-Language-Action Alignment**|通用机器人理解并执行自然语言指令是长期愿景，尽管视觉-语言-动作（VLA）模型已取得显著进展，但其生成动作仍可能与指令不符。为缩小“意图-动作差距”，本文研究了测试时验证方法。通过分析具身指令遵循的测试时缩放定律，发现联合缩放复述指令和生成动作的数量能更有效地增加测试时样本多样性。在此基础上，提出了CoVer，一种用于VLA对齐的对比验证器，该架构能随计算资源和数据的增加而良好扩展。进一步引入“启动时计算”和分层验证推理流程：在部署时预计算多样化的复述指令，为每条指令重复生成动作候选，然后使用验证器选择最优高层提示和低层动作块。实验表明，相比扩展策略预训练，CoVer在SIMPLER基准上实现了22%的分布内和13%的分布外增益，并在真实世界实验中进一步提高了45%，在PolaRiS基准上任务进度和成功率分别提升了14%和9%。|Marco Pavone Team|[2602.12281](http://arxiv.org/abs/2602.12281)|null|
|**2026-02-12**|**GigaBrain-0.5M*: a VLA That Learns From World Model-Based Reinforcement Learning**|针对现有视觉-语言-动作（VLA）模型在场景理解和未来预测方面的局限性，本研究提出GigaBrain-0.5M*，一个通过世界模型强化学习训练的VLA模型。该模型基于已在大量机器人操作数据上预训练的GigaBrain-0.5，并整合了RAMP（Reinforcement learning via world Model-conditioned Policy）以实现鲁棒的跨任务适应。实验结果表明，RAMP在RECAP基线之上取得了显著的性能提升，在洗衣折叠、箱子包装和咖啡制作等挑战性任务上提升约30%，并在真实部署中展示了可靠的长期执行能力，能够无故障完成复杂的操纵任务。|Zheng Zhu Team|[2602.12099](http://arxiv.org/abs/2602.12099)|**[link](https://gigabrain05m.github.io/)**|
|**2026-02-12**|**VLAW: Iterative Co-Improvement of Vision-Language-Action Policy and World Model**|为提升视觉-语言-动作（VLA）模型性能和可靠性，并解决真实世界数据收集成本高及现有世界模型物理保真度不足的问题，本研究提出一个迭代改进算法。该算法利用少量真实世界试错数据提高世界模型的保真度，然后世界模型生成补充合成数据以改进VLA模型。在真实机器人上的实验表明，该方法使先进VLA模型的成功率相较于基础策略绝对提升39.2%，并且通过生成的合成试错数据训练，又额外提升了11.6%。|Chelsea Finn Team|[2602.12063](http://arxiv.org/abs/2602.12063)|null|
|**2026-02-12**|**HoloBrain-0 Technical Report**|针对基础模型研究与可靠机器人真实部署之间的差距，本研究提出了HoloBrain-0，一个全面的视觉-语言-动作（VLA）框架。其核心是一个新颖的VLA架构，通过明确整合机器人具身先验（如多视角相机参数和运动学描述）来增强3D空间推理并支持多样化具身形态。该设计通过“预训练后微调”范式验证，在多个仿真基准和长时程真实世界操作任务中取得领先结果，且一个高效的0.2B参数变体能支持低延迟部署。研究还全面开源了HoloBrain生态系统，旨在加速研究和实际应用。|Zhizhong Su Team|[2602.12062](http://arxiv.org/abs/2602.12062)|null|
|**2026-02-12**|**When would Vision-Proprioception Policies Fail in Robotic Manipulation?**|针对视觉-本体感受策略在机器人运动过渡阶段视觉模态作用受限、策略倾向于利用本体感受信号导致视觉学习受抑制的问题，本研究提出了梯度调整与阶段引导（GAP）算法。该算法通过利用本体感受信息估计运动过渡阶段的概率，并自适应地调整本体感受梯度的幅值，以实现视觉和本体感受模态的动态协同。综合实验表明，GAP算法能够提升视觉-本体感受策略的鲁棒性和泛化性，适用于模拟和真实环境、单臂和双臂设置，并兼容多种VLA模型。|Di Hu Team|[2602.12032](http://arxiv.org/abs/2602.12032)|null|
|**2026-02-12**|**JEPA-VLA: Video Predictive Embedding is Needed for VLA Models**|针对现有视觉-语言-动作（VLA）模型存在的样本效率低和泛化能力有限问题，本研究发现其根源在于预训练视觉表示在环境理解和策略先验方面的知识不足。通过深入分析，研究指出在视频上预训练的预测嵌入，特别是V-JEPA 2，能更有效地捕捉任务相关时态动态并忽略不可预测因素，从而弥补了现有视觉表示的缺陷。在此基础上，提出了JEPA-VLA，一种将预测嵌入自适应整合到现有VLA中的方法。实验证明，JEPA-VLA在LIBERO、LIBERO-plus、RoboTwin2.0和真实机器人任务等一系列基准测试中均取得了显著性能提升。|Mingsheng Long Team|[2602.11832](http://arxiv.org/abs/2602.11832)|null|
|**2026-02-12**|**ABot-N0: Technical Report on the VLA Foundation Model for Versatile Embodied Navigation**|针对具身导航任务碎片化的问题，本研究引入了ABot-N0，一个统一的视觉-语言-动作（VLA）基础模型，旨在实现点目标、物体目标、指令遵循、兴趣点目标和人员跟踪五大核心任务的“大一统”。该模型采用分层“大脑-动作”架构，利用大型语言模型进行语义推理，并结合流匹配专家生成精确轨迹。为支持大规模学习，研究构建了ABot-N0数据引擎，整理了海量专家轨迹和推理样本。实验结果表明，ABot-N0在7个基准测试中取得了最先进的性能，并能通过集成的Agentic导航系统实现动态真实世界环境中的鲁棒、长时程任务执行。|Mu Xu Team|[2602.11598](http://arxiv.org/abs/2602.11598)|**[link](https://amap-cvlab.github.io/ABot-Navigation/ABot-N0/)**|
|**2026-02-12**|**Scaling World Model for Hierarchical Manipulation Policies**|针对视觉-语言-动作（VLA）模型在域外（OOD）设置下泛化能力不足的问题，本研究引入了一个分层VLA框架VISTA。VISTA利用大规模预训练世界模型的泛化能力实现鲁棒且可泛化的视觉子目标任务分解，其中高层世界模型规划任务分解为带有目标图像的子任务序列，低层VLA策略遵循文本和视觉指导生成动作。这些合成的目标图像为低层策略提供了视觉和物理上的详细指导，使其能够泛化到未见过的物体和新场景。实验结果表明，在世界模型生成的指导下，VISTA在大量OOD场景中显著提升了VLA性能，在novel场景中性能从14%提高到69%。|Xinghang Li Team|[2602.10983](http://arxiv.org/abs/2602.10983)|null|
|**2026-02-12**|**ForeAct: Steering Your VLA with Efficient Visual Foresight Planning**|视觉-语言-动作（VLA）模型将高级语言指令转换为具体可执行动作，在开放世界环境中尤具挑战性。本文提出了Visual Foresight Planning (ForeAct)，这是一种通用高效的规划器，通过想象的未来观察和子任务描述逐步指导VLA。该规划器包含一个高效的预测图像生成模块（在0.33秒内预测高质量未来观察）和一个视觉-语言模型，后者负责推理任务并生成子任务描述。重要的是，ForeAct能够无缝集成到现有VLA中，只需扩充视觉输入而无需修改架构。经过百万级跨具身任务的预训练，预测生成器学习了鲁棒的具身动力学。在包含11项多样化、多步骤真实世界任务的基准测试中，ForeAct实现了87.4%的平均成功率，比基线 $π_0$提高了40.9%，比带有文本子任务指导的$π_0$ 提高了30.3%。|Song Han Team|[2602.12322](http://arxiv.org/abs/2602.12322)|null|
|**2026-02-11**|**H-WM: Robotic Task and Motion Planning Guided by Hierarchical World Model**|针对现有世界模型在长时程机器人规划中累积误差的问题，以及传统符号逻辑世界模型缺乏视觉感知接地的局限性，本研究提出分层世界模型（H-WM）。H-WM在一个统一的双层框架内联合预测逻辑和视觉状态转换，将符号推理的鲁棒性与视觉观察的感知基础相结合。为训练H-WM，研究引入了一个对齐机器人运动与符号状态、动作和视觉观察的机器人数据集。实验证明，分层输出为长时程任务提供了稳定一致的中间指导，有效减轻了误差积累，并在VLA控制策略上展示了该方法的有效性和通用性。|Yingxue Zhang Team|[2602.11291](http://arxiv.org/abs/2602.11291)|null|
|**2026-02-11**|**RISE: Self-Improving Robot Policy with Compositional World Model**|针对视觉-语言-动作（VLA）模型在接触密集和动态操作任务中易受执行偏差影响的脆弱性，以及物理世界中在线强化学习（RL）的限制，本研究提出了RISE，一个通过想象进行机器人强化学习的可扩展框架。其核心是一个组合世界模型，该模型能预测多视角未来并通过进度价值模型评估想象结果，从而为策略改进提供信息丰富的优势。这些组件被整合到一个闭环自改进流水线中，在想象空间中持续生成试错并更新策略。在三个真实世界任务中，RISE相对于现有技术取得了显著性能提升，如在动态砖块分类、背包包装和盒子关闭任务中，绝对性能分别提升超过35%、45%和35%。|Hongyang Li Team|[2602.11075](http://arxiv.org/abs/2602.11075)|**[link](https://opendrivelab.com/kai0-rl/)**|
|**2026-02-11**|**RADAR: Benchmarking Vision-Language-Action Generalization via Real-World Dynamics, Spatial-Physical Intelligence, and Autonomous Evaluation**|针对现有视觉-语言-动作（VLA）模型评估主要局限于仿真或高度受限的真实世界，导致现实差距大、泛化能力差的问题，本研究提出RADAR（Real-world Autonomous Dynamics And Reasoning）基准。RADAR旨在系统评估VLA在真实条件下的泛化能力，集成了物理动力学套件、专门测试空间推理和物理理解的任务，以及基于3D指标的全自主评估流程。通过RADAR对多个先进VLA模型进行审计，发现模型在适度物理动态下性能急剧下降，例如在传感器噪声下3D IoU从0.261下降到0.068，且空间推理能力有限，揭示了模型在真实世界条件下的严重脆弱性，强调了RADAR作为可靠泛化评估基准的必要性。|Guangrun Wang Team|[2602.10980](http://arxiv.org/abs/2602.10980)|null|
|**2026-02-11**|**From Representational Complementarity to Dual Systems: Synergizing VLM and Vision-Only Backbones for End-to-End Driving**|VLA驾驶将语言功能引入端到端规划，但其核心变化除准确性-成本权衡外尚不明确。本文通过RecogDrive进行3-RQ分析，比较VLM和纯视觉骨干网络在相同规划器下的表现。研究发现VLM能引入独特子空间，导致在长尾场景下行为差异，VLM更激进而ViT更保守。基于此，本文提出HybridDriveVLA，通过学习评分器融合两者的轨迹，将PDMS提升至92.10。进一步，DualDriveVLA实现了快慢策略，仅在低置信度时调用VLM，在15%场景中调用VLM即可达到91.00 PDMS，并使吞吐量提高3.2倍。|Yan Wang Team|[2602.10719](http://arxiv.org/abs/2602.10719)|null|
|**2026-02-11**|**Say, Dream, and Act: Learning Video World Models for Instruction-Driven Robot Manipulation**|机器人操作需要预测环境对动作的响应，但现有系统常缺乏此能力，导致效率低下。鉴于VLMs无法明确预测未来状态且现有世界模型存在预测短期或空间不一致帧的问题，本文提出了一种快速且预测性的视频条件动作框架。该方法首先适配鲁棒的视频生成模型以确保未来预测的可靠性，接着通过对抗蒸馏实现快速视频生成，最后训练一个动作模型利用生成视频和真实观测纠正空间错误。实验结果表明，该方法生成的视频预测在时间连贯性和空间准确性方面表现优异，显著提升了具身一致性、空间指代能力和任务完成度。|Yanwei Fu Team|[2602.10717](http://arxiv.org/abs/2602.10717)|null|
|**2026-02-11**|**AugVLA-3D: Depth-Driven Feature Augmentation for Vision-Language-Action Models**|当前VLA模型主要依赖2D图像训练，限制了其在复杂3D环境中的空间理解和动作落地。为解决此问题，本文提出了一个将深度估计集成到VLA模型中的新框架，以丰富3D特征表示。该方法利用VGGT从RGB输入中提取3D几何线索，并引入“动作助手”模块，通过动作先验约束3D表示以确保与下游控制任务的一致性。实验结果表明，所提出的方法不仅增强了几何模糊场景中的感知，还显著提高了动作预测准确性，强调了深度驱动数据增强对弥合2D观测与3D决策差距的潜力。|F. Richard Yu Team|[2602.10698](http://arxiv.org/abs/2602.10698)|null|
|**2026-02-11**|**Improving Medical Visual Reinforcement Fine-Tuning via Perception and Reasoning Augmentation**|强化微调（RFT）在语言模型后训练中展现潜力，但在跨模态、以视觉为中心的医学影像领域应用不足，而该领域需要强大的视觉感知和结构化推理。为此，本文提出了VRFT-Aug，一个专为医学领域设计的视觉强化微调框架。VRFT-Aug通过引入先验知识注入、感知驱动策略优化、医学知情奖励塑造和行为模仿等训练策略来增强感知和推理。实验结果表明，该方法在多个医学数据集上持续优于标准监督微调和RFT基线，并提供了可推广到其他医学图像任务的实用训练启发。|Qicheng Lao Team|[2602.10619](http://arxiv.org/abs/2602.10619)|null|
|**2026-02-11**|**LAP: Language-Action Pre-Training Enables Zero-shot Cross-Embodiment Transfer**|机器人学长期目标是实现零样本部署的通用策略，但现有VLA模型与训练实体紧密耦合。针对此问题，本文提出了语言-动作预训练（LAP），一种将低级机器人动作直接用自然语言表示的方法，使动作监督与预训练VLM的输入-输出分布对齐，无需定制化设计或昂贵标注。基于LAP，本文提出了LAP-3B，实现了在未见机器人上的显著零样本迁移，平均成功率超过50%，比现有VLA模型提升约2倍。此外，LAP还支持高效适应、有利扩展，并通过统一的语言-动作格式在协同训练中获得额外收益。|Anirudha Majumdar Team|[2602.10556](http://arxiv.org/abs/2602.10556)|**[link](https://lap-vla.github.io)**|
|**2026-02-11**|**Found-RL: foundation model-enhanced reinforcement learning for autonomous driving**|强化学习在自动驾驶中面临样本效率低和语义可解释性不足问题，而基础模型（特别是VLM）虽能提供丰富知识，但高推理延迟阻碍其在RL训练中的实时部署。为此，本文提出了Found-RL平台，通过异步批量推理框架将VLM推理与仿真循环解耦，解决了延迟瓶颈。该平台引入了价值边际正则化和优势加权动作指导等监督机制，将VLM建议有效提炼到RL策略中。此外，通过条件对比动作对齐解决CLIP的动态盲区问题，实现密集奖励塑造。Found-RL使得轻量级RL模型在保持实时推理（500 FPS）的同时，达到接近十亿参数VLM的性能。|Sikai Chen Team|[2602.10458](http://arxiv.org/abs/2602.10458)|null|
|**2026-02-10**|**Hardware Co-Design Scaling Laws via Roofline Modelling for On-Device LLMs**|VLA模型在资源受限设备部署中面临LLM骨干网络选择挑战，需平衡准确性与推理延迟及硬件效率。本文提出了硬件协同设计法则，将模型训练损失建模为架构超参数的函数，并通过屋脊线模型表征推理延迟，从而建立了准确性-延迟的直接对应关系。通过对1,942个候选架构进行经验评估和训练，拟合出架构与训练损失的缩放定律。将该定律与延迟建模结合，本文确定了硬件协同设计LLM的帕累托前沿，并将架构搜索公式化为精度与性能的联合优化。结果显示，该方法将架构选择时间从数月缩短到数天，并在相同延迟下，其协同设计的架构在WikiText-2上的困惑度降低了19.42%。|Cheng Deng Team|[2602.10377](http://arxiv.org/abs/2602.10377)|null|
|**2026-02-10**|**ST4VLA: Spatially Guided Training for Vision-Language-Action Models**|大型VLM虽擅长多模态理解，但在具身任务中将指令转化为低级运动动作时表现不足。为此，本文引入了ST4VLA，一个双系统VLA框架，通过空间引导训练使动作学习与VLM中的空间先验对齐。ST4VLA包含两阶段：首先是空间基础预训练，利用大规模和机器人数据通过点、框、轨迹预测为VLM注入可迁移先验；其次是空间引导动作后训练，通过空间提示促使模型生成更丰富的空间先验以指导动作生成。实验表明，ST4VLA显著提升了Google Robot和WidowX Robot的性能，在SimplerEnv上创造了SOTA，并展现出对未见物体、转述指令的更强泛化能力及在真实世界中对扰动的鲁棒性。|Jiangmiao Pang Team|[2602.10109](http://arxiv.org/abs/2602.10109)|null|
|**2026-02-10**|**EgoHumanoid: Unlocking In-the-Wild Loco-Manipulation with Robot-Free Egocentric Demonstration**|人形机器人运动-操作任务面临数据需求高和具身差距挑战。针对此，本文提出了EgoHumanoid框架，首次利用大量以自我为中心的人类演示数据与有限机器人数据共同训练视觉-语言-动作策略，使人形机器人在多样真实环境中执行运动-操作。为弥合人类与机器人间的形态和视点差异，本文引入了包含硬件设计、数据处理的系统对齐流程，并开发了便携式人类数据收集系统。其核心是视点对齐和动作对齐，分别减少视觉域差异和将人类动作映射到机器人动作空间。真实世界实验表明，整合人类演示数据显著优于仅用机器人数据的基线（51%），尤其是在未见环境中。|Li Chen Team|[2602.10106](http://arxiv.org/abs/2602.10106)|**[link](https://opendrivelab.com/EgoHumanoid)**|

<p align=right>(<a href=#updated-on-20260219>back to top</a>)</p>

## Humanoid

|Publish Date|Title|Chinese Summary|Authors|PDF|Code|
|---|---|---|---|---|---|
|**2026-02-18**|**Learning Humanoid End-Effector Control for Open-Vocabulary Visual Loco-Manipulation**|人形机器人在野外对任意物体进行视觉定位-操作需要精确的末端执行器控制和通用的场景理解，但现有基于模仿学习的方法因数据收集困难而泛化能力有限。为此，本文提出HERO范式，结合大型视觉模型的强大泛化能力与模拟训练的控制性能。该方法通过设计一个精确的残差感知型末端执行器跟踪策略，整合逆运动学、神经正向模型、目标调整和重新规划。实验证明，该方法将末端执行器跟踪误差降低了3.2倍，使机器人能在多样真实环境中可靠操作各类日常物体，展示了其在模拟和真实世界中的有效性。|Saurabh Gupta Team|[2602.16705](http://arxiv.org/abs/2602.16705)|**[link](https://hero-humanoid.github.io/)**|
|**2026-02-18**|**VIGOR: Visual Goal-In-Context Inference for Unified Humanoid Fall Safety**|人形机器人在复杂环境中的可靠跌倒恢复至关重要，但现有方法通常将跌倒安全问题碎片化处理或依赖无视觉的端到端策略，导致泛化能力受限。本文提出一种统一的跌倒安全方法，基于人类跌倒姿态的可转移性以及整合的感知-运动表示。该方法训练一个特权教师模型，利用稀疏人类演示数据，并将其蒸馏成一个仅依赖自我中心深度和本体感受的学生模型，学生通过匹配教师的目标-上下文潜在表示来学习反应。模拟和真实Unitree G1人形机器人上的结果表明，该方法在多样非平坦环境中实现了鲁棒、零样本的跌倒安全，无需真实世界微调。|Stella X. Yu Team|[2602.16511](http://arxiv.org/abs/2602.16511)|null|
|**2026-02-17**|**Perceptive Humanoid Parkour: Chaining Dynamic Human Skills via Motion Matching**|尽管人形机器人步态已在复杂地形上取得进展，但捕捉高动态人类动作的敏捷性和适应性仍是挑战，特别是复杂环境中的跑酷任务。本文提出了感知人形跑酷（PHP）模块化框架，使人形机器人能自主执行长时程、基于视觉的复杂障碍跑酷。该方法首先利用运动匹配将人类技能组合成长时程运动轨迹，再训练运动跟踪强化学习专家策略并将其蒸馏为基于深度的多技能学生策略。通过结合感知与技能组合，机器人仅凭机载深度传感和速度指令即可进行自主、上下文感知的决策。在Unitree G1人形机器人上的真实实验验证了该框架在攀爬高障碍和多障碍穿越等动态跑酷技能上的有效性。|C. Karen Liu Team|[2602.15827](http://arxiv.org/abs/2602.15827)|null|
|**2026-02-17**|**MeshMimic: Geometry-Aware Humanoid Motion Learning through 3D Scene Reconstruction**|人形机器人运动控制虽有突破，但高度依赖昂贵的动作捕捉（MoCap）数据，且MoCap数据常缺乏环境几何上下文，导致运动与场景脱节和物理不一致。针对此问题，本文提出MeshMimic框架，通过桥接3D场景重建与具身智能，使人形机器人能直接从视频中学习耦合的“运动-地形”交互。该框架利用先进3D视觉模型精确重建人类轨迹和3D地形/物体几何，并引入基于运动学一致性的优化算法提取高质量运动数据，再通过接触不变重定向方法转移人-环境交互特征。实验证明，MeshMimic在多样复杂地形上实现了鲁棒、高动态性能，且仅需消费级单目传感器，为人形机器人自主进化提供了可扩展路径。|Yijie Guo Team|[2602.15733](http://arxiv.org/abs/2602.15733)|null|
|**2026-02-16**|**Replanning Human-Robot Collaborative Tasks with Vision-Language Models via Semantic and Physical Dual-Correction**|人机协作（HRC）在装配任务中至关重要，但人类指令的模糊性常阻碍机器人生成可行的协作行为。现有基于视觉-语言模型（VLMs）的方法存在幻觉推理和无法预测物理执行失败的问题。本文提出一种HRC框架，通过双重校正机制增强VLM的推理能力：内部校正模型在动作执行前验证逻辑一致性和任务可行性，外部校正模型则通过执行后反馈检测并纠正物理失败。仿真研究显示该方法提高了成功率，真实世界协作装配任务实验也证实了其在响应人类指令、进行交互式重新规划方面的实用性。|Kensuke Harada Team|[2602.14551](http://arxiv.org/abs/2602.14551)|null|
|**2026-02-16**|**AdaptManip: Learning Adaptive Whole-Body Object Lifting and Delivery with Online Recurrent State Estimation**|现有基于模仿学习的人形机器人移动操作方法依赖人类演示，且易受扰动影响，缺乏鲁棒的自主框架。本文提出AdaptManip，一个完全自主的框架，使人形机器人能执行集成导航、物体举升和递送任务。该框架通过强化学习训练鲁棒策略，无需人类演示，包含循环对象状态估计器、全身基础移动与残差操作控制策略以及基于LiDAR的全局定位器。所有组件均在模拟中训练并以零样本方式部署到真实硬件。实验结果表明，AdaptManip在适应性和成功率上显著优于基线方法，精确的对象状态估计提升了操作性能，并成功实现了人形机器人在真实世界中的完全自主导航、举升和递送。|Sehoon Ha Team|[2602.14363](http://arxiv.org/abs/2602.14363)|**[link](https://morganbyrd03.github.io/adaptmanip/)**|
|**2026-02-15**|**WIMLE: Uncertainty-Aware World Models with IMLE for Sample-Efficient Continuous Control**|模型基强化学习虽具有高样本效率潜力，但常因累积模型误差、平均多模态动态的单模态世界模型及过度自信预测而表现不佳。本文引入WIMLE，一种模型基方法，将隐式最大似然估计（IMLE）扩展到模型基RL框架中，以学习随机、多模态的世界模型，并利用集成和潜在采样估计预测不确定性。训练期间，WIMLE根据预测置信度加权合成转换，保留有用模型推演并减弱不确定预测偏差。在多项连续控制任务中，WIMLE展现了卓越的样本效率和具竞争力的渐近性能，尤其在Humanoid-run任务上样本效率提升超过50%，并解决了HumanoidBench上更多任务，突显了IMLE基多模态和不确定性感知加权对稳定模型基RL的价值。|Ke Li Team|[2602.14351](http://arxiv.org/abs/2602.14351)|**[link](https://openreview.net/forum?id=mzLOnTb3WH)**|
|**2026-02-15**|**ProAct: A Dual-System Framework for Proactive Embodied Social Agents**|具身社交代理在语音和手势同步生成上有所进展，但多数系统仍仅限于短时窗内反应式交互，难以实现需要深思熟虑和意图推理的主动社交行为。本文提出ProAct双系统框架，通过解耦低延迟“行为系统”与较慢的“认知系统”，协调实时交互与长时程社交推理的时间尺度冲突。为将意图流畅地转化为非语言行为，引入了通过ControlNet以意图为条件的流匹配模型，支持异步意图注入。将ProAct部署在物理人形机器人上，用户研究表明参与者和观察者在感知主动性、社交存在感和整体参与度方面更偏好ProAct，证明了双系统主动控制在具身社交交互中的益处。|Libin Liu Team|[2602.14048](http://arxiv.org/abs/2602.14048)|**[link](https://proactrobot.github.io/)**|
|**2026-02-14**|**Humanoid Hanoi: Investigating Shared Whole-Body Control for Skill-Based Box Rearrangement**|人形机器人通过任务级排序可重用技能以实现长时程箱子重排，但简单重用预训练的共享全身控制器（WBC）可能因新技能引入的状态和命令分布偏移而降低鲁棒性。本文提出一种数据聚合程序，通过利用领域随机化下闭环技能执行的推演数据来增强共享WBC训练，从而解决这一问题。为评估该方法，引入了“人形汉诺塔”这一长时程箱子重排基准。在模拟和Digit V3人形机器人上的实验结果表明，该框架在扩展时程上实现了完全自主的重排，并量化了共享WBC方法相对于非共享基线的优势。|Alan Fern Team|[2602.13850](http://arxiv.org/abs/2602.13850)|null|
|**2026-02-14**|**Impact-Robust Posture Optimization for Aerial Manipulation**|扭矩控制机器人（尤其具运动冗余者）在冲击时常出现状态和输入命令的急剧峰值，影响安全性和鲁棒性。本文提出一种通过优化运动冗余机器人姿态以提高冲击鲁棒性的新方法。该方法基于刚性冲击模型构建构型相关指标，量化冲击前后速度变化，并通过基于梯度的运动任务迭代引导机器人至最小化该指标的构型。此任务嵌入任务空间逆动力学全身控制器中，实现与其他控制目标的无缝集成。实验表明，该方法使冲击后机器人构型峰值（相对于标准TSID）降低高达51%，并避免执行器饱和。此外，数值模拟证明运动冗余对冲击鲁棒性至关重要，可使冲击后机器人状态峰值降低高达45%。|Antonio Franchi Team|[2602.13762](http://arxiv.org/abs/2602.13762)|null|
|**2026-02-14**|**A Kung Fu Athlete Bot That Can Do It All Day: Highly Dynamic, Balance-Challenging Motion Dataset and Autonomous Fall-Resilient Tracking**|针对现有类人机器人运动追踪系统在处理高动态行为时性能和鲁棒性不足的问题，特别是在武术这类复杂运动中数据集稀缺且机器人易失稳跌倒，本研究构建了一个高动态武术运动数据集KungFuAthlete。在此基础上，提出了一种新颖的训练范式，使单个策略能够联合学习高动态运动追踪和跌倒恢复，从而将敏捷执行和稳定性统一在一个框架内。实验结果表明，该数据集具有显著更高的运动强度和复杂性，且该框架扩展了机器人能力，实现了具备恢复能力的执行，提高了类人机器人在真实世界高动态场景中的鲁棒性和自主性。|Xuesong Li Team|[2602.13656](http://arxiv.org/abs/2602.13656)|null|
|**2026-02-13**|**Adding internal audio sensing to internal vision enables human-like in-hand fabric recognition with soft robotic fingertips**|鉴于机器人难以复制人类区分织物纹理的丰富动态感知能力，因为传统触觉传感器难以兼顾高空间分辨率和高时间采样率，本研究提出了一个能够感知 spatio-temporal 力模式和纹理诱导振动的系统。该系统在机器人手上集成Minsight传感器测量形变和力，以及Minsound传感器通过MEMS麦克风捕捉振动。受人类评估织物启发，机器人主动夹持摩擦织物。实验结果显示，音频传感器对分类性能具有高实用性，且基于Transformer的方法在20种常见织物数据集上达到了97%的分类准确率，并能泛化学习织物弹性、厚度和粗糙度的通用表征。|Katherine J. Kuchenbecker Team|[2602.12918](http://arxiv.org/abs/2602.12918)|null|
|**2026-02-13**|**PMG: Parameterized Motion Generator for Human-like Locomotion Control**|尽管深度强化学习和运动追踪技术显著提升了类人机器人运动能力，但现有全身参考引导方法在适应高层指令接口和多样任务场景时仍面临数据集、鲁棒性和标定等挑战。为解决这些限制，本研究提出了参数化运动生成器（PMG），它基于对人类运动结构分析，仅使用紧凑的参数化运动数据和高维控制指令合成参考轨迹。结合模仿学习流水线和基于优化的sim-to-real电机参数识别模块，该系统在类人机器人原型ZERITH Z1上实现了自然、类人的运动，精确响应高维控制输入（包括VR遥操作），并支持高效、可验证的sim-to-real迁移，为实用化类人机器人控制提供了途径。|Houde Liu Team|[2602.12656](http://arxiv.org/abs/2602.12656)|null|
|**2026-02-13**|**CLOT: Closed-Loop Global Motion Tracking for Whole-Body Humanoid Teleoperation**|针对长时间全身类人机器人遥操作中存在的全局姿态漂移积累问题，本研究提出了CLOT（Closed-Loop Global Motion Tracking），一个通过高频定位反馈实现闭环全局运动追踪的实时全身类人机器人遥操作系统。为解决直接施加全局追踪奖励导致的激进行为，研究提出数据驱动的随机化策略，解耦观测轨迹与奖励评估，实现平滑稳定的全局校正，并结合对抗性运动先验抑制非自然行为。经过20小时人类运动数据训练的Transformer策略，在仿真和真实世界实验中验证了系统在高动态运动、高精度追踪和sim-to-real鲁棒性方面的卓越性能，实现了长时间无漂移的人机模仿。|Yichao Yan Team|[2602.15060](http://arxiv.org/abs/2602.15060)|null|
|**2026-02-12**|**General Humanoid Whole-Body Control via Pretraining and Fast Adaptation**|为解决类人机器人全身控制器在运动多样性、快速适应性和高动态场景鲁棒平衡方面面临的挑战，本研究提出了FAST框架，旨在实现快速适应和稳定运动追踪。FAST引入了Parseval-Guided残差策略适应机制，该机制在正交性和KL约束下学习轻量级增量动作策略，从而高效适应异分布运动并缓解灾难性遗忘。此外，为增强物理鲁棒性，提出了质心感知控制，通过整合质心相关观测和目标，提升追踪挑战性参考运动时的平衡能力。实验结果表明，FAST在鲁棒性、适应效率和泛化能力方面均持续优于现有SOTA基线方法。|Zongqing Lu Team|[2602.11929](http://arxiv.org/abs/2602.11929)|null|
|**2026-02-12**|**HAIC: Humanoid Agile Object Interaction Control via Dynamics-Aware World Model**|针对类人机器人在非结构化环境中与具有独立动力学和非完整约束的欠驱动物体进行人-物交互时面临的控制挑战，本研究提出了HAIC框架。其核心贡献是一个动力学预测器，它仅通过本体感知历史估计高阶物体状态，并将预测结果投射到静态几何先验上，形成动态占据图，使策略能够推断盲区内的碰撞边界和接触可供性。此外，HAIC采用不对称微调，确保世界模型在分布变化下能鲁棒地适应学生策略的探索。实验表明，HAIC在滑板、推拉购物车等敏捷任务中实现了高成功率，并通过预测多物体动力学，成功掌握了如在复杂地形搬运箱子等多物体长时任务。|Renjing Xu Team|[2602.11758](http://arxiv.org/abs/2602.11758)|**[link](https://haic-humanoid.github.io/)**|
|**2026-02-12**|**Future Mining: Learning for Safety and Security**|针对采矿业向AI驱动网络物理生态系统演进中面临的恶劣环境限制和网络物理威胁，以及能量受限传感器导致的盲区问题，本研究提出了一个统一的智能安全与安保架构愿景。该架构集成了多模态感知、安全联邦学习、强化学习、DTN通信和能量感知传感，并设计了矿工定位、多模态态势感知、后门攻击监测、可信联邦学习和物联网设备健康监测等五个核心模块。这些模块旨在协同解决矿工定位、危害理解、联邦鲁棒性和预测性维护等问题，从而在对抗条件下实现弹性、可信的智能采矿系统，保障运营连续性。|Sanjay Madria Team|[2602.11472](http://arxiv.org/abs/2602.11472)|null|
|**2026-02-12**|**Humanoid Manipulation Interface: Humanoid Whole-Body Manipulation from Robot-Free Demonstrations**|当前人形机器人全身操作方法受限于硬件物流和复杂奖励工程，导致自主技能有限且通常仅限于受控环境。为解决这些问题，本文提出了Humanoid Manipulation Interface (HuMI)，一个便携高效的框架，用于在各种环境中学习多样化的全身操作任务。HuMI通过便携硬件捕捉丰富的全身运动，实现无机器人数据收集，并利用分层学习流程将人类运动转化为灵巧且可行的人形技能。广泛实验表明，HuMI的数据收集效率比遥操作提高3倍，并在未知环境中取得了70%的成功率，有效提升了人形机器人的泛化操作能力。|Yang Gao Team|[2602.06643](http://arxiv.org/abs/2602.06643)|**[link](https://humanoid-manipulation-interface.github.io)**|
|**2026-02-11**|**ExtremControl: Low-Latency Humanoid Teleoperation with Direct Extremity Control**|针对现有类人机器人遥操作系统因依赖重度预处理和位置PD控制而导致高延迟，严重限制响应能力的问题，本研究提出了ExtremControl，一个低延迟全身控制框架。该框架的核心思想是：直接在选定刚性连杆（特别是末端肢体）的SE(3)姿态上操作，避免全身重定向；利用笛卡尔空间映射将人类运动直接转换为机器人连杆目标；并在低层融入速度前馈控制，以支持快速变化的控制接口下的高响应行为。仿真和真实世界实验验证了ExtremControl的有效性，实现了低至50ms的端到端延迟，从而使得机器人能够执行乒乓球平衡、杂耍等高响应性任务，显著超越了先前200ms的延迟限制。|Chuang Gan Team|[2602.11321](http://arxiv.org/abs/2602.11321)|**[link](https://owenowl.github.io/extremcontrol)**|
|**2026-02-11**|**APEX: Learning Adaptive High-Platform Traversal for Humanoid Robots**|为解决类人机器人高平台攀爬中，现有深度强化学习方法常收敛于高冲击、不安全的跳跃式解决方案的问题，本研究提出了APEX系统。该系统实现了基于感知的攀爬式高平台穿越，通过地形适应行为（攀爬、行走、姿态重构）组合多种技能。其核心是引入通用棘轮进度奖励，为接触丰富的目标达成机动提供密集且无速度的监督，同时通过双重策略（训练时建模映射伪影，部署时滤波高程图）减少sim-to-real感知差距。实验表明，该系统在29自由度Unitree G1类人机器人上实现了对0.8米平台（约腿长的114%）的零样本sim-to-real穿越，展现了对平台高度和初始姿态的鲁棒适应性及平稳的多技能转换。|Ding Zhao Team|[2602.11143](http://arxiv.org/abs/2602.11143)|**[link](https://apex-humanoid.github.io/)**|
|**2026-02-11**|**Towards Learning a Generalizable 3D Scene Representation from 2D Observations**|针对现有神经辐射场（NeRF）方法多在相机坐标系下操作，难以直接应用于机器人操作的挑战，本研究提出了一种可泛化的神经辐射场方法，用于从机器人自我中心观测预测三维工作空间占用。与以往方法不同，该模型在全局工作空间坐标系中构建占用表示，使其能直接应用于机器人操作，并集成了灵活的源视图，无需场景特定的微调即可泛化到未见过的物体排列。在类人机器人上进行的实验验证了该方法，结果显示，在40个真实场景训练后，该模型实现了26mm的重建误差（包括遮挡区域），证明了其超越传统立体视觉方法推断完整3D占用信息的能力。|Stefan Wermter Team|[2602.10943](http://arxiv.org/abs/2602.10943)|null|
|**2026-02-11**|**MOSAIC: Bridging the Sim-to-Real Gap in Generalist Humanoid Motion Tracking and Teleoperation with Rapid Residual Adaptation**|鉴于通用人形运动追踪器在模拟中表现优异，但在实际硬件持续遥操作时易受接口和动力学误差影响，本文提出了开源全栈系统MOSAIC。该系统首先通过强化学习在多源运动库上训练面向遥操作的通用运动追踪器，采用自适应重采样和强调世界坐标系运动一致性的奖励。为弥合模拟到真实世界的接口差距，MOSAIC通过快速残差适应，使用少量接口特定数据训练一个接口特定策略，并通过加性残差模块将其蒸馏到通用追踪器中，优于传统微调方法。实验结果（包括系统消融、分布外基准测试和真实机器人实验）证明，MOSAIC在实际延迟和噪声下能实现稳健的离线运动回放和在线长周期遥操作。|Alois Knoll Team|[2602.08594](http://arxiv.org/abs/2602.08594)|null|
|**2026-02-11**|**MotionWeaver: Holistic 4D-Anchored Framework for Multi-Humanoid Image Animation**|鉴于现有角色图像动画方法难以泛化到涉及多样人形形式、复杂交互和频繁遮挡的多人场景，本文提出了MotionWeaver框架。该框架引入统一的运动表示，提取与身份无关的运动并明确绑定到角色，以泛化到多样人形并扩展到多人场景。同时，提出了整体4D锚定范式，构建共享4D空间融合运动与视频潜空间，并通过分层4D级别监督强化交互和遮挡处理。为支持此研究，构建了46小时多人视频数据集和300视频基准。定量和定性实验结果表明，MotionWeaver在自建基准上达到SOTA，并能有效泛化至复杂多人场景。|Weizhan Zhang Team|[2602.13326](http://arxiv.org/abs/2602.13326)|null|
|**2026-02-10**|**EgoHumanoid: Unlocking In-the-Wild Loco-Manipulation with Robot-Free Egocentric Demonstration**|针对人形机器人动作操作对数据需求高，而现有方法未能充分利用人类演示数据，且存在人机体现差异的问题，本文提出了EgoHumanoid框架。该框架首次利用大量自我中心人类演示和少量机器人数据共同训练视觉-语言-动作策略，使人形机器人能在多样真实世界环境中执行动作操作。通过硬件设计到数据处理的系统对齐流水线，包括视图对齐和动作对齐，成功弥合了人机之间的形态和视角差异。广泛的真实世界实验表明，整合无机器人自我中心数据相比仅机器人基线性能显著提升51%，尤其是在未见过的环境中，且分析揭示了行为有效迁移及扩展人类数据的潜力。|Li Chen Team|[2602.10106](http://arxiv.org/abs/2602.10106)|**[link](https://opendrivelab.com/EgoHumanoid)**|
|**2026-02-10**|**Humanoid Factors: Design Principles for AI Humanoids in Human Worlds**|随着人形机器人开始与人类共享空间，传统人因工程需要扩展，不仅考虑人类因素，也要考虑人形机器人因素。当前人形机器人带来了人类行为、沟通和社会存在的期望，重塑了可用性、信任和安全。本文引入“人形机器人因素”框架，围绕物理、认知、社会和伦理四大支柱，指导人形机器人开发，使其能有效与人类共存和协作，并表征了人机能力间的重叠与差异。通过评估真实人形机器人控制算法，该框架揭示了传统机器人任务指标如何忽视关键人类认知和交互原则，为设计、评估和管理持续人机共存提供了基础性框架。|Lixiao Huang Team|[2602.10069](http://arxiv.org/abs/2602.10069)|null|
|**2026-02-10**|**TeleGate: Whole-Body Humanoid Teleoperation via Gated Expert Selection with Motion Prior**|针对人形机器人实时全身遥操作中，现有方法通过知识蒸馏将多专家策略整合，常导致高动态运动性能下降的挑战，本文提出了TeleGate统一遥操作框架。其核心思想是训练一个轻量级门控网络，根据本体感知状态和参考轨迹实时动态激活领域特定专家策略，从而保留其完整能力，避免知识蒸馏的性能损失。此外，引入基于VAE的运动先验模块，从历史观测中提取未来运动意图，实现预期控制。在模拟和Unitree G1机器人上的实验表明，TeleGate仅需2.5小时训练数据，即在跑步、跌倒恢复和跳跃等多样动态运动中实现了高精度实时遥操作，显著优于基线方法。|Rongyun Cao Team|[2602.09628](http://arxiv.org/abs/2602.09628)|null|
|**2026-02-09**|**Characteristics, Management, and Utilization of Muscles in Musculoskeletal Humanoids: Empirical Study on Kengoro and Musashi**|当前肌肉骨骼人形机器人研究中，对其生物仿生结构固有的多样属性及其管理利用方式缺乏统一讨论。本研究基于作者团队开发的Kengoro和Musashi机器人，将肌肉骨骼结构特征归纳为冗余性、独立性、各向异性、可变力臂和非线性弹性五大属性。文章进一步探讨了这些属性组合所带来的优势与劣势，并重点讨论了身体图式学习、反射控制、肌肉分组及身体图式适应等机制。最后，研究阐述了通过集成系统实现运动的实践，并展望了未来的研究挑战。|Masayuki Inaba Team|[2602.08518](http://arxiv.org/abs/2602.08518)|null|
|**2026-02-09**|**Learning Human-Like Badminton Skills for Humanoid Robots**|人形机器人实现羽毛球等高强度运动的类人表现面临巨大挑战，尤其是在运动学模仿与功能性、物理感知击打之间难以兼顾自然风格。为解决此问题，本文提出了Imitation-to-Interaction渐进式强化学习框架，旨在使机器人从“模仿者”进化为“击球手”。该方法通过人类数据建立运动先验，蒸馏到模型化状态表示中，并利用对抗性先验稳定动力学，同时引入流形扩展策略以应对稀疏的专家演示。实验结果显示，该框架在仿真中掌握了多样羽毛球技能，并首次实现了类人羽毛球技能从仿真到真实机器人的零样本迁移，展示了物理世界中的优雅和精准打击。|Peng Lu Team|[2602.08370](http://arxiv.org/abs/2602.08370)|null|
|**2026-02-07**|**VividFace: Real-Time and Realistic Facial Expression Shadowing for Humanoid Robots**|人形机器人面部表情实时模仿对于实现逼真、情感丰富的人机交互至关重要，但现有方法常因离线推理和细节捕捉不足而难以同时达到实时性和逼真性。为解决这些局限，本文提出了VividFace，一个实时且逼真的人形机器人面部表情阴影系统。该系统通过优化模仿框架X2CNet++，并引入特征适应训练策略，显著增强了表情表现力；同时，通过视频流兼容推理管线和基于异步I/O的工作流，实现了高效的实时模仿。广泛的真实世界演示验证了VividFace在0.05秒内模仿人类表情并生成生动人形面部的实用能力，且能泛化至多种面部配置。|Yang Zhang Team|[2602.07506](http://arxiv.org/abs/2602.07506)|null|
|**2026-02-07**|**TextOp: Real-time Interactive Text-Driven Humanoid Robot Motion Generation and Control**|现有的人形机器人全身控制器在灵活性和自主性方面存在局限，难以实现实时和交互式驱动。为解决这一问题，本文提出了TextOp，一个实时文本驱动的人形运动生成与控制框架，支持流式语言指令和即时修改。TextOp采用两级架构：高级运动扩散模型根据文本生成短时域轨迹，低级运动跟踪策略则在机器人上执行这些轨迹。广泛的真实机器人实验和离线评估表明，TextOp实现了即时响应、平滑全身运动和精确控制，在舞蹈、跳跃等复杂行为中展现出自由形式的意图表达和流畅过渡。|Xuelong Li Team|[2602.07439](http://arxiv.org/abs/2602.07439)|**[link](https://text-op.github.io/)**|
|**2026-02-07**|**Bridging Speech, Emotion, and Motion: a VLM-based Multimodal Edge-deployable Framework for Humanoid Robots**|为解决多数人形机器人缺乏协调的语音、面部表情和手势，以及在设备上自主运行的需求，本文提出了SeM²，一个基于视觉语言模型的框架。SeM²通过多模态感知模块捕捉用户上下文，结合思维链推理规划响应，并利用语义序列对齐机制确保言语内容与物理表达的精确时间协调，从而实现情感一致的多模态交互。研究实现了云端及边缘部署版本，其中边缘版本通过知识蒸馏高效运行。综合评估显示，SeM²在自然度、情感清晰度和模态一致性方面显著优于单模态基线，推动了社交表达性人形机器人在多样现实环境中的应用。|Miao Li Team|[2602.07434](http://arxiv.org/abs/2602.07434)|null|
|**2026-02-06**|**Cerebellar-Inspired Residual Control for Fault Recovery: From Inference-Time Adaptation to Structural Consolidation**|针对机器人策略在真实世界部署中常遇到的训练后故障，且不便重新训练的问题，本文提出了一种推理时、受小脑启发的残差控制框架。该框架通过在线校正动作增强冻结的强化学习策略，无需修改基础策略参数即可实现故障恢复。它实例化了小脑核心原理，如高维模式分离、并行残差路径和局部误差驱动可塑性，并通过保守的元适应调节残差权限。实验结果表明，在MuJoCo基准测试中，该框架在执行器、动力学和环境扰动下，对HalfCheetah-v5和Humanoid-v5在适度故障下性能显著提升，并在严重故障下表现出优雅的性能下降。|Amit Ranjan Trivedi Team|[2602.07227](http://arxiv.org/abs/2602.07227)|null|
|**2026-02-06**|**DynaRetarget: Dynamically-Feasible Retargeting using Sampling-Based Trajectory Optimization**|将人类运动重定向到人形机器人控制策略并确保其动态可行性是一项挑战。本文介绍了DynaRetarget，一个将人类运动重定向到人形控制策略的完整流程。其核心是新颖的基于采样的轨迹优化（SBTO）框架，该框架能将不完善的运动学轨迹细化为动态可行的运动，并通过逐步推进优化范围来处理长时域任务。DynaRetarget在重定向数百个人形-物体演示中取得了比现有技术更高的成功率，并能泛化到不同物体属性的场景，为生成大规模人形局部操作轨迹合成数据集提供了可能。|Majid Khadiv Team|[2602.06827](http://arxiv.org/abs/2602.06827)|null|
|**2026-02-06**|**ECO: Energy-Constrained Optimization with Reinforcement Learning for Humanoid Walking**|为解决仿人机器人节能稳定运动中现有方法需大量超参数调优且易导致次优策略的问题，本研究提出了ECO（Energy-Constrained Optimization）约束强化学习框架。该框架将能量指标明确定义为不等式约束，通过拉格朗日法强制执行能量消耗和参考运动约束，从而实现稳定、对称且节能的行走。ECO在仿真和真实机器人（BRUCE）上的实验结果表明，在保持鲁棒行走性能的同时，其能耗显著低于MPC、标准RL及其他SOTA约束RL方法，实现了仿人机器人节能运动的实质性进步。|Yao Su Team|[2602.06445](http://arxiv.org/abs/2602.06445)|null|
|**2026-02-06**|**Now You See That: Learning End-to-End Humanoid Locomotion from Raw Pixels**|为解决基于视觉的仿人机器人鲁棒运动中模拟到真实差距产生的感知噪声和多样地形下学习目标冲突的挑战，本研究提出了一个端到端的视觉驱动运动框架。为实现鲁棒的sim-to-real迁移，该框架开发了高保真深度传感器模拟，并提出了视觉感知行为蒸馏方法。为适应多样地形，引入了地形特定奖励塑形，并结合多评论家和多判别器学习。在配备不同立体深度摄像头的两个仿人机器人平台上，该策略展现出在多样环境中的鲁棒性能，能处理极端挑战和精细任务，包括双向长期楼梯遍历。|Zongwu Xie Team|[2602.06382](http://arxiv.org/abs/2602.06382)|null|

<p align=right>(<a href=#updated-on-20260219>back to top</a>)</p>
