---
layout: default
---

## Updated on 2026.02.20

## Categories

- [Manipulation](#manipulation)
- [World Model](#world-model)
- [VLM](#vlm)
- [VLA](#vla)
- [Humanoid](#humanoid)

## Manipulation

| Publish Date | Title | Chinese Summary | Authors | PDF | Code |
|:---------|:-----------------------|:------------------------|:---------|:------|:------|
|**2026-02-19**|**RoboGene: Boosting VLA Pre-training via Diversity-Driven Agentic Framework for Real-World Task Generation**|鉴于通用机器人操作受限于稀缺的真实世界交互数据，以及现有任务生成方法效率低、易产生不可行指令，本文提出了RoboGene框架。该框架通过多样性驱动采样、自反思机制和人工参与完善，自动化生成单臂、双臂和移动机器人多样化、物理上可行的操作任务。定量分析和大规模真实世界实验表明，RoboGene显著优于现有基础模型，且其生成的数据能显著提升VLA模型的成功率和泛化能力。|Jian Tang Team|[2602.16444](http://arxiv.org/abs/2602.16444)|null|
|**2026-02-19**|**IRIS: Learning-Driven Task-Specific Cinema Robot Arm for Visuomotor Motion Control**|针对工业级机器人相机系统成本高、操作复杂的问题，本文提出了智能机器人成像系统 (IRIS)，这是一个低成本的6自由度机械臂。该系统结合了3D打印硬件与基于Action Chunking with Transformers (ACT) 的视觉运动模仿学习框架，能够直接从人类演示中学习电影级的相机轨迹，无需显式编程。实验结果表明，IRIS平台成本低于1000美元，具有高精度和可靠的自主执行能力，并在真实世界中展现了良好的泛化性。|Ali Bereyhi Team|[2602.17537](http://arxiv.org/abs/2602.17537)|null|
|**2026-02-19**|**Benchmarking the Effects of Object Pose Estimation and Reconstruction on Robotic Grasping Success**|当前3D重建的几何评估未能充分反映其对机器人抓取等下游操作任务的影响。本文提出了一个大规模、基于物理的基准来评估6D姿态估计器和3D网格模型在抓取功能上的有效性。研究发现，重建缺陷会显著减少抓取姿态候选，但在姿态估计准确时对抓取性能影响较小；抓取成功率主要受空间误差影响。这项工作为理解感知系统与机器人物体操作之间的关系提供了重要洞察。|Torsten Sattler Team|[2602.17101](http://arxiv.org/abs/2602.17101)|null|
|**2026-02-18**|**BPP: Long-Context Robot Imitation Learning by Focusing on Key History Frames**|在机器人任务中，策略需要关注历史观测信息，但现有方法常因训练时历史状态覆盖不足，导致模型易受虚假相关性影响而泛化性差。针对此问题，本文提出“宏观策略”（Big Picture Policies, BPP），通过视觉-语言模型识别关键帧，将多样轨迹映射至紧凑的任务相关事件集，从而显著减少训练与部署间的分布偏移。实验结果表明，BPP在四项真实世界和三项模拟操作任务中，成功率比现有最佳方法高出70%。|Aviral Kumar Team|[2602.15010](http://arxiv.org/abs/2602.15010)|null|
|**2026-02-18**|**Learning Humanoid End-Effector Control for Open-Vocabulary Visual Loco-Manipulation**|针对人形机器人在野外视觉定位-操作中末端执行器控制精度和场景理解泛化能力不足的问题，本文提出了HERO范式。该方法结合了大型视觉模型的强泛化能力和仿真训练的精确控制性能，设计了一个残差感知末端执行器跟踪策略。通过逆运动学、学习型正向模型及目标调整等创新，将跟踪误差降低了3.2倍。HERO系统在多样化的真实环境中，能够可靠地操作各种日常物品，验证了其在复杂场景下的有效性和实用性。|Saurabh Gupta Team|[2602.16705](http://arxiv.org/abs/2602.16705)|**[link](https://hero-humanoid.github.io/)**|
|**2026-02-18**|**VIGOR: Visual Goal-In-Context Inference for Unified Humanoid Fall Safety**|针对人形机器人在杂乱环境中跌倒恢复的挑战，本文提出了一种统一的跌倒安全方法，涵盖跌倒恢复的所有阶段。该方法基于人类跌倒姿态的可迁移性和整合式感知-运动表征，通过在平坦和复杂地形上利用稀疏人类演示训练特权教师模型，并蒸馏到仅依赖自我中心深度和本体感受的学生模型。仿真和真实机器人实验结果表明，该方法在多样化非平坦环境中实现了鲁棒、零样本的跌倒安全，无需真实世界微调。|Stella X. Yu Team|[2602.16511](http://arxiv.org/abs/2602.16511)|null|
|**2026-02-18**|**SparTa: Sparse Graphical Task Models from a Handful of Demonstrations**|为解决机器人从演示中高效学习长周期操作任务的挑战，本文提出一种方法，通过图形化对象关系序列来推断机器人应实现的目标，而非直接学习动作。该方法引入了演示分割和池化，提取涵盖完整对象交互的操作图，并结合预训练视觉特征进行对象匹配，以提高鲁棒性。实验证明，该方法能准确分割演示并从多演示中高效学习，其任务表示在仿真和真实机器人上均支持可靠执行。|Abhinav Valada Team|[2602.16911](http://arxiv.org/abs/2602.16911)|null|
|**2026-02-18**|**MALLVI: a multi agent framework for integrated generalized robotics manipulation**|针对现有大型语言模型 (LLMs) 在机器人任务规划中缺乏环境反馈、鲁棒性差的问题，本文提出了MALLVi (Multi Agent Large Language and Vision framework)，一个支持闭环反馈驱动的机器人操作框架。MALLVi协调多个专门代理（如Decomposer, Thinker, Reflector）处理感知、推理和规划，并通过视觉语言模型 (VLM) 评估环境反馈以实现迭代决策和错误恢复。仿真和真实世界实验表明，这种闭环多代理协调显著提升了零样本操作任务的泛化能力和成功率。|Babak Khalaj Team|[2602.16898](http://arxiv.org/abs/2602.16898)|null|
|**2026-02-17**|**Learning to Retrieve Navigable Candidates for Efficient Vision-and-Language Navigation**|针对大型语言模型 (LLMs) 在视觉语言导航 (VLN) 中决策效率低下的问题，本文提出了一个检索增强框架。该框架在不修改或微调LLM的情况下，通过指令级嵌入检索器在情节层面提供任务特定先验，并通过模仿学习的候选检索器在步骤层面修剪不相关的可导航方向，从而减少动作模糊性和提示复杂性。在R2R基准上的实验结果显示，该方法在成功率和决策效率方面均取得显著提升，验证了检索增强决策支持的有效性。|Lina Yao Team|[2602.15724](http://arxiv.org/abs/2602.15724)|null|
|**2026-02-17**|**Selective Perception for Robot: Task-Aware Attention in Multimodal VLA**|针对现有机器人视觉-语言-动作 (VLA) 模型静态融合多视角输入导致计算开销大和噪声干扰的问题，本文提出了一个动态信息融合框架。该框架引入轻量级自适应路由架构，实时分析文本提示和腕部摄像头观察，预测不同视图的任务相关性，并有条件地衰减低效视图的计算。通过自动化标注流程高效获取训练数据，实验结果表明该方法在真实世界机器人操作中显著提高了推理效率和控制性能，验证了动态信息融合在资源受限环境下的实用性。|Soo-Chul Lim Team|[2602.15543](http://arxiv.org/abs/2602.15543)|null|
|**2026-02-17**|**Feasibility-aware Imitation Learning from Observation with Multimodal Feedback**|针对模仿学习中演示者与机器人物理特性差异导致演示动作数据不全或不可行的问题，本文提出了可行性感知行为克隆 (FABCO)。FABCO将行为克隆与可行性估计相结合，利用机器人动力学模型评估演示动作的可复现性，并通过多模态反馈引导演示者改进动作。同时，可行性感知策略学习减少了不可行演示动作的影响。实验结果表明，FABCO将模仿学习性能提升了3.2倍以上，显著增强了机器人稳定执行能力。|Takamitsu Matsubara Team|[2602.15351](http://arxiv.org/abs/2602.15351)|null|
|**2026-02-16**|**PhyScensis: Physics-Augmented LLM Agents for Complex Physical Scene Arrangement**|自动生成交互式3D环境对机器人模拟数据收集至关重要，但现有方法常忽略物体间的物理关系，难以生成复杂的物理场景。为解决物体密度高、支撑关系复杂以及需准确建模空间与物理属性的挑战，本文提出PhyScensis框架。该框架以LLM代理为核心，结合物理引擎，通过迭代提出资产与物理谓词、实现3D场景及反馈优化，并利用概率编程和启发式方法确保场景稳定性与空间关系的可控性。实验证明，PhyScensis在场景复杂度、视觉质量和物理精度上均超越现有方法，为机器人操纵提供了统一的复杂物理场景生成方案。|Chuang Gan Team|[2602.14968](http://arxiv.org/abs/2602.14968)|null|
|**2026-02-16**|**Affordance Transfer Across Object Instances via Semantically Anchored Functional Map**|传统示教学习（LfD）需大量物理演示，难以扩展；从人类视频学习虽避免了直接机器人参与，但将演示泛化至几何差异大的对象仍是挑战。为此，本文提出“语义锚定功能映射”（SemFM）框架，旨在通过单次视觉演示实现功能跨对象迁移。该方法从粗糙网格重建开始，识别物体间语义对应的功能区域，选择互斥语义锚点，并利用功能映射在表面传播约束，获得密集且语义一致的对应关系。实验结果表明，SemFM能以较低计算成本准确迁移可用性，适用于实际机器人感知-动作流水线。|Weiming Zhi Team|[2602.14874](http://arxiv.org/abs/2602.14874)|null|
|**2026-02-16**|**DriveFine: Refining-Augmented Masked Diffusion VLA for Precise and Robust Driving**|自动驾驶领域的视觉-语言-动作（VLA）模型，如基于扩散和基于Token的规划器，存在模态对齐困难、训练效率低、泛化性不足及累积误差等问题。本文提出DriveFine，一个结合灵活解码和自校正能力的掩码扩散VLA模型。其核心是创新的即插即用块级MoE设计，将“精炼专家”无缝注入“生成专家”之上，通过显式专家选择和梯度阻断实现专家解耦。此外，设计了一种混合强化学习策略，平衡精炼专家的探索与训练稳定性。实验证明，DriveFine在NAVSIM v1、v2和Navhard基准测试中表现出强大的有效性和鲁棒性。|Yan Wang Team|[2602.14577](http://arxiv.org/abs/2602.14577)|null|
|**2026-02-16**|**RoboSolver: A Multi-Agent Large Language Model Framework for Solving Robotic Arm Problems**|当前机器人领域难以有效整合大型语言模型（LLM）和视觉语言模型（VLM）与计算工具，以实现机器人操纵器问题的自动化分析与求解。本研究提出一个基于LLM和VLM的智能多智能体框架，专门用于机器人技术。该框架能接收文本和视觉输入，并自动执行正逆运动学、速度/加速度计算、3D仿真及运动控制。基准测试显示，与原始模型相比，集成GPT-4o的框架在文本输入正运动学任务中准确率从0.30提升至0.97，在视觉输入任务中准确率从约0.73提升至0.93，在综合机器人任务中达到0.97，显著提高了LLM/VLM在机器人任务中的性能。|Alireza Taheri Team|[2602.14438](http://arxiv.org/abs/2602.14438)|null|
|**2026-02-16**|**A Soft Wrist with Anisotropic and Selectable Stiffness for Robust Robot Learning in Contact-rich Manipulation**|在非结构化环境中进行接触丰富的机器人操纵任务时，现有软末端执行器因形变范围有限、缺乏方向刚度控制或系统复杂而面临鲁棒性挑战。为此，本文提出CLAW（Compliant Leaf-spring Anisotropic soft Wrist），一种新型软腕机构，通过简单的正交板簧与锁定旋转关节设计，实现了大范围6自由度形变（横向40mm，垂直20mm）和可调的三种模式各向异性刚度，同时保持轻量化和低成本。模仿学习实验表明，CLAW在插销任务中成功率达76%，显著优于Fin Ray夹具和刚性夹具，展现了其在复杂接触场景下实现鲁棒机器人学习的潜力。|Masashi Hamaya Team|[2602.14434](http://arxiv.org/abs/2602.14434)|null|
|**2026-02-16**|**AdaptManip: Learning Adaptive Whole-Body Object Lifting and Delivery with Online Recurrent State Estimation**|人形机器人进行导航、物体抓取和递送等全身运动操纵任务，现有模仿学习方法鲁棒性差且易受干扰。本文提出AdaptManip，一个全自主框架，通过强化学习训练鲁棒的全身运动操纵策略，无需人类演示。该框架包含：实时跟踪物体的循环状态估计器、实现鲁棒运动和稳定操纵的全身基础策略，以及提供抗漂移定位的基于LiDAR的全局位置估计器。所有组件均在仿真中训练并零样本部署到真实硬件。实验结果表明，AdaptManip在适应性和总成功率上显著优于基线，即使在遮挡下，精确的物体状态估计也提升了操纵性能，并成功演示了真实世界的全自主任务。|Sehoon Ha Team|[2602.14363](http://arxiv.org/abs/2602.14363)|**[link](https://morganbyrd03.github.io/adaptmanip/)**|
|**2026-02-15**|**GRAIL: Goal Recognition Alignment through Imitation Learning**|从代理行为中理解其目标是使AI系统与人类意图对齐的关键，但现有方法常依赖最优策略表示，与行为者真实行为不符，影响目标识别准确性。本文提出GRAIL（Goal Recognition Alignment through Imitation Learning），利用模仿学习和逆强化学习，直接从（可能次优的）演示轨迹中为每个候选目标学习目标导向策略。GRAIL通过单次前向传播对轨迹进行评分，在保留经典目标识别一次性推理能力的同时，能捕获次优和系统偏倚行为。实验结果显示，GRAIL在系统偏倚、次优和噪声轨迹下显著提高了F1-分数，并在最优设置中保持竞争力。|Reuth Mirsky Team|[2602.14252](http://arxiv.org/abs/2602.14252)|null|
|**2026-02-15**|**Learning Part-Aware Dense 3D Feature Field for Generalizable Articulated Object Manipulation**|铰接物体操纵的泛化性是机器人领域的挑战，关键在于理解功能部件，但现有2D特征在提升至3D空间时面临运行时间长、不一致和分辨率低等问题。为此，本文提出Part-Aware 3D Feature Field (PA3FF)，一种新型的、具有部件感知能力的密集3D特征，通过对比学习利用大规模3D部件提案训练。PA3FF能从点云前向预测连续3D特征场，其特征距离反映功能部件的接近程度。在此基础上，提出Part-Aware Diffusion Policy (PADP)，一个模仿学习框架，旨在提高机器人操纵的样本效率和泛化能力。实验证明，PA3FF在操纵场景中持续优于多种2D和3D表示，并能支持其他下游任务。|Hao Dong Team|[2602.14193](http://arxiv.org/abs/2602.14193)|**[link](https://pa3ff.github.io)**|
|**2026-02-15**|**RoboAug: One Annotation to Hundreds of Scenes via Region-Contrastive Data Augmentation for Robotic Manipulation**|提升机器人学习在多样、未见场景中的泛化能力是一个重要挑战，现有方法常依赖大规模预训练或需完美目标检测。本文提出RoboAug，一个生成式数据增强框架，通过仅需单张图像的边界框标注，显著减少对大规模预训练和完美视觉识别的依赖。RoboAug利用预训练生成模型进行精确语义数据增强，并集成即插即用区域对比损失，引导模型聚焦任务相关区域以提升泛化性。在三台机器人上超过3.5万次实验表明，RoboAug在未见场景中泛化能力显著优于SOTA基线，成功率在UR-5e、AgileX和Tien Kung 2.0上分别提升至0.47、0.60和0.67，验证了其在真实世界操纵任务中的优越性和有效性。|Jian Tang Team|[2602.14032](http://arxiv.org/abs/2602.14032)|null|
|**2026-02-15**|**WoVR: World Models as Reliable Simulators for Post-Training VLA Policies with RL**|为克服强化学习在机器人VLA模型中因大量真实世界交互需求而难以直接部署，以及世界模型模拟器在长周期想象轨迹中易产生幻觉和误差累积的问题，本研究提出了WoVR框架。WoVR通过可控的动作条件视频世界模型提高轨迹稳定性，利用关键帧初始化轨迹减少有效误差深度，并通过世界模型-策略协同演化保持对齐。实验结果显示，WoVR在LIBERO基准和真实机器人操作任务中实现了稳定的长周期想象轨迹和有效的策略优化，平均成功率在LIBERO上提升了29.3个百分点，在真实机器人上提升了30.0个百分点，证明了在有效控制幻觉的情况下，学习到的世界模型可作为实用的强化学习模拟器。|Dongbin Zhao Team|[2602.13977](http://arxiv.org/abs/2602.13977)|null|
|**2026-02-14**|**Enabling Option Learning in Sparse Rewards with Hindsight Experience Replay**|针对分层强化学习框架（如MOC）在稀疏奖励多目标环境中表现不佳，特别是在对象操作任务中难以发现与对象的交互策略问题，本研究首先提出了MOC-HER，将回溯经验回放（HER）集成到MOC中。在此基础上，为更有效处理对象操作任务，进一步引入了Dual Objectives Hindsight Experience Replay (2HER)，通过同时生成对象最终状态目标和智能体效应器位置目标，奖励智能体与对象的交互和任务完成。实验结果表明，MOC-2HER在机器人操作环境中的成功率高达90%，远高于MOC和MOC-HER的不足11%，验证了双目标重标记策略的有效性。|Gabriel de Oliveira Ramos Team|[2602.13865](http://arxiv.org/abs/2602.13865)|null|
|**2026-02-14**|**Mean Flow Policy with Instantaneous Velocity Constraint for One-step Action Generation**|针对流基策略在强化学习中表达能力与计算负担的权衡问题，本研究提出了一种新的生成式策略函数——平均速度策略（MVP）。MVP通过建模平均速度场实现最快的一步动作生成，并引入瞬时速度约束（IVC）以确保高表达能力。理论上证明IVC作为关键边界条件可提高学习精度和策略表达力。实验结果表明，MVP在Robomimic和OGBench的多个机器人操作任务中取得了最先进的成功率，并在训练和推理速度上显著优于现有流基策略基线。|Shengbo Eben Li Team|[2602.13810](http://arxiv.org/abs/2602.13810)|null|
|**2026-02-14**|**Gaussian Sequences with Multi-Scale Dynamics for 4D Reconstruction from Monocular Casual Videos**|为解决从单目日常视频中进行四维（4D）动态场景重建的ill-posed问题，本研究基于真实世界动态的多尺度规律性，设计了多尺度动态机制以分解复杂运动场。在此基础上，提出了具有多尺度动态的高斯序列，通过多级运动组合构建动态3D高斯表示，显著减轻了重建歧义并促进物理合理性。同时，结合视觉基础模型的多模态先验提供补充监督，进一步约束解空间并提高重建保真度。实验证明，该方法在动态新视角合成任务中，在基准和真实世界操作数据集上均显著优于现有方法，实现了从单目视频中准确且全局一致的4D重建。|Lei Sun Team|[2602.13806](http://arxiv.org/abs/2602.13806)|null|
|**2026-02-14**|**MOTIF: Learning Action Motifs for Few-shot Cross-Embodiment Transfer**|尽管视觉-语言-动作（VLA）模型推动了通用机器人学习，但由于运动学异构性以及收集足够真实世界示范数据进行微调的高成本，跨具身（cross-embodiment）迁移仍然充满挑战。现有跨具身策略通常依赖共享-私有架构，其私有参数容量有限且缺乏明确的适应机制。为解决这些局限性，本文提出了MOTIF框架，旨在实现高效的小样本跨具身迁移，它将具身无关的时空模式（称为动作基序）与异构动作数据解耦。具体而言，MOTIF首先通过带有进度感知对齐和具身对抗约束的矢量量化学习统一的基序，以确保时间和跨具身一致性。然后，设计一个轻量级预测器从实时输入预测这些基序，并将其与机器人特定状态融合，以指导流匹配策略在新的具身上生成动作。模拟和真实世界环境的评估均验证了MOTIF的优越性，在小样本迁移场景中显著优于强基线，模拟中提升6.5%，真实世界中提升43.7%。|Heng Tao Shen Team|[2602.13764](http://arxiv.org/abs/2602.13764)|null|
|**2026-02-14**|**HybridFlow: A Two-Step Generative Policy for Robotic Manipulation**|现有的机器人操作策略受推理延迟限制，缺乏足够的实时环境交互能力。尽管流匹配等更快的生成方法正逐步取代扩散方法，但其精度仍难以满足机器人操作的严格要求。本文关注MeanFlow作为流匹配的单步变体虽然快速但在动作生成精度上的不足。为平衡推理速度和生成质量，本文提出了HybridFlow，这是一种具有2-NFE（函数评估次数）的三阶段方法，包括MeanFlow模式下的全局跳转、用于分布对齐的ReNoise以及ReFlow模式下的局部细化。该方法利用MeanFlow单步生成的快速优势，同时以最少的生成步骤确保动作精度。真实世界实验表明，HybridFlow在成功率上比16步扩散策略高出15-25%，并将推理时间从152毫秒缩短到19毫秒（8倍加速，约52赫兹）；在未见颜色OOD抓取和可变形物体折叠任务上分别达到了70.0%和66.3%的成功率。这些结果表明HybridFlow是一种实用的低延迟方法，能增强机器人操作策略的真实世界交互能力。|Yide Liu Team|[2602.13718](http://arxiv.org/abs/2602.13718)|null|
|**2026-02-14**|**Symmetry-Aware Fusion of Vision and Tactile Sensing via Bilateral Force Priors for Robotic Manipulation**|机器人插入任务需要精密的、富接触的交互，仅凭视觉难以解决。尽管触觉反馈具有直观价值，但现有研究表明，朴素的视觉-触觉融合往往未能持续提供改进。为解决此问题，本文提出了一种用于视觉-触觉融合的跨模态Transformer（CMT），它通过结构化的自注意力与交叉注意力机制整合腕部摄像头观测和触觉信号。为稳定触觉嵌入，本文进一步引入了物理信息正则化，鼓励双边力平衡，反映了人类运动控制的原理。在TacSL基准上的实验表明，带有对称正则化的CMT实现了96.59%的插入成功率，超越了朴素和门控融合基线，并与“腕部+接触力”的优越配置（96.09%）非常接近。这些结果突出表明：触觉感知对于精确对齐不可或缺，以及经过物理信息正则化强化的原则性多模态融合，能够充分发挥视觉和触觉的互补优势，在现实感知条件下接近最优性能。|Tao Yu Team|[2602.13689](http://arxiv.org/abs/2602.13689)|null|
|**2026-02-14**|**Hierarchical Audio-Visual-Proprioceptive Fusion for Precise Robotic Manipulation**|现有机器人操作方法主要依赖视觉和本体感受，在部分可观测的真实世界环境中难以推断接触相关的交互状态。而声学线索能自然编码丰富的接触动态，但在多模态融合中却未被充分利用，且大多数融合方法错误地假设模态作用均一。为实现基于声学信息的精确机器人操作，本文提出一种分层表示融合框架，逐步整合音频、视觉和本体感受。该方法首先将视觉和本体感受表示条件化于声学线索，然后明确建模高阶跨模态交互以捕捉模态间的互补依赖。融合后的表示被扩散策略用于直接从多模态观测生成连续机器人动作。在真实世界机器人操作任务（如倒液体和开柜门）上的广泛实验表明，该方法持续优于现有最先进的多模态融合框架，尤其是在声学线索提供视觉无法轻易获得的任务相关信息时。此外，通过互信息分析解释了音频线索在机器人操作中的作用。|Peng Liu Team|[2602.13640](http://arxiv.org/abs/2602.13640)|null|
|**2026-02-13**|**Imitating What Works: Simulation-Filtered Modular Policy Learning from Human Videos**|通过观看人类视频学习操作技能，有望为机器人学习提供大规模数据的新来源。然而，人类视频在学习抓取后动作方面提供了强信号，但在学习抓取行为方面用处较小，特别是对于没有类似人手的机器人而言，任意稳定的抓取通常不兼容任务。为解决这一挑战，本文提出了Perceive-Simulate-Imitate (PSI) 框架，用于使用经过模拟中成对抓取-轨迹过滤处理的人类视频运动数据训练模块化操作策略。这一模拟步骤通过抓取适用性标签扩展了轨迹数据，从而能够监督学习面向任务的抓取能力。真实世界实验表明，该框架可以无需任何机器人数据高效学习精确操作技能，与简单使用抓取生成器相比，性能显著更鲁棒。|Wei-Chiu Ma Team|[2602.13197](http://arxiv.org/abs/2602.13197)|null|
|**2026-02-13**|**UniManip: General-Purpose Zero-Shot Robotic Manipulation with Agentic Operational Graph**|针对现有机器人操作方法在零样本泛化方面的不足，即端到端VLA模型缺乏精度而传统规划器语义刚性问题，本文提出了UniManip框架。该框架基于双层Agentic Operational Graph (AOG)，通过高层Agentic层进行任务编排和低层Scene层表示动态状态，实现语义推理与物理接地的统一，并以动态智能体循环方式主动实例化场景图、规划无碰撞轨迹并自主恢复失败。实验结果表明，UniManip在未见对象和任务上展现出鲁棒的零样本能力，成功率显著高于现有VLA和分层基线，且支持从固定基座到移动操作的零样本迁移。|Ziwei Wang Team|[2602.13086](http://arxiv.org/abs/2602.13086)|**[link](https://henryhcliu.github.io/unimanip)**|
|**2026-02-13**|**How Swarms Differ: Challenges in Collective Behaviour Comparison**|针对群体行为分析中数值特征集通常缺乏通用性且难以定量衡量行为相似性的问题，本研究深入探讨了特征集对集体行为的影响。我们从现有群体机器人学工作中筛选出特征集和相似性度量，并评估了它们在特定行为背景外的鲁棒性。研究发现，特征集和相似性度量的相互作用决定了区分相似行为群体的有效性，并提出了一种基于自组织图的方法来识别特征空间中行为难以区分的区域。|Jonas Kuckling Team|[2602.13016](http://arxiv.org/abs/2602.13016)|null|
|**2026-02-13**|**SafeFlowMPC: Predictive and Safe Trajectory Planning for Robot Manipulators with Learning-based Policies**|面对机器人日益融入日常生活对灵活性和实时反应能力的需求，以及学习方法缺乏安全保证和优化方法泛化能力不足的挑战，本文提出了SafeFlowMPC框架。该框架结合了流匹配与在线优化，旨在融合学习和优化方法的优势，并通过次优模型预测控制公式，实时确保操作安全性。在KUKA 7自由度机械臂上的真实世界实验（包括抓取和人机交接任务）中，SafeFlowMPC展现了强大的性能。|Andreas Kugi Team|[2602.12794](http://arxiv.org/abs/2602.12794)|null|
|**2026-02-13**|**Scaling Single Human Demonstrations for Imitation Learning using Generative Foundational Models**|鉴于模仿学习中收集机器人演示数据的困难以及人类演示到机器人转移的挑战，本文提出了Real2Gen框架，仅通过单个人类演示来训练操作策略。Real2Gen从人类演示中提取关键信息并传输到模拟环境，利用可编程专家智能体生成无限量的训练数据来学习流匹配策略。实验结果表明，Real2Gen平均成功率提高了26.6%，并且由于训练数据的丰富性，训练出的策略具有更强的泛化能力，实现了纯模拟训练策略的零样本真实世界部署。|Abhinav Valada Team|[2602.12734](http://arxiv.org/abs/2602.12734)|null|
|**2026-02-13**|**$\mathcal{X}$-KD: General Experiential Knowledge Distillation for Large Language Models**|针对大语言模型知识蒸馏（KD）中，现有方法常忽视教师模型原始学习环境的问题，本文提出了Experiential Knowledge Distillation ($\mathcal{X}$-KD) 框架。受经验学习理论和逆强化学习启发，$\mathcal{X}$-KD采用Approximated Variational Reward Imitation Learning (AVRIL) 框架，联合建模教师的原始奖励函数并执行策略蒸馏，使学生模型能在教师的原始学习环境中学习。实验证明，$\mathcal{X}$ -KD在抽象摘要、机器翻译和算术推理任务上均优于基线方法，并实现了更好的性能-多样性权衡和数据效率。|Yuyu Yuan Team|[2602.12674](http://arxiv.org/abs/2602.12674)|null|
|**2026-02-13**|**PMG: Parameterized Motion Generator for Human-like Locomotion Control**|为解决人形机器人运动中，现有全身体参考引导方法对高级命令接口适应性差、对数据和校准敏感等实际挑战，本文提出了Parameterized Motion Generator (PMG)。PMG是一种基于人类运动结构分析的实时运动生成器，通过紧凑的参数化运动数据和高维控制命令合成参考轨迹，并结合模仿学习流水线和仿真到现实电机参数识别模块。实验证明，该集成系统能生成自然、类人运动，精确响应高维控制输入（如VR远程操作），并实现高效、可验证的仿真到现实迁移。|Houde Liu Team|[2602.12656](http://arxiv.org/abs/2602.12656)|null|
|**2026-02-13**|**Real-to-Sim for Highly Cluttered Environments via Physics-Consistent Inter-Object Reasoning**|为解决从单视图观测重建物理有效3D场景时，现有方法常忽略物理约束导致无效状态，进而影响下游模拟可靠性的问题，本文提出了一种新颖的物理约束Real-to-Sim管道。该管道能够从单视图RGB-D数据重建物理一致的3D场景，其核心是一个可微分优化管道，通过接触图建模空间依赖，并利用可微分刚体模拟联合优化物体姿态和物理属性。实验结果表明，重建场景具有高物理保真度，能忠实复现真实世界接触动力学，从而实现稳定可靠的接触密集型操作。|Jun Ma Team|[2602.12633](http://arxiv.org/abs/2602.12633)|**[link](https://physics-constrained-real2sim.github.io)**|
|**2026-02-13**|**FlowHOI: Flow-based Semantics-Grounded Generation of Hand-Object Interactions for Dexterous Robot Manipulation**|近期视觉-语言-动作（VLA）模型能够生成看似合理的末端执行器运动，但在长程、富接触的任务中常常失败，因为缺乏对手-物体交互（HOI）结构的显式表示。为解决此问题，本文提出了FlowHOI，一个两阶段流匹配框架，它能根据第一人称观测、语言指令和3D高斯飞溅（3DGS）场景重建，生成语义接地、时间连贯的HOI序列，包括手部姿态、物体姿态和手-物体接触状态。该框架将以几何为中心的抓取与以语义为中心的操作解耦，后者通过紧凑的3D场景令牌进行条件化，并采用运动-文本对齐损失来语义化生成的交互。为解决高保真HOI监督数据稀缺的问题，本文引入了一个重建流水线，从大规模第一人称视频中恢复对齐的手-物体轨迹和网格，为鲁棒生成提供了HOI先验。FlowHOI在GRAB和HOT3D基准上实现了最高的动作识别精度和比最强扩散基线高1.7倍的物理模拟成功率，同时推理速度提升了40倍。此外，通过将生成的HOI表示重定向到真实机器人执行流程，本文在四个灵巧操作任务上验证了真实机器人执行的可行性。|Xingxing Zuo Team|[2602.13444](http://arxiv.org/abs/2602.13444)|**[link](https://huajian-zeng.github.io/projects/flowhoi/)**|
|**2026-02-12**|**FAIL: Flow Matching Adversarial Imitation Learning for Image Generation**|鉴于流匹配模型后训练与模仿学习的数学等同性，以及监督微调无法纠正策略漂移而偏好优化成本高昂的问题，本文提出了Flow Matching Adversarial Imitation Learning (FAIL) 框架。该框架通过对抗训练最小化策略与专家之间的散度，无需明确奖励或成对比较，并推导出了FAIL-PD和FAIL-PG两种算法。实验证明，FAIL在仅使用少量演示数据的情况下，能在提示遵循和美学基准上取得竞争性性能，并能有效泛化至离散图像和视频生成，同时作为鲁棒正则化器减轻奖励欺骗。|Weidi Xie Team|[2602.12155](http://arxiv.org/abs/2602.12155)|null|
|**2026-02-12**|**GigaBrain-0.5M*: a VLA That Learns From World Model-Based Reinforcement Learning**|针对传统VLA模型在场景理解和未来预测上的局限性，本研究提出了GigaBrain-0.5M*，一个基于世界模型强化学习的VLA模型。该模型在预训练的GigaBrain-0.5基础上，通过RAMP（Reinforcement leArning via world Model-conditioned Policy）进一步整合了世界模型强化学习，以实现鲁棒的跨任务适应性。实验结果表明，RAMP在洗衣折叠、箱子包装和意式浓缩咖啡制作等复杂任务中，相较于RECAP基线性能提升了约30%，并且在实际部署中展示了可靠的长期执行能力，能够无故障完成复杂操作任务。|Zheng Zhu Team|[2602.12099](http://arxiv.org/abs/2602.12099)|**[link](https://gigabrain05m.github.io/)**|
|**2026-02-12**|**Affordance-Graphed Task Worlds: Self-Evolving Task Generation for Scalable Embodied Learning**|鉴于在真实世界中训练机器人策略成本高昂且难以扩展，而现有生成模拟方法难以生成逻辑连贯的长时任务并处理动态物理不确定性，本研究提出了Affordance-Graphed Task Worlds (AGT-World) 框架。该框架能够根据真实世界观测自主构建交互式模拟环境和相应的机器人任务策略，通过将任务空间形式化为结构化图实现复杂目标的精确分层分解，并引入结合视觉-语言模型推理和几何验证的混合反馈自进化机制来完善策略。广泛实验证明，AGT-World在成功率和泛化能力上显著优于现有方法，实现了提议、执行和修正的自改进循环，以支持可扩展的机器人学习。|Changshui Zhang Team|[2602.12065](http://arxiv.org/abs/2602.12065)|null|
|**2026-02-12**|**HoloBrain-0 Technical Report**|为了弥合基础模型研究与可靠的真实世界机器人部署之间的差距，本研究引入了HoloBrain-0，一个全面的视觉-语言-动作 (VLA) 框架。该框架的核心是一个新颖的VLA架构，明确融入了机器人本体先验信息（如多视图相机参数和运动学描述），以增强3D空间推理并支持多样化的本体。通过“预训练-后训练”范式进行验证，该系统在RoboTwin 2.0、LIBERO和GenieSim等模拟基准测试中取得了最先进的成果，并在长时程真实世界操作任务中表现出色。值得注意的是，其高效的0.2B参数变体能与大得多的基线媲美，并支持低延迟的设备部署。为加速研究和实际应用，HoloBrain生态系统已完全开源。|Zhizhong Su Team|[2602.12062](http://arxiv.org/abs/2602.12062)|null|
|**2026-02-12**|**When would Vision-Proprioception Policies Fail in Robotic Manipulation?**|针对视觉-本体感受策略在复杂任务中泛化能力不一致的问题，本研究发现，在机器人运动转换的子阶段，视觉模态的作用有限，策略倾向于更简洁的本体感受信号，抑制了视觉学习。为此，我们提出了梯度调整与阶段引导 (GAP) 算法，通过利用本体感受估计运动转换阶段的概率，并据此自适应地调节本体感受梯度的幅度，从而实现视觉与本体感受的动态协作。综合实验表明，GAP算法在模拟和真实世界环境、单臂和双臂设置以及不同模型类型中均适用，并能形成鲁棒且可泛化的视觉-本体感受策略。|Di Hu Team|[2602.12032](http://arxiv.org/abs/2602.12032)|null|
|**2026-02-12**|**Accelerating Robotic Reinforcement Learning with Agent Guidance**|强化学习在机器人操作中面临严重的样本效率问题，而现有人机协作 (HIL) 方法虽能加速训练，但受限于可扩展性、操作员疲劳和不一致的人类专业知识。为解决此问题，本研究提出了Agent-guided Policy Search (AGPS) 框架，通过多模态智能体取代人工监督者，实现训练流程自动化。其核心思想是将智能体视为语义世界模型，注入内在价值先验来结构化物理探索，并利用可执行工具通过纠正性路点和空间约束提供精确指导。实验证明，AGPS在样本效率方面优于HIL方法，从而实现了无劳动力的可扩展机器人学习路径。|Yaodong Yang Team|[2602.11978](http://arxiv.org/abs/2602.11978)|null|
|**2026-02-12**|**Robot-DIFT: Distilling Diffusion Features for Geometrically Consistent Visuomotor Control**|本研究认为机器人操作泛化性的瓶颈在于当前视觉骨干网络与闭环控制物理需求之间的结构性不匹配，尤其在于现有模型缺乏精细的几何敏感性。鉴于生成扩散模型内在地编码了几何依赖性，但其随机性和延迟阻碍了直接应用，我们提出了Robot-DIFT框架。该框架通过流形蒸馏 (Manifold Distillation) 将冻结的扩散教师模型蒸馏到确定性空间-语义特征金字塔网络 (S2-FPN) 中，从而在保持生成模型丰富几何先验的同时，确保了时间稳定性、实时执行和抗漂移鲁棒性。在DROID数据集上的预训练结果表明，Robot-DIFT在几何一致性和控制性能上均优于领先的判别式基线，验证了视觉学习方式对机器人行为能力的关键影响。|Georgia Chalvatzaki Team|[2602.11934](http://arxiv.org/abs/2602.11934)|null|
|**2026-02-12**|**JEPA-VLA: Video Predictive Embedding is Needed for VLA Models**|现有VLA模型在机器人操作中仍面临样本效率低和泛化能力有限的问题，本研究认为这与预训练视觉表示在环境理解和策略先验方面知识不足有关。通过深入分析，我们发现现有VLA中常用的视觉表示未能有效捕获关键任务相关信息及诱导有效策略先验，而通过视频预训练的预测嵌入，特别是V-JEPA 2，能够灵活地处理不可预测因素并编码任务相关的时间动态。基于此，我们提出了JEPA-VLA，一种将预测嵌入自适应集成到现有VLA的简单有效方法。实验证明，JEPA-VLA在LIBERO、LIBERO-plus、RoboTwin2.0和真实机器人任务等基准测试中均取得了显著的性能提升。|Mingsheng Long Team|[2602.11832](http://arxiv.org/abs/2602.11832)|null|
|**2026-02-12**|**Clutt3R-Seg: Sparse-view 3D Instance Segmentation for Language-grounded Grasping in Cluttered Scenes**|针对杂乱环境中3D实例分割在遮挡、有限视角和噪声掩码下的挑战，本研究提出了Clutt3R-Seg，一种用于语言引导抓取的零样本鲁棒3D实例分割流水线。该方法的核心思想是引入语义线索的分层实例树，通过跨视图分组和条件替换，将噪声掩码作为信息线索，从而抑制过分割和欠分割，产生视图一致的掩码和鲁棒的3D实例。为应对多阶段任务中的场景变化，该方法还引入了一致性感知更新机制。在合成和真实世界数据集及真实机器人上的验证表明，Clutt3R-Seg在杂乱和稀疏视图场景中持续优于现有最先进基线，尤其在重度杂乱序列中表现出超过2.2倍的性能提升。|Ayoung Kim Team|[2602.11660](http://arxiv.org/abs/2602.11660)|null|
|**2026-02-12**|**ViTaS: Visual Tactile Soft Fusion Contrastive Learning for Visuomotor Learning**|针对现有机器人操作中视觉与触觉信息融合方法在遮挡场景下效果不佳、未能充分利用两种模态互补性且集成机制多为直接拼接的问题，本研究提出了ViTaS框架。该框架旨在结合视觉和触觉信息指导智能体行为，并引入了软融合对比学习（Soft Fusion Contrastive Learning）以及一个CVAE模块，以更好地利用视觉-触觉表示中的对齐和互补性。在12个模拟环境和3个真实世界环境中的实验验证表明，ViTaS显著优于现有基线，证明了其在利用多模态信息方面的有效性。|Huazhe Xu Team|[2602.11643](http://arxiv.org/abs/2602.11643)|null|
|**2026-02-12**|**EasyMimic: A Low-Cost Framework for Robot Imitation Learning from Human Videos**|鉴于大规模真实世界数据采集成本高昂阻碍了机器人模仿学习，尤其对低成本家用机器人而言，本研究提出了EasyMimic框架。该框架是一个低成本、可复制的解决方案，使机器人能通过标准RGB相机捕获的人类视频演示快速学习操作策略。其方法首先从视频中提取3D手部轨迹，并利用动作对齐模块将其映射到低成本机器人的夹持器控制空间；为弥合人机领域差距，引入了简单的手部视觉增强策略，并通过协同训练方法在处理过的人类数据和少量机器人数据上微调模型。实验证明，EasyMimic在LeRobot平台上在多种操作任务中取得了高性能，显著减少了对昂贵机器人数据采集的依赖，为智能机器人进入家庭提供了实用途径。|Qin Jin Team|[2602.11464](http://arxiv.org/abs/2602.11464)|null|
|**2026-02-12**|**Scaling World Model for Hierarchical Manipulation Policies**|视觉-语言-动作（VLA）模型在通用机器人操作中虽有前景，但在有限真实机器人数据下，其在分布外（OOD）场景中的泛化能力仍脆弱。本研究提出VISTA，一个分层视觉-语言-动作框架，利用大规模预训练世界模型的泛化能力实现鲁棒且可泛化的视觉子目标任务分解。该框架将世界模型作为高层规划器来生成带有目标图像的子任务序列，VLA作为低层执行器遵循指导生成动作。实验结果表明，与原始文本目标相比，合成的目标图像提供了更具视觉和物理细节的指导，使低层策略能泛化到新物体和场景，将VLA在OOD场景中的性能从14%提升至69%。|Xinghang Li Team|[2602.10983](http://arxiv.org/abs/2602.10983)|null|
|**2026-02-12**|**Self-Refining Vision Language Model for Robotic Failure Detection and Reasoning**|针对机器人故障推理中，真实世界故障的复杂性及丰富推理标签获取成本高昂的问题，本文提出了ARMOR框架。该框架将故障检测和推理建模为一个多任务自细化过程，模型通过迭代预测检测结果和自然语言推理，并从大规模稀疏二元标签和少量丰富推理标注的异构监督中学习。实验结果表明，ARMOR在故障检测率上比现有方法提升高达30%，在LLM模糊匹配分数测量的推理能力上提升高达100%，展现了对异构监督和开放式推理的鲁棒性。|Yesh Dattatreya Team|[2602.12405](http://arxiv.org/abs/2602.12405)|null|
|**2026-02-11**|**Human Preference Modeling Using Visual Motion Prediction Improves Robot Skill Learning from Egocentric Human Video**|当前从第一视角人类视频中学习机器人的方法，在度量视觉状态长期价值时存在假设限制，且在跨实体和环境时需要迁移学习到的价值函数。本研究提出一种新方法，通过预测后续图像中跟踪点的运动来建模人类偏好，并将奖励函数定义为机器人行为中预测与观察到的物体运动的一致性。随后，利用改进的Soft Actor Critic (SAC) 算法（通过10次机器人演示初始化）来估计此奖励的价值函数，并在机器人上优化策略。结果显示，该方法在真实机器人上有效，并且在模拟和真实机器人上的多项任务中，其学习到的策略与现有工作相当或超越。|Christopher G. Atkeson Team|[2602.11393](http://arxiv.org/abs/2602.11393)|null|
|**2026-02-11**|**MolmoSpaces: A Large-Scale Open Ecosystem for Robot Navigation and Manipulation**|机器人大规模部署要求对日常情境具备鲁棒性，但现有基准测试缺乏足够多样化的场景。本研究引入了MolmoSpaces，一个支持机器人策略大规模基准测试的开放生态系统，包含超过23万个多样化室内环境和13万个丰富注释的物体资产，且与模拟器无关。同时设计了MolmoSpaces-Bench基准套件。实验证明，MolmoSpaces-Bench具有强大的模拟到真实关联性，验证了新型零样本策略的优越性，并揭示了对提示措辞、初始关节位置和相机遮挡的关键敏感性，为机器人学习研究提供了可扩展的基础。|Ranjay Krishna Team|[2602.11337](http://arxiv.org/abs/2602.11337)|null|
|**2026-02-11**|**YOR: Your Own Mobile Manipulator for Generalizable Robotics**|随着机器人学习的发展，低成本机器人平台日益增多，但移动操作的最佳外形仍然是待解问题。本研究推出YOR，一个开源、低成本的移动操纵器，集成了全向基座、伸缩式垂直升降器和带抓手的双臂，以实现全身移动和操作。其设计强调模块化、易于组装和经济性（物料清单成本低于1万美元）。YOR在需要协调全身控制、双臂操作和自主导航的任务中展现出其能力，以远低于现有平台的成本提供了具有竞争力的移动操作研究功能。|Zichen Jeff Cui Team|[2602.11150](http://arxiv.org/abs/2602.11150)|null|
|**2026-02-11**|**ABot-M0: VLA Foundation Model for Robotic Manipulation with Action Manifold Learning**|在异构硬件上构建通用具身智能体是机器人学的一大挑战，面临数据碎片化、表示不一致和训练目标不匹配等问题。本研究提出了ABot-M0框架，它通过建立系统的数据整理管道，同时优化模型架构和训练策略，将异构原始数据转换为统一、高效的表示。通过构建大规模UniACT数据集，统一预训练提高了跨平台和任务的知识迁移和泛化能力。为提升动作预测效率和稳定性，引入了动作流形学习（AML）。此外，框架通过双流机制实现模块化感知，整合VLM语义与几何先验和多视角输入。实验结果表明各组件独立且具有累加效益。|Mu Xu Team|[2602.11236](http://arxiv.org/abs/2602.11236)|**[link](https://amap-cvlab.github.io/ABot-Manipulation/)**|
|**2026-02-11**|**OSIL: Learning Offline Safe Imitation Policies with Safety Inferred from Non-preferred Trajectories**|离线安全模仿学习面临在缺乏每时间步安全成本或奖励信息的演示数据中学习安全和奖励最大化策略的难题。本研究针对此问题提出了一种新颖的离线安全模仿学习算法OSIL，它通过非首选轨迹推断安全性。OSIL将安全策略学习表述为约束马尔可夫决策过程（CMDP），并通过推导奖励最大化目标的下界并学习一个估计非首选行为可能性的成本模型来重新构建CMDP问题，从而无需明确的安全成本和奖励标注。实验证明，OSIL能学习到更安全的策略，满足成本约束而不降低奖励性能，优于现有基线方法。|Balaraman Ravindran Team|[2602.11018](http://arxiv.org/abs/2602.11018)|null|
|**2026-02-11**|**Towards Learning a Generalizable 3D Scene Representation from 2D Observations**|现有方法在相机坐标系中构建表示，限制了其在机器人操作中的直接应用。本研究引入了一种可泛化的神经辐射场方法，用于根据以机器人自我为中心的观察来预测3D工作空间占用。该模型在全局工作空间框架中构建占用表示，使其直接适用于机器人操作，并能集成灵活的源视图，无需场景特定微调即可泛化到未见物体布局。在人形机器人上的实验证明，该模型在40个真实场景上训练后，实现了26毫米的重建误差（包括遮挡区域），验证了其推断完整3D占用（超越传统立体视觉方法）的能力。|Stefan Wermter Team|[2602.10943](http://arxiv.org/abs/2602.10943)|null|
|**2026-02-11**|**Semi-Supervised Cross-Domain Imitation Learning**|跨域模仿学习（CDIL）通过领域知识迁移加速策略学习，但在专家数据收集成本高昂时尤为重要。现有CDIL方法或依赖监督（不稳定），或无监督（不稳健）。本研究提出了半监督CDIL（SS-CDIL）设置及其首个具有理论依据的算法。该方法仅使用少量目标专家演示和未标记的不完善轨迹等离线数据。为解决域差异，提出一种新颖的跨域损失函数来学习域间状态-动作映射，并设计自适应权重函数以平衡源域和目标域知识。实验表明，该方法在MuJoCo和Robosuite上始终优于基线，实现了最小监督下稳定且数据高效的策略学习。|Ping-Chun Hsieh Team|[2602.10793](http://arxiv.org/abs/2602.10793)|null|
|**2026-02-11**|**Say, Dream, and Act: Learning Video World Models for Instruction-Driven Robot Manipulation**|机器人操作通常缺乏预测环境响应动作的能力，导致错误和低效，且现有视觉-语言模型（VLM）和世界模型在预测未来状态或生成空间一致帧方面存在局限。本研究提出一种用于快速且可预测的视频条件动作框架。该方法首先选择并适应一个鲁棒的视频生成模型以确保可靠的未来预测，然后应用对抗蒸馏进行快速、少步骤的视频生成，最后训练一个动作模型，利用生成的视频和真实观察来纠正空间误差。大量实验表明，该方法生成的时间连贯、空间准确的视频预测直接支持精确操作，在具身一致性、空间指代能力和任务完成度方面显著优于现有基线。|Yanwei Fu Team|[2602.10717](http://arxiv.org/abs/2602.10717)|null|

<p align=right>(<a href=#updated-on-20260220>back to top</a>)</p>

## World Model

| Publish Date | Title | Chinese Summary | Authors | PDF | Code |
|:---------|:-----------------------|:------------------------|:---------|:------|:------|
|**2026-02-19**|**Reverso: Efficient Time Series Foundation Models for Zero-shot Forecasting**|在时间序列预测领域，现有的大规模基础模型虽然性能出色，但因参数量巨大（上亿）而导致实际应用中的低效与高成本。本文提出了一种学习高效时间序列基础模型的简洁方案，表明无需大型Transformer架构，小型混合模型（结合长卷积和线性RNN层，特别是DeltaNet层）即可在保持性能的同时，将模型尺寸缩小上百倍。该方案结合了数据增强和推理策略，最终形成Reverso系列模型，显著推动了零样本时间序列预测的性能与效率前沿。|Yoon Kim Team|[2602.17634](http://arxiv.org/abs/2602.17634)|null|
|**2026-02-19**|**AI Gamestore: Scalable, Open-Ended Evaluation of Machine General Intelligence with Human Games**|评估机器通用智能，尤其是在快速发展的技术时代，变得越来越重要和具有挑战性。传统的AI基准测试范围狭窄且易饱和。本文提出通过研究AI系统如何玩“所有可能的人类游戏”（即“人类游戏多重宇宙”）来评估其类人通用智能。为此，论文引入了AI GameStore平台，该平台利用大语言模型和人类循环，从流行的数字游戏平台自动合成和改编新的代表性人类游戏。初步概念验证中，在基于Apple App Store和Steam生成了100款游戏并测试了七个视觉-语言模型后，结果显示最佳模型在大多数游戏中的表现不到人类平均水平的10%，尤其在需要世界模型学习、记忆和规划的游戏中表现不佳。|Joshua B. Tenenbaum Team|[2602.17594](http://arxiv.org/abs/2602.17594)|null|
|**2026-02-19**|**Systematic Evaluation of Single-Cell Foundation Model Interpretability Reveals Attention Captures Co-Expression Rather Than Unique Regulatory Signal**|为评估单细胞基础模型的机制可解释性，本文提出了一个包含37项分析、153项统计测试、四种细胞类型和两种扰动模式的系统评估框架，并将其应用于scGPT和Geneformer模型。研究发现，注意力模式确实编码了分层组织的生物信息（早期层是蛋白质-蛋白质相互作用，晚期层是转录调控）。然而，这种结构对扰动预测没有增量价值：简单的基因水平基线模型在性能上优于注意力和相关性边缘（AUROC 0.81-0.88 对比 0.70），成对边缘分数没有预测贡献，且调控头部的因果消融并未导致性能下降。这些发现具有普遍性，并且细胞状态分层可解释性（CSSI）通过解决注意力特定的扩展失效问题，将基因调控网络恢复能力提高了1.85倍。|Ihor Kendiukhov Team|[2602.17532](http://arxiv.org/abs/2602.17532)|null|
|**2026-02-19**|**Dataless Weight Disentanglement in Task Arithmetic via Kronecker-Factored Approximate Curvature**|任务算术（Task Arithmetic）提供了一种模块化、可扩展的基础模型适应方法，但结合多个任务向量可能导致跨任务干扰，进而引发表征漂移并降低性能。现有的表征漂移正则化方法通常需要外部任务数据，这与模块化和数据可用性（如隐私）的要求相冲突。本文提出了一种无数据方法，将表征漂移的正则化问题构造成曲率矩阵近似问题。通过利用Kronecker因子近似曲率（K-FAC）等成熟技术，本文获得了一个实用的正则化器。实验结果表明，该方法在任务添加和任务否定方面均取得了最先进的性能，其复杂度与任务数量无关，并提升了对任务向量重缩放的鲁棒性，从而无需进行额外的调优。|Simone Calderara Team|[2602.17385](http://arxiv.org/abs/2602.17385)|null|
|**2026-02-19**|**Computer-Using World Model**|在复杂软件环境中操作的智能体需要推理其行为的后果，因为即使是单一错误的UI操作也可能破坏漫长的工作流程。在真实执行不支持反事实探索的计算机使用场景中，大规模试错学习和规划变得不切实际。本文引入了计算机使用世界模型（CUWM），该模型通过两阶段分解UI动态：首先预测与智能体相关的状态变化的文本描述，然后视觉化地实现这些变化以合成下一个屏幕截图，从而预测给定当前状态和候选操作的下一个UI状态。CUWM在与Microsoft Office应用交互的离线UI转换数据上进行训练，并通过轻量级强化学习阶段进行优化。通过测试时动作搜索评估，CUWM指导的决策质量和执行鲁棒性在各种Office任务中均得到提升。|Dongmei Zhang Team|[2602.17365](http://arxiv.org/abs/2602.17365)|null|
|**2026-02-19**|**FRAPPE: Infusing World Modeling into Generalist Policies via Multiple Future Representation Alignment**|使VLA模型预测环境动态（即世界建模）对于提升机器人推理和泛化能力至关重要，但现有方法面临像素级重建过度强调导致语义学习受限和推理时依赖预测未来观测导致误差累积的问题。本文提出了通过并行渐进扩展实现未来表征对齐（FRAPPE）的方法。该方法采用两阶段微调策略：在中期训练阶段，模型学习预测未来观测的潜在表征；在后期训练阶段，并行扩展计算工作量并同时与多个不同的视觉基础模型对齐表征。FRAPPE显著提高了微调效率并减少了对动作标注数据的依赖。在RoboTwin基准测试和真实世界任务上的实验表明，FRAPPE超越了最先进的方法，并在长时序和未见过场景中展现出强大的泛化能力。|Donglin Wang Team|[2602.17259](http://arxiv.org/abs/2602.17259)|**[link](https://h-zhao1997.github.io/frappe)**|
|**2026-02-19**|**Structured Prototype-Guided Adaptation for EEG Foundation Models**|脑电图（EEG）基础模型（EFMs）在完全微调下表现出色，但在受试者级监督有限时（临床常见情况）泛化能力差，这源于嘈杂、有限的监督与EFM高度可塑的参数空间之间的结构性不匹配。本文提出SCOPE框架，一个结构化置信度感知原型引导的EFM微调框架，分两阶段进行。第一阶段通过学习几何正则化任务先验、构建平衡的类级别原型，并根据它们的一致性生成置信度感知的伪标签来过滤无标签数据中的不可靠信号，从而构建可靠的外部监督。第二阶段引入ProAdapter，通过一个基于结构化原型的轻量级适配器来适应冻结的EEG基础模型。实验表明，SCOPE在三种EEG任务和五种基础模型骨干下，在标签受限的跨受试者设置中始终实现强大的性能和效率。|Mengling Feng Team|[2602.17251](http://arxiv.org/abs/2602.17251)|null|
|**2026-02-19**|**Decoding the Human Factor: High Fidelity Behavioral Prediction for Strategic Foresight**|在关键环境中预测人类决策是人工智能的核心挑战。尽管大型语言模型（LLMs）展现出强大的通用推理能力，但它们在生成一致的、个体特异性行为方面常常力不从心，尤其当准确预测依赖于心理特征和情境约束之间的复杂交互时。基于提示的方法在这种设定下表现脆弱，存在身份漂移且利用详细个性描述的能力有限。为解决这些局限性，本文引入了大型行为模型（LBM），这是一个经过微调的行为基础模型，能高保真地预测个体战略选择。LBM通过将结构化的、高维度的特质档案（来源于全面的心理测量评估）作为条件，从瞬态的个性提示转向行为嵌入。LBM在一个专有数据集上训练，该数据集将稳定的倾向、动机状态和情境约束与观察到的选择关联起来。在留出的情景评估中，LBM的微调改善了行为预测，超越了未经适应的Llama-3.1-8B-Instruct基线，并在以大五人格特质为条件时与前沿基线模型表现相当。此外，与基于提示的基线方法存在复杂性上限不同，LBM持续受益于日益密集的特质档案，性能随额外特质维度的提供而提升。这些结果确立了LBM作为高保真行为模拟的可扩展方法。|Shula Grinapol Team|[2602.17222](http://arxiv.org/abs/2602.17222)|null|
|**2026-02-19**|**Continual learning and refinement of causal models through dynamic predicate invention**|智能体在复杂环境中高效导航需要内化其世界的底层逻辑，但标准的世界建模方法往往存在样本效率低下、缺乏透明度以及可扩展性差的问题，特别是在命题方法因组合爆炸而失效的复杂关系动态领域。本文提出了一个框架，通过将连续模型学习和修复整合到智能体的决策循环中，利用元解释学习和谓词发明来发现具有语义意义和可重用性的抽象，从而完全在线地构建符号因果世界模型。该方法使智能体能够从其观察中构建分层、解耦、高质量的概念。实验证明，我们提升推理方法能够扩展到具有复杂关系动态的领域，同时比基于PPO神经网络的基线方法实现高出数个数量级的样本效率。|Peter Flach Team|[2602.17217](http://arxiv.org/abs/2602.17217)|null|
|**2026-02-19**|**JEPA-DNA: Grounding Genomic Foundation Models through Joint-Embedding Predictive Architectures**|基因组基础模型（GFMs）主要依赖掩码语言建模（MLM）或下一词元预测（NTP）来学习生命语言。这些范式擅长捕捉局部基因组语法和精细的基序模式，但往往未能捕捉更广泛的功能背景，导致表征缺乏全局生物学视角。本文提出了JEPA-DNA，一种新颖的预训练框架，将联合嵌入预测架构（JEPA）与传统生成目标相结合。JEPA-DNA通过在潜在空间中将词元级恢复与预测目标耦合（监督CLS词元），引入了潜在接地，这促使模型预测掩码基因组片段的高级功能嵌入，而非仅仅关注单个核苷酸。JEPA-DNA扩展了NTP和MLM范式，既可作为独立的从头训练目标，也可作为现有GFM的持续预训练增强。在多样化的基因组基准测试中，JEPA-DNA在有监督和零样本任务中均表现出优于仅生成式基线的性能，提供了更鲁棒和生物学上更接地气的表征。|Yoli Shavit Team|[2602.17162](http://arxiv.org/abs/2602.17162)|null|
|**2026-02-19**|**AudioChat: Unified Audio Storytelling, Editing, and Understanding with Transfusion Forcing**|针对现有音频基础模型在处理复杂多源声学场景（“音频故事”）时面临的语义、时间、物理复杂性挑战，本文提出了AudioChat框架。该框架利用大型语言模型（LLM）驱动的工具调用代理模拟用户交互生成训练数据，并引入“Audio Transfusion Forcing”目标，使模型能通过结构化思维链推理实现高级指令分解和交互式多轮音频理解/生成。为评估模型性能，研究者开发了三项新的任务导向型评估指标。|Zeyu Jin Team|[2602.17097](http://arxiv.org/abs/2602.17097)|null|
|**2026-02-18**|**Parameter-free representations outperform single-cell foundation models on downstream benchmarks**|针对单细胞RNA测序（scRNA-seq）数据中基因表达基础模型（如TranscriptFormer）依赖计算密集型深度学习方法以取得最先进性能的现状，本文探讨了是否能通过更简洁的线性方法达到类似效果。研究采用一套基于精细归一化和线性方法的简单、可解释的流水线进行数据处理。实验结果表明，在多个单细胞基础模型常用基准测试中，该方法取得了与最先进模型相当或更优的性能，甚至在涉及训练数据中未出现的新细胞类型和生物体的域外任务中超越了基础模型。这强调了严格基准测试的重要性，并提示细胞身份的生物学特征或可通过单细胞基因表达数据的简单线性表示来有效捕捉。|Pankaj Mehta Team|[2602.16696](http://arxiv.org/abs/2602.16696)|null|
|**2026-02-18**|**Are Object-Centric Representations Better At Compositional Generalization?**|针对机器缺乏人类组合泛化能力的问题，以及物体中心（OC）表示在此方面有效性缺乏系统性验证的现状，本文提出了一个跨CLEVRTex、Super-CLEVR和MOVi-C三个视觉世界的视觉问答（VQA）基准。研究通过控制训练数据多样性、样本量等变量，全面比较了DINOv2和SigLIP2及其OC对应版本在未见物体属性组合上的泛化性能。实验结果显示，OC方法在更具挑战性的组合泛化任务中表现更优；原始密集表示仅在较简单任务中占优且需要更多计算资源；OC模型展现出更高的样本效率。总体而言，当数据集规模、数据多样性或计算资源受限时，物体中心表示在组合泛化方面具有显著优势。|Andrea Dittadi Team|[2602.16689](http://arxiv.org/abs/2602.16689)|null|
|**2026-02-18**|**Scaling Open Discrete Audio Foundation Models with Interleaved Semantic, Acoustic, and Text Tokens**|当前音频语言模型多以文本为先，限制了通用音频建模。本文提出并系统研究了原生音频基础模型，通过大规模的下一令牌预测，联合建模语义内容、声学细节和文本，以支持通用音频生成和跨模态能力。研究者系统探索了数据源、文本混合比例等设计选择，并首次对离散音频模型进行了缩放定律研究。基于这些发现，他们训练了SODA模型套件，并在保留说话者声音的语音-语音翻译等任务中展示了其作为灵活骨干网络的有效性。|Diyi Yang Team|[2602.16687](http://arxiv.org/abs/2602.16687)|null|
|**2026-02-18**|**Retrieval-Augmented Foundation Models for Matched Molecular Pair Transformations to Recapitulate Medicinal Chemistry Intuition**|针对药物化学中类似物设计缺乏高效、可控的分子编辑方法，现有机器学习方法存在局限性的问题，本文提出了一种基于大规模匹配分子对转换（MMPTs）的变异到变异（variable-to-variable）类似物生成基础模型。该模型通过提示机制实现用户对转换模式的精确控制，并引入MMPT-RAG框架利用外部参考类似物进行上下文指导。实验证明，该方法在通用化学语料库和专利数据集上显著提升了生成多样性、新颖性和可控性，能在实际发现场景中恢复出真实的类似物结构。|Liang Zhao Team|[2602.16684](http://arxiv.org/abs/2602.16684)|null|
|**2026-02-18**|**Learning Situated Awareness in the Real World**|多模态基础模型（MFMs）现有基准多侧重环境中心空间关系，忽视了以观察者为中心的、需推理智能体视角和动作的情境感知。为填补此空白，研究者引入了SAW-Bench，一个基于Ray-Ban Meta智能眼镜录制真实世界视频的新型基准，用于评估自我中心情境感知。该基准包含786个视频和2071个问答对，通过六项感知任务探测模型的观察者中心理解。评估显示，即使是最佳MFM，与人类仍有显著差距，模型常未能推断出连贯的相机几何导致空间推理错误，突显了超越被动观察、理解以观察者为中心的物理动态的重要性。|Xin Eric Wang Team|[2602.16682](http://arxiv.org/abs/2602.16682)|null|
|**2026-02-18**|**VETime: Vision Enhanced Zero-Shot Time Series Anomaly Detection**|时间序列异常检测（TSAD）需要识别点异常和上下文异常，但现有基础模型在精细点定位和全局上下文理解之间存在权衡。为解决这一困境，本文提出了VETime，首个通过细粒度视觉-时间对齐和动态融合统一时间与视觉模态的TSAD框架。VETime引入可逆图像转换和补丁级时间对齐模块，以建立共享的视觉-时间轴并保留细节。此外，它设计了异常窗口对比学习和任务自适应多模态融合机制。大量实验表明，VETime在零样本场景中显著优于先进模型，实现了更高的定位精度和更低的计算开销。|Chen Zhang Team|[2602.16681](http://arxiv.org/abs/2602.16681)|null|
|**2026-02-18**|**Learning to unfold cloth: Scaling up world models to deformable object manipulation**|机器人布料操作因其复杂的物理特性而极具挑战性，需要通用策略以适应不同布料特性。本文提出一种改进的DreamerV2强化学习架构应用于空中布料操作，通过引入表面法线作为输入，并优化回放缓冲区及数据增强程序，增强了机器人使用的世界模型来应对物理复杂性。在仿真和物理机器人的零样本部署实验中，该方法成功实现了多种布料的空中展开，证明了所提出架构在泛化性能上的显著优势。|Subramanian Ramamoorthy Team|[2602.16675](http://arxiv.org/abs/2602.16675)|null|
|**2026-02-18**|**A Systematic Evaluation of Sample-Level Tokenization Strategies for MEG Foundation Models**|神经影像数据的基础模型需将连续神经时间序列数据“令牌化”，但不同令牌化策略的影响尚不明确。本文对应用于脑磁图（MEG）数据的基于Transformer的大型神经影像模型（LNMs）的样本级令牌化策略进行了系统评估。通过比较可学习（引入基于自编码器的新方法）和不可学习令牌器在信号重建保真度、基础建模性能及下游任务上的表现，研究发现在多个MEG数据集上，两者均实现了高重建精度和大致相当的性能，表明简单的固定样本级令牌化策略足以支持神经基础模型的开发。|Mark W. Woolrich Team|[2602.16626](http://arxiv.org/abs/2602.16626)|null|
|**2026-02-18**|**Why Thinking Hurts? Diagnosing and Rectifying the Reasoning Shift in Foundation Recommender Models**|将思维链（CoT）推理整合到语义ID推荐基础模型中，常导致推荐性能下降，究其原因在于“通用子空间”中冗长推理的文本惯性，使得模型忽视关键语义ID。为解决此问题，本文提出一个训练无关的“推理时子空间对齐”框架。该方法通过压缩推理链和应用偏差减去的对比解码，有效缓解了无根据的文本漂移。实验证明，此框架能有效校准推理过程，使基础模型在利用推理能力的同时，不牺牲基于ID的推荐准确性。|Enhong Chen Team|[2602.16587](http://arxiv.org/abs/2602.16587)|null|
|**2026-02-18**|**Arc2Morph: Identity-Preserving Facial Morphing with Arc2Face**|人脸融合攻击对电子身份文档中的人脸识别系统构成严峻威胁，尤其是在护照注册过程中缺乏实时监督采集时。本文提出一种基于身份条件化人脸基础模型Arc2Face的新型人脸融合技术，能够从紧凑的身份表示合成逼真的人脸图像。通过在多个大规模数据集上与现有先进方法进行比较，实验结果表明，所提出的深度学习方法在融合攻击潜力方面达到了与传统上最具挑战性的基于地标技术相当的水平，证实了其在融合生成过程中有效保留和管理身份信息的能力。|Davide Maltoni Team|[2602.16569](http://arxiv.org/abs/2602.16569)|null|
|**2026-02-18**|**MMA: Multimodal Memory Agent**|长时程多模态智能体在依赖外部记忆时，常因检索到过时、低可信或冲突信息而产生过度自信的错误。针对此问题，本文提出了多模态记忆智能体（MMA），该智能体结合来源可信度、时间衰减和冲突感知网络共识，动态评估每个记忆项的可靠性，并利用此信号加权证据或在支持不足时弃权。同时，引入了MMA-Bench基准来研究信念动态。实验结果表明，在FEVER数据集上，MMA在保持基线准确率的同时，将方差降低了35.2%；在LoCoMo上，提高了操作准确性并减少了错误答案；在MMA-Bench上，视觉模式下MMA的准确率显著优于基线，并揭示了RAG智能体中潜在的“视觉安慰剂效应”。|Hao Tang Team|[2602.16493](http://arxiv.org/abs/2602.16493)|null|
|**2026-02-18**|**RoboGene: Boosting VLA Pre-training via Diversity-Driven Agentic Framework for Real-World Task Generation**|通用机器人操作受限于稀缺且成本高昂的真实世界交互数据，且现有任务策划方法不可扩展或易产生不可行指令。为解决这一问题，本文提出了RoboGene框架，旨在自动化生成多样化且物理上可行的单臂、双臂和移动机器人操作任务。RoboGene包含多样性驱动采样、自反思机制以强制物理约束以及人机协作精炼等核心组件。实验结果表明，RoboGene在定量分析和大规模真实世界实验中显著优于现有基础模型，且使用RoboGene预训练的VLA模型展现出更高的成功率和泛化能力，强调了高质量任务生成的重要性。|Jian Tang Team|[2602.16444](http://arxiv.org/abs/2602.16444)|null|
|**2026-02-18**|**Automated Histopathology Report Generation via Pyramidal Feature Extraction and the UNI Foundation Model**|从千兆像素级的组织病理学全玻片图像（WSIs）生成精确的诊断文本面临巨大挑战。本文提出了一个分层视觉语言框架，将一个冻结的病理学基础模型与Transformer解码器相结合进行报告生成。该方法通过多分辨率金字塔补丁选择和背景伪影去除技术处理WSI，并利用UNI Vision Transformer提取特征，随后由Transformer解码器生成文本，并使用BioGPT进行分词。为提高可靠性，该框架还引入了一个基于检索的验证步骤，通过比较生成报告与参考语料库来修正报告内容。|Serkan Sokmen Team|[2602.16422](http://arxiv.org/abs/2602.16422)|null|
|**2026-02-18**|**CADEvolve: Creating Realistic CAD via Program Evolution**|计算机辅助设计（CAD）的AI自动化受限于缺乏复杂操作和设计意图的数据集，导致现有方法难以生成工业级程序。为此，本文提出了CADEvolve，一个基于进化的管道和数据集，它从简单基元开始，通过VLM引导的编辑和验证，逐步生成工业级复杂性的CAD程序。该方法生成了8k个复杂零件作为可执行的CadQuery参数化生成器，并经过处理和增强后，形成了一个包含130万个脚本的统一数据集。实验结果表明，在CADEvolve上微调的VLM在DeepCAD、Fusion 360和MCB基准测试的Image2CAD任务上均达到了最先进的性能。|Dmitrii Zhemchuzhnikov Team|[2602.16317](http://arxiv.org/abs/2602.16317)|null|
|**2026-02-18**|**AFFMAE: Scalable and Efficient Vision Pretraining for Desktop Graphics Cards**|自监督预训练虽提高了计算机视觉效率，但高分辨率训练仍需服务器级基础设施，限制了基础模型的开发。传统MAE与分层架构结合时面临密集网格和掩码感知设计挑战。本文提出了AFFMAE，一个基于自适应、非网格token合并的掩码友好型分层预训练框架，通过丢弃被掩码的token并仅对可见token执行动态合并，消除了密集网格假设并保持了分层可扩展性。实验结果表明，在相同参数量下，AFFMAE在高分辨率电子显微镜分割任务上匹配了ViT-MAE的性能，同时将FLOPs减少高达7倍，内存使用减半，并在单个RTX 5090上实现了更快的训练。|Behzad Najafian Team|[2602.16249](http://arxiv.org/abs/2602.16249)|null|
|**2026-02-18**|**EasyControlEdge: A Foundation-Model Fine-Tuning for Edge Detection**|在实际边缘检测中，清晰度和数据效率至关重要，但用有限数据生成清晰边缘图极具挑战，且图像生成基础模型在边缘检测领域的潜力尚未充分挖掘。本文提出了EasyControlEdge，旨在将图像生成基础模型适应于边缘检测任务，以实现高清晰度和数据高效性。该方法通过引入边缘导向目标和高效像素空间损失来专门化基础模型，并在推理时利用基于无条件动力学的引导，实现通过引导尺度控制边缘密度。实验结果显示，EasyControlEdge在多个基准测试中持续优于现有方法，特别是在无后处理清晰度评估和有限训练数据条件下表现突出。|Tadahiro Taniguchi Team|[2602.16238](http://arxiv.org/abs/2602.16238)|null|
|**2026-02-18**|**Factored Latent Action World Models**|从无动作视频中学习潜在动作是构建可控世界模型的强大范式，但现有方法通常依赖单一动力学模型控制整个场景，难以应对多实体复杂环境。为解决此问题，本文提出了因子化潜在动作模型（FLAM），该框架将场景分解为独立因子，每个因子推断其自身潜在动作并预测下一时刻状态。这种因子化结构能够更准确地建模复杂的多实体动力学。实验结果表明，FLAM在模拟和真实世界多实体数据集上的预测准确性和表示质量均优于现有方法，改善了视频生成质量，并促进了下游策略学习。|Peter Stone Team|[2602.16229](http://arxiv.org/abs/2602.16229)|null|
|**2026-02-18**|**World Model Failure Classification and Anomaly Detection for Autonomous Inspection**|自主检查机器人可降低工业现场监测的成本与风险，但遮挡、视角受限等问题使准确读数充满挑战。本文提出了一个混合框架，结合监督式故障分类与异常检测，将检查任务分类为成功、已知故障或异常情况。该方法以带有压缩视频输入的世界模型为骨干，并通过共形预测阈值确定的两个决策函数在人类观察者之前进行分类。在办公室和工业现场仪表检查上的实验结果显示，该框架在区分成功、故障和OOD情况方面准确率超过90%，且分类发生时间早于人类观察者，展现了其在自主检查中实现鲁棒、预见性故障检测的潜力。|Shayegan Omidshafiei Team|[2602.16182](http://arxiv.org/abs/2602.16182)|null|
|**2026-02-18**|**BrainRVQ: A High-Fidelity EEG Foundation Model via Dual-Domain Residual Quantization and Hierarchical Autoregression**|鉴于脑电图（EEG）信号信噪比低和时频谱非平稳性导致基础模型开发困难，且现有方法未能充分利用神经动力学的分层潜在结构，本文提出了BrainRVQ模型。BrainRVQ通过“双域残差向量量化”（DD-RVQ）分词器将EEG信号解耦为分层离散代码，并采用分层自回归预训练目标，结合重要性引导的课程掩码策略，实现从粗到细的代码重建。在8个多样化下游数据集上的广泛实验表明，BrainRVQ显著超越了现有最先进基线，证明了其学习鲁棒和可泛化神经表征的有效性。|Luca Mainardi Team|[2602.16951](http://arxiv.org/abs/2602.16951)|null|
|**2026-02-18**|**How should AI knowledge be governed? Epistemic authority, structural transparency, and the case for open cognitive graphs**|针对教育AI系统在发挥认知权威时缺乏制度性问责机制的治理挑战，本文将教育AI视为公共教育认知基础设施。研究提出“开放认知图”（OCG）作为技术接口，旨在外部化教学结构，使其认知逻辑可检查和修订。在此基础上，引入了“主干-分支治理模型”来组织认知权威。一个社区治理教育基础模型的案例研究表明，该框架能通过制度化过程有效整合分布式专业知识，为实现教育公平、AI政策和可持续发展提供了结构性治理方案。|Yi Hua Team|[2602.16949](http://arxiv.org/abs/2602.16949)|null|
|**2026-02-18**|**BEMEval-Doc2Schema: Benchmarking Large Language Models for Structured Data Extraction in Building Energy Modeling**|针对基础模型在自动化建筑能耗建模（BEM）中缺乏系统评估基准的挑战，本文提出了BEMEval框架。该框架首个基准BEMEval-Doc2Schema专注于从建筑文档中提取结构化数据，并引入“键值重叠率”（KVOR）指标来量化模型输出与真实模式的一致性。研究者使用此框架评估了GPT-5和Gemini 2.5在零样本和少样本策略下对三个数据集的表现。实验结果表明Gemini 2.5性能优于GPT-5，少样本提示能提升两者准确性，且不同数据模式对性能有影响。BEMEval-Doc2Schema为LLM在BEM任务中的评估建立了首个社区驱动基准。|Liang Zhang Team|[2602.16926](http://arxiv.org/abs/2602.16926)|null|
|**2026-02-18**|**Beyond the Flag: A Framework for Integrating Cybersecurity Competitions into K-12 Education for Cognitive Apprenticeship and Ethical Skill Development**|针对夺旗赛（CTF）在K-12网络安全教育中面临的教师专业鸿沟和公平性挑战，本文提出了“网络安全中的伦理-认知学徒制”（ECAC）框架。该框架系统整合了认知学徒理论和嵌入式伦理发展，分为基础建模、搭建环境、指导阐述、伦理困境注入和反思探索五个阶段。ECAC旨在为多元化学生群体提供“低门槛、高上限”的学习路径，培养可迁移技能，并通过将教师角色重塑为“首席学习者”来弥补专业知识差距。该框架为CTF活动转型为培养高技能、高道德、多样化网络安全人才的综合学习体验提供了实用路线图。|Nam Son Nguyen Team|[2602.16921](http://arxiv.org/abs/2602.16921)|null|
|**2026-02-18**|**StereoAdapter-2: Globally Structure-Consistent Underwater Stereo Depth Estimation**|针对水下立体深度估计中由于水下域偏移及现有GRU-based迭代方法在长距离视差传播上效率低下的问题，本文提出了StereoAdapter-2。该模型用基于选择性状态空间模型的ConvSS2D算子替代ConvGRU更新器，通过四向扫描策略，实现在单个更新步骤中高效且线性复杂度的长距离空间传播。此外，研究构建了大规模合成水下立体数据集UW-StereoDepth-80K以丰富训练数据。实验结果显示，结合动态LoRA适应，StereoAdapter-2在TartanAir-UW和SQUID基准上分别提升17%和7.2%，达到了最先进的零样本性能，并在BlueROV2平台进行了真实世界验证。|Hao Tang Team|[2602.16915](http://arxiv.org/abs/2602.16915)|null|
|**2026-02-18**|**SparTa: Sparse Graphical Task Models from a Handful of Demonstrations**|针对机器人从演示中学习长时程操作任务的挑战，本文提出一种侧重于推断任务目标而非具体动作的学习方法。该方法通过一系列图形化对象关系表示场景状态演变，并设计了演示分割与池化策略，以提取完整对象交互图并估计任务各阶段的对象状态分布。为增强多演示学习的鲁棒性，研究还利用预训练视觉特征进行对象匹配。实验结果表明，该方法在演示分割精度和从多演示中学习最小任务模型方面表现良好，且其学习到的任务表示在模拟和真实机器人环境中均能支持可靠执行。|Abhinav Valada Team|[2602.16911](http://arxiv.org/abs/2602.16911)|null|
|**2026-02-18**|**Position: Why a Dynamical Systems Perspective is Needed to Advance Time Series Modeling**|针对时间序列（TS）建模领域虽有进步但仍需突破的现状，本文强调引入动力系统（DS）视角的必要性。文章指出，TS数据常源于潜在动力系统，而动力系统重建（DSR）技术有望通过数据推断DS代理模型，从而实现理论最优预测。DS原理不仅能提供超越短期预测的长期统计数据预测能力，还能深入揭示TS生成机制，指导模型性能边界、泛化能力及控制策略。研究回顾了DS理论与DSR的核心概念，并提出了将DSR的深刻见解转化为TS建模的具体建议，以期实现计算和内存效率更高的卓越预测。|Lukas Eisenmann Team|[2602.16864](http://arxiv.org/abs/2602.16864)|null|
|**2026-02-17**|**MedProbCLIP: Probabilistic Adaptation of Vision-Language Foundation Model for Reliable Radiograph-Report Retrieval**|视觉-语言基础模型在多模态理解方面潜力巨大，但其确定性嵌入难以满足高风险生物医学应用对可靠性的要求。本文提出了MedProbCLIP，一个用于胸部X射线和放射学报告表示学习及双向检索的概率视觉-语言学习框架。MedProbCLIP通过概率对比目标将图像和文本表示建模为高斯嵌入，显式捕获不确定性和多对多对应关系，并利用变分信息瓶颈减轻过度自信预测。实验结果表明，在MIMIC-CXR数据集上，MedProbCLIP在检索和零样本分类方面均优于现有基线，并展现出卓越的校准性、风险-覆盖行为、选择性检索可靠性以及对临床相关损坏的鲁棒性，提升了放射学图像-文本检索系统的可信度和安全性。|Gongbo Liang Team|[2602.16019](http://arxiv.org/abs/2602.16019)|null|
|**2026-02-17**|**Neural Scaling Laws for Boosted Jet Tagging**|大型语言模型（LLMs）的成功表明计算规模扩展是性能提升的关键，然而高能物理（HEP）领域最先进模型的训练计算量远低于工业界。针对该背景，本文研究了使用公共JetClass数据集进行增压射流分类的神经网络扩展定律。研究推导了计算最优的扩展定律，并识别出一个可通过增加计算持续接近的有效性能极限。研究结果表明，增加计算能可靠地推动性能接近渐近极限，且更具表达能力的低级特征可以提高性能极限并在固定数据集大小下改善结果，同时量化了数据重复对有效数据集大小的增益影响。|Lukas Heinrich Team|[2602.15781](http://arxiv.org/abs/2602.15781)|null|
|**2026-02-16**|**EditCtrl: Disentangled Local and Global Control for Real-Time Generative Video Editing**|背景：高保真生成式视频编辑依赖预训练视频基础模型，但其计算成本高昂，即使是局部编辑也需处理整个视频上下文。方法：本文提出EditCtrl，一个高效的视频修复控制框架。它引入了新颖的局部视频上下文模块，仅对掩码标记进行操作，使计算成本与编辑区域大小成正比。同时，一个轻量级的时间全局上下文嵌入器确保视频整体上下文的一致性。结果：EditCtrl比现有先进方法计算效率高10倍，并提升了编辑质量。该方法还支持多区域文本提示编辑和自回归内容传播。|Caleb Leak Team|[2602.15031](http://arxiv.org/abs/2602.15031)|**[link](https://yehonathanlitman.github.io/edit_ctrl)**|
|**2026-02-16**|**Generalization from Low- to Moderate-Resolution Spectra with Neural Networks for Stellar Parameter Estimation: A Case Study with DESI**|背景：在恒星光谱分析中，跨巡天泛化能力（特别是从低分辨率到中分辨率光谱的迁移）是一个关键挑战。方法：本文研究了使用预训练多层感知机（MLPs）解决此问题，以LAMOST低分辨率光谱到DESI中分辨率光谱的迁移为例。作者在LAMOST低分辨率光谱或其嵌入上预训练MLPs，并在DESI光谱上进行微调，比较了直接在光谱上训练的MLPs与基于Transformer模型嵌入训练的MLPs，并评估了不同的微调策略。结果：预训练在LAMOST低分辨率光谱上的MLPs表现出色，即便不微调也能获得良好性能，适度微调可进一步提升。研究表明，简单预训练MLPs能提供有竞争力的跨巡天泛化能力，但光谱基础模型在跨巡天恒星参数估计中的作用仍需深入探索。|Viska Wei Team|[2602.15021](http://arxiv.org/abs/2602.15021)|null|
|**2026-02-16**|**Cold-Start Personalization via Training-Free Priors from Structured World Models**|背景：冷启动个性化（即在无用户历史数据时推断用户偏好）是一个挑战，因为用户只关心少数偏好维度，且关键维度因人而异。现有强化学习方法在多轮交互中难以有效利用偏好数据的分因子结构。方法：本文提出Pep（Preference Elicitation with Priors）框架，将冷启动偏好获取分解为离线结构学习和在线贝叶斯推理。Pep离线从完整用户档案中学习偏好相关性的结构化世界模型，然后在线进行免训练的贝叶斯推理，以选择信息丰富的提问并预测完整的偏好档案。结果：Pep在生成响应与用户偏好的一致性方面达到80.8%，远高于强化学习的68.5%，且交互次数减少3-5倍。它仅用约1万参数就实现此效果，而强化学习需80亿参数，突显了利用偏好数据分因子结构的重要性。|Asli Celikyilmaz Team|[2602.15012](http://arxiv.org/abs/2602.15012)|null|
|**2026-02-16**|**PDE foundation models are skillful AI weather emulators for the Martian atmosphere**|背景：为火星大气层构建熟练的预测天气模拟器，面临训练数据和计算资源不足的挑战。方法：本文展示了如何将预训练在多源偏微分方程数值解上的AI基础模型（Poseidon PDE基础模型）适配并微调，以构建火星大气的预测天气模拟器。研究扩展了Poseidon模型从二维到三维的方法，同时保留了预训练信息，并探讨了在稀疏初始条件下的模型性能。结果：通过预训练与模型扩展的结合，模型在独立验证年份的性能提升了34.4%。这表明偏微分方程基础模型不仅能近似其他偏微分方程的解，还能作为解决实际世界复杂交互问题的锚定模型，尤其是在训练数据或计算预算有限的情况下。|Juan Bernabe-Moreno Team|[2602.15004](http://arxiv.org/abs/2602.15004)|null|
|**2026-02-16**|**Use What You Know: Causal Foundation Models with Partial Graphs**|背景：传统的因果量估计依赖于为特定假设定制的估计器。新兴的因果基础模型（CFMs）提供统一方法，但目前无法融入领域知识，导致预测次优。方法：本文提出将因果信息（如完整因果图或部分祖先信息）条件化到CFMs中的方法。研究系统评估了不同的条件化策略，发现将可学习偏差注入注意力机制是利用完整和部分因果信息最有效的方法。结果：通过条件化，通用CFM的性能可以与针对特定因果结构训练的专用模型相媲美。这一方法克服了构建一体化因果基础模型的核心障碍，使其能够以数据驱动的方式回答因果查询，同时有效利用任何程度的领域专业知识。|Bernhard Schölkopf Team|[2602.14972](http://arxiv.org/abs/2602.14972)|null|
|**2026-02-16**|**Model Context Protocol (MCP) Tool Descriptions Are Smelly! Towards Improving AI Agent Efficiency with Augmented MCP Tool Descriptions**|背景：基于基础模型（FM）的智能体通过工具描述与外部系统交互，但这些自然语言描述中的缺陷可能误导FM，其普遍性和影响尚不明确。方法：本文对103个MCP服务器上的856个工具进行了大规模实证研究。通过识别工具描述的六个组成部分，开发了评分标准并据此形式化了“工具描述异味”。利用FM-based扫描器进行操作化评估，并增强描述以评估其对智能体性能的影响。结果：97.1%的工具描述至少含有一种“异味”，其中56%未能清晰阐明目的。尽管增强所有组件的描述可使任务成功率中位数提升5.85个百分点，但执行步骤也增加了67.46%，并导致性能下降，揭示了性能与成本之间的权衡。|Ahmed E. Hassan Team|[2602.14878](http://arxiv.org/abs/2602.14878)|null|
|**2026-02-16**|**World Models for Policy Refinement in StarCraft II**|背景：尽管大型语言模型（LLMs）展现出强大的推理能力，但现有基于LLM的《星际争霸II》（SC2）智能体主要关注策略优化，缺乏可学习的、动作条件化的转移模型来辅助决策。方法：本文提出StarWM，这是首个在部分可观测环境下预测SC2未来观测的世界模型。为学习SC2的混合动态，作者引入了一种将观测分解为五个语义模块的结构化文本表示，并构建了首个用于SC2动态预测的指令微调数据集SC2-Dynamics-50k。StarWM被集成到“生成-模拟-细化”决策循环中，形成StarWM-Agent。结果：StarWM在资源预测准确性等指标上比零样本基线提升近60%。在线评估中，StarWM-Agent对不同难度级别（Hard、Harder、VeryHard）的胜率分别提升了30%、15%和30%，同时改善了宏观管理稳定性和战术风险评估。|Bo Xu Team|[2602.14857](http://arxiv.org/abs/2602.14857)|null|
|**2026-02-16**|**SAILS: Segment Anything with Incrementally Learned Semantics for Task-Invariant and Training-Free Continual Learning**|背景：持续学习在类增量语义分割（CISS）中面临重复训练、高计算成本和灾难性遗忘的限制，制约了其实际应用。方法：本文提出SAILS（Segment Anything with Incrementally Learned Semantics），一个免训练的CISS框架，它将CISS解耦为两个阶段：首先利用Segment Anything Model (SAM)进行零样本区域提取，然后通过固定特征空间中的原型进行语义关联。SAILS通过选择性类内聚类，为每个类生成多个原型以更好地建模类内变异性。结果：SAILS无需增量训练，但在标准CISS数据集上通常超越了现有的基于训练的方法，特别是在遗忘问题严重的长期和挑战性任务序列中。SAILS完全消除了遗忘，保持了任务不变的一致性能，并展现出正向反向迁移。|René Schuster Team|[2602.14767](http://arxiv.org/abs/2602.14767)|null|
|**2026-02-16**|**Depth Completion as Parameter-Efficient Test-Time Adaptation**|背景：现有深度补全方法通常通过训练任务特定编码器来利用辅助输入，但易过拟合且泛化性差。而3D基础模型可提供更强的几何先验。方法：本文提出CAPA，一个参数高效的测试时优化框架，用于利用稀疏几何线索对预训练3D基础模型进行深度补全。CAPA冻结基础模型骨干，仅通过参数高效微调（如LoRA或VPT）更新少量参数，并利用推理时稀疏观测直接计算梯度进行指导。对于视频，CAPA引入序列级参数共享以利用时间相关性并强制多帧一致性。结果：CAPA与任何基于ViT的基础模型兼容，并在室内外数据集的各种条件下取得了最先进的结果。它有效地将基础模型的几何先验与场景特定测量相结合，修正了畸变。|Shengyu Huang Team|[2602.14751](http://arxiv.org/abs/2602.14751)|null|
|**2026-02-16**|**WebWorld: A Large-Scale World Model for Web Agent Training**|背景：网页智能体需要大量轨迹以实现泛化，但真实世界训练受网络延迟、速率限制和安全风险制约。现有模拟器局限于封闭环境。方法：本文推出WebWorld系列，首个大规模训练的开放网络模拟器。它利用可扩展数据管道在超过100万次开放网络交互中进行训练，支持推理、多格式数据和超过30步的长周期模拟。结果：WebWorld在WebWorld-Bench上实现了与Gemini-3-Pro相当的模拟性能。在WebWorld合成轨迹上训练的Qwen3-14B在WebArena上性能提升9.2%，达到与GPT-4o相当的水平。WebWorld作为世界模型，在推理时搜索方面超越了GPT-5。此外，它还展现出跨领域泛化能力，为构建世界模型提供了可复现的方法。|Zuozhu Liu Team|[2602.14721](http://arxiv.org/abs/2602.14721)|null|
|**2026-02-16**|**Arbor: A Framework for Reliable Navigation of Critical Conversation Flows**|针对大型语言模型在医疗分诊等高风险领域难以遵循结构化工作流程的问题，以及单一提示方法在长提示下易导致指令依从性下降的挑战，本文提出了Arbor框架。该框架将决策树导航分解为节点级任务，通过基于DAG的编排机制动态检索和评估转换，并将响应生成解耦。实验结果表明，与单一提示基线相比，Arbor在真实临床分诊对话中将平均轮次准确率提高了29.4个百分点，同时显著降低了延迟和成本，证明了架构分解能有效提升模型性能并降低对模型固有能力的依赖。|Luís Ungaro Team|[2602.14643](http://arxiv.org/abs/2602.14643)|null|
|**2026-02-16**|**Tabular Foundation Models Can Learn Association Rules**|传统的关联规则挖掘（ARM）方法存在规则爆炸和可扩展性差的问题，而现有神经方法在低数据量下性能不佳，但表格基础模型（TFMs）为解决这些局限性提供了基础。为此，本文提出了一个模型无关的关联规则学习框架，能够利用TFMs从任何条件概率模型中提取关联规则，并实例化了TabProbe。实验结果显示，TabProbe利用TFMs作为条件概率估计器，无需频繁项集挖掘即可生成简洁、高质量的关联规则，在低数据设置下仍保持强大的预测性能和鲁棒性。|Victoria Degeler Team|[2602.14622](http://arxiv.org/abs/2602.14622)|null|
|**2026-02-16**|**MATEO: A Multimodal Benchmark for Temporal Reasoning and Planning in LVLMs**|AI智能体在实现复杂目标时需要规划，但现有研究对基础模型时间执行顺序（TEO）的理解有限，多局限于线性近似或纯文本输入。为解决此问题，本文引入了MATEO（MultimodAl Temporal Execution Order）基准，旨在评估和提升大型视觉语言模型（LVLMs）的时间推理能力。通过收集高质量多模态食谱语料库及相应的TEO图注释，作者使用MATEO评估了六个最先进的LVLM，考察了不同模型规模、语言上下文、多模态输入结构和微调策略对时间推理能力的影响。|Giuseppe Riccardi Team|[2602.14589](http://arxiv.org/abs/2602.14589)|null|
|**2026-02-16**|**MedVAR: Towards Scalable and Efficient Medical Image Generation via Next-scale Autoregressive Prediction**|医学图像生成在数据增强和隐私保护中至关重要，但现有方法在架构效率、多器官数据和原则性评估方面存在不足。为此，本文提出了MedVAR，首个基于自回归的医学基础模型，采用“下一尺度预测”范式，实现快速可扩展的医学图像合成。MedVAR以粗到精的方式生成图像，并构建了一个包含约44万张CT和MRI图像的协调数据集。综合实验表明，MedVAR在图像保真度、多样性和可扩展性方面均达到最先进水平，为未来的医学生成基础模型提供了 promising 的架构方向。|Yueming Jin Team|[2602.14512](http://arxiv.org/abs/2602.14512)|null|
|**2026-02-16**|**Covariance-Aware Transformers for Quadratic Programming and Decision Making**|针对Transformer在涉及协方差矩阵的决策问题中的应用潜力，本文首先证明线性注意力机制可通过模拟梯度下降求解无约束二次规划（QP），并扩展至求解L1惩罚/约束QP。在此基础上，本文提出了Time2Decide，一种通过显式输入协方差矩阵来增强时间序列基础模型（TSFM）的通用方法。实验结果表明，Time2Decide在经典的投资组合优化问题上，其性能普遍优于基础TSFM模型，并在特定条件下甚至超越了传统的“预测-优化”流程，证明Transformer通过显式利用二阶统计量能有效解决复杂决策问题。|Samet Oymak Team|[2602.14506](http://arxiv.org/abs/2602.14506)|null|
|**2026-02-16**|**A Soft Wrist with Anisotropic and Selectable Stiffness for Robust Robot Learning in Contact-rich Manipulation**|在非结构化环境中，机器人进行接触密集型操作任务时，现有软末端执行器因形变范围有限、缺乏定向刚度控制或系统复杂而面临挑战。本文介绍了一种名为CLAW（Compliant Leaf-spring Anisotropic soft Wrist）的新型软腕机构，它通过简单的板簧和锁定旋转关节设计，实现了大范围6自由度形变和可调的各向异性刚度，同时保持轻量和低成本。在模仿学习实验中，CLAW在插销任务中实现了76%的成功率，显著优于其他夹具，并在处理高精度装配和精细物体操作等接触密集型场景中表现出强大潜力，预示其能增强机器人学习的鲁棒性。|Masashi Hamaya Team|[2602.14434](http://arxiv.org/abs/2602.14434)|null|
|**2026-02-15**|**WIMLE: Uncertainty-Aware World Models with IMLE for Sample-Efficient Continuous Control**|基于模型的强化学习（MBRL）常因模型误差累积、世界模型处理多模态动力学不佳及预测过度自信而表现受限。本文提出了WIMLE，一种将隐式最大似然估计（IMLE）扩展到MBRL框架的方法，旨在学习随机、多模态的世界模型，并利用集成和潜在采样估计预测不确定性。WIMLE在训练中根据预测置信度加权合成转换，以稳定学习。在40个连续控制任务上的实验结果表明，WIMLE实现了卓越的样本效率和有竞争力的渐近性能，尤其在挑战性任务上显著提升了样本效率，凸显了IMLE基多模态和不确定性感知加权对稳定MBRL的价值。|Ke Li Team|[2602.14351](http://arxiv.org/abs/2602.14351)|**[link](https://openreview.net/forum?id=mzLOnTb3WH)**|
|**2026-02-15**|**Multi-Agent Debate: A Unified Agentic Framework for Tabular Anomaly Detection**|表格异常检测常依赖单一或静态集成检测器，但异构模型在分布漂移、缺失数据和稀有异常下常出现分歧。本文提出了MAD（Multi-Agent Debating）框架，将这种分歧作为核心信号，通过数学协调层解决。框架中每个代理是一个ML检测器，提供异常分数、置信度和证据，并由LLM评论员增强。协调器将消息转换为损失并更新代理影响力，生成最终异常分数和可审计的辩论轨迹。实验表明，MAD在各种表格异常基准测试上提高了鲁棒性，并提供了更清晰的模型分歧追踪。|Sheng Li Team|[2602.14251](http://arxiv.org/abs/2602.14251)|null|
|**2026-02-15**|**Towards Spatial Transcriptomics-driven Pathology Foundation Models**|空间转录组学（ST）能够提供超越组织学评估的分子景观，多模态基础模型也显示了形态分子耦合提升组织学表征的潜力。为整合局部分子信息到病理视觉编码器，本文提出了Spatial Expression-Aligned Learning (SEAL) 框架，作为一种参数高效的视觉-组学自监督微调方法，可应用于现有病理学基础模型。SEAL通过在涵盖14个器官的70多万个配对基因表达点-组织区域示例上进行训练，在38项幻灯片级和15项补丁级下游任务上，持续优于纯视觉和ST预测基线，并展示了强大的域泛化能力和基因到图像检索等跨模态能力，为病理学基础模型的ST引导微调提供了通用且实用的框架。|Faisal Mahmood Team|[2602.14177](http://arxiv.org/abs/2602.14177)|null|
|**2026-02-15**|**ARport: An Augmented Reality System for Markerless Image-Guided Port Placement in Robotic Surgery**|精确的端口放置是机器人辅助手术的关键步骤，但术前规划与术中执行之间存在差距。本文提出了ARport，一个增强现实（AR）系统，旨在自动将预规划的套管布局映射到患者体表，提供直观的术中空间指导。ARport在光学透视头戴式显示器（OST-HMD）上实现，无需外部传感器或标记，通过基础模型提取患者体表并进行无标记配准，实现术前解剖模型与患者体表的对齐，从而现场可视化套管布局。全尺寸人体模型实验表明，ARport能够准确叠加预规划的套管位置，实现虚拟规划与真实解剖之间的一致空间对应，为临床工作流程的无缝集成提供了高效且极简的解决方案。|Qi Dou Team|[2602.14153](http://arxiv.org/abs/2602.14153)|null|
|**2026-02-13**|**Imitating What Works: Simulation-Filtered Modular Policy Learning from Human Videos**|针对机器人通过观察人类视频学习抓取操作技能时，传统方法难以有效学习与任务兼容的抓取行为的问题，本研究提出了Perceive-Simulate-Imitate (PSI) 框架。该框架利用仿真中的抓取轨迹过滤技术，对人类视频数据进行处理，并生成带有抓取适用性标签的扩展轨迹数据，从而实现面向任务的抓取能力监督学习。真实世界实验表明，PSI无需任何机器人数据即可高效学习精确的操纵技能，并且相比简单使用抓取生成器的方法，性能显著提升，鲁棒性更强。|Wei-Chiu Ma Team|[2602.13197](http://arxiv.org/abs/2602.13197)|null|
|**2026-02-13**|**Order Matters in Retrosynthesis: Structure-aware Generation via Reaction-Center-Guided Discrete Flow Matching**|为解决现有免模板逆合成方法学习效率低和半模板方法泛化受限的问题，本研究提出了一种结构感知的免模板框架，核心在于利用原子排序信息。该方法将反应中心原子置于序列头部，通过位置归纳偏差编码化学反应的两阶段特性，并采用RetroDiT骨干网络与离散流匹配相结合。实验结果表明，该方法在USPTO-50k和USPTO-Full数据集上取得了SOTA性能，且在预测反应中心下，性能超越了使用更多数据训练的基础模型，并验证了结构先验的重要性。|Tianshu Yu Team|[2602.13136](http://arxiv.org/abs/2602.13136)|null|
|**2026-02-13**|**A Calibrated Memorization Index (MI) for Detecting Training Data Leakage in Generative MRI Models**|鉴于图像生成模型可能复制训练数据，尤其在医学图像生成中引发隐私问题，本研究提出了一种校准的逐样本度量方法来检测训练数据的记忆化和重复。该方法利用MRI基础模型提取图像特征，聚合多层白化最近邻相似度，并映射为有界的“过拟合/新颖性指数”（ONI）和“记忆化指数”（MI）分数。在多个MRI数据集上的实验结果表明，该度量能稳健检测重复数据，并提供一致的度量值，在样本级别实现了近乎完美的重复项检测。|Ibrahim Habli Team|[2602.13066](http://arxiv.org/abs/2602.13066)|null|
|**2026-02-13**|**INHerit-SG: Incremental Hierarchical Semantic Scene Graphs with RAG-Style Retrieval**|针对现有语义场景图在机器人导航中难以支持可解释的人类意图推理的问题，本研究提出了INHerit-SG框架。该框架将地图定义为RAG-ready的知识库，通过引入自然语言描述作为语义锚点对齐人类意图，并采用异步双进程架构和分层结构解耦几何分割与语义推理，通过事件触发机制保持地图长期一致性。实验在新建数据集和真实世界环境中进行，结果表明INHerit-SG在复杂查询上达到了最先进性能，并提高了检索成功率和可靠性，展现了其在下游导航任务中的可扩展性。|Yang Gao Team|[2602.12971](http://arxiv.org/abs/2602.12971)|null|
|**2026-02-13**|**Information-theoretic analysis of world models in optimal reward maximizers**|为量化最优行为对世界内部表示的需求，本研究考虑了一个具有n个状态和m个动作的受控马尔可夫过程，并假设转移动态存在均匀先验。研究证明，观察一个对任何非恒定奖励函数最优的确定性策略，可以精确地传达n log m比特关于环境的信息。具体来说，环境与最优策略之间的互信息为n log m比特。这些发现为实现最优性所需的“隐式世界模型”提供了精确的信息理论下限，适用于多种奖励最大化目标。|Alex Altair Team|[2602.12963](http://arxiv.org/abs/2602.12963)|null|
|**2026-02-13**|**Scaling Single Human Demonstrations for Imitation Learning using Generative Foundational Models**|鉴于机器人模仿学习中收集演示数据耗时且不易从人类演示直接迁移，本研究提出Real2Gen框架，旨在从单个“人类”演示中训练机器人操纵策略。该方法从人类演示中提取必要信息并传输至仿真环境，在仿真中利用可编程专家智能体生成无限数据来训练流匹配策略。实验结果显示，Real2Gen在三个真实世界任务上成功率平均提升26.6%，并且由于训练数据丰富多样，训练策略的泛化能力显著提高，纯仿真训练的策略还能零样本部署到真实世界。|Abhinav Valada Team|[2602.12734](http://arxiv.org/abs/2602.12734)|null|
|**2026-02-13**|**MedXIAOHE: A Comprehensive Recipe for Building Medical MLLMs**|为提升真实世界临床应用中的通用医学理解和推理能力，本研究提出了医疗视觉-语言基础模型MedXIAOHE。该模型采用实体感知持续预训练框架，组织异构医学语料以拓宽知识覆盖并减少长尾问题；通过强化学习和工具增强的代理训练，整合多样化医学推理模式以支持带可验证决策轨迹的多步骤诊断推理；并融合用户偏好规则、证据推理和低幻觉长文本报告生成，提高真实世界使用的可靠性。MedXIAOHE在多项医学基准测试中取得了最先进的性能，并超越了领先的闭源多模态系统。|Zhixiong Yang Team|[2602.12705](http://arxiv.org/abs/2602.12705)|null|
|**2026-02-13**|**RelBench v2: A Large-Scale Benchmark and Repository for Relational Data**|为推动关系深度学习（RDL）的发展，并应对日益增长的模型规模需求，本研究引入了RelBench v2，一个大规模、真实的关系数据库基准扩展。RelBench v2新增了四个大型数据集和“自动完成任务”，旨在直接推理关系表中缺失的属性值，并整合了外部基准和评估框架以实现统一的关系-时间评估。实验结果表明，RDL模型在自动完成、预测和推荐任务中始终优于单表基线，突出了显式建模关系结构的重要性。|Jure Leskovec Team|[2602.12606](http://arxiv.org/abs/2602.12606)|**[link](https://relbench.stanford.edu)**|
|**2026-02-13**|**The Constant Eye: Benchmarking and Bridging Appearance Robustness in Autonomous Driving**|针对自动驾驶算法在OOD条件下易受外观变化影响，且难以区分外观与结构场景变化导致规划器失效的问题，本研究建立了Navdream，一个高保真鲁棒性基准。该基准利用生成式像素对齐风格迁移，隔离外观变化对驾驶性能的影响。为弥合这一差距，研究提出了一种通用感知接口，利用冻结的视觉基础模型（DINOv3）提取外观不变特征作为规划器的稳定接口。实验表明，现有规划算法在OOD外观下性能显著下降，而该即插即用解决方案在各种规划范式中实现了卓越的零样本泛化，在极端外观变化下仍保持一致性能。|Yiyi Liao Team|[2602.12563](http://arxiv.org/abs/2602.12563)|null|
|**2026-02-13**|**Self-Supervised JEPA-based World Models for LiDAR Occupancy Completion and Forecasting**|鉴于自动驾驶需要世界模型来支持长期规划，且模型学习需具备自监督的可扩展性，本研究提出AD-LiST-JEPA，一个基于联合嵌入预测架构（JEPA）的自监督世界模型。该模型旨在利用JEPA框架从激光雷达数据预测未来时空演变。通过下游基于激光雷达的占用完成和预测（OCF）任务评估学习到的表示质量，概念验证实验表明，经过JEPA世界模型学习预训练后的编码器在OCF性能上有所提升，证明了该方法在感知和预测联合任务中的潜力。|Anna Choromanska Team|[2602.12540](http://arxiv.org/abs/2602.12540)|null|
|**2026-02-13**|**Multi-Agent Model-Based Reinforcement Learning with Joint State-Action Learned Embeddings**|在部分可观察和高度动态环境中，多智能体协调学习面临表示学习和数据效率挑战。为此，本文提出了一种新颖的基于模型的强化学习框架，该框架将联合状态-动作表示学习与想象式展开相结合。作者设计了一个使用变分自编码器训练的世界模型，并利用学习到的状态-动作嵌入（SALE）进行增强，将其注入到预测未来展开的想象模块和估计联合动作值函数的联合智能体网络中。在星际争霸II微管理、多智能体MuJoCo和基于级别的觅食挑战等基准测试中，该方法在有限真实环境交互下，通过将想象轨迹与基于SALE的动作值相结合，显著优于基线算法，验证了其在多智能体模型范式中学习联合状态-动作嵌入的有效性。|David Meger Team|[2602.12520](http://arxiv.org/abs/2602.12520)|null|
|**2026-02-12**|**The Observer Effect in World Models: Invasive Adaptation Corrupts Latent Physics**|判断神经网络模型是内化了物理定律还是仅利用统计捷径，尤其是在分布外（OOD）变化下，仍是一个难题。传统的适应性评估方法（如微调或高容量探针）可能改变被测量的表示，从而混淆自监督学习（SSL）期间的真实学习内容。为解决此问题，本文提出了一种非侵入性评估协议PhyIP，该协议基于线性表示假设，通过测试物理量能否从冻结表示中线性解码来评估。在流体动力学和轨道力学任务中，实验发现当SSL错误率较低时，潜在结构变得线性可访问，PhyIP在OOD测试中成功恢复了内能和牛顿反平方标度（ρ>0.90）。相比之下，基于适应性的评估可能使这种结构崩溃（ρ≈0.05）。这表明低容量探针能更准确地评估物理世界模型，而适应性评估可能掩盖潜在结构。|Barbara Hammer Team|[2602.12218](http://arxiv.org/abs/2602.12218)|null|
|**2026-02-12**|**LDA-1B: Scaling Latent Dynamics Action Model via Universal Embodied Data Ingestion**|当前机器人基础模型多依赖大规模行为克隆，忽视了异构具身数据中可迁移的动力学知识，而现有统一世界模型（UWM）因粗糙数据使用和碎片化数据集难以扩展。为此，本文提出了LDA-1B，一个通过通用具身数据摄取进行扩展的机器人基础模型，它联合学习动力学、策略和视觉预测，并为不同质量数据分配不同角色。为支持大规模训练，作者构建并标准化了EI-30k数据集（超过3万小时的人类和机器人轨迹）。通过在结构化的DINO潜在空间中进行预测，实现了异构数据的可扩展动力学学习，避免了冗余的像素空间外观建模，并采用多模态扩散Transformer处理异步视觉和动作流，实现了10亿参数规模的稳定训练。实验结果表明，LDA-1B在接触密集型、灵巧型和长程任务上分别比现有方法（如π0.5）提高了21%、48%和23%，并能通过利用30%通常有害且被丢弃的低质量轨迹，实现数据高效微调，性能提升10%。|He Wang Team|[2602.12215](http://arxiv.org/abs/2602.12215)|**[link](https://pku-epic.github.io/LDA)**|
|**2026-02-12**|**DreamID-Omni: Unified Framework for Controllable Human-Centric Audio-Video Generation**|尽管基础模型在音视频生成方面取得进展，但以人物为中心的多任务（如参考音视频生成、视频编辑、音频驱动动画）仍被孤立处理，且难以在单一框架内实现对多角色身份和音色的精确解耦控制。本文提出了DreamID-Omni，一个统一的可控人物中心音视频生成框架。作者设计了一个对称条件扩散Transformer，通过对称条件注入方案整合异构条件信号。为解决多人物场景中普遍存在的身份-音色绑定失败和说话人混淆问题，提出了双层解耦策略：在信号层面采用同步RoPE确保严格的注意力空间绑定，在语义层面采用结构化字幕建立显式属性-主题映射。此外，还设计了多任务渐进训练方案，利用弱约束生成先验来正则化强约束任务。大量实验证明，DreamID-Omni在视频、音频和音视频一致性方面均达到了全面的最先进性能，甚至超越了领先的商业模型。|Xiangwang Hou Team|[2602.12160](http://arxiv.org/abs/2602.12160)|**[link](https://guoxu1233.github.io/DreamID-Omni/)**|
|**2026-02-12**|**It's TIME: Towards the Next Generation of Time Series Forecasting Benchmarks**|时间序列基础模型（TSFMs）正在革新预测领域，但现有基准在数据构成、数据完整性、任务制定和分析视角上存在局限。为解决这些问题，本文提出了TIME，一个新一代任务中心基准，包含50个全新数据集和98个预测任务，专为严格的零样本TSFM评估而设计，避免数据泄露。通过整合大型语言模型和人类专业知识，建立了严格的人机协作基准构建流程，确保高数据完整性，并根据真实操作需求和变量可预测性重新定义任务。此外，作者提出了一种新颖的模式级评估视角，超越了基于静态元标签的传统数据集级评估，通过利用结构化时间序列特征表征内在时间属性，为模型能力提供了更具普适性的见解。对12个代表性TSFMs进行评估，并建立了一个多粒度排行榜，以促进深入分析和可视化检查。|Chenghao Liu Team|[2602.12147](http://arxiv.org/abs/2602.12147)|null|
|**2026-02-12**|**Commencing-Student Enrolment Forecasting Under Data Sparsity with Time Series Foundation Models**|许多大学面临日益增长的财政压力，亟需准确预测新生入学人数，然而高等教育入学预测通常数据稀疏，年度序列短且受报告变化和体制转变影响。流行的经典方法因短样本导致参数估计和模型选择不稳定，以及结构性中断导致外推能力下降而不可靠。近期，TSFMs在泄漏受限的协变量构建下，为年度、数据稀疏的机构预测提供了强大的零样本先验。本文在零样本设置下，对多种TSFM家族进行了基准测试，并测试了一组紧凑、防泄漏的协变量集。作者引入了“机构运营状况指数”（IOCI），这是一个从时间戳文件证据中提取的可转移的0-100区间状态协变量，并结合了具有稳定特征工程的Google Trends需求代理。使用严格对齐的回溯测试，结果表明，在没有机构特定训练的情况下，条件化TSFMs的表现与经典基准相当，具体表现差异因群体和模型而异。|Surangika Ranathunga Team|[2602.12120](http://arxiv.org/abs/2602.12120)|null|
|**2026-02-12**|**The Pensieve Paradigm: Stateful Language Models Mastering Their Own Context**|当前的大型语言模型（LLMs）虽有强大的数据库和检索系统，但缺乏主动管理自身状态和记忆的机制，被动接受手动提供的上下文，受限于固定窗口大小。本研究引入了StateLM，一种新型基础模型，它具备内部推理循环来管理自身状态。该模型配备了一系列记忆工具（如上下文剪枝、文档索引、笔记）并被训练主动管理这些工具，从而摆脱固定窗口的架构限制。实验结果表明，StateLM在所有模型规模的长文档问答任务中持续优于标准LLMs，在聊天记忆任务中绝对准确率提升10%至20%，在深度研究任务BrowseComp-Plus上更是达到52%的准确率，远超标准LLM的5%左右。这使得LLMs从被动预测器转变为状态感知代理，实现了状态化和可管理的推理过程。|Yan Wang Team|[2602.12108](http://arxiv.org/abs/2602.12108)|null|
|**2026-02-12**|**GigaBrain-0.5M*: a VLA That Learns From World Model-Based Reinforcement Learning**|直接从当前观察预测多步动作块的视觉-语言-动作（VLA）模型，因场景理解和未来预测能力受限而存在不足。相比之下，预训练于网络规模视频语料库的视频世界模型具有强大的时空推理和准确的未来预测能力，为增强VLA学习提供了基础。本研究提出了GigaBrain-0.5M*，一个通过世界模型驱动强化学习训练的VLA模型，其基于在超过1万小时机器人操作数据上预训练的GigaBrain-0.5。GigaBrain-0.5M*通过RAMP（通过世界模型条件策略进行强化学习）整合强化学习，以实现鲁棒的跨任务适应。实验结果表明，RAMP相对于RECAP基线取得了显著性能提升，在“叠衣服”、“装箱”和“制作浓缩咖啡”等挑战性任务上提升了约30%，并且在真实世界部署中展示了可靠的长时序复杂操作能力。|Zheng Zhu Team|[2602.12099](http://arxiv.org/abs/2602.12099)|**[link](https://gigabrain05m.github.io/)**|
|**2026-02-12**|**Affordance-Graphed Task Worlds: Self-Evolving Task Generation for Scalable Embodied Learning**|在现实世界中直接训练机器人策略成本高昂且难以扩展，而生成式仿真虽能合成大规模数据，但现有方法常难以生成逻辑连贯的长时序任务，且因开环执行难以应对动态物理不确定性。本研究提出了Affordance-Graphed Task Worlds (AGT-World)，一个统一框架，能够根据现实世界观察自主构建交互式仿真环境及相应的机器人任务策略。AGT-World通过将任务空间形式化为结构化图，实现复杂目标的精确分层分解为原子原语，并引入结合视觉-语言模型推理和几何验证的混合反馈自进化机制来自主优化策略。大量实验证明，AGT-World在成功率和泛化能力上显著优于现有方法，实现了提议、执行和纠错的自改进循环，从而促进可扩展的机器人学习。|Changshui Zhang Team|[2602.12065](http://arxiv.org/abs/2602.12065)|null|
|**2026-02-12**|**VLAW: Iterative Co-Improvement of Vision-Language-Action Policy and World Model**|视觉-语言-动作（VLA）模型的性能和可靠性可通过迭代在线交互提升，但现实世界策略回放数据收集昂贵。虽然学习到的模拟器（动作条件视频生成模型）有望生成额外回放数据，但现有世界模型缺乏物理保真度，难以精确建模接触密集物体操纵中的关键物理细节。本研究提出一种简单的迭代改进算法，利用真实世界回放数据提升世界模型的保真度，然后该世界模型再用于生成补充合成数据以改进VLA模型。在真实机器人实验中，该方法显著提升了最先进VLA模型在多个下游任务上的性能，相比基础策略，成功率绝对提升39.2%，相比使用生成合成回放数据进行训练，提升了11.6%。|Chelsea Finn Team|[2602.12063](http://arxiv.org/abs/2602.12063)|null|
|**2026-02-12**|**HoloBrain-0 Technical Report**|基础模型研究与可靠的真实世界机器人部署之间存在差距。为弥合此差距，本研究引入了HoloBrain-0，一个全面的视觉-语言-动作（VLA）框架。其核心是一个新颖的VLA架构，明确融入了机器人具身先验（如多视角相机参数和运动学描述），以增强3D空间推理并支持多样化的具身形态。该设计通过可扩展的“预训练-后训练”范式进行验证，在RoboTwin 2.0、LIBERO和GenieSim等仿真基准测试中取得了最先进的结果，并在具有挑战性的长时序真实世界操作任务中表现出色。值得一提的是，其高效的0.2B参数变体能与更大的基线模型相媲美，支持低延迟的设备端部署。为加速研究，HoloBrain生态系统已完全开源，包括预训练VLA基础模型、后训练检查点和全栈VLA基础设施RoboOrchard。|Zhizhong Su Team|[2602.12062](http://arxiv.org/abs/2602.12062)|null|
|**2026-02-12**|**FedGRPO: Privately Optimizing Foundation Models with Group-Relative Rewards from Domain Client**|针对联邦基础模型(FedFMs)中现有知识迁移方法存在的训练成本高、通信开销大及隐私风险问题，本研究将其重构为强化学习式评估过程。提出了FedGRPO框架，通过构建轻量级置信图进行能力专家选择，并利用"组相对"概念将问题与解决方案打包为候选策略，仅聚合客户端返回的标量奖励信号，而非直接交换数据或模型更新。实验结果表明，FedGRPO在保证隐私和降低通信开销的同时，实现了优于传统FedFMs基线的下游任务精度和通信效率。|Yuxing Han Team|[2602.12014](http://arxiv.org/abs/2602.12014)|null|
|**2026-02-12**|**Accelerating Robotic Reinforcement Learning with Agent Guidance**|强化学习(RL)在机器人操作技能学习中面临样本效率低下，而现有的人在环(HIL)方法受限于可扩展性与人类监督的低效性。本研究提出了Agent-guided Policy Search (AGPS)框架，用多模态智能体取代人类监督者，将智能体视为语义世界模型，注入内在价值先验来指导物理探索。通过可执行工具，智能体提供精确的纠正航点和空间约束来修剪探索。实验结果表明，AGPS在精细插入和变形物体操作任务中，样本效率优于HIL方法，从而实现了自动化监督，为可扩展的机器人学习提供了新途径。|Yaodong Yang Team|[2602.11978](http://arxiv.org/abs/2602.11978)|null|
|**2026-02-12**|**Where Bits Matter in World Model Planning: A Paired Mixed-Bit Study for Efficient Spatial Reasoning**|为实现高效空间推理，本研究探讨了低比特规划行为在有限精度预算下，是否主要由总比特位宽决定，或由比特位在模型模块间的分配方式决定。通过在DINO-WM上进行混合位评估，观察到8比特和6比特设置接近FP16，3比特设置性能崩溃，而4比特设置对位宽分配敏感。在4比特过渡区，保留编码器精度可提升规划性能。这些发现表明，模块感知、预算感知的量化策略对于高效空间推理具有重要研究意义。|Vaishak Menon Team|[2602.11882](http://arxiv.org/abs/2602.11882)|null|
|**2026-02-12**|**PuYun-LDM: A Latent Diffusion Model for High-Resolution Ensemble Weather Forecasts**|潜在扩散模型(LDMs)在 <=0.25° 的高分辨率集合天气预报中存在扩散性受限问题，且现有频率方法因缺乏对变量间频谱异质性的考虑而导致正则化强度不均。本研究提出了PuYun-LDM模型，结合3D掩码自编码器(3D-MAE)编码天气状态演化特征作为扩散模型的额外条件，并采用变量感知掩码频率建模(VA-MFM)策略自适应选择阈值。实验证明，PuYun-LDM增强了潜在扩散性，在短时效预报中表现优于ENS，长时效预报中与ENS相当，并能在短时间内高效生成全球预报。|Bin Wang Team|[2602.11807](http://arxiv.org/abs/2602.11807)|null|
|**2026-02-12**|**HAIC: Humanoid Agile Object Interaction Control via Dynamics-Aware World Model**|类人机器人在处理非结构化环境中复杂全身任务时，现有方法难以应对具有独立动力学和非完整约束的欠驱动物体，且缺乏外部状态估计。本研究提出了HAIC统一框架，通过仅利用本体感受历史数据估计高阶物体状态的动力学预测器，并将预测投影至静态几何先验形成空间接地动态占用图，使策略能在盲区推断碰撞边界和接触可供性。通过非对称微调，世界模型持续适应学生策略。实验表明，HAIC在敏捷任务和多物体长时序任务中均取得了高成功率，有效应对惯性扰动并预测多物体动力学。|Renjing Xu Team|[2602.11758](http://arxiv.org/abs/2602.11758)|**[link](https://haic-humanoid.github.io/)**|
|**2026-02-12**|**ABot-N0: Technical Report on the VLA Foundation Model for Versatile Embodied Navigation**|具身导航领域长期受限于任务特定的架构。本研究引入了ABot-N0，一个统一的视觉-语言-动作(VLA)基础模型，旨在实现点目标、物体目标、指令跟随、POI目标和人物跟随五项核心导航任务的“大一统”。模型采用分层“大脑-行动”架构，将基于大型语言模型的认知大脑与基于流匹配的动作专家结合。为支持大规模学习，研究团队开发了ABot-N0数据引擎， curating 16.9M专家轨迹和5.0M推理样本。ABot-N0在7个基准测试中取得了新的SOTA性能，并整合规划器与分层拓扑记忆，实现动态真实世界环境中的鲁棒长时序任务。|Mu Xu Team|[2602.11598](http://arxiv.org/abs/2602.11598)|**[link](https://amap-cvlab.github.io/ABot-Navigation/ABot-N0/)**|
|**2026-02-12**|**Brain4FMs: A Benchmark of Foundation Models for Electrical Brain Signal**|脑基础模型(BFMs)在神经科学领域快速发展，但缺乏统一的方法理解和标准化评估框架。本研究旨在填补这一空白，通过自监督学习(SSL)分类法组织BFMs，并从数据集角度总结常见下游任务，整理临床和以人为中心神经技术应用的代表性公共数据集。在此基础上，推出了Brain4FMs开放评估平台，集成了15个代表性BFM和18个公共数据集。该平台支持标准化比较和分析预训练数据、SSL策略和架构对泛化和下游性能的影响，以指导更准确、可迁移的BFMs开发。|Yang Yang Team|[2602.11558](http://arxiv.org/abs/2602.11558)|null|
|**2026-02-12**|**TS-Memory: Plug-and-Play Memory for Time Series Foundation Models**|时间序列基础模型(TSFMs)在分布偏移下的下游领域适应面临挑战，现有参数化适应易导致灾难性遗忘，而非参数化检索则引入高推理延迟。本研究提出了参数化记忆蒸馏方法并实现为TS-Memory，一个轻量级记忆适配器。TS-Memory分两阶段训练：首先构建离线、无信息泄露的kNN教师模型合成置信度感知的量化目标；其次通过置信度门控监督将检索诱导的分布校正蒸馏到记忆适配器中。实验表明，TS-Memory在多种TSFMs和基准测试上持续改进点预测和概率预测性能，且效率与冻结骨干网络相当，实现了无检索部署。|Yuxuan Liang Team|[2602.11550](http://arxiv.org/abs/2602.11550)|null|
|**2026-02-12**|**Budget-Constrained Agentic Large Language Models: Intention-Based Planning for Costly Tool Use**|在预算约束下，大型语言模型调用外部工具解决多步骤任务的工具增强型智能体面临巨大状态-动作空间、高结果方差和过高探索成本导致直接规划难以处理的问题。本研究提出了INTENT，一个推理时规划框架，利用意图感知的层次世界模型来预测未来的工具使用和风险校准成本，并在线指导决策。实验证明，INTENT在成本增强型StableToolBench上严格遵循硬预算可行性，显著提高了任务成功率，并在工具价格和预算动态变化下表现出鲁棒性。|Qi Qi Team|[2602.11541](http://arxiv.org/abs/2602.11541)|null|
|**2026-02-12**|**Vascular anatomy-aware self-supervised pre-training for X-ray angiogram analysis**|X射线血管造影图像分析受限于带标注数据稀缺，大规模自监督学习(SSL)在该领域潜力未被充分挖掘。本研究引入了VasoMIM框架，通过解剖引导的掩码策略强制模型学习血管语义，并利用解剖一致性损失保留血管结构一致性，从而增强学习表征的判别力。同时，策展了迄今最大的X射线血管造影预训练数据集XA-170K。VasoMIM在多个下游任务中展现出卓越的可迁移性和领先的性能，凸显了其作为X射线血管造影分析基础模型的巨大潜力。|Zeng-Guang Hou Team|[2602.11536](http://arxiv.org/abs/2602.11536)|null|
|**2026-02-12**|**Semantic-aware Adversarial Fine-tuning for CLIP**|当前研究表明，通过对抗性微调CLIP图像编码器可增强其零样本分类的对抗鲁棒性，但生成对抗样本（AEs）时仅依赖图像与单一手动模板的余弦相似度，不足以衡量图文对的语义相似性，导致微调后的模型鲁棒性不足。为解决此问题，本文提出了一种语义集成攻击方法，通过最小化原始图像与一组精炼文本描述（由基础模型生成并去除了幻觉）之间的平均相似度来生成语义感知的AEs。在此基础上，作者提出了语义感知对抗微调（SAFT）框架。实验结果表明，SAFT在16个数据集上的零样本对抗鲁棒性方面显著优于现有方法，实现了实质性提升。|Feng Liu Team|[2602.12461](http://arxiv.org/abs/2602.12461)|null|
|**2026-02-12**|**Stabilizing Native Low-Rank LLM Pretraining**|基础模型日益增长的参数量带来了巨大的计算和内存挑战，而低秩分解是降低成本的潜在途径，但从头开始仅使用低秩权重训练模型且性能匹配全秩模型仍缺乏稳定的方法。本文研究表明，无需先验方法的“全秩”辅助指导，大型语言模型（LLMs）可以从头开始仅使用低秩分解权重训练所有非嵌入矩阵。作者发现权重矩阵更新中谱范数（最大奇异值）的失控增长是导致原生低秩训练不稳定和损失尖峰的主要因素，并提出Spectron方法：通过正交化进行谱重归一化，根据因子当前的谱范数动态限制所得权重更新。实验证明，Spectron实现了稳定、端到端的低秩训练，开销可忽略不计，并为原生低秩Transformer建立了计算最优的缩放定律，展示了可预测的幂律行为和相对于全秩模型改进的推理效率。|Eugene Belilovsky Team|[2602.12429](http://arxiv.org/abs/2602.12429)|null|
|**2026-02-12**|**Policy4OOD: A Knowledge-Guided World Model for Policy Intervention Simulation against the Opioid Overdose Crisis**|阿片类药物危机是美国严重的公共卫生问题，但由于政策互动复杂且系统动态，评估干预措施极具挑战。本文提出Policy4OOD，一个知识引导的时空世界模型，旨在整合预测、反事实推理和优化三种关键能力来有效评估阿片类政策。该模型通过策略知识图谱、州级空间依赖性及社会经济时间序列的联合编码，构建一个策略条件化的Transformer来预测阿片类药物相关结果。训练完成后，世界模型可作为模拟器，通过前向传播进行预测，通过替换历史策略编码进行反事实分析，并通过蒙特卡洛树搜索进行策略优化。实验结果表明，空间依赖性和结构化策略知识显著提高了预测准确性，验证了该模型在数据驱动的公共卫生决策支持中的潜力。|Yanfang Ye Team|[2602.12373](http://arxiv.org/abs/2602.12373)|null|
|**2026-02-12**|**Free Lunch in Medical Image Foundation Model Pre-training via Randomized Synthesis and Disentanglement**|医学图像基础模型（MIFMs）在临床任务中展现巨大潜力，但其发展受限于大规模标注数据集的稀缺、异质性和高成本。本文提出RaSD（Randomized Synthesis and Disentanglement），一个可扩展的框架，可完全利用合成数据预训练MIFMs。RaSD通过随机高斯分布模拟解剖结构和外观变异，使模型接触足够的多尺度结构和外观扰动，从而迫使其依赖不变和任务相关的解剖线索而非数据集特有纹理，实现鲁棒和可迁移的表示学习。在120万3D体和960万2D图像上进行预训练后，RaSD模型在6种成像模态、48个数据集和56个下游任务中，持续优于从零开始训练的模型，在17个任务上取得了最佳性能，并在大多数其他任务上与使用大型真实数据集预训练的模型表现相当。这些结果证明了仅合成数据即可驱动鲁棒表示学习的能力，为医学AI领域带来了范式转变。|Hao Chen Team|[2602.12317](http://arxiv.org/abs/2602.12317)|null|

<p align=right>(<a href=#updated-on-20260220>back to top</a>)</p>

## VLM

| Publish Date | Title | Chinese Summary | Authors | PDF | Code |
|:---------|:-----------------------|:------------------------|:---------|:------|:------|
|**2026-02-19**|**Pushing the Frontier of Black-Box LVLM Attacks via Fine-Grained Detail Targeting**|针对大型视觉-语言模型（LVLMs）的黑盒对抗攻击因缺少梯度和复杂的多模态边界而面临挑战，现有方法M-Attack存在梯度高方差和优化不稳定的问题。为此，本研究提出了M-Attack-V2，通过将局部匹配重构为非对称期望，并在源端引入多裁剪对齐（MCA）以降低梯度方差，在目标端引入辅助目标对齐（ATA）以生成更平滑的目标流形。此外，还提出了补丁动量（Patch Momentum）和优化的补丁尺寸集成，以强化可转移方向。实验结果表明，M-Attack-V2显著提高了LVLMs上的黑盒攻击成功率，例如在Claude-4.0上从8%提升至30%，在GPT-5上达到100%，超越了现有SOTA方法。|Zhiqiang Shen Team|[2602.17645](http://arxiv.org/abs/2602.17645)|**[link](https://github.com/vila-lab/M-Attack-V2)**|
|**2026-02-19**|**Catastrophic Forgetting Resilient One-Shot Incremental Federated Learning**|现代大数据系统产生的增量数据流给集中化和联邦学习（FL）带来了通信开销和灾难性遗忘的挑战。本论文提出了One-Shot Incremental Federated Learning (OSI-FL) 框架，旨在解决这两个难题。OSI-FL通过在单轮通信中发送由冻结视觉-语言模型（VLM）生成的客户端类别特定嵌入，并由服务器端的扩散模型合成新数据用于训练。为对抗灾难性遗忘，引入了选择性样本保留（SSR）机制，根据样本损失识别并保留最具信息量的样本。实验结果显示，OSI-FL在类别增量和域增量场景下，在多个基准数据集上均优于包括传统和一次性FL方法在内的基线。|Monowar Bhuyan Team|[2602.17625](http://arxiv.org/abs/2602.17625)|null|
|**2026-02-19**|**AI Gamestore: Scalable, Open-Ended Evaluation of Machine General Intelligence with Human Games**|评估机器智能与人类通用智能的广谱能力变得日益重要，但传统AI基准存在范围狭窄、易饱和的问题。本研究提出通过通用游戏玩法来评估类人通用智能，即考察AI系统玩和学习玩“所有可想象的人类游戏”的能力，并引入了AI GameStore平台。该平台利用大型语言模型与人工协作，通过改编流行数字游戏环境来合成新的代表性人类游戏。初步实验中，对100款生成的游戏和七个前沿视觉-语言模型进行了评估，结果显示最佳模型在大多数游戏上的得分不到人类平均分的10%，尤其在挑战世界模型学习、记忆和规划的游戏中表现不佳。|Joshua B. Tenenbaum Team|[2602.17594](http://arxiv.org/abs/2602.17594)|null|
|**2026-02-19**|**LATA: Laplacian-Assisted Transductive Adaptation for Conformal Uncertainty in Medical VLMs**|医疗视觉-语言模型（VLMs）在零样本识别方面表现良好，但在领域偏移下，其可靠性依赖于校准不确定性。分裂共形预测（SCP）虽提供覆盖率保证，但在少样本、不平衡情况下，预测集效率低且类别覆盖不平衡。为解决此问题，本研究提出了LATA (Laplacian-Assisted Transductive Adaptation)，一种“无需训练和标签”的改进方法，通过在图像-图像k-NN图上平滑零样本概率来优化校准和测试池，并引入“失败感知”共形分数以提高预测集效率和类别平衡。实验结果表明，LATA在多种医疗VLM和下游任务中，持续降低集合大小和类别条件覆盖率差距，同时匹配或收紧目标覆盖率，优于现有基线并大幅减少计算量。|Zongyuan Ge Team|[2602.17535](http://arxiv.org/abs/2602.17535)|null|
|**2026-02-19**|**Selective Training for Large Vision Language Models via Visual Information Gain**|大型视觉语言模型（LVLMs）常表现出语言偏见，即在缺乏视觉证据时也能生成答案。现有方法虽试图缓解，但缺乏量化视觉输入对训练样本或token的实际益处的指标。本研究引入了视觉信息增益（VIG），一种基于困惑度的指标，用于衡量视觉输入对预测不确定性的降低程度。VIG支持在样本和token级别进行细粒度分析，能够有效突出视觉基础元素。基于此，论文提出了一种VIG引导的选择性训练方案，优先处理高VIG的样本和token。实验结果表明，该方法通过专注于视觉信息丰富的样本和token，有效提高了视觉接地能力，减轻了语言偏见，并在显著减少监督的情况下实现了卓越性能。|Sangheum Hwang Team|[2602.17186](http://arxiv.org/abs/2602.17186)|null|
|**2026-02-18**|**Saliency-Aware Multi-Route Thinking: Revisiting Vision-Language Reasoning**|视觉-语言模型（VLMs）在推理过程中，视觉输入通常只在生成开始时提供一次，导致推理逐渐以文本为主导，并可能累积早期视觉接地错误。同时，粗糙且嘈杂的视觉接地引导难以在长文本上有效 steer 推理。针对这些挑战，本研究提出了显著性感知原则（SAP）选择方法。SAP作用于高级推理原则而非token级轨迹，能在嘈杂反馈下稳定控制离散生成，并允许后续推理步骤在需要时重新参考视觉证据。SAP还支持多路径推理。该方法无需额外训练，实验结果表明，SAP在可比的token生成预算下实现了竞争性性能，尤其在减少物体幻觉方面，同时产生更稳定的推理和更低的响应延迟。|Jundong Li Team|[2602.16702](http://arxiv.org/abs/2602.16702)|null|
|**2026-02-18**|**A Contrastive Learning Framework Empowered by Attention-based Feature Adaptation for Street-View Image Classification**|街景图像属性分类是一项计算密集型任务，现有视觉-语言模型（如CLIP）的适应或微调方法通常依赖全局图像嵌入，难以捕捉复杂街景中细粒度、局部属性。为解决此问题，本研究提出了CLIP-MHAdapter，一种轻量级CLIP适应范式变体。CLIP-MHAdapter在现有模型基础上，增加了一个配备多头自注意力机制的瓶颈多层感知器（MLP），该MLP作用于patch tokens，以有效建模patch之间的依赖关系。实验结果显示，CLIP-MHAdapter以约140万可训练参数，在Global StreetScapes数据集的八个属性分类任务中取得了卓越或具有竞争力的准确性，达到了新的SOTA水平，同时保持了较低的计算成本。|James Haworth Team|[2602.16590](http://arxiv.org/abs/2602.16590)|null|
|**2026-02-18**|**DressWild: Feed-Forward Pose-Agnostic Garment Sewing Pattern Generation from In-the-Wild Images**|针对现有服装版型生成方法在处理多样姿态和视角时的不足，以及优化方法计算昂贵的问题，本文提出了DressWild。该前馈流水线能从单张“in-the-wild”图像中，利用视觉-语言模型（VLMs）提取姿态感知、3D增强的服装特征，并通过Transformer编码器预测出物理一致的2D缝纫版型和对应的3D服装参数。实验证明，该方法无需多视图或迭代优化，即可稳健地恢复多样化版型，为逼真的服装模拟提供了高效可扩展的解决方案。|Chenfanfu Jiang Team|[2602.16502](http://arxiv.org/abs/2602.16502)|null|
|**2026-02-18**|**Visual Self-Refine: A Pixel-Guided Paradigm for Accurate Chart Parsing**|针对大型视觉-语言模型（LVLMs）在图表解析等视觉密集任务中常出现数据遗漏和幻觉的问题，本文受人类阅读策略启发，提出了视觉自我修正（VSR）范式。VSR使模型能生成并反馈像素级定位可视化结果，从而直观纠正视觉感知错误。具体而言，ChartVSR模型将解析过程分为修正和解码两阶段，迭代确保数据点定位的准确性。同时，构建了更具挑战性的ChartP-Bench基准。研究强调VSR作为一种通用视觉反馈机制，有望提升广泛视觉中心任务的准确性。|Dahua Lin Team|[2602.16455](http://arxiv.org/abs/2602.16455)|null|
|**2026-02-18**|**Designing Production-Scale OCR for India: Multilingual and Domain-Specific Systems**|为构建适应印度语言多样性和文档异构性的OCR系统，本文研究了两种多语言OCR训练策略。通过Chitrapathak系列，对比了端到端训练和微调现有模型（即便未经目标语言训练）的效果。结果表明，第二种策略在准确性与延迟之间取得了更好的平衡，Chitrapathak-2在泰卢固语OCR上达到SOTA，并实现了显著加速。此外，本文还推出了专门针对9种印度政府文档的Parichay系列，以更快的推理速度达到89.8%的精确匹配分数，为印度生产级OCR提供了实用指导。|Shubham Agarwal Team|[2602.16430](http://arxiv.org/abs/2602.16430)|null|
|**2026-02-18**|**Peeking Ahead of the Field Study: Exploring VLM Personas as Support Tools for Embodied Studies in HCI**|针对实地研究成本高昂且耗时的问题，本文受快速原型启发，提出了一种利用视觉-语言模型（VLM）角色模拟实地研究结果的低成本评估方法。为探究VLM角色模仿人类反应的能力，作者开展了平行研究：一项有真实参与者的街头过马路任务和一项使用VLM角色的视频研究。结果显示，VLM角色能够模仿人类的反应模式（如平均过马路时间），但在行为变异性和深度上有所欠缺，有望用于形成性研究、实地研究准备和人类数据增强。|Takeo Igarashi Team|[2602.16157](http://arxiv.org/abs/2602.16157)|null|
|**2026-02-18**|**Evaluating Demographic Misrepresentation in Image-to-Image Portrait Editing**|文本到图像（T2I）生成中的人口统计偏差已得到广泛研究，但指令引导的图像到图像（I2I）编辑中与人口统计相关的失败模式仍未被充分探索。本文形式化了“软擦除”和“刻板印象替换”两种失败模式，并通过一个受控基准和VLM评分及人工评估，研究了开放权重I2I编辑器中编辑指令对不同人口统计学主体产生的差异性结果。分析发现，身份保留失败普遍存在且具有人口统计学上的不均匀性，并受隐性社会先验的影响。研究还表明，提示级的身份约束可在不更新模型的情况下减少少数群体的图像编辑后人口统计变化，揭示了现有编辑器中不对称的身份先验，并呼吁开发对人口统计学鲁棒的编辑系统。|Jean Oh Team|[2602.16149](http://arxiv.org/abs/2602.16149)|null|
|**2026-02-18**|**IRIS: Intent Resolution via Inference-time Saccades for Open-Ended VQA in Large Vision-Language Models**|针对开放式视觉问答（VQA）中的歧义问题及其对大型视觉-语言模型（LVLMs）性能的限制，本文提出了IRIS（Intent Resolution via Inference-time Saccades），一种无需训练的实时眼动追踪方法。通过一项包含500个图像-问题对的用户研究，发现用户开始提问时最近的注视点对LVLMs的消歧最具信息量。实验表明，IRIS将歧义问题的回答准确率提高了一倍以上（从35.2%到77.2%），同时保持了非歧义问题的性能，并在不同架构的SOTA VLMs上均显示出一致改进。研究还发布了一个新的数据集和评估套件。|Miguel P. Eckstein Team|[2602.16138](http://arxiv.org/abs/2602.16138)|null|
|**2026-02-18**|**OmniCT: Towards a Unified Slice-Volume LVLM for Comprehensive CT Analysis**|针对现有大型视觉-语言模型（LVLMs）在CT图像切片级理解和体素级理解之间的碎片化问题，本文提出了OmniCT，一个强大的统一切片-体积LVLM。OmniCT通过空间一致性增强（SCE）引入体积一致性，并利用MoE混合投影实现高效切片-体积适应；通过器官级语义增强（OSE）强调病灶和器官级语义；并构建了最大的切片-体积CT数据集MedEval-CT进行统一评估。OmniCT在多项临床任务中大幅超越现有方法，在微观细节敏感性和宏观空间推理方面均表现出色，为跨模态医学图像理解建立了新范式。|Beng Chin Ooi Team|[2602.16110](http://arxiv.org/abs/2602.16110)|null|
|**2026-02-18**|**Narrow fine-tuning erodes safety alignment in vision-language agents**|终身多模态智能体在后训练阶段适应新任务时，能力获取与安全对齐之间存在矛盾。本研究发现，对齐的视觉-语言模型在狭域有害数据集上进行微调会导致严重的紧急不对齐，且这种不对齐会广泛泛化到无关任务和模态。通过在Gemma3-4B上的实验表明，不对齐程度随LoRA秩单调递增，且多模态评估揭示的不对齐程度远高于纯文本评估。研究进一步发现，有害行为占据低维子空间，并评估了良性窄域微调和基于激活的引导两种缓解策略。结果显示，这两种策略虽能显著减少不对齐，但均未能完全消除学到的有害行为，强调了对鲁棒持续学习框架的需求。|Shivam Raval Team|[2602.16931](http://arxiv.org/abs/2602.16931)|null|
|**2026-02-18**|**MALLVI: a multi agent framework for integrated generalized robotics manipulation**|大型语言模型（LLMs）在机器人操作任务规划中的现有方法往往以开环方式运行，缺乏鲁棒的环境反馈，导致在动态环境中不稳定。本研究提出了MALLVi，一个多智能体大型语言与视觉框架，旨在实现闭环反馈驱动的机器人操作。MALLVi协调专门智能体（如分解器、定位器、思考者、反射器）来管理感知、定位、推理和高级规划，其中反射器支持有针对性的错误检测和恢复，避免全面重新规划。在模拟和真实世界环境中的实验证明，迭代闭环多智能体协调显著提高了零样本操作任务的泛化能力和成功率。|Babak Khalaj Team|[2602.16898](http://arxiv.org/abs/2602.16898)|null|
|**2026-02-18**|**DODO: Discrete OCR Diffusion Models**|光学字符识别（OCR）是信息数字化的关键任务，但现代视觉-语言模型（VLM）通常依赖的自回归解码在处理长文档时计算成本高昂且速度缓慢。本研究指出OCR本质上是高度确定性任务，理论上可通过扩散模型实现高效并行解码，然而现有掩码扩散模型因结构不稳定性不适用于OCR。为此，本文引入了DODO，首个利用块离散扩散（block discrete diffusion）为OCR任务解锁加速潜力的VLM。DODO通过将生成分解为块来缓解全局扩散的同步错误。实验结果表明，DODO在保持接近SOTA精度的同时，推理速度比自回归基线快3倍。|Niv Nayman Team|[2602.16872](http://arxiv.org/abs/2602.16872)|null|
|**2026-02-17**|**MedProbCLIP: Probabilistic Adaptation of Vision-Language Foundation Model for Reliable Radiograph-Report Retrieval**|鉴于确定性视觉-语言基础模型在生物医学应用中可靠性不足，本文引入了MedProbCLIP，一个用于胸部X射线和放射学报告表示学习及双向检索的概率视觉-语言学习框架。MedProbCLIP通过概率对比目标将图像和文本表示建模为高斯嵌入，显式捕获不确定性和多对多对应关系，并利用变分信息瓶颈减轻过度自信预测。在MIMIC-CXR数据集上的评估表明，MedProbCLIP在检索和零样本分类方面均优于现有基线，并展示出卓越的校准性、风险覆盖行为和对临床损坏的鲁棒性，强调了概率建模在提高医学图像-文本检索系统信任度和安全性方面的价值。|Gongbo Liang Team|[2602.16019](http://arxiv.org/abs/2602.16019)|null|
|**2026-02-17**|**BTReport: A Framework for Brain Tumor Radiology Report Generation with Clinically Relevant Features**|针对神经肿瘤学放射学报告生成（RRG）领域因缺乏配对图像-报告数据集而进展受限的问题，本文提出了BTReport，一个开源的脑肿瘤RRG框架。BTReport通过将RRG过程解耦为确定性特征提取和报告生成两步，使用确定性提取的成像特征构建自然语言报告，而大型语言模型仅用于句法和叙事格式化。这种方法使得生成的报告完全可解释且不易产生幻觉。研究表明，该特征可预测关键临床结果，且BTReport生成的报告与参考临床报告更吻合。此外，本文还推出了增强BraTS图像的配套数据集BTReport-BraTS。|Mehmet Kurt Team|[2602.16006](http://arxiv.org/abs/2602.16006)|null|
|**2026-02-17**|**Can Vision-Language Models See Squares? Text-Recognition Mediates Spatial Reasoning Across Three Model Families**|本文通过一个简单实验揭示了视觉-语言模型（VLMs）在定位缺乏文本标识的二元网格填充单元格时存在的根本性限制。实验中，要求三个前沿VLM转录两种图像类型的15x15网格：文本符号（.和#）和无网格线的实心方块。结果显示，在文本符号条件下，VLM表现良好，但在实心方块条件下，所有模型的准确率和F1分数均大幅下降。这表明VLMs可能拥有一条高保真的文本识别路径，其空间推理能力显著优于其原生视觉路径。尽管各模型在非文本视觉元素上表现出不同的失败模式，但都揭示了其空间定位能力严重下降的共同缺陷。|Yuval Levental Team|[2602.15950](http://arxiv.org/abs/2602.15950)|null|
|**2026-02-17**|**Visual Memory Injection Attacks for Multi-Turn Conversations**|鉴于大型生成式视觉-语言模型（LVLM）在长上下文多轮对话中安全性的研究不足，本论文提出了一种名为视觉记忆注入（VMI）的隐蔽攻击。该攻击通过操纵图像，使得LVLM在正常提示下行为正常，但在触发提示下输出预设的目标信息以操纵用户。实验证明，VMI攻击在多轮对话中依然有效，并在多个开源LVLM上得到验证，揭示了通过扰动图像进行大规模用户操纵的可行性，强调了提高LVLM对抗此类攻击的鲁棒性需求。|Matthias Hein Team|[2602.15927](http://arxiv.org/abs/2602.15927)|null|
|**2026-02-17**|**Concept-Enhanced Multimodal RAG: Towards Interpretable and Accurate Radiology Report Generation**|针对放射学报告生成（RRG）中视觉-语言模型（VLM）存在的解释性差和易产生“幻觉”的问题，本研究提出了概念增强多模态检索增强生成（CEMRAG）框架。该框架将视觉表示分解为可解释的临床概念，并将其与多模态RAG结合，通过丰富的上下文提示提升RRG的解释性和事实准确性。实验结果表明，CEMRAG在多个数据集和VLM架构下，在临床准确性和NLP指标上均优于现有基线，挑战了可解释性与性能之间的权衡假设，为医疗VLM提供了可信赖的AI辅助放射学途径。|Valerio Guarrasi Team|[2602.15650](http://arxiv.org/abs/2602.15650)|null|
|**2026-02-17**|**CARE Drive A Framework for Evaluating Reason-Responsiveness of Vision Language Models in Automated Driving**|针对自动驾驶中基础模型评估主要关注结果性能而忽视决策是否反映人类相关考量的问题，本研究提出了CARE Drive框架。该框架独立于模型，通过在受控上下文变化下比较基线和原因增强模型决策，评估人类原因对决策行为的因果影响。在一个骑车人超车场景中，结果显示明确的人类原因显著影响模型决策，提高了与专家推荐行为的一致性，证明了可以在不修改模型参数的情况下系统评估基础模型的原因响应性。|Arkady Zgonnikov Team|[2602.15645](http://arxiv.org/abs/2602.15645)|null|
|**2026-02-17**|**Req2Road: A GenAI Pipeline for SDV Test Artifact Generation and On-Vehicle Execution**|为解决软件定义汽车（SDV）功能测试中需求分散、规范复杂以及测试资产异构的挑战，本研究提出了一种自动化管道。该管道利用大型语言模型和视觉-语言模型提取信号和行为逻辑，自动生成Gherkin场景并转换为可运行的测试脚本，并通过VSS集成和检索增强生成（RAG）实现标准化。在儿童存在检测系统（CPDS）上的评估显示，89%的需求可转化为可执行场景，验证了该端到端需求-测试管道在仿真和在环测试环境中的可行性，但仍需人工审查。|Alois Knoll Team|[2602.15591](http://arxiv.org/abs/2602.15591)|null|
|**2026-02-17**|**VLM-DEWM: Dynamic External World Model for Verifiable and Resilient Vision-Language Planning in Manufacturing**|针对视觉-语言模型（VLM）在智能制造动态工作单元中面临的无状态操作和不透明推理挑战，本研究提出了VLM-DEWM认知架构。该架构通过一个持久且可查询的动态外部世界模型（DEWM），将VLM推理与世界状态管理解耦，并结构化每个VLM决策为可外部化的推理轨迹。实验结果表明，VLM-DEWM显著提高了状态跟踪准确性（从56%提升至93%）和恢复成功率（从低于5%提升至95%），并通过结构化内存减少了计算开销，为动态制造环境中的长周期机器人操作提供了可靠且弹性的解决方案。|Ning Ji Team|[2602.15549](http://arxiv.org/abs/2602.15549)|null|
|**2026-02-17**|**Selective Perception for Robot: Task-Aware Attention in Multimodal VLA**|针对机器人领域中视觉-语言-动作（VLA）模型静态融合多视角输入导致的计算冗余和噪声问题，本研究提出了一种动态信息融合框架。该框架引入轻量级自适应路由架构，实时分析文本提示和腕部摄像头观测，预测多摄像头视图的任务相关性，并有选择地向策略网络提供必要视觉特征。为路由器训练，本研究还开发了利用VLM的自动化标注管道。实验结果表明，该方法在真实世界机器人操纵场景中显著提高了推理效率和控制性能，验证了动态信息融合在资源受限实时机器人控制环境中的有效性。|Soo-Chul Lim Team|[2602.15543](http://arxiv.org/abs/2602.15543)|null|
|**2026-02-17**|**Semantic-Guided 3D Gaussian Splatting for Transient Object Removal**|为解决3D Gaussian Splatting (3DGS) 重建中瞬态物体导致的重影伪影问题，本研究提出了一种基于视觉-语言模型（VLM）的语义过滤框架。该方法通过计算渲染视图与干扰文本提示之间的CLIP相似度得分，并累积到每个高斯函数上，对超出校准阈值的高斯函数进行不透明度正则化和周期性修剪。实验结果表明，该方法在RobustNeRF基准测试中持续改善了重建质量，同时保持了最小内存开销和实时渲染性能，通过语义分类有效解决了视差模糊问题。|Priyesh Shukla Team|[2602.15516](http://arxiv.org/abs/2602.15516)|null|
|**2026-02-17**|**On the Out-of-Distribution Generalization of Reasoning in Multimodal LLMs for Simple Visual Planning Tasks**|本研究旨在深入探究大型语言模型和视觉-语言模型中思维链（CoT）推理方法的泛化能力，尤其是在简单规划任务上的表现。研究者提出了一个评估框架，对基于网格的导航任务中不同输入表示和CoT策略的模型进行了微调和系统评估。实验结果表明，CoT推理能提高分布内泛化能力，但对分布外（如更大地图）的泛化能力多数情况下仍非常有限；值得注意的是，结合多种文本格式的推理轨迹产生了最佳的分布外泛化效果，且纯文本模型表现优于基于图像输入的模型。|Francesco Croce Team|[2602.15460](http://arxiv.org/abs/2602.15460)|null|
|**2026-02-16**|**BPP: Long-Context Robot Imitation Learning by Focusing on Key History Frames**|在机器人任务中，传统策略因仅依赖当前观测而无法有效利用历史信息，导致在需要记忆的任务中泛化性差，尤其是在部署时容易受训练中虚假关联的影响。为解决此问题，本研究提出大图策略（BPP），通过视觉-语言模型识别并利用一组最小的关键帧作为历史观测条件。实验结果表明，BPP显著减少了训练与部署间的分布偏移，并在四项真实世界操作任务和三项仿真任务中，成功率比现有最佳方法提高了70%。|Aviral Kumar Team|[2602.15010](http://arxiv.org/abs/2602.15010)|null|
|**2026-02-16**|**ThermEval: A Structured Benchmark for Evaluation of Vision-Language Models on Thermal Imagery**|当前视觉语言模型（VLMs）在RGB图像上表现优异，但无法泛化至夜间监控、搜救等关键场景的热成像图像，且现有基准无法评估其对温度感知和推理能力。本研究引入ThermEval-B，一个包含5.5万个热视觉问答对的结构化基准，并整合了ThermEval-D数据集，首次提供带有语义身体部位标注的密集逐像素温度图。实验评估25个VLM后发现，模型在温度推理上普遍失败，在色图变换下性能下降，且容易依赖语言先验，提示或微调仅带来微弱提升，证实了热图像理解需专门评估。 |Nipun Batra Team|[2602.14989](http://arxiv.org/abs/2602.14989)|null|
|**2026-02-16**|**DM0: An Embodied-Native Vision-Language-Action Model towards Physical AI**|针对传统方法将互联网预训练模型适配到物理任务的局限性，本研究提出了DM0，一个具身原生（Embodied-Native）的视觉-语言-动作（VLA）框架，旨在为物理AI提供统一的具身操作和导航能力。该框架采用三阶段训练流水线：首先对VLM进行大规模统一预训练，整合网络文本、自动驾驶和具身交互日志数据，随后构建流匹配动作专家，并通过混合训练策略及具身空间支架策略实现高层推理与低层控制的协调。DM0在RoboChallenge基准测试中，于专业和通用设置下均取得了最先进的性能。|Tiancai Wang Team|[2602.14974](http://arxiv.org/abs/2602.14974)|**[link](https://github.com/Dexmal/dexbotic)**|
|**2026-02-16**|**MATEO: A Multimodal Benchmark for Temporal Reasoning and Planning in LVLMs**|AI智能体实现复杂目标需要精细规划，其中时间执行顺序（TEO）至关重要，但现有基础模型对TEO的理解研究不足，多限于线性近似或纯文本输入。为弥补这一空白，本研究引入MATEO基准，旨在评估和提升大型视觉语言模型（LVLMs）的时间推理能力。通过收集高质量多模态菜谱语料并利用众包构建TEO图谱，MATEO提供了丰富的标注。对六个主流LVLMs的评估揭示了语言上下文、多模态输入结构和微调策略对时间推理能力的关键影响，突显了当前LVLMs在该领域的局限性。|Giuseppe Riccardi Team|[2602.14589](http://arxiv.org/abs/2602.14589)|null|
|**2026-02-16**|**Replanning Human-Robot Collaborative Tasks with Vision-Language Models via Semantic and Physical Dual-Correction**|人机协作（HRC）在装配任务中面临人类指令模糊导致机器人行为不可靠的问题，现有基于VLM的方法虽能解释指令，但易产生幻觉推理和物理执行失败。为此，本研究提出了一个HRC框架，通过引入双重校正机制增强VLM的推理能力。该机制包含一个内部校正模型在执行前验证逻辑和可行性，以及一个外部校正模型通过事后反馈纠正物理失败。仿真和真实世界实验均表明，该框架显著提高了任务成功率，并能有效支持机器人根据人类指令进行交互式重规划。|Kensuke Harada Team|[2602.14551](http://arxiv.org/abs/2602.14551)|null|
|**2026-02-16**|**Error Patterns in Historical OCR: A Comparative Analysis of TrOCR and a Vision-Language Model**|18世纪印刷文本的OCR因降级打印、古体字和非标准化拼写而充满挑战，现有基于Transformer和VLM的OCR系统虽总体准确率高，但CER和WER等指标对学术使用可靠性洞察有限。本研究通过比较专用OCR Transformer（TrOCR）和通用VLM（Qwen）在历史英文文本线级识别上的表现，发现Qwen虽在总体准确率和鲁棒性方面占优，但存在选择性语言正则化和拼写规范化，可能改变历史原貌。TrOCR则能更好地保持拼写忠实度，但易出现级联错误。研究强调了架构归纳偏差对OCR错误结构的影响，以及在历史文献数字化中进行架构感知评估的必要性。|Mikko Tolonen Team|[2602.14524](http://arxiv.org/abs/2602.14524)|null|
|**2026-02-16**|**S2D: Selective Spectral Decay for Quantization-Friendly Conditioning of Neural Activations**|大规模Transformer模型中的激活异常值对模型量化构成严重挑战，导致量化精度显著下降，且随预训练规模的增加而加剧。本研究通过理论分析和实证观察，揭示了激活异常值与权重的主奇异值之间的直接联系。在此基础上，提出选择性谱衰减（S²D）方法，在微调阶段仅对最大奇异值对应的权重分量进行正则化。实验证明S²D显著减少了激活异常值，使得模型在W4A4量化下PTQ精度提升高达7%，结合QAT时提升4%，且泛化至下游任务和视觉-语言模型，提高了大规模模型的部署效率。|Deepak Gupta Team|[2602.14432](http://arxiv.org/abs/2602.14432)|null|
|**2026-02-16**|**Multi-Turn Adaptive Prompting Attack on Large Vision-Language Models**|针对多轮越狱攻击在大型视觉-语言模型（LVLMs）中因视觉输入过于恶意而易被防御的问题，本研究提出了MAPA（Multi-turn Adaptive Prompting Attack）方法。MAPA在每个回合中交替使用文本和视觉攻击动作以引发最恶意响应，并在跨回合中通过迭代式来回调整攻击轨迹，逐步放大响应的恶意性。这种双层设计使MAPA持续优于现有最先进方法，在对抗Llava-V1.6-Mistral-7B、Qwen2.5-VL-7B-Instruct、Llama-3.2-Vision-11B-Instruct和GPT-4o-mini等模型的最新基准测试中，攻击成功率提高了11-35%。|Yiliao Song Team|[2602.14399](http://arxiv.org/abs/2602.14399)|null|
|**2026-02-15**|**Moving Beyond Sparse Grounding with Complete Screen Parsing Supervision**|现代计算机使用智能体（CUA）需要对屏幕进行结构化感知才能可靠地理解指令和执行操作，但现有标注数据集稀疏且多样性不足，限制了其泛化能力，且实际部署需要高效率。本研究引入ScreenParse，一个用于完整屏幕解析的大规模数据集，包含771K个网页截图的密集标注。通过Webshot自动化流程及VLM辅助，ScreenParse提供了详尽的UI元素信息。基于此数据集，我们训练了紧凑型ScreenVLM，其在密集解析上显著优于更大规模的基础VLM，并在公共基准上展现了强大的迁移能力，证明了密集屏幕监督能为UI理解提供可迁移的结构先验。|Peter Staar Team|[2602.14276](http://arxiv.org/abs/2602.14276)|null|
|**2026-02-15**|**Dual-Signal Adaptive KV-Cache Optimization for Long-Form Video Understanding in Vision-Language Models**|视觉-语言模型（VLMs）在处理长视频时面临内存瓶颈，因KV缓存随序列长度线性增长，而现有驱逐策略先计算完整注意力矩阵再丢弃token，导致计算浪费。本研究提出Sali-Cache，一个先验优化框架，通过主动内存管理实现双信号自适应缓存。该方法结合光流分析的时间滤波器和显著性检测的空间滤波器，在注意力操作前智能管理内存。实验表明，Sali-Cache在LLaVA 1.6架构上实现了2.20倍的内存压缩比，同时保持100%的准确率，并在相同内存预算下能更长时间保留上下文特征，实现了在消费级硬件上高效处理长视频内容。|Priyesh Shukla Team|[2602.14236](http://arxiv.org/abs/2602.14236)|null|
|**2026-02-15**|**Index Light, Reason Deep: Deferred Visual Ingestion for Visual-Dense Document Question Answering**|当前多模态文档问答系统采用“供给侧摄取”策略，即在索引阶段全面生成视觉描述，导致成本高昂且不可靠。本文提出了“延迟视觉摄取（DVI）”框架，该框架采用“需求侧摄取”策略，仅在索引阶段进行轻量级元数据提取以实现页面定位，将视觉理解延迟到用户提问时，再将原始图像与特定问题发送给视觉语言模型进行分析。实验结果表明，DVI在零摄取视觉语言模型成本下，取得了与现有方法相当的整体准确率（46.7% vs 48.9%），在视觉必要查询上的有效率达50%，并实现了100%的页面定位率，有效将问答准确率问题转化为页面定位问题。|Tao Xu Team|[2602.14162](http://arxiv.org/abs/2602.14162)|null|
|**2026-02-15**|**Annotation-Efficient Vision-Language Model Adaptation to the Polish Language Using the LLaVA Framework**|现有视觉-语言模型（VLMs）多基于英语数据训练，限制了其在其他语言和文化背景下的应用。为解决此问题，本研究复现并调整了LLaVA-Next方法，通过全自动化流程翻译、过滤现有数据集并补充合成数据，构建了一套波兰语VLM。实验结果显示，该方法在波兰语改编的MMBench上相较于LLaVA-1.6-Vicuna-13B实现了9.5%的性能提升，并在生成性评估中，其生成的标题在语言正确性方面获得了更高评价，证明大规模自动化翻译结合轻量级过滤能有效为低资源语言引导高质量多模态模型。|Wojciech Kusa Team|[2602.14073](http://arxiv.org/abs/2602.14073)|null|
|**2026-02-15**|**MarsRetrieval: Benchmarking Vision-Language Models for Planetary-Scale Geospatial Retrieval on Mars**|行星科学中，现有深度学习基准多局限于监督视觉任务，不支持文本引导的地理空间发现。为此，本研究引入了MarsRetrieval，一个用于评估视觉-语言模型在火星地理空间发现能力的检索基准，包含图像-文本检索、地貌检索和全球地理定位等任务。研究提出统一的检索协议以评估多模态嵌入架构。实验结果表明MarsRetrieval极具挑战性，即使是强大的基础模型也难以捕捉领域特定的地貌区别，且领域特定微调对于行星环境中的可泛化地理空间发现至关重要。|Hongxin Wei Team|[2602.13961](http://arxiv.org/abs/2602.13961)|null|
|**2026-02-14**|**RMPL: Relation-aware Multi-task Progressive Learning with Stage-wise Training for Multimedia Event Extraction**|多媒体事件抽取（MEE）受标注数据缺乏限制，现有基准M2E2仅提供评估标注，导致直接监督训练困难，现有方法未能有效学习结构化事件表示。为解决这些局限，本文提出了RMPL（Relation-aware Multi-task Progressive Learning）框架，用于低资源条件下的MEE。RMPL通过阶段式训练，整合了来自单模态事件抽取和多媒体关系抽取的异构监督，先学习事件中心表示，再进行微调。实验结果表明，在M2E2基准上，结合多个视觉语言模型的RMPL在不同模态设置下均显示出持续的性能改进。|Yu Hong Team|[2602.13748](http://arxiv.org/abs/2602.13748)|null|
|**2026-02-14**|**Fine-tuned Vision Language Model for Localization of Parasitic Eggs in Microscopic Images**|土壤传播性蠕虫（STH）感染在全球范围广泛，但诊断专业知识有限，手动显微镜诊断耗时且易错。为实现自动化诊断，本文旨在利用经过微调的视觉语言模型（VLM），例如Microsoft Florence，来定位显微图像中的所有寄生虫卵。初步实验结果显示，该定位VLM的mIOU达到了0.94，优于其他目标检测方法，表明其有望成为自动化框架的核心组件，为智能寄生虫诊断提供可扩展的工程解决方案。|Nouar AlDahoul Team|[2602.13712](http://arxiv.org/abs/2602.13712)|null|
|**2026-02-14**|**LeafNet: A Large-Scale Dataset and Comprehensive Benchmark for Foundational Vision-Language Understanding of Plant Diseases**|基础模型和视觉-语言预训练在VLM领域取得显著进展，但在植物病理学等农业特定领域的应用受限于缺乏大规模、全面的多模态数据集和基准。为弥补此空白，本研究引入了LeafNet数据集和LeafBench视觉问答基准，涵盖97种病害的18.6万张图像和13,950个问答对。对12个先进VLM的基准测试揭示了其疾病理解能力的显著差异，二元分类准确率超90%，但细粒度识别低于65%。研究证实，多模态架构整合语言表示显著增强了诊断精度，凸显了LeafBench对VLM在植物病理学应用中方法学进展和评估的重要性。|Luyl-Da Quach Team|[2602.13662](http://arxiv.org/abs/2602.13662)|null|
|**2026-02-14**|**KorMedMCQA-V: A Multimodal Benchmark for Evaluating Vision-Language Models on the Korean Medical Licensing Examination**|当前视觉-语言模型（VLM）评估基准多为英语或通用领域，缺乏针对韩语医疗领域的多模态问答基准。为此，本研究引入了KorMedMCQA-V，一个韩语医学执照考试风格的多模态多项选择问答基准，包含1534个问题和2043张图像，涵盖多种临床模态，且约30%的问题需整合多图像证据。在统一零样本评估协议下，对50多个VLM的基准测试显示，最佳专有模型准确率达96.9%，最佳开源模型为83.7%，而最佳韩语专用模型仅为43.2%。研究还发现推理导向模型性能显著提升，医学领域专业化收益不一，所有模型在多图像问题上表现下降，且性能因成像模态而异。|Edward Choi Team|[2602.13650](http://arxiv.org/abs/2602.13650)|null|
|**2026-02-14**|**Towards Sparse Video Understanding and Reasoning**|现有视频问答（VQA）方法通常均匀采样视频帧，效率低下且未能有效捕捉关键信息，在多轮VQA中存在挑战。本研究提出了REVISE（Reasoning with Video Sparsity），一个多轮视频问答智能体，它选择少量信息帧，在多轮中维护摘要状态，并在有信心时提前停止。为微调开源模型，引入了EAGER（Evidence-Adjusted Gain for Efficient Reasoning）这一无标注奖励机制，包含置信度增益、摘要充分性和正确且提前停止三项。在多个VQA基准测试中，REVISE在提高准确率的同时，显著减少了帧数、轮次和提示token，展现了实用的稀疏视频推理能力。|Han Liu Team|[2602.13602](http://arxiv.org/abs/2602.13602)|null|
|**2026-02-14**|**AdaVBoost: Mitigating Hallucinations in LVLMs via Token-Level Adaptive Visual Attention Boosting**|大规模视觉-语言模型（LVLMs）存在幻觉问题，现有视觉注意力增强方法通过预定义缩放缓解，但固定缩放因子可能在不同生成步骤中表现出过弱或过强的局限性。为解决此问题，本文提出了AdaVBoost，一个token级别的自适应视觉注意力增强框架，旨在每个生成步骤动态确定注意力增强的程度。该框架引入视觉接地熵（VGE）来估计幻觉风险，并根据VGE对高风险token施加强视觉注意力增强，对低风险token施加弱增强。实验结果表明，AdaVBoost在多个LVLMs和幻觉基准测试中显著优于基线方法。|Tianyu Pang Team|[2602.13600](http://arxiv.org/abs/2602.13600)|null|
|**2026-02-14**|**OpAgent: Operator Agent for Web Navigation**|自主网络智能体在复杂且不稳定的真实网站环境中，面临现有SFT或离线RL方法因分布漂移而导致的性能局限。本文提出了一个强大的在线强化学习WebAgent，通过与非受限广域网站的直接迭代交互来优化策略。该方法包含分层多任务微调，建立了强大的VLM；开发了在线交互环境和RL管道，引入混合奖励机制缓解长期导航中的信用分配挑战；并提出了OpAgent模块化框架，整合规划器、接地器、反射器和总结器以实现错误恢复和自校正。实验结果显示，RL增强模型在WebArena上成功率达38.1%，而OpAgent框架进一步提升至71.6%，达到新的SOTA水平。|Peng Di Team|[2602.13559](http://arxiv.org/abs/2602.13559)|null|
|**2026-02-13**|**Steerable Vision-Language-Action Policies for Embodied Reasoning and Hierarchical Control**|预训练视觉语言模型(VLMs)拥有丰富的常识先验知识，但在机器人控制中有效落地仍面临挑战，现有分层方法中VLM对低层行为的引导受限于自然语言接口。为此，研究者提出了“可操控策略”(Steerable Policies)，通过在不同抽象级别（如子任务、动作、像素坐标）的丰富合成指令上训练视觉语言动作模型(VLA)，以提升低层可控性并释放VLM的预训练知识。实验结果表明，无论是通过学习型高层具身推理器还是即插即用VLM控制，Steerable Policies在真实的机器人操作实验中均优于现有基线，尤其在泛化和长任务方面表现出色。|Sergey Levine Team|[2602.13193](http://arxiv.org/abs/2602.13193)|null|
|**2026-02-13**|**Implicit-Scale 3D Reconstruction for Multi-Food Volume Estimation from Monocular Images**|现有膳食评估方法多依赖单图像分析或基于外观的推断，缺乏明确几何推理且对尺度模糊敏感，难以在真实用餐场景中准确估计食物份量。本研究提出了Implicit-Scale 3D Reconstruction from Monocular Multi-Food Images基准数据集，将食物份量估计重构为单目观测下的隐式尺度3D重建问题，通过餐盘、餐具等上下文线索而非显式度量来推断尺度，并着重于复杂的多食物场景。实验结果显示，几何重建方法相比强大的视觉语言基线，在准确性和鲁棒性上均有提升，最佳方法在体积估计上达到了0.21 MAPE，几何精度为5.7 L1 Chamfer Distance。|Jiangpeng He Team|[2602.13041](http://arxiv.org/abs/2602.13041)|**[link](https://www.kaggle.com/competitions/3d-reconstruction-from-monocular-multi-food-images/data)**|
|**2026-02-13**|**Training-Free Acceleration for Document Parsing Vision-Language Model with Hierarchical Speculative Decoding**|文档解析是多模态理解中的核心任务，但基于视觉语言模型(VLM)的端到端方法在处理长文档时，常因自回归生成长序列而导致显著的推理延迟。针对这一问题，本研究提出了一种免训练且高效的加速方法，借鉴推测解码的思想，使用轻量级文档解析流水线作为草稿模型预测未来批次token，并由更精确的VLM并行验证。此外，该方法还利用文档的布局结构将页面划分为独立区域进行并行解码。实验结果表明，该方法在通用OmniDocBench上为dots.ocr模型提供了2.42倍的无损加速，在长文档解析任务上加速高达4.89倍。|Lianwen Jin Team|[2602.12957](http://arxiv.org/abs/2602.12957)|null|
|**2026-02-13**|**HoRAMA: Holistic Reconstruction with Automated Material Assignment for Ray Tracing using NYURay**|下一代无线网络需要精确的特定站点确定性信道传播预测，而无线射线追踪(RT)依赖高精度3D环境模型和材料属性，手动建模耗时且传统视觉3D重建方法缺乏RT兼容性。为此，本研究提出了HoRAMA（Holistic Reconstruction with Automated Material Assignment）系统，该系统能利用智能手机捕获的RGB视频，结合MASt3R-SLAM的密集点云生成和视觉语言模型辅助的材料分配，自动生成RT兼容的3D模型。实验结果表明，HoRAMA的射线追踪预测在匹配多径分量功率预测方面与手动创建的3D模型基线表现相当（2.28 dB RMSE vs 2.18 dB），同时将3D重建时间从两个月缩短到16小时，显著提高了效率。|Theodore S. Rappaport Team|[2602.12942](http://arxiv.org/abs/2602.12942)|null|
|**2026-02-13**|**RoadscapesQA: A Multitask, Multimodal Dataset for Visual Question Answering on Indian Roads**|理解道路场景对自动驾驶至关重要，但现有数据集可能无法充分覆盖印度复杂多样的驾驶环境。本研究推出了Roadscapes，一个多任务多模态数据集，包含多达9,000张在印度不同驾驶环境中拍摄的图像，并附带手动验证的边界框。该数据集利用基于规则的启发式方法推断场景属性，并生成用于对象定位、推理和场景理解的问答对，涵盖了印度城市和乡村的多种昼夜场景。Roadscapes旨在推动非结构化环境中视觉场景理解的研究，并提供了使用视觉语言模型进行图像问答任务的初步基线。|Jyothikamalesh S Team|[2602.12877](http://arxiv.org/abs/2602.12877)|null|
|**2026-02-13**|**Thinking Like a Radiologist: A Dataset for Anatomy-Guided Interleaved Vision Language Reasoning in Chest X-ray Interpretation**|放射学诊断涉及视觉检查与语言推理的反复交织，但现有医学大型视觉语言模型(LVLMs)多依赖纯文本思维链推理，易产生幻觉，且现有伪视觉解决方案仍缺乏丰富的视觉细节。为此，本研究提出了MMRad-IVL-22K，这是首个专为胸部X射线解读中原生交织的视觉语言推理设计的大规模数据集，反映了放射科医生反复推理和视觉检查的工作流程。实验结果表明，多模态思维链引导的报告生成在临床准确性和报告质量方面显著优于纯文本思维链（RadGraph指标提高6%），证实高保真交织视觉语言证据是可靠医疗AI不可替代的组成部分。在MMRad-IVL-22K上微调的模型在推理一致性和报告质量方面也优于通用和医学专用LVLMs。|Wei Shen Team|[2602.12843](http://arxiv.org/abs/2602.12843)|null|
|**2026-02-13**|**X-SYS: A Reference Architecture for Interactive Explanation Systems**|可解释AI (XAI) 方法虽多，但将其部署为交互式系统仍面临挑战，因其需要兼顾算法与系统能力以维持解释可用性。本研究将可解释性视为一个信息系统问题，提出了X-SYS，一个交互式解释系统的参考架构。X-SYS围绕STAR（可伸缩性、可追溯性、响应性、适应性）四个质量属性，并指定了包含XUI服务、解释服务等五个组件的分解结构，将交互模式映射到系统能力以解耦用户界面和后端计算。通过SemanticLens系统实现X-SYS，展示了其如何通过契约化服务边界实现独立演进、通过离线/在线分离确保响应性以及通过持久状态管理支持可追溯性，为在操作约束下设计交互式解释系统提供了可重用蓝图和具体实例。|Sebastian Lapuschkin Team|[2602.12748](http://arxiv.org/abs/2602.12748)|null|
|**2026-02-13**|**SignScene: Visual Sign Grounding for Mapless Navigation**|导航标志能帮助人类在陌生环境中无地图导航，但机器人如何利用标志进行无地图导航是一个挑战，核心在于如何解释复杂多样的标志及其抽象语义内容，并将其与局部3D场景匹配。本研究将此形式化为“标志接地”(sign grounding)问题，即把标志上的语义指令映射到对应的场景元素和导航动作。研究者利用视觉语言模型(VLMs)的语义常识和推理能力，并提出了SignScene，一种以标志为中心的空-语义表示，旨在以利于VLM有效推理的形式呈现导航相关场景元素和标志信息。在包含九种环境类型、114个查询的数据集上，该方法实现了88%的接地准确率，显著优于基线，并成功使Spot机器人在真实世界中仅依赖标志进行无地图导航。|David Hsu Team|[2602.12686](http://arxiv.org/abs/2602.12686)|null|
|**2026-02-13**|**IndicFairFace: Balanced Indian Face Dataset for Auditing and Mitigating Geographical Bias in Vision-Language Models**|视觉语言模型(VLMs)常从训练数据中继承并放大社会偏见，印度群体尤其被错误代表，现有公平性数据集将印度视为单一类别，忽视了其内部地理多样性。为解决这一局限，本研究提出了IndicFairFace，一个包含14,400张图像的新颖且平衡的人脸数据集，旨在代表印度的地理多样性，图像伦理获取并在各邦和性别间均匀平衡。通过IndicFairFace，研究量化了基于CLIP的VLM中存在的印度国内地理偏见，并利用后验迭代零空间投影去偏方法成功减少了这种偏见。实验证明，该去偏方法对现有嵌入空间的影响很小，基准数据集上的检索准确率平均下降不到1.5%，确立了IndicFairFace作为研究印度背景下VLM地理偏见的第一个基准。|Jiechao Gao Team|[2602.12659](http://arxiv.org/abs/2602.12659)|null|
|**2026-02-13**|**PISHYAR: A Socially Intelligent Smart Cane for Indoor Social Navigation and Multimodal Human-Robot Interaction for Visually Impaired People**|现有智能助行设备多侧重物理导航，但缺乏社交智能和多模态人机交互能力。本研究提出了PISHYAR，一款结合社交感知导航和多模态人机交互的智能拐杖。该系统包含社交导航框架（集成RGB-D感知、对象检测、活动识别、路径规划和触觉反馈）和代理式多模态LLM-VLM交互框架（集成语音识别、VLM、LLM和文本转语音，并支持动态模式路由）。通过仿真、真实世界实验和用户研究，PISHYAR在避障和社会顺从性导航中表现可靠，整体准确率约80%，集体活动识别稳健。初步用户研究显示，视障用户对其交互框架的可用性、信任度和感知社交性给予高度评价，突显了PISHYAR作为多模态辅助移动设备在提供社交互动支持方面的潜力。|Alireza Taheri Team|[2602.12597](http://arxiv.org/abs/2602.12597)|null|
|**2026-02-13**|**On Robustness and Chain-of-Thought Consistency of RL-Finetuned VLMs**|在视觉语言模型（VLM）中，尽管强化学习（RL）微调能提升推理任务性能，但仍面临视觉基础薄弱、幻觉和过度依赖文本线索的问题。研究发现，简单的文本扰动（如误导性描述）会显著降低模型的鲁棒性和置信度，并揭示了RL微调中存在的准确性与忠实性之间的权衡。具体来说，微调提高了基准准确性，却可能损害推理链的可靠性和模型对上下文变化的鲁棒性。这些结果强调了仅凭准确性评估的局限性，并呼吁在训练和评估中同时关注正确性、鲁棒性以及视觉基础推理的忠实性。|Arnab Mondal Team|[2602.12506](http://arxiv.org/abs/2602.12506)|null|
|**2026-02-13**|**Layer-Specific Fine-Tuning for Improved Negation Handling in Medical Vision-Language Models**|由于视觉语言模型（VLM）在区分肯定和否定医疗陈述方面存在局限性，本研究引入了一个放射学诊断基准来系统评估VLM对极性的敏感性。为解决此问题，我们构建了一个包含结构化声明和属性级否定上下文临床否定数据集，并提出了一种名为否定感知选择性训练（NAST）的自适应方法。NAST利用因果追踪效应（CTE）根据各层对否定处理的因果贡献来调整梯度更新。实验结果表明，NAST在不损害通用视觉-语言对齐的情况下，显著提高了VLM对肯定和否定临床陈述的辨别能力，凸显了因果可解释性在安全关键医疗领域中进行有针对性模型适应的价值。|Rahmatollah Beheshti Team|[2602.12498](http://arxiv.org/abs/2602.12498)|null|
|**2026-02-12**|**Scaling Verification Can Be More Effective than Scaling Policy Learning for Vision-Language-Action Alignment**|针对视觉-语言-动作（VLA）模型在执行自然语言指令时存在的"意图-动作差距"问题，本研究探索了测试时验证方法。我们首先揭示了具身指令遵循的测试时标度定律，发现联合扩展复述指令和生成动作数量能更高效地恢复正确动作。为利用这些规律，我们提出了CoVer，一个对比验证器，并引入了“启动时计算”和分层验证推理管道。在部署时，该框架预先计算VLM生成的复述指令，生成动作候选项，然后使用验证器选择最优提示和动作块。实验结果表明，CoVer在SIMPLER基准上取得了显著提升（分布内22%，分布外13%），并在真实世界实验中进一步提升45%，在PolaRiS基准上任务进展提升14%，成功率提升9%。|Marco Pavone Team|[2602.12281](http://arxiv.org/abs/2602.12281)|null|
|**2026-02-12**|**ExStrucTiny: A Benchmark for Schema-Variable Structured Information Extraction from Document Images**|尽管通用视觉语言模型（VLM）在传统文档理解基准上表现良好，但它们在多样化文档类型和灵活模式下进行整体、细粒度结构化信息提取的能力仍未充分研究。现有数据集在实体本体、查询复杂度或文档类型上存在局限。为解决这些不足，本研究引入了ExStrucTiny，一个新的文档图像结构化信息提取（IE）基准数据集，它统一了关键实体提取、关系提取和视觉问答的特性。该数据集通过结合人工和合成并经过人工验证的样本构建，涵盖了更广泛的文档类型和提取场景。对开放和封闭式VLM在该基准上的分析揭示了模式适应、查询欠规范和答案定位等挑战，为未来改进通用模型在文档结构化信息提取方面的能力奠定了基础。|Manuela Veloso Team|[2602.12203](http://arxiv.org/abs/2602.12203)|null|
|**2026-02-12**|**3DGSNav: Enhancing Vision-Language Model Reasoning for Object Navigation via Active 3D Gaussian Splatting**|针对零样本对象导航（ZSON）中视觉语言模型（VLM）决策受低级感知准确性限制的问题，本研究提出了3DGSNav框架。3DGSNav利用3D高斯泼溅（3DGS）作为VLM的持久记忆来增强空间推理能力。通过主动感知，该框架逐步构建环境的3DGS表示，从而实现轨迹引导的、边界感知的第一人称视角的自由视点渲染。此外，研究设计了结构化视觉提示并结合思维链（CoT）提示来进一步提升VLM的推理能力。在导航过程中，实时对象检测器用于过滤潜在目标，而VLM驱动的主动视点切换则进行目标再验证，确保高效可靠的识别。在多个基准测试和真实世界四足机器人实验中，3DGSNav展现出鲁棒且具竞争力的性能。|Xinyi Yu Team|[2602.12159](http://arxiv.org/abs/2602.12159)|null|
|**2026-02-12**|**Affordance-Graphed Task Worlds: Self-Evolving Task Generation for Scalable Embodied Learning**|鉴于在真实世界中训练机器人策略成本高昂且现有生成模拟难以生成逻辑连贯的长周期任务并处理动态物理不确定性，本研究提出了Affordance-Graphed Task Worlds (AGT-World) 统一框架。该框架基于真实世界观测自主构建交互式模拟环境和机器人任务策略，通过将任务空间形式化为结构化图，实现复杂目标的精确分层分解，并引入了结合VLM推理和几何验证的自演化机制来优化策略。实验结果表明，该方法在成功率和泛化能力上显著优于现有方法，实现了提议、执行和修正的自我改进循环，以支持可扩展的机器人学习。|Changshui Zhang Team|[2602.12065](http://arxiv.org/abs/2602.12065)|null|
|**2026-02-12**|**Can Local Vision-Language Models improve Activity Recognition over Vision Transformers? -- Case Study on Newborn Resuscitation**|鉴于新生儿复苏活动准确记录的重要性及其在实践中的不足，以及现有方法在识别细粒度活动上的挑战，本研究探索了生成式AI（GenAI）方法，特别是结合大型语言模型（LLM）的局部视觉-语言模型（VLM），以改进新生儿复苏视频中的活动识别。研究将几种零样本VLM策略和微调VLM（包括LoRA）与分类头进行评估，并与监督式TimeSFormer基线进行比较。结果表明，小型VLM虽然容易产生幻觉，但经过LoRA微调后，F1分数可达0.91，显著超越TimeSFormer的0.70。|Øyvind Meinich-Bache Team|[2602.12002](http://arxiv.org/abs/2602.12002)|null|
|**2026-02-12**|**Benchmarking Vision-Language Models for French PDF-to-Markdown Conversion**|针对PDF到Markdown转换作为RAG管道关键步骤，以及现有基准多侧重英文或中文并可能过度惩罚无关格式差异的问题，本研究引入了一个专门针对法语文档、通过模型不一致采样挑选的挑战性新基准。该基准涵盖手写表格、复杂布局、密集表格和富含图形的页面，并采用单元测试式检查和类别特定归一化进行评估，以减少无关的呈现差异。实验结果显示，在15个模型中，最强的专有模型在手写和表格处理上表现出更高的鲁棒性，同时一些开源系统在标准打印布局上仍具竞争力。|Nicolas Mery Team|[2602.11960](http://arxiv.org/abs/2602.11960)|null|
|**2026-02-12**|**Are Two LLMs Better Than One? A Student-Teacher Dual-Head LLMs Architecture for Pharmaceutical Content Optimization**|针对大型语言模型（LLM）在制药等受监管领域内容创作中面临的科学准确性和合规性挑战，以及手动质量控制（QC）低效易错的问题，本研究引入了LRBTC，一个模块化的LLM和VLM驱动的QC架构。该架构涵盖语言、法规、品牌、技术和内容结构检查，并结合了师生双模型架构、人工干预（HITL）工作流和瀑布式规则过滤机制。实验结果表明，LRBTC在AIReg-Bench上取得了83.0%的F1和97.5%的召回率，相比Gemini 2.5 Pro，遗漏违规减少5倍；在CSpelling上平均准确率提高26.7%。误差分析揭示当前模型擅长检测拼写错误，但在复杂医学语法和标点错误识别方面仍有提升空间。|Anubhav Girdhar Team|[2602.11957](http://arxiv.org/abs/2602.11957)|null|
|**2026-02-12**|**LAMP: Implicit Language Map for Robot Navigation**|为解决现有视觉-语言模型（VLM）零样本导航方法在大环境中因显式存储语言向量而导致的内存需求过高和细粒度规划分辨率有限的问题，本研究提出了LAMP（Language Map）框架。该框架通过学习连续的、语言驱动的隐式神经语言场来编码语言特征，并结合稀疏图实现高效的粗略路径规划，进而通过梯度优化在学习场中细化目标姿态。该方法还采用贝叶斯框架建模嵌入不确定性以增强泛化能力，并通过图采样策略扩展到大环境。实验证明，LAMP在内存效率和细粒度目标到达精度方面均优于现有显式方法。|Sunwook Choi Team|[2602.11862](http://arxiv.org/abs/2602.11862)|**[link](https://lab-of-ai-and-robotics.github.io/LAMP/)**|
|**2026-02-12**|**JEPA-VLA: Video Predictive Embedding is Needed for VLA Models**|针对现有视觉-语言-动作（VLA）模型在机器人操作中存在的样本效率低和泛化能力有限的问题，本研究认为这与被忽视的预训练视觉表征在环境理解和策略先验方面知识不足有关。研究发现，视频上预训练的预测嵌入（特别是V-JEPA 2）能有效捕捉任务相关的时态动态并过滤不可预测的环境因素，从而弥补现有视觉表征的不足。基于此，论文提出了JEPA-VLA，一种将预测嵌入自适应集成到现有VLA中的简单而有效的方法。实验结果表明，JEPA-VLA在一系列基准和真实机器人任务中均取得了显著的性能提升。|Mingsheng Long Team|[2602.11832](http://arxiv.org/abs/2602.11832)|null|
|**2026-02-12**|**Revis: Sparse Latent Steering to Mitigate Object Hallucination in Large Vision-Language Models**|为解决大型视觉-语言模型（LVLMs）普遍存在的物体幻觉问题，该问题源于视觉特征和预训练文本表示在网络深层中的纠缠，本研究提出了REVIS，一个无需训练的框架。REVIS基于潜在空间几何原理，通过正交投影精确提取纯视觉信息向量，并采用校准策略，仅在视觉信息被抑制的精确深度进行稀疏干预，从而有效恢复视觉信息。经验评估结果显示，REVIS与最先进的基线相比，将目标幻觉率降低了约19%，同时保持了模型原有的通用推理能力。|Zhou Yang Team|[2602.11824](http://arxiv.org/abs/2602.11824)|null|
|**2026-02-12**|**Adaptive Debiasing Tsallis Entropy for Test-Time Adaptation**|现有主流的视觉-语言模型测试时自适应（TTA）方法依赖香农熵（SE）衡量预测不确定性，但由于预训练数据偏差，SE会产生有偏的不确定性估计。为解决此问题，研究发现并证明Tsallis熵（TE）通过引入非广延参数q，天然适合表征有偏分布。在此基础上，本文提出了自适应去偏Tsallis熵（ADTE），为每个类别定制类特定的q^l参数，该参数通过对持续传入的测试实例估计标签偏差进行归一化得到。实验结果表明，ADTE在ImageNet及其五种变体上超越了最先进的方法，并在10个跨域基准测试中取得了最高的平均性能，证实了其有效性。|Jianfeng Lu Team|[2602.11743](http://arxiv.org/abs/2602.11743)|null|
|**2026-02-12**|**Adapting Vision-Language Models for E-commerce Understanding at Scale**|电商产品理解需要文本、图像和结构化属性等多模态的强大理解能力。通用视觉-语言模型（VLMs）虽能提供泛化能力，但如何将其有效地适应电商数据中以属性为中心、多图像和噪声多的特性，同时不牺牲通用性能，是一个未被充分探索的问题。本文通过大规模实验研究，展示了对通用VLM进行有针对性的适配，可以在大幅提升电商任务性能的同时，保持其广泛的多模态能力。此外，本文还提出了一个新颖且全面的评估套件，涵盖了深度产品理解、严格指令遵循和动态属性提取等任务。|Shahram Khadivi Team|[2602.11733](http://arxiv.org/abs/2602.11733)|null|
|**2026-02-12**|**STVG-R1: Incentivizing Instance-Level Reasoning and Grounding in Videos via Reinforcement Learning**|在视觉-语言模型（VLMs）中，文本描述与视觉坐标之间的不匹配常导致幻觉问题，在时空视频定位（STVG）等密集预测任务中尤为突出。现有方法通常增强视觉-文本对齐或添加辅助解码器，但这会引入额外的可训练模块，导致高昂的标注和计算成本。本文提出了一种新颖的视觉提示范式，通过将每帧坐标预测重新表述为实例级识别问题，为每个对象分配唯一且时间一致的ID，并将其嵌入视频作为视觉提示。此外，研究引入了STVG-R1，这是首个用于STVG的强化学习框架，通过任务驱动奖励联合优化时间精度、空间一致性和结构化格式正则化。实验结果表明，STVG-R1在六个基准测试中表现出色，在HCSTVG-v2上m_IoU比基线Qwen2.5-VL-7B高出20.9%，达到SOTA，并展现出强大的零样本泛化能力。|Qing Li Team|[2602.11730](http://arxiv.org/abs/2602.11730)|null|
|**2026-02-12**|**ScalSelect: Scalable Training-Free Multimodal Data Selection for Efficient Visual Instruction Tuning**|大规模视觉指令微调（VIT）是提升视觉-语言模型（VLMs）在多模态任务中性能的关键范式，但其在大型数据集上的训练因数据冗余而计算昂贵且低效。现有的数据选择方法要么需要昂贵的训练或梯度计算，要么依赖代理模型或指令无关表示且复杂度高，限制了可扩展性。本文提出了ScalSelect，一种可扩展的无训练多模态数据选择方法，其复杂度与样本数量呈线性关系，无需外部模型或辅助数据集。ScalSelect通过提取指令令牌在目标VLM中最受关注的视觉特征来构建样本表示，然后识别其表示最能近似整个数据集主导子空间的样本，从而实现无需成对比较的可扩展重要性评分。广泛的实验表明，ScalSelect仅使用16%的数据即可达到全数据集训练97.5%以上的性能，在某些设置下甚至超越了全数据训练。|Kai Chen Team|[2602.11636](http://arxiv.org/abs/2602.11636)|**[link](https://github.com/ChangtiWu/ScalSelect}{ScalSelect})**|
|**2026-02-12**|**SkillRater: Untangling Capabilities in Multimodal Data**|传统数据筛选方法通常只给样本分配一个单一的质量分数，但这在模型需要学习多种不同能力时显得局限。本文认为质量应是多维的，每个维度对应模型需掌握的一种能力。为此，我们提出了SkillRater框架，将数据过滤分解为多个专业的评估器，每个评估器负责一种能力，并通过元学习在独立验证目标上进行训练。这些评估器的分数通过渐进式选择规则组合：在每个训练阶段，只要任一评估器将样本排名高于随时间收紧的阈值，该样本即被保留，以在早期保持多样性，后期聚焦高价值样本。在视觉语言模型上的验证表明，SkillRater在视觉理解、OCR和STEM推理能力上均显著优于未过滤基线，且学习到的评估器信号几乎正交，证实了多维度分解的有效性。|Akshat Shrivastava Team|[2602.11615](http://arxiv.org/abs/2602.11615)|null|
|**2026-02-12**|**Chatting with Images for Introspective Visual Thinking**|当前大型视觉-语言模型（LVLMs）通常依赖基于单次视觉编码的纯文本推理，这常导致细粒度视觉信息丢失，尤其是在处理跨区域或多图像的视觉语义或几何关系推理时。为解决这些挑战，本文提出了“与图像对话”这一新框架，将视觉操作重新定义为语言引导的特征调制。在该框架下，模型在富有表现力的语言提示指导下，动态地对多个图像区域进行联合再编码，从而实现语言推理与视觉状态更新之间更紧密的耦合。我们通过ViLaVT实例化了这一范式，它是一个配备动态视觉编码器的新型LVLM，并采用两阶段课程（监督微调和强化学习）进行训练。广泛的实验表明，ViLaVT在八个基准测试中取得了显著且一致的改进，尤其在复杂的多图像和基于视频的空间推理任务上表现突出。|Tieniu Tan Team|[2602.11073](http://arxiv.org/abs/2602.11073)|null|
|**2026-02-12**|**Self-Refining Vision Language Model for Robotic Failure Detection and Reasoning**|针对机器人系统故障推理面临的挑战，即真实世界故障的复杂性及丰富推理标签获取成本高昂，本研究提出了ARMOR模型。ARMOR将故障检测与自然语言推理视为一个多任务自完善过程，通过迭代预测和基于历史输出的条件推理进行训练。它利用大规模稀疏二元标签和少量丰富推理标注的异构监督，并通过离线和在线模仿学习进行优化。在推理阶段，ARMOR生成多条完善轨迹并利用自确定性指标选择最可靠的预测。实验结果显示，ARMOR在故障检测率上比现有方法提升了30%，在LLM模糊匹配分数衡量的推理能力上提升了100%，证明了其在异构监督下的鲁棒性及超越预定义故障模式的开放式推理能力。|Yesh Dattatreya Team|[2602.12405](http://arxiv.org/abs/2602.12405)|null|
|**2026-02-12**|**What does RL improve for Visual Reasoning? A Frankenstein-Style Analysis**|为了理解强化学习（RL）在视觉语言模型（VLM）视觉推理中相比监督微调（IN）的具体贡献，本研究提出了一种“弗兰肯斯坦式”分析框架。该框架通过因果探测定位功能、参数比较表征更新以及模型合并测试可迁移性。研究发现，RL主要通过在模型的中间到后期层诱导一致的推理时段转移来提升性能，并且这些中间到后期的优化对于RL的性能增益是可迁移和必要的。这些结果表明，RL对视觉推理的可靠贡献并非统一增强视觉感知，而是系统地细化了Transformer中间到后期层的计算，从而改善了视觉到推理的对齐和推理性能，揭示了仅通过基准评估来理解多模态推理改进的局限性。|Tianyi Zhou Team|[2602.12395](http://arxiv.org/abs/2602.12395)|null|
|**2026-02-12**|**Synthetic Image Detection with CLIP: Understanding and Assessing Predictive Cues**|鉴于现代生成模型能产生近乎真实的照片，合成图像检测（SID）的泛化能力及其在实际应用中的表现面临挑战，特别是在新生成模型面前。本研究旨在探究CLIP作为SID基础模型的有效性及其内在线索。为此，我们构建了SynthCLIC数据集以减少语义偏差，并利用可解释的线性分类器和文本概念模型分析CLIP特征。结果显示，CLIP检测器在GAN基准上表现优异，但在高质量扩散数据集SynthCLIC上性能略有下降，且跨不同生成器家族的泛化能力显著受限。研究发现，检测器主要依赖高层摄影属性而非明显的生成器伪影。这些发现强调了持续模型更新和更广泛训练暴露的必要性，并肯定了CLIP作为更通用、鲁棒SID方法的强大基础。|Michael Graber Team|[2602.12381](http://arxiv.org/abs/2602.12381)|null|
|**2026-02-12**|**ForeAct: Steering Your VLA with Efficient Visual Foresight Planning**|在开放世界环境中，视觉-语言-动作（VLA）模型将高级语言指令转换为具体可执行动作面临挑战。本研究提出了视觉预见规划器（ForeAct），一个通用高效的规划器，它通过想象未来观察和子任务描述来逐步指导VLA。ForeAct包含一个高效的预见图像生成模块，能从当前视觉输入和语言指令在0.33秒内预测高质量的未来观察，并结合一个视觉-语言模型生成子任务描述。最先进的VLA模型无需修改架构，只需通过增强视觉输入即可无缝集成ForeAct。预见生成器在超过100万个多任务、跨具身情景中进行预训练，学习了鲁棒的具身动力学。在包含11个真实世界任务的基准测试中，ForeAct取得了87.4%的平均成功率，比基线模型有显著提升。|Song Han Team|[2602.12322](http://arxiv.org/abs/2602.12322)|null|
|**2026-02-12**|**LatentAM: Real-Time, Large-Scale Latent Gaussian Attention Mapping via Online Dictionary Learning**|为解决开放词汇机器人感知中从流式RGB-D观测构建可扩展潜在特征地图的挑战，并克服传统VLM嵌入方法缺乏通用性和依赖预训练的问题，本研究提出了LatentAM框架。LatentAM是一种在线3D高斯泼溅（3DGS）映射框架，它采用模型无关且无需预训练的在线字典学习方法，实现了与不同VLM的即插即用集成。该方法将高斯基元与紧凑查询向量关联，通过带有可学习字典的注意力机制转换为近似VLM嵌入。字典在流式观测中高效初始化并在线优化，同时结合基于体素哈希的地图管理策略以实现大规模环境下的GPU内存有界使用。实验结果表明，LatentAM在特征重建保真度上显著优于现有方法，并在评估数据集上实现了接近实时的速度（12-35 FPS）。|Yulun Tian Team|[2602.12314](http://arxiv.org/abs/2602.12314)|null|
|**2026-02-11**|**Hierarchical Concept Embedding & Pursuit for Interpretable Image Classification**|可解释设计模型在计算机视觉中日益受到关注，因为它们能为其预测提供忠实的解释。在图像分类中，这类模型通常从图像中提取人类可解释的概念并用于分类。稀疏概念恢复方法利用视觉-语言模型的潜在空间将图像嵌入表示为概念嵌入的稀疏组合，但这类方法忽视了概念的层次结构，可能导致预测正确但解释与层次结构不一致。为解决此问题，本文提出了分层概念嵌入与追踪（HCEP）框架，它在潜在空间中诱导概念嵌入的层次结构，并利用分层稀疏编码来恢复图像中存在的概念。实验结果表明，HCEP在概念精度和召回率上均优于基线，同时保持了有竞争力的分类准确率，并且在样本数量有限时展现出卓越的分类和概念恢复性能，证明了将层次结构融入稀疏编码能产生更可靠和可解释的图像分类模型。|René Vidal Team|[2602.11448](http://arxiv.org/abs/2602.11448)|null|
|**2026-02-11**|**Beyond VLM-Based Rewards: Diffusion-Native Latent Reward Modeling**|扩散模型和流匹配模型的偏好优化需要既具有判别鲁棒性又计算高效的奖励函数。视觉-语言模型（VLMs）已成为主要的奖励提供者，利用其丰富的多模态先验来指导对齐。然而，它们的计算和内存成本很高，并且通过像素空间奖励优化潜在扩散生成器会引入域不匹配，使对齐复杂化。本文提出了DiNa-LRM，一种扩散原生的潜在奖励模型，它直接在噪声扩散状态上构建偏好学习。DiNa-LRM引入了一种噪声校准的Thurstone似然，其不确定性依赖于扩散噪声，并利用预训练的潜在扩散骨干网络与时间步条件奖励头。实验结果显示，DiNa-LRM在图像对齐基准测试中显著优于现有基于扩散的奖励基线，并以极低的计算成本实现了与最先进VLM相当的性能，显著改善了偏好优化动态，实现了更快、更资源高效的模型对齐。|Wenhan Luo Team|[2602.11146](http://arxiv.org/abs/2602.11146)|**[link](https://github.com/HKUST-C4G/diffusion-rm)**|
|**2026-02-11**|**Active Zero: Self-Evolving Vision-Language Models through Active Environment Exploration**|自博弈已使大型语言模型（LLMs）通过自生成挑战实现自主提升。然而，现有视觉-语言模型（VLMs）的自博弈方法依赖与静态图像集的被动交互，导致对初始数据集的强依赖和学习效率低下。为解决这些局限性，本文提出了Active-Zero框架，将模型学习从被动交互转向主动探索视觉环境。Active-Zero采用三个共同进化的智能体：一个Searcher根据模型能力前沿从开放世界存储库检索图像；一个Questioner合成校准的推理任务；一个Solver通过准确性奖励进行优化。这种闭环机制实现了自我脚手架的自动课程学习。在Qwen2.5-VL-7B-Instruct的12个基准测试中，Active-Zero在推理任务上平均准确率达到53.97%（提升5.7%），在通用理解上达到59.77%（提升3.9%），持续优于现有自博弈基线，表明主动探索是可扩展和自适应自进化视觉-语言系统的关键要素。|Tat-Seng Chua Team|[2602.11241](http://arxiv.org/abs/2602.11241)|null|
|**2026-02-11**|**Safe mobility support system using crowd mapping and avoidance route planning using VLM**|自主移动机器人在动态、尤其是拥挤环境中安全有效导航仍面临挑战。本文提出了一种新颖的框架，该框架集成了视觉-语言模型（VLM）和高斯过程回归（GPR），用于生成动态人群密度图（“抽象图”），以实现自主机器人导航。我们的方法利用VLM识别抽象环境概念（如人群密度）的能力，并通过GPR以概率方式表示这些概念。在大学校园进行的真实世界试验结果表明，机器人成功地生成了避开静态障碍物和动态人群的路径，从而增强了导航的安全性和适应性。|Koichi Ozaki Team|[2602.10910](http://arxiv.org/abs/2602.10910)|null|

<p align=right>(<a href=#updated-on-20260220>back to top</a>)</p>

## VLA

| Publish Date | Title | Chinese Summary | Authors | PDF | Code |
|:---------|:-----------------------|:------------------------|:---------|:------|:------|
|**2026-02-19**|**When Vision Overrides Language: Evaluating and Mitigating Counterfactual Failures in VLAs**|现有视觉-语言-动作（VLA）模型在遵循语言指令时常因视觉捷径和数据集偏差导致“反事实失败”。为系统研究此问题，本研究引入了LIBERO-CF基准。针对性地，提出了反事实动作引导（CAG）方法，这是一种双分支推理方案，将标准VLA策略与非语言条件下的视觉-动作（VA）模块结合，通过动作选择中的反事实比较来减少对视觉捷径的依赖。实验表明，CAG在LIBERO-CF上显著提升了语言遵循准确率和任务成功率，例如在未充分观察的任务中，语言遵循准确率提高了9.7%，任务成功率提高了3.6%，与VA模型结合时增益更大，并在真实世界中减少了反事实失败并提高了任务成功率。|Mingyu Ding Team|[2602.17659](http://arxiv.org/abs/2602.17659)|**[link](https://vla-va.github.io/)**|
|**2026-02-19**|**Web Verbs: Typed Abstractions for Reliable Task Composition on the Agentic Web**|随着网络演变为智能体操作环境，现有Web智能体依赖低级操作（如点击、按键），效率低且不易验证。本研究提出“Web Verbs”概念，这是一个Web规模的、类型化、语义化的函数集合，通过统一接口暴露网站功能，旨在为Web动作构建语义层。这些动词作为稳定且可组合的单元，使大型语言模型（LLM）能合成可靠且可审计的工作流。概念验证和案例研究表明，Web Verbs相对于现有智能体实现了更简洁和稳健的执行，显著提高了可靠性、效率和可验证性。|Suman Nath Team|[2602.17245](http://arxiv.org/abs/2602.17245)|null|
|**2026-02-18**|**EgoScale: Scaling Dexterous Manipulation with Diverse Egocentric Human Data**|人类行为数据在学习物理智能方面具有巨大潜力，但如何有效利用其实现精细、高自由度灵巧操作尚不明确。本研究提出了EgoScale框架，通过大规模第一人称人类数据实现人到灵巧操作的迁移。研究人员在一个包含20,854小时带动作标签的第一人称人类视频数据集上训练了一个视觉-语言-动作（VLA）模型，发现人类数据规模与验证损失之间存在对数线性关系，且验证损失与下游真实机器人性能强相关。通过大规模人类数据预训练与轻量级人-机器人校准相结合的两阶段迁移方法，最终策略使22自由度灵巧机械手的平均成功率比无预训练基线提高了54%，并能有效迁移到低自由度机械手，证明大规模人类运动提供了可复用且与具体形态无关的运动先验。|Linxi Fan Team|[2602.16710](http://arxiv.org/abs/2602.16710)|null|
|**2026-02-18**|**MALLVI: a multi agent framework for integrated generalized robotics manipulation**|大型语言模型（LLMs）在机器人操作任务规划中的应用面临开环操作、缺乏环境反馈的挑战，导致在动态环境中表现脆弱。本研究提出了MALLVi，一个多智能体大语言与视觉框架，可实现闭环反馈驱动的机器人操作。MALLVi协调专门的智能体（分解器、定位器、思考者、反思器等）来管理感知、定位、推理和高级规划，并利用视觉语言模型（VLM）评估环境反馈。仿真和真实世界实验证明，迭代的闭环多智能体协调显著提高了零样本操作任务的泛化能力和成功率。|Babak Khalaj Team|[2602.16898](http://arxiv.org/abs/2602.16898)|null|
|**2026-02-17**|**Learning to Retrieve Navigable Candidates for Efficient Vision-and-Language Navigation**|基于大型语言模型（LLM）的视觉-语言导航（VLN）在决策效率上存在不足，因为模型需反复解释指令并处理冗余的导航候选。本研究提出一种检索增强框架，旨在不修改或微调LLM的情况下提高其效率和稳定性。该方法在两个层面引入检索：剧集层面的指令级嵌入检索器选择语义相似的成功轨迹作为上下文示例；步骤层面的模仿学习候选检索器在LLM推理前修剪不相关的导航方向。在Room-to-Room (R2R) 基准测试上的实验结果表明，该方法在成功率、Oracle成功率和SPL上均取得了一致提升，并且两种检索模块对全局引导和分步决策效率贡献互补。|Lina Yao Team|[2602.15724](http://arxiv.org/abs/2602.15724)|null|
|**2026-02-17**|**The Next Paradigm Is User-Centric Agent, Not Platform-Centric Service**|尽管数字服务和大型语言模型（LLM）技术进步显著，但现有平台中心化模式常优先平台指标而非用户真实需求，导致用户利益冲突。本研究提出未来数字服务应从平台中心化转向以用户为中心的智能体，此类智能体优先用户隐私，与用户定义目标保持一致，并赋予用户偏好和行为的控制权。论文探讨了实现用户中心智能的机遇与挑战，提出了一个实用的设备-云管道实现方案，并讨论了其采纳所需的治理和生态系统结构，认为LLM和设备智能的进步使这一愿景变得可行。|Enhong Chen Team|[2602.15682](http://arxiv.org/abs/2602.15682)|null|
|**2026-02-17**|**CARE Drive A Framework for Evaluating Reason-Responsiveness of Vision Language Models in Automated Driving**|基础模型在自动驾驶中用于场景解释和决策，但现有评估方法主要关注结果性能，未能评估模型决策是否反映人类相关考量，可能导致安全关键领域出现虚假信心。本研究提出了CARE Drive框架，一个模型无关的上下文感知理由评估方法，用于评估自动驾驶中视觉语言模型的理由响应性。该框架通过在受控上下文变化下比较基线和理由增强的模型决策，分两阶段评估：提示校准确保输出稳定，系统性上下文扰动衡量决策对人类理由（如安全裕度、社会压力、效率）的敏感性。实验表明，人类明确的理由显著影响模型决策，提高了与专家推荐行为的一致性，但不同上下文因素的响应性存在差异。|Arkady Zgonnikov Team|[2602.15645](http://arxiv.org/abs/2602.15645)|null|
|**2026-02-17**|**World Action Models are Zero-shot Policies**|现有视觉-语言-动作（VLA）模型在语义泛化方面表现出色，但在新环境中难以泛化到未见过的物理动作。本研究提出了DreamZero，一个基于预训练视频扩散骨干的世界动作模型（WAM）。与VLA不同，WAM通过将视频作为世界演变的密集表示，预测未来的世界状态和动作来学习物理动力学。DreamZero通过联合建模视频和动作，能从异构机器人数据中有效学习多样技能，无需重复演示。真实机器人实验显示，DreamZero在泛化到新任务和环境方面比现有VLA模型提升超过2倍，并能实现7Hz的实时闭环控制。此外，它还展示了跨形态迁移能力，仅需少量视频或游戏数据即可显著提高未见任务性能并实现零样本泛化。|Joel Jang Team|[2602.15922](http://arxiv.org/abs/2602.15922)|**[link](https://dreamzero0.github.io/)**|
|**2026-02-17**|**VLM-DEWM: Dynamic External World Model for Verifiable and Resilient Vision-Language Planning in Manufacturing**|视觉-语言模型（VLM）在智能制造中的高级规划方面前景广阔，但面临无状态操作导致世界状态漂移和推理不透明导致故障诊断困难两大挑战。本研究提出了VLM-DEWM认知架构，通过一个持久、可查询的动态外部世界模型（DEWM）将VLM推理与世界状态管理解耦。每个VLM决策都被构建成可外部化的推理轨迹（ERT），在执行前对照DEWM进行验证。当发生故障时，通过预测与观察状态之间的差异分析实现有针对性的恢复。实验表明，VLM-DEWM将状态跟踪准确性从56%提高到93%，恢复成功率从低于5%提高到95%，并显著降低了计算开销，为动态制造环境中的长周期机器人操作提供了可验证且弹性强的解决方案。|Ning Ji Team|[2602.15549](http://arxiv.org/abs/2602.15549)|null|
|**2026-02-17**|**Selective Perception for Robot: Task-Aware Attention in Multimodal VLA**|机器人学中多视图输入的视觉-语言-动作（VLA）模型常采用静态融合，导致不必要的计算开销和来自任务无关背景信息的噪声。本研究受人类主动感知启发，提出了一种动态信息融合框架，旨在最大化VLA模型的效率和鲁棒性。该方法引入了一个轻量级自适应路由架构，实时分析文本提示和腕部摄像头观测，预测多摄像头视图的任务相关性，从而有条件地减弱低信息效用视图的计算，仅向策略网络提供必要的视觉特征。通过自动化标签流水线高效获取训练数据。真实世界机器人操作实验证明，该方法在推理效率和控制性能上均显著优于现有VLA模型，验证了动态信息融合在资源受限的实时机器人控制环境中的有效性。|Soo-Chul Lim Team|[2602.15543](http://arxiv.org/abs/2602.15543)|null|
|**2026-02-17**|**One Agent to Guide Them All: Empowering MLLMs for Vision-and-Language Navigation via Explicit World Representation**|导航智能体需要同时理解高层语义指令和精确空间感知，但现有多模态大语言模型（MLLMs）驱动的导航代理存在紧密耦合设计和对简化文本地图的依赖问题。为此，本研究提出一种解耦设计，将低层空间状态估计与高层语义规划分离，并引入交互式度量世界表示，使MLLMs能在此丰富一致的表示上进行推理。此外，通过反事实推理进一步挖掘MLLMs潜力，并确保动作的物理有效性。实验结果表明，该方法在R2R-CE和RxR-CE基准测试中分别取得了48.8%和42.2%的成功率，创下了新的零样本SOTA，并成功实现了零样本从模拟到真实环境的迁移，验证了其作为具身视觉-语言导航的通用接口的鲁棒性。|Qi Wu Team|[2602.15400](http://arxiv.org/abs/2602.15400)|null|
|**2026-02-17**|**ActionCodec: What Makes for Good Action Tokenizers**|视觉-语言-动作（VLA）模型利用视觉-语言模型（VLMs）的原生自回归范式，在指令遵循和训练效率方面表现出色，但动作分词的设计主要关注重建保真度，忽视了其对VLA优化的直接影响，导致优化原则尚不明确。本研究从VLA优化角度，基于信息论见解，提出了最大化时间令牌重叠、最小化词汇冗余、增强多模态互信息和令牌独立性等设计原则，并指导开发了高性能动作分词器ActionCodec。实验结果显示，ActionCodec显著提升了训练效率和VLA性能，在LIBERO基准测试中，经ActionCodec微调的SmolVLM2-2.2B模型在未进行机器人预训练的情况下实现了95.5%的成功率，并结合架构改进达到了97.4%，刷新了VLA模型的SOTA表现。|Jianye Hao Team|[2602.15397](http://arxiv.org/abs/2602.15397)|null|
|**2026-02-17**|**EAA: Automating materials characterization with vision language model agents**|自动化复杂的实验显微镜工作流程是当前面临的挑战。本研究提出了实验自动化代理（EAA），一个由视觉-语言模型驱动的代理系统，旨在实现复杂实验显微镜工作流的自动化。EAA集成了多模态推理、工具增强行动和可选的长期记忆，支持自主程序和交互式用户引导测量，并构建在灵活的任务管理器架构之上，提供与模型上下文协议（MCP）兼容的现代工具生态系统。在先进光源成像光束线的实验演示中，EAA成功完成了自动变焦、自然语言描述特征搜索和交互式数据采集，结果表明该系统能有效提高光束线效率，减轻操作负担，并降低用户的专业门槛。|Mathew J. Cherukara Team|[2602.15294](http://arxiv.org/abs/2602.15294)|null|
|**2026-02-16**|**Beyond Imitation: Reinforcement Learning-Based Sim-Real Co-Training for VLA Models**|模拟提供了可扩展且低成本的方式来丰富视觉-语言-动作（VLA）训练，减少对昂贵真实机器人演示的依赖，但多数模拟-现实协同训练方法依赖于监督微调（SFT），将模拟视为静态演示源，未充分利用大规模闭环交互，从而限制了真实世界收益和泛化。针对此问题，本文提出了RL-based sim-real Co-training (RL-Co) 框架，该框架利用交互式模拟同时保留真实世界能力。其设计分为两阶段：首先，通过对真实和模拟演示的混合数据进行SFT来预热策略，然后通过在模拟中进行强化学习来微调策略，并添加辅助的真实世界数据监督损失以锚定策略并缓解灾难性遗忘。在四项真实世界桌面操作任务上，RL-Co在两种VLA架构（OpenVLA和π₀.₅）上均持续优于仅真实世界微调和基于SFT的协同训练，例如OpenVLA的真实世界成功率提升24%，π₀.₅提升20%，同时在未见任务变体上表现出更强的泛化能力和显著提升的真实世界数据效率。|Yu Wang Team|[2602.12628](http://arxiv.org/abs/2602.12628)|null|
|**2026-02-16**|**DM0: An Embodied-Native Vision-Language-Action Model towards Physical AI**|传统的具身智能方法常将互联网预训练模型适应于物理任务，忽视了物理基础的早期融合。本研究提出DM0，一个专为具身AI设计的具身原生视觉-语言-动作（VLA）框架，旨在通过从一开始就从异构数据源学习来统一具身操作和导航。该方法包含预训练、中训练和后训练三个阶段：首先，在大规模统一预训练阶段，将网络文本、自动驾驶场景和具身交互日志等多样化语料无缝整合到VLM中，共同学习语义知识和物理先验；随后，在VLM之上构建流匹配动作专家；并采用混合训练策略协调高层推理与低层控制，同时引入具身空间支架策略构建空间思维链（CoT）推理以约束动作解空间。实验结果表明，DM0在RoboChallenge基准测试的Table30上，在专业型和通用型设置中均实现了最先进的性能。|Tiancai Wang Team|[2602.14974](http://arxiv.org/abs/2602.14974)|**[link](https://github.com/Dexmal/dexbotic)**|
|**2026-02-16**|**DriveFine: Refining-Augmented Masked Diffusion VLA for Precise and Robust Driving**|自动驾驶中的VLA模型常用生成式规划器，但基于扩散的规划器存在模态对齐困难、训练效率低、泛化能力受限，而基于令牌的规划器则受累于累积因果误差和不可逆解码。针对这两种主流范式各有利弊的问题，本研究提出了DriveFine，一个结合灵活解码与自校正能力的掩码扩散VLA模型。该模型设计了一种即插即用的块-MoE（mixture-of-experts）模块，在生成专家之上无缝注入一个细化专家，并通过推理时的显式专家选择和训练时的梯度阻断实现专家完全解耦，以保留预训练权重的基础能力。此外，还设计了一种混合强化学习策略，鼓励细化专家进行有效探索并保持训练稳定性。在NAVSIM v1、v2和Navhard基准上的大量实验证明了DriveFine的强大功效和鲁棒性。|Yan Wang Team|[2602.14577](http://arxiv.org/abs/2602.14577)|null|
|**2026-02-16**|**Replanning Human-Robot Collaborative Tasks with Vision-Language Models via Semantic and Physical Dual-Correction**|人机协作（HRC）在装配任务中面临人类指令模糊和欠明确的问题，导致机器人难以生成可行且协作的行为；同时，现有基于VLM的方法常出现幻觉推理和无法预测物理执行失败。为解决这些挑战，本研究提出了一个增强VLM推理的双重纠正机制HRC框架。该框架包含一个内部纠正模型，用于在动作执行前验证逻辑一致性和任务可行性；以及一个外部纠正模型，通过执行后反馈检测并纠正物理故障。模拟消融实验表明，该方法比没有纠正机制的基线模型提高了成功率。真实世界协作装配任务（如上半身人形机器人辅助固定物体或准备工具）的实验进一步证实了该框架在响应人类指令进行交互式重新规划方面的有效性和实用性。|Kensuke Harada Team|[2602.14551](http://arxiv.org/abs/2602.14551)|null|
|**2026-02-16**|**TikArt: Aperture-Guided Observation for Fine-Grained Visual Reasoning via Reinforcement Learning**|多模态大语言模型（MLLMs）在细粒度视觉推理中面临挑战，因为关键证据可能存在于微小物体、杂乱区域或细微标记中，这些在单一全局图像编码下容易丢失。为解决此问题，本研究引入了TikArt（Thinking Aperture），一种光圈引导代理，将多步视觉-语言推理视为对感兴趣区域的决策过程。TikArt遵循“思考-光圈-观察”循环，在语言生成和两种光圈操作（Zoom用于提取矩形裁剪，Segment调用SAM2获取基于掩码的不规则目标裁剪）之间交替。每次操作后，模型会产生显式观察，将局部视觉线索转化为持久的语言记忆。TikArt基于Qwen3-VL-8B，使用AGRPO（一种GRPO风格的强化学习算法）优化其推理策略。实验结果表明，TikArt在V*、HR-Bench-4K/8K、MME-RealWorld-Lite、MMStar、RefCOCO和ReasonSeg等数据集上均取得了显著提升，并能生成可解释的光圈轨迹以支持高分辨率推理。|Lei Zhao Team|[2602.14482](http://arxiv.org/abs/2602.14482)|null|
|**2026-02-16**|**Hierarchical Vision-Language Interaction for Facial Action Unit Detection**|面部动作单元（AU）检测旨在识别面部肌肉的细微激活，但其主要挑战是在有限标注数据下有效学习判别性和泛化性的AU表示。为解决此问题，本研究提出了一种分层视觉-语言交互AU理解（HiVA）方法，利用AU文本描述作为语义先验来指导和增强AU检测。HiVA通过大语言模型生成多样化的AU描述以强化基于语言的表示学习，并引入AU感知动态图模块以捕获细粒度和整体的视觉-语言关联。这些特征通过分层跨模态注意力架构进一步整合，包括解耦双重交叉注意力（DDCA）和上下文双重交叉注意力（CDCA），分别建立细粒度AU特定交互和建模全局AU间依赖。广泛实验表明，HiVA持续超越最先进方法，且定性分析揭示其生成语义上有意义的激活模式，证实了其在学习鲁棒和可解释的跨模态对应关系方面的有效性。|Cuntai Guan Team|[2602.14425](http://arxiv.org/abs/2602.14425)|null|
|**2026-02-16**|**Multi-Turn Adaptive Prompting Attack on Large Vision-Language Models**|多轮越狱攻击对文本大语言模型（LLMs）有效，但将其扩展到多模态大语言模型（LVLMs）时，简单添加视觉输入容易触发防御机制，导致模型回应保守。为解决此问题，本研究提出了MAPA：一种多轮自适应提示攻击方法。该方法包含两级设计：在每一轮中，交替使用文本-视觉攻击动作以引发最恶意的回应；在多轮之间，通过迭代往复的细化调整攻击轨迹，逐步放大回应的恶意性。实验结果表明，MAPA持续优于现有SOTA方法，在针对LLaVA-V1.6-Mistral-7B、Qwen2.5-VL-7B-Instruct、Llama-3.2-Vision-11B-Instruct和GPT-4o-mini的最新基准测试中，攻击成功率提高了11-35%。|Yiliao Song Team|[2602.14399](http://arxiv.org/abs/2602.14399)|null|
|**2026-02-15**|**WoVR: World Models as Reliable Simulators for Post-Training VLA Policies with RL**|强化学习（RL）有望提升VLA模型超越模仿学习的能力，但其需要大量真实世界交互，难以直接部署到物理机器人。现有研究尝试使用学习到的世界模型作为模拟器进行策略优化，但闭环的想象轨迹不可避免地存在幻觉和长期误差累积，这会破坏优化信号。本研究提出了WoVR，一个可靠的基于世界模型的强化学习框架，用于VLA策略的后训练，它明确地调节RL与不完美想象动态的交互。WoVR通过可控的动作条件视频世界模型提高轨迹稳定性，通过关键帧初始化轨迹减少有效误差深度，并通过世界模型-策略协同演化维持策略-模拟器对齐。在LIBERO基准和真实机器人操作上的广泛实验表明，WoVR实现了稳定的长视距想象轨迹和有效的策略优化，将LIBERO平均成功率从39.95%提高到69.2%，真实机器人成功率从61.7%提高到91.7%，验证了在有效控制幻觉的前提下，学习到的世界模型可作为强化学习的实用模拟器。|Dongbin Zhao Team|[2602.13977](http://arxiv.org/abs/2602.13977)|null|
|**2026-02-14**|**Semantic-Contact Fields for Category-Level Generalizable Tactile Tool Manipulation**|通用工具操作需要语义规划和精确物理控制，但现有通用机器人策略缺乏高保真物理基础，而接触感知策略又往往实例特定，难以泛化。大规模真实世界触觉数据难以获取，且软传感器复杂动力学使零样本仿真到真实迁移充满挑战。针对此，本文提出了语义-接触场（SCFields），一种融合视觉语义和密集接触估计的统一3D表示。通过两阶段Sim-to-Real接触学习管道实现：首先在大规模模拟数据上预训练以学习通用接触物理，再通过少量真实数据和伪标签进行微调以对齐传感器特性，从而实现对未见工具的物理泛化。SCFields作为扩散策略的密集观测输入，在刮擦、蜡笔画和剥皮任务中表现出鲁棒的类别级泛化能力，显著优于纯视觉和原始触觉基线。|Yan Wu Team|[2602.13833](http://arxiv.org/abs/2602.13833)|null|
|**2026-02-14**|**MOTIF: Learning Action Motifs for Few-shot Cross-Embodiment Transfer**|视觉-语言-动作（VLA）模型在通用机器人学习方面取得进展，但由于运动学异质性和高昂的数据收集成本，跨具身（cross-embodiment）迁移仍具挑战。现有跨具身策略多依赖共享-私有架构，存在私有参数容量有限和缺乏明确适应机制的问题。为解决这些限制，本文提出了MOTIF框架，旨在通过解耦具身无关的时空模式（称为动作基序）实现高效的少样本跨具身迁移。MOTIF首先通过带有进度感知对齐和具身对抗约束的向量量化学习统一基序，确保时空和跨具身一致性；随后，设计轻量级预测器从实时输入中预测基序，并将其与机器人特定状态融合，指导流匹配策略生成新具身动作。在仿真和真实世界环境中的评估均验证了MOTIF的优越性，少样本迁移成功率显著提升。|Heng Tao Shen Team|[2602.13764](http://arxiv.org/abs/2602.13764)|null|
|**2026-02-14**|**HBVLA: Pushing 1-Bit Post-Training Quantization for Vision-Language-Action Models**|视觉-语言-动作（VLA）模型因其巨大的计算和内存开销，难以部署在资源受限的机器人和边缘平台。尽管权重二值化可提高效率，但现有方法无法弥合二值化与全精度权重之间的分布差距，导致长期闭环执行中量化误差累积并严重降低动作质量。针对此，本文提出了HBVLA，一个VLA定制的二值化框架。HBVLA首先利用策略感知的增强Hessian识别对动作生成关键的权重，然后对非显著权重进行稀疏正交变换以引入低熵中间状态，最后在Harr域中对所有权重进行组式1比特量化。实验结果表明，HBVLA在LIBERO和SimplerEnv上，量化模型分别保留了92.2%和93.6%的全精度性能，并显著优于现有最先进的二值化方法，在真实世界评估中也仅造成微小的成功率下降，展示了在严格硬件约束下的鲁棒部署能力。|Ivor Tsang Team|[2602.13710](http://arxiv.org/abs/2602.13710)|null|
|**2026-02-13**|**Steerable Vision-Language-Action Policies for Embodied Reasoning and Hierarchical Control**|预训练的视觉-语言模型（VLMs）虽能提供丰富的常识先验，但将其有效落地到机器人行为仍具挑战，现有分层方法通过自然语言指令连接VLM与VLA，限制了VLM对低层行为的引导。为增强VLM对低层行为的控制，本文引入“可控策略”（Steerable Policies），即在多抽象层次（如子任务、运动、像素坐标）的丰富合成指令上训练VLA模型。这使得VLM能够通过上下文学习引导这些策略，从而解锁其预训练知识并提升任务泛化能力。广泛的真实世界操作实验表明，与现有基于VLM的VLA和分层基线相比，新方法在泛化和长周期任务中表现更优。|Sergey Levine Team|[2602.13193](http://arxiv.org/abs/2602.13193)|null|
|**2026-02-13**|**UniManip: General-Purpose Zero-Shot Robotic Manipulation with Agentic Operational Graph**|通用机器人操作需要机器人无缝连接高层语义意图与低层物理交互，但现有方法在零样本泛化方面存在不足。针对此问题，本文提出了UniManip框架，其核心是双层代理操作图（AOG），旨在统一语义推理与物理接地。该框架通过高层代理层进行任务编排，低层场景层表示动态状态，实现抽象规划与几何约束的持续对齐，从而支持鲁棒的零样本执行。作为一个动态代理循环，UniManip能从非结构化感知中实例化以物体为中心的场景图，通过安全感知局部规划器参数化无碰撞轨迹，并利用结构化记忆自主诊断和从执行失败中恢复。广泛实验证明，该系统在未见过物体和任务上的零样本成功率分别比VLA和分层基线高22.5%和25.0%，并能直接零样本迁移到移动操作任务。|Ziwei Wang Team|[2602.13086](http://arxiv.org/abs/2602.13086)|**[link](https://henryhcliu.github.io/unimanip)**|
|**2026-02-13**|**Learning Native Continuation for Action Chunking Flow Policies**|动作分块能使视觉-语言-动作（VLA）模型实时运行，但简单的分块执行常导致块边界处的不连续性。现有的实时分块（RTC）方法虽能缓解此问题，但由于其在策略外部，会引起虚假的多模态切换和非内在平滑的轨迹。为此，本文提出了Legato，一种针对基于动作分块流的VLA策略的训练时序延续方法。Legato通过将去噪初始化为已知动作和噪声的调度形混合物，使模型接触部分动作信息，并重塑学习到的流动力学以确保训练和推理在每步引导下保持一致性，同时使用随机调度条件训练以支持不同推理延迟和实现可控平滑性。实验证明，Legato能生成更平滑的轨迹，减少执行时的虚假多模态切换，从而减少犹豫和缩短任务完成时间，在五项操作任务中均比RTC表现更优，轨迹平滑度和任务完成时间均提升约10%。|Yang Gao Team|[2602.12978](http://arxiv.org/abs/2602.12978)|**[link](https://lyfeng001.github.io/Legato/)**|
|**2026-02-13**|**ALOE: Action-Level Off-Policy Evaluation for Vision-Language-Action Model Post-Training**|在真实世界中通过在线强化学习（RL）改进大型视觉-语言-动作（VLA）系统时，价值函数估计是关键，但其通常从混合数据源中收集的轨迹片段进行，这本质上是一个离策略评估问题，而现有工作常采用保守的同策略估计，限制了学习效果。为解决这一问题，本文提出了ALOE（Action-Level Off-Policy Evaluation）框架，用于VLA的后期训练。ALOE采用基于分块的时间差分自举法来评估单个动作序列而非预测最终任务结果，这在稀疏奖励下能更好地将信用归因于关键动作分块，并支持稳定的策略改进。在三项真实世界操作任务上的评估表明，ALOE在不影响执行速度的前提下提高了学习效率，验证了离策略RL在VLA后期训练中可被可靠地重新引入。|Maoqing Yao Team|[2602.12691](http://arxiv.org/abs/2602.12691)|null|
|**2026-02-13**|**SignScene: Visual Sign Grounding for Mapless Navigation**|人类可利用导航标识在陌生环境中无需地图进行导航，本研究旨在使机器人也能利用标识实现开放世界中的无图导航。核心挑战在于解释标识：真实世界标识多样复杂，其抽象语义内容需与局部3D场景进行接地。本文将此形式化为标识接地问题，即把标识上的语义指令映射到对应的场景元素和导航动作。考虑到视觉-语言模型（VLMs）具备所需的语义常识和推理能力但对空间表示敏感，我们提出了SignScene，一种以标识为中心的空间-语义表示方法，它捕获导航相关的场景元素和标识信息，并以有利于VLM有效推理的形式呈现。在包含9种不同环境类型、114个查询的数据集上评估，SignScene达到了88%的接地准确率，显著优于基线方法，并能驱动Spot机器人在真实世界中仅依赖标识进行无图导航。|David Hsu Team|[2602.12686](http://arxiv.org/abs/2602.12686)|null|
|**2026-02-13**|**Xiaomi-Robotics-0: An Open-Sourced Vision-Language-Action Model with Real-Time Execution**|本文介绍了小米机器人0号（Xiaomi-Robotics-0），一个针对高性能、快速且平滑实时执行优化的先进视觉-语言-动作（VLA）模型。该方法的核心在于精心设计的训练配方和部署策略。模型首先在大规模跨实体机器人轨迹和视觉-语言数据上进行预训练，赋予其广泛且泛化的动作生成能力，同时避免灾难性遗忘底层VLM的视觉语义知识。在后期训练中，作者提出了多种技术用于异步执行训练，以解决真实机器人执行时的推理延迟问题。部署时，仔细对齐连续预测动作块的时间步，确保实时执行的连续性和无缝性。广泛的模拟基准测试和两个需要精确灵巧双臂操作的真实机器人任务评估表明，该方法在所有模拟基准上均达到了最先进的性能，并在真实机器人任务上使用消费级GPU实现了高成功率和吞吐量，代码和模型检查点已开源。|Quanyun Zhou Team|[2602.12684](http://arxiv.org/abs/2602.12684)|**[link](https://xiaomi-robotics-0.github.io)**|
|**2026-02-13**|**CRAFT: Adapting VLA Models to Contact-rich Manipulation via Force-aware Curriculum Fine-tuning**|视觉-语言-动作（VLA）模型在处理接触密集型操作任务时面临挑战，因为成功需要精确对齐、稳定接触和处理变形物体，而高熵视觉语言输入与低熵但关键的力信号之间存在不平衡，导致模型过度依赖感知并产生不稳定控制。针对此问题，本文引入CRAFT，一个力感知课程微调框架，该框架集成了一个变分信息瓶颈模块，以在早期训练中调节视觉和语言嵌入，鼓励模型优先处理力信号，然后逐步恢复完整的多模态信息。为实现力感知学习，作者设计了一个同源主从遥操作系统，用于收集各种接触密集型任务中同步的视觉、语言和力数据。真实世界实验表明，CRAFT持续提高了任务成功率，泛化到未见物体和新任务变体，并能有效适应不同VLA架构，从而实现鲁棒且可泛化的接触密集型操作。|Jingtao Sun Team|[2602.12532](http://arxiv.org/abs/2602.12532)|null|
|**2026-02-13**|**AsyncVLA: An Asynchronous VLA for Fast and Robust Navigation on the Edge**|当前机器人基础模型虽泛化能力强，但推理延迟高，导致在动态环境中不安全。针对此问题，本文提出了AsyncVLA异步控制框架，将语义推理与反应性执行解耦。该框架通过远程工作站上的大型基础模型提供高层指导，同时由轻量级板载Edge Adapter高频精细化动作，并通过端到端微调协议和轨迹重加权策略弥合异步流间的域间隙。在面对高达6秒通信延迟的真实视觉导航任务中，AsyncVLA的成功率比现有最佳基线高出40%，成功连接了大型模型的语义智能与边缘机器人所需的实时反应能力。|Sergey Levine Team|[2602.13476](http://arxiv.org/abs/2602.13476)|null|
|**2026-02-13**|**FlowHOI: Flow-based Semantics-Grounded Generation of Hand-Object Interactions for Dexterous Robot Manipulation**|现有视觉-语言-动作（VLA）模型在长周期、接触密集型任务中表现不佳，原因在于缺乏对手-物体交互（HOI）结构的明确表示。为解决此问题，本文提出FlowHOI，一个两阶段流匹配框架，可根据自我中心观察、语言指令和3D高斯泼溅场景重建生成语义明确、时间连贯的HOI序列。该方法将以几何为中心的抓取与以语义为中心的操控解耦，并利用3D场景令牌和运动-文本对齐损失进行语义接地，同时通过从大规模自我中心视频重建HOI轨迹的方法弥补高保真HOI监督的稀缺性。实验结果显示，FlowHOI在GRAB和HOT3D基准测试中实现了最高的动作识别准确率，物理模拟成功率比扩散基线高1.7倍，推理速度提升40倍，并成功在真实机器人上执行了灵巧操作任务。|Xingxing Zuo Team|[2602.13444](http://arxiv.org/abs/2602.13444)|**[link](https://huajian-zeng.github.io/projects/flowhoi/)**|
|**2026-02-12**|**Scaling Verification Can Be More Effective than Scaling Policy Learning for Vision-Language-Action Alignment**|通用机器人理解并执行自然语言指令是长期愿景，尽管视觉-语言-动作（VLA）模型已取得显著进展，但其生成动作仍可能与指令不符。为缩小“意图-动作差距”，本文研究了测试时验证方法。通过分析具身指令遵循的测试时缩放定律，发现联合缩放复述指令和生成动作的数量能更有效地增加测试时样本多样性。在此基础上，提出了CoVer，一种用于VLA对齐的对比验证器，该架构能随计算资源和数据的增加而良好扩展。进一步引入“启动时计算”和分层验证推理流程：在部署时预计算多样化的复述指令，为每条指令重复生成动作候选，然后使用验证器选择最优高层提示和低层动作块。实验表明，相比扩展策略预训练，CoVer在SIMPLER基准上实现了22%的分布内和13%的分布外增益，并在真实世界实验中进一步提高了45%，在PolaRiS基准上任务进度和成功率分别提升了14%和9%。|Marco Pavone Team|[2602.12281](http://arxiv.org/abs/2602.12281)|null|
|**2026-02-12**|**GigaBrain-0.5M*: a VLA That Learns From World Model-Based Reinforcement Learning**|针对现有视觉-语言-动作（VLA）模型在场景理解和未来预测方面的局限性，本研究提出GigaBrain-0.5M*，一个通过世界模型强化学习训练的VLA模型。该模型基于已在大量机器人操作数据上预训练的GigaBrain-0.5，并整合了RAMP（Reinforcement learning via world Model-conditioned Policy）以实现鲁棒的跨任务适应。实验结果表明，RAMP在RECAP基线之上取得了显著的性能提升，在洗衣折叠、箱子包装和咖啡制作等挑战性任务上提升约30%，并在真实部署中展示了可靠的长期执行能力，能够无故障完成复杂的操纵任务。|Zheng Zhu Team|[2602.12099](http://arxiv.org/abs/2602.12099)|**[link](https://gigabrain05m.github.io/)**|
|**2026-02-12**|**VLAW: Iterative Co-Improvement of Vision-Language-Action Policy and World Model**|为提升视觉-语言-动作（VLA）模型性能和可靠性，并解决真实世界数据收集成本高及现有世界模型物理保真度不足的问题，本研究提出一个迭代改进算法。该算法利用少量真实世界试错数据提高世界模型的保真度，然后世界模型生成补充合成数据以改进VLA模型。在真实机器人上的实验表明，该方法使先进VLA模型的成功率相较于基础策略绝对提升39.2%，并且通过生成的合成试错数据训练，又额外提升了11.6%。|Chelsea Finn Team|[2602.12063](http://arxiv.org/abs/2602.12063)|null|
|**2026-02-12**|**HoloBrain-0 Technical Report**|针对基础模型研究与可靠机器人真实部署之间的差距，本研究提出了HoloBrain-0，一个全面的视觉-语言-动作（VLA）框架。其核心是一个新颖的VLA架构，通过明确整合机器人具身先验（如多视角相机参数和运动学描述）来增强3D空间推理并支持多样化具身形态。该设计通过“预训练后微调”范式验证，在多个仿真基准和长时程真实世界操作任务中取得领先结果，且一个高效的0.2B参数变体能支持低延迟部署。研究还全面开源了HoloBrain生态系统，旨在加速研究和实际应用。|Zhizhong Su Team|[2602.12062](http://arxiv.org/abs/2602.12062)|null|
|**2026-02-12**|**When would Vision-Proprioception Policies Fail in Robotic Manipulation?**|针对视觉-本体感受策略在机器人运动过渡阶段视觉模态作用受限、策略倾向于利用本体感受信号导致视觉学习受抑制的问题，本研究提出了梯度调整与阶段引导（GAP）算法。该算法通过利用本体感受信息估计运动过渡阶段的概率，并自适应地调整本体感受梯度的幅值，以实现视觉和本体感受模态的动态协同。综合实验表明，GAP算法能够提升视觉-本体感受策略的鲁棒性和泛化性，适用于模拟和真实环境、单臂和双臂设置，并兼容多种VLA模型。|Di Hu Team|[2602.12032](http://arxiv.org/abs/2602.12032)|null|
|**2026-02-12**|**JEPA-VLA: Video Predictive Embedding is Needed for VLA Models**|针对现有视觉-语言-动作（VLA）模型存在的样本效率低和泛化能力有限问题，本研究发现其根源在于预训练视觉表示在环境理解和策略先验方面的知识不足。通过深入分析，研究指出在视频上预训练的预测嵌入，特别是V-JEPA 2，能更有效地捕捉任务相关时态动态并忽略不可预测因素，从而弥补了现有视觉表示的缺陷。在此基础上，提出了JEPA-VLA，一种将预测嵌入自适应整合到现有VLA中的方法。实验证明，JEPA-VLA在LIBERO、LIBERO-plus、RoboTwin2.0和真实机器人任务等一系列基准测试中均取得了显著性能提升。|Mingsheng Long Team|[2602.11832](http://arxiv.org/abs/2602.11832)|null|
|**2026-02-12**|**ABot-N0: Technical Report on the VLA Foundation Model for Versatile Embodied Navigation**|针对具身导航任务碎片化的问题，本研究引入了ABot-N0，一个统一的视觉-语言-动作（VLA）基础模型，旨在实现点目标、物体目标、指令遵循、兴趣点目标和人员跟踪五大核心任务的“大一统”。该模型采用分层“大脑-动作”架构，利用大型语言模型进行语义推理，并结合流匹配专家生成精确轨迹。为支持大规模学习，研究构建了ABot-N0数据引擎，整理了海量专家轨迹和推理样本。实验结果表明，ABot-N0在7个基准测试中取得了最先进的性能，并能通过集成的Agentic导航系统实现动态真实世界环境中的鲁棒、长时程任务执行。|Mu Xu Team|[2602.11598](http://arxiv.org/abs/2602.11598)|**[link](https://amap-cvlab.github.io/ABot-Navigation/ABot-N0/)**|
|**2026-02-12**|**Scaling World Model for Hierarchical Manipulation Policies**|针对视觉-语言-动作（VLA）模型在域外（OOD）设置下泛化能力不足的问题，本研究引入了一个分层VLA框架VISTA。VISTA利用大规模预训练世界模型的泛化能力实现鲁棒且可泛化的视觉子目标任务分解，其中高层世界模型规划任务分解为带有目标图像的子任务序列，低层VLA策略遵循文本和视觉指导生成动作。这些合成的目标图像为低层策略提供了视觉和物理上的详细指导，使其能够泛化到未见过的物体和新场景。实验结果表明，在世界模型生成的指导下，VISTA在大量OOD场景中显著提升了VLA性能，在novel场景中性能从14%提高到69%。|Xinghang Li Team|[2602.10983](http://arxiv.org/abs/2602.10983)|null|
|**2026-02-12**|**ForeAct: Steering Your VLA with Efficient Visual Foresight Planning**|视觉-语言-动作（VLA）模型将高级语言指令转换为具体可执行动作，在开放世界环境中尤具挑战性。本文提出了Visual Foresight Planning (ForeAct)，这是一种通用高效的规划器，通过想象的未来观察和子任务描述逐步指导VLA。该规划器包含一个高效的预测图像生成模块（在0.33秒内预测高质量未来观察）和一个视觉-语言模型，后者负责推理任务并生成子任务描述。重要的是，ForeAct能够无缝集成到现有VLA中，只需扩充视觉输入而无需修改架构。经过百万级跨具身任务的预训练，预测生成器学习了鲁棒的具身动力学。在包含11项多样化、多步骤真实世界任务的基准测试中，ForeAct实现了87.4%的平均成功率，比基线 $π_0$提高了40.9%，比带有文本子任务指导的$π_0$ 提高了30.3%。|Song Han Team|[2602.12322](http://arxiv.org/abs/2602.12322)|null|
|**2026-02-11**|**H-WM: Robotic Task and Motion Planning Guided by Hierarchical World Model**|针对现有世界模型在长时程机器人规划中累积误差的问题，以及传统符号逻辑世界模型缺乏视觉感知接地的局限性，本研究提出分层世界模型（H-WM）。H-WM在一个统一的双层框架内联合预测逻辑和视觉状态转换，将符号推理的鲁棒性与视觉观察的感知基础相结合。为训练H-WM，研究引入了一个对齐机器人运动与符号状态、动作和视觉观察的机器人数据集。实验证明，分层输出为长时程任务提供了稳定一致的中间指导，有效减轻了误差积累，并在VLA控制策略上展示了该方法的有效性和通用性。|Yingxue Zhang Team|[2602.11291](http://arxiv.org/abs/2602.11291)|null|
|**2026-02-11**|**RISE: Self-Improving Robot Policy with Compositional World Model**|针对视觉-语言-动作（VLA）模型在接触密集和动态操作任务中易受执行偏差影响的脆弱性，以及物理世界中在线强化学习（RL）的限制，本研究提出了RISE，一个通过想象进行机器人强化学习的可扩展框架。其核心是一个组合世界模型，该模型能预测多视角未来并通过进度价值模型评估想象结果，从而为策略改进提供信息丰富的优势。这些组件被整合到一个闭环自改进流水线中，在想象空间中持续生成试错并更新策略。在三个真实世界任务中，RISE相对于现有技术取得了显著性能提升，如在动态砖块分类、背包包装和盒子关闭任务中，绝对性能分别提升超过35%、45%和35%。|Hongyang Li Team|[2602.11075](http://arxiv.org/abs/2602.11075)|**[link](https://opendrivelab.com/kai0-rl/)**|
|**2026-02-11**|**RADAR: Benchmarking Vision-Language-Action Generalization via Real-World Dynamics, Spatial-Physical Intelligence, and Autonomous Evaluation**|针对现有视觉-语言-动作（VLA）模型评估主要局限于仿真或高度受限的真实世界，导致现实差距大、泛化能力差的问题，本研究提出RADAR（Real-world Autonomous Dynamics And Reasoning）基准。RADAR旨在系统评估VLA在真实条件下的泛化能力，集成了物理动力学套件、专门测试空间推理和物理理解的任务，以及基于3D指标的全自主评估流程。通过RADAR对多个先进VLA模型进行审计，发现模型在适度物理动态下性能急剧下降，例如在传感器噪声下3D IoU从0.261下降到0.068，且空间推理能力有限，揭示了模型在真实世界条件下的严重脆弱性，强调了RADAR作为可靠泛化评估基准的必要性。|Guangrun Wang Team|[2602.10980](http://arxiv.org/abs/2602.10980)|null|
|**2026-02-11**|**From Representational Complementarity to Dual Systems: Synergizing VLM and Vision-Only Backbones for End-to-End Driving**|VLA驾驶将语言功能引入端到端规划，但其核心变化除准确性-成本权衡外尚不明确。本文通过RecogDrive进行3-RQ分析，比较VLM和纯视觉骨干网络在相同规划器下的表现。研究发现VLM能引入独特子空间，导致在长尾场景下行为差异，VLM更激进而ViT更保守。基于此，本文提出HybridDriveVLA，通过学习评分器融合两者的轨迹，将PDMS提升至92.10。进一步，DualDriveVLA实现了快慢策略，仅在低置信度时调用VLM，在15%场景中调用VLM即可达到91.00 PDMS，并使吞吐量提高3.2倍。|Yan Wang Team|[2602.10719](http://arxiv.org/abs/2602.10719)|null|
|**2026-02-11**|**Say, Dream, and Act: Learning Video World Models for Instruction-Driven Robot Manipulation**|机器人操作需要预测环境对动作的响应，但现有系统常缺乏此能力，导致效率低下。鉴于VLMs无法明确预测未来状态且现有世界模型存在预测短期或空间不一致帧的问题，本文提出了一种快速且预测性的视频条件动作框架。该方法首先适配鲁棒的视频生成模型以确保未来预测的可靠性，接着通过对抗蒸馏实现快速视频生成，最后训练一个动作模型利用生成视频和真实观测纠正空间错误。实验结果表明，该方法生成的视频预测在时间连贯性和空间准确性方面表现优异，显著提升了具身一致性、空间指代能力和任务完成度。|Yanwei Fu Team|[2602.10717](http://arxiv.org/abs/2602.10717)|null|
|**2026-02-11**|**AugVLA-3D: Depth-Driven Feature Augmentation for Vision-Language-Action Models**|当前VLA模型主要依赖2D图像训练，限制了其在复杂3D环境中的空间理解和动作落地。为解决此问题，本文提出了一个将深度估计集成到VLA模型中的新框架，以丰富3D特征表示。该方法利用VGGT从RGB输入中提取3D几何线索，并引入“动作助手”模块，通过动作先验约束3D表示以确保与下游控制任务的一致性。实验结果表明，所提出的方法不仅增强了几何模糊场景中的感知，还显著提高了动作预测准确性，强调了深度驱动数据增强对弥合2D观测与3D决策差距的潜力。|F. Richard Yu Team|[2602.10698](http://arxiv.org/abs/2602.10698)|null|
|**2026-02-11**|**Improving Medical Visual Reinforcement Fine-Tuning via Perception and Reasoning Augmentation**|强化微调（RFT）在语言模型后训练中展现潜力，但在跨模态、以视觉为中心的医学影像领域应用不足，而该领域需要强大的视觉感知和结构化推理。为此，本文提出了VRFT-Aug，一个专为医学领域设计的视觉强化微调框架。VRFT-Aug通过引入先验知识注入、感知驱动策略优化、医学知情奖励塑造和行为模仿等训练策略来增强感知和推理。实验结果表明，该方法在多个医学数据集上持续优于标准监督微调和RFT基线，并提供了可推广到其他医学图像任务的实用训练启发。|Qicheng Lao Team|[2602.10619](http://arxiv.org/abs/2602.10619)|null|
|**2026-02-11**|**LAP: Language-Action Pre-Training Enables Zero-shot Cross-Embodiment Transfer**|机器人学长期目标是实现零样本部署的通用策略，但现有VLA模型与训练实体紧密耦合。针对此问题，本文提出了语言-动作预训练（LAP），一种将低级机器人动作直接用自然语言表示的方法，使动作监督与预训练VLM的输入-输出分布对齐，无需定制化设计或昂贵标注。基于LAP，本文提出了LAP-3B，实现了在未见机器人上的显著零样本迁移，平均成功率超过50%，比现有VLA模型提升约2倍。此外，LAP还支持高效适应、有利扩展，并通过统一的语言-动作格式在协同训练中获得额外收益。|Anirudha Majumdar Team|[2602.10556](http://arxiv.org/abs/2602.10556)|**[link](https://lap-vla.github.io)**|
|**2026-02-11**|**Found-RL: foundation model-enhanced reinforcement learning for autonomous driving**|强化学习在自动驾驶中面临样本效率低和语义可解释性不足问题，而基础模型（特别是VLM）虽能提供丰富知识，但高推理延迟阻碍其在RL训练中的实时部署。为此，本文提出了Found-RL平台，通过异步批量推理框架将VLM推理与仿真循环解耦，解决了延迟瓶颈。该平台引入了价值边际正则化和优势加权动作指导等监督机制，将VLM建议有效提炼到RL策略中。此外，通过条件对比动作对齐解决CLIP的动态盲区问题，实现密集奖励塑造。Found-RL使得轻量级RL模型在保持实时推理（500 FPS）的同时，达到接近十亿参数VLM的性能。|Sikai Chen Team|[2602.10458](http://arxiv.org/abs/2602.10458)|null|
|**2026-02-10**|**Hardware Co-Design Scaling Laws via Roofline Modelling for On-Device LLMs**|VLA模型在资源受限设备部署中面临LLM骨干网络选择挑战，需平衡准确性与推理延迟及硬件效率。本文提出了硬件协同设计法则，将模型训练损失建模为架构超参数的函数，并通过屋脊线模型表征推理延迟，从而建立了准确性-延迟的直接对应关系。通过对1,942个候选架构进行经验评估和训练，拟合出架构与训练损失的缩放定律。将该定律与延迟建模结合，本文确定了硬件协同设计LLM的帕累托前沿，并将架构搜索公式化为精度与性能的联合优化。结果显示，该方法将架构选择时间从数月缩短到数天，并在相同延迟下，其协同设计的架构在WikiText-2上的困惑度降低了19.42%。|Cheng Deng Team|[2602.10377](http://arxiv.org/abs/2602.10377)|null|
|**2026-02-10**|**ST4VLA: Spatially Guided Training for Vision-Language-Action Models**|大型VLM虽擅长多模态理解，但在具身任务中将指令转化为低级运动动作时表现不足。为此，本文引入了ST4VLA，一个双系统VLA框架，通过空间引导训练使动作学习与VLM中的空间先验对齐。ST4VLA包含两阶段：首先是空间基础预训练，利用大规模和机器人数据通过点、框、轨迹预测为VLM注入可迁移先验；其次是空间引导动作后训练，通过空间提示促使模型生成更丰富的空间先验以指导动作生成。实验表明，ST4VLA显著提升了Google Robot和WidowX Robot的性能，在SimplerEnv上创造了SOTA，并展现出对未见物体、转述指令的更强泛化能力及在真实世界中对扰动的鲁棒性。|Jiangmiao Pang Team|[2602.10109](http://arxiv.org/abs/2602.10109)|null|
|**2026-02-10**|**EgoHumanoid: Unlocking In-the-Wild Loco-Manipulation with Robot-Free Egocentric Demonstration**|人形机器人运动-操作任务面临数据需求高和具身差距挑战。针对此，本文提出了EgoHumanoid框架，首次利用大量以自我为中心的人类演示数据与有限机器人数据共同训练视觉-语言-动作策略，使人形机器人在多样真实环境中执行运动-操作。为弥合人类与机器人间的形态和视点差异，本文引入了包含硬件设计、数据处理的系统对齐流程，并开发了便携式人类数据收集系统。其核心是视点对齐和动作对齐，分别减少视觉域差异和将人类动作映射到机器人动作空间。真实世界实验表明，整合人类演示数据显著优于仅用机器人数据的基线（51%），尤其是在未见环境中。|Li Chen Team|[2602.10106](http://arxiv.org/abs/2602.10106)|**[link](https://opendrivelab.com/EgoHumanoid)**|

<p align=right>(<a href=#updated-on-20260220>back to top</a>)</p>

## Humanoid

| Publish Date | Title | Chinese Summary | Authors | PDF | Code |
|:---------|:-----------------------|:------------------------|:---------|:------|:------|
|**2026-02-18**|**Learning Humanoid End-Effector Control for Open-Vocabulary Visual Loco-Manipulation**|针对人形机器人在野外进行物体视觉局部操作时末端执行器控制精度和场景泛化理解能力有限的问题，本文提出了HERO范式。该方法通过设计一种精确的残差感知末端执行器跟踪策略，结合了逆运动学、学习型神经正向模型、目标调整和重规划等技术，并利用开放词汇大型视觉模型增强视觉泛化能力。实验表明，该策略将末端执行器跟踪误差降低了3.2倍，使机器人在多样化的真实环境中能够可靠地操作各种日常物品。|Saurabh Gupta Team|[2602.16705](http://arxiv.org/abs/2602.16705)|**[link](https://hero-humanoid.github.io/)**|
|**2026-02-18**|**VIGOR: Visual Goal-In-Context Inference for Unified Humanoid Fall Safety**|为解决人形机器人在杂乱环境中跌倒恢复时现有方法碎片化且泛化性差的问题，本文提出了一种统一的跌倒安全方法，涵盖所有恢复阶段。该方法基于人类跌倒姿态的约束性及感知-运动整合表征的必要性，通过在平坦和复杂地形上训练特权教师模型，并将其蒸馏成一个仅依赖自我中心深度和本体感受的学生模型。模拟和真实Unitree G1人形机器人上的实验结果验证了该方法在多样非平坦环境中无需微调即可实现鲁棒、零样本的跌倒安全。|Stella X. Yu Team|[2602.16511](http://arxiv.org/abs/2602.16511)|null|
|**2026-02-18**|**Humanoid Hanoi: Investigating Shared Whole-Body Control for Skill-Based Box Rearrangement**|为使人形机器人实现长周期箱子重新排列任务，并解决简单复用全身控制器（WBC）在长周期任务中因状态和指令分布变化导致的鲁棒性下降问题，本文提出了一个基于技能的框架。该架构通过共享的、任务无关的WBC执行所有技能，并通过数据聚合过程，利用领域随机化下的闭环技能执行rollout来增强WBC训练。在“Humanoid Hanoi”基准测试及Digit V3人形机器人上的实验结果证明，该方法能实现扩展周期内的完全自主重新排列，并量化了共享WBC方法相对于非共享基线的显著优势。|Alan Fern Team|[2602.13850](http://arxiv.org/abs/2602.13850)|null|
|**2026-02-17**|**Perceptive Humanoid Parkour: Chaining Dynamic Human Skills via Motion Matching**|针对人形机器人难以捕捉高动态人类运动敏捷性和适应性，尤其是在复杂环境中进行跑酷的挑战，本文提出了Perceptive Humanoid Parkour (PHP) 模块化框架。该框架首先通过运动匹配将人类原子技能组合成长周期运动轨迹，接着训练专家策略并蒸馏为基于深度感知、多技能的学生策略，以实现自主、上下文感知的决策。在Unitree G1人形机器人上的真实世界实验展示了其高度动态的跑酷技能，包括攀爬高障碍物和长周期多障碍物穿越，并能闭环适应实时扰动。|C. Karen Liu Team|[2602.15827](http://arxiv.org/abs/2602.15827)|null|
|**2026-02-17**|**MeshMimic: Geometry-Aware Humanoid Motion Learning through 3D Scene Reconstruction**|鉴于现有深度强化学习驱动的人形机器人运动控制依赖昂贵的动作捕捉数据且常因运动与场景解耦导致物理不一致，本文提出了MeshMimic框架。该框架通过桥接3D场景重建和具身智能，利用先进3D视觉模型精确重建人类轨迹与地形几何，并引入基于运动学一致性的优化算法和接触不变重定向方法。实验证明，MeshMimic能够仅使用消费级单目传感器从视频中学习耦合的“运动-地形”交互，并在多样复杂地形上实现鲁棒、高动态的机器人性能。|Yijie Guo Team|[2602.15733](http://arxiv.org/abs/2602.15733)|null|
|**2026-02-16**|**Replanning Human-Robot Collaborative Tasks with Vision-Language Models via Semantic and Physical Dual-Correction**|为解决人机协作中视觉-语言模型（VLMs）在解释模糊指令时出现的幻觉推理和物理执行失败预测问题，本文提出了一个增强VLM推理的双重纠正机制HRC框架。该机制包含一个在执行前验证逻辑一致性和任务可行性的内部纠正模型，以及一个通过执行后反馈检测并纠正物理失败的外部纠正模型。仿真和真实世界协作装配任务的实验结果表明，该方法有效提高了成功率，并能支持机器人对人类指令进行交互式重新规划，从而提升了协作效率与鲁棒性。|Kensuke Harada Team|[2602.14551](http://arxiv.org/abs/2602.14551)|null|
|**2026-02-16**|**AdaptManip: Learning Adaptive Whole-Body Object Lifting and Delivery with Online Recurrent State Estimation**|针对现有模仿学习方法在人形机器人导航、抓取和递送任务中对干扰缺乏鲁棒性的问题，本文提出了AdaptManip框架。该框架通过强化学习训练一个无需人类演示的鲁棒局部操作策略，并集成了循环对象状态估计器、全身基础策略（结合残差操作控制）以及基于LiDAR的抗漂移全局位置估计器。实验结果表明，AdaptManip在适应性和成功率上显著优于基线方法，并在真实世界中实现了人形机器人完全自主的导航、物体抓取和递送任务，即使在遮挡下也能保持操作性能。|Sehoon Ha Team|[2602.14363](http://arxiv.org/abs/2602.14363)|**[link](https://morganbyrd03.github.io/adaptmanip/)**|
|**2026-02-15**|**WIMLE: Uncertainty-Aware World Models with IMLE for Sample-Efficient Continuous Control**|针对模型基强化学习（MBRL）中因模型误差累积、单模态世界模型及过度自信预测导致的性能不佳问题，本文提出了WIMLE方法。该方法将隐式最大似然估计（IMLE）扩展到MBRL框架，以学习随机、多模态的世界模型并利用集成和潜在采样估计不确定性，在训练中根据预测置信度加权合成转换。实验结果表明，WIMLE在多项连续控制任务上实现了优越的样本效率和竞争性的渐近性能，尤其在挑战性任务中将样本效率提高50%以上，突显了多模态和不确定性感知加权对MBRL的价值。|Ke Li Team|[2602.14351](http://arxiv.org/abs/2602.14351)|**[link](https://openreview.net/forum?id=mzLOnTb3WH)**|
|**2026-02-15**|**ProAct: A Dual-System Framework for Proactive Embodied Social Agents**|为解决具身社交代理在实时交互中，主动社交行为所需的长时间上下文推理与低延迟响应之间的冲突，本文提出了ProAct双系统框架。该框架将低延迟的“行为系统”与较慢的“认知系统”解耦，后者执行长周期社交推理并产生高层主动意图，并通过ControlNet条件化的流式流量匹配模型实现意图到连续非语言行为的无缝转换。在物理人形机器人上的用户研究表明，ProAct在感知到的主动性、社交存在感和整体参与度方面显著优于被动系统，验证了双系统主动控制的有效性。|Libin Liu Team|[2602.14048](http://arxiv.org/abs/2602.14048)|**[link](https://proactrobot.github.io/)**|
|**2026-02-14**|**Impact-Robust Posture Optimization for Aerial Manipulation**|针对力矩控制冗余度机器人在冲击过程中状态和指令波动大、鲁棒性差的问题，本文提出了一种优化机器人姿态以提高冲击鲁棒性的方法。该方法利用刚体冲击模型定义配置相关度量，量化冲击前后速度变化，并通过基于梯度的运动任务将其最小化，从而引导机器人达到冲击鲁棒姿态，并将其嵌入任务空间逆动力学（TSID）全身控制器。模拟实验结果显示，该方法使机器人冲击后的状态尖峰最多减少51%，有效避免了执行器饱和，并证明了运动学冗余对冲击鲁棒性的重要性。|Antonio Franchi Team|[2602.13762](http://arxiv.org/abs/2602.13762)|null|
|**2026-02-14**|**A Kung Fu Athlete Bot That Can Do It All Day: Highly Dynamic, Balance-Challenging Motion Dataset and Autonomous Fall-Resilient Tracking**|针对当前类人机器人运动跟踪系统在高动态（如武术）行为下性能受限、缺乏处理不安全状态及恢复策略的问题，本文构建了高动态武术运动数据集KungFuAthlete。在此基础上，提出了一种新颖的训练范式，使单个策略能同时学习高动态运动跟踪和跌倒恢复。实验结果表明，KungFuAthlete数据集在运动强度和复杂性上显著高于现有数据集，且该框架将机器人能力从纯粹的运动跟踪扩展到具备恢复功能的执行，显著提升了类人机器人在真实世界高动态场景中的鲁棒性和自主性。|Xuesong Li Team|[2602.13656](http://arxiv.org/abs/2602.13656)|null|
|**2026-02-13**|**Adding internal audio sensing to internal vision enables human-like in-hand fabric recognition with soft robotic fingertips**|人类区分织物质地的触觉感知能力难以在机器人中复现，因现有触觉传感器难以同时实现高空间分辨率和高时间采样率。本研究提出一种结合了视觉（Minsight）和听觉（Minsound）两种触觉传感器的系统，并通过模拟人类摩擦织物的动作进行感知。研究发现，基于音频的Minsound传感器对织物分类具有高实用性，基于Transformer的方法在20种常见织物数据集上取得了97%的分类精度。该方法还能够泛化学习织物的弹性、厚度和粗糙度，且增加外部麦克风可提高系统在嘈杂环境中的鲁棒性。|Katherine J. Kuchenbecker Team|[2602.12918](http://arxiv.org/abs/2602.12918)|null|
|**2026-02-13**|**PMG: Parameterized Motion Generator for Human-like Locomotion Control**|现有类人机器人全身参考引导方法难以适应高级指令和多样任务，存在对数据量要求高、对速度和姿态敏感以及对机器人标定敏感的问题。本文提出参数化运动生成器（PMG），它基于对人类运动结构分析，利用紧凑的参数化运动数据和高维控制指令合成参考轨迹。结合模仿学习和基于优化的Sim-to-Real电机参数识别模块，在ZERITH Z1类人机器人上验证了该方法能生成自然、类人的运动，精确响应高维控制输入（包括VR遥操作），并实现了高效可验证的Sim-to-Real迁移，为类人机器人自然且可部署的控制提供了实用途径。|Houde Liu Team|[2602.12656](http://arxiv.org/abs/2602.12656)|null|
|**2026-02-13**|**CLOT: Closed-Loop Global Motion Tracking for Whole-Body Humanoid Teleoperation**|当前类人机器人全身长程遥操作面临全局姿态漂移累积的挑战，而现有方法缺乏全局反馈。本文提出CLOT系统，通过高频定位反馈实现闭环全局运动跟踪，确保长时间运行无漂移。为避免直接奖励导致的激进修正，引入数据驱动的随机化策略解耦观察轨迹与奖励评估，并利用对抗运动先验抑制不自然行为。经过20小时人类运动数据和1300 GPU小时训练的Transformer策略，在模拟和真实31自由度类人机器人上验证了其在高动态运动、高精度跟踪及Sim-to-Real传递上的强大鲁棒性。|Yichao Yan Team|[2602.15060](http://arxiv.org/abs/2602.15060)|null|
|**2026-02-12**|**General Humanoid Whole-Body Control via Pretraining and Fast Adaptation**|通用类人机器人全身控制器面临运动分布多样性、快速适应困难及高动态场景下稳定平衡等挑战。现有方法常需任务特定训练或在新运动适应时性能下降。本文提出FAST框架，实现了快速适应和稳定运动跟踪。该框架引入Parseval引导的残差策略适应机制，通过正交性和KL约束学习轻量级增量动作策略，有效适应分布外运动并避免灾难性遗忘。为提升物理鲁棒性，还提出重心感知控制，通过整合重心相关观测和目标来增强跟踪挑战性参考运动时的平衡。仿真和真实世界实验均验证了FAST在鲁棒性、适应效率和泛化性方面优于现有基线。|Zongqing Lu Team|[2602.11929](http://arxiv.org/abs/2602.11929)|null|
|**2026-02-12**|**HAIC: Humanoid Agile Object Interaction Control via Dynamics-Aware World Model**|类人机器人在非结构化环境中执行复杂全身任务时，与欠驱动物体（具有独立动力学和非完整约束）的交互仍是挑战。本文提出HAIC统一框架，无需外部状态估计即可实现与多样对象动力学的鲁棒交互。其核心是一个动力学预测器，仅根据本体感受历史估计高阶物体状态。这些预测被投射到静态几何先验上，形成动态占据图，使策略能在盲区推断碰撞边界和接触可供性。通过不对称微调，世界模型持续适应学生策略的探索，确保分布变化下的状态估计鲁棒性。实验证明HAIC在滑板、推拉推车等敏捷任务中成功率高，并通过预测多物体动力学掌握了跨地形运送箱子等多物体长程任务。|Renjing Xu Team|[2602.11758](http://arxiv.org/abs/2602.11758)|**[link](https://haic-humanoid.github.io/)**|
|**2026-02-12**|**Future Mining: Learning for Safety and Security**|鉴于采矿环境的严苛条件（如光照差、GPS受限、连接不稳定）以及日益增长的网络物理威胁，当前的安全和操作可靠性面临挑战。本文提出一种统一的智能安全与安保架构愿景，旨在整合多模态感知、安全联邦学习、强化学习、DTN通信和能耗感知传感。该架构包含矿工定位、多模态态势感知、后门攻击监控、联邦学习信任和物联网设备健康监测五个核心模块。这些模块协同工作，以应对矿工定位、危险理解、联邦鲁棒性和预测性维护等问题，最终构建一个在对抗条件下能保持操作连续性的弹性智能采矿系统。|Sanjay Madria Team|[2602.11472](http://arxiv.org/abs/2602.11472)|null|
|**2026-02-12**|**Humanoid Manipulation Interface: Humanoid Whole-Body Manipulation from Robot-Free Demonstrations**|当前人形机器人全身操作方法受限于硬件物流和复杂奖励工程，导致自主技能有限且通常仅限于受控环境。为解决这些问题，本文提出了Humanoid Manipulation Interface (HuMI)，一个便携高效的框架，用于在各种环境中学习多样化的全身操作任务。HuMI通过便携硬件捕捉丰富的全身运动，实现无机器人数据收集，并利用分层学习流程将人类运动转化为灵巧且可行的人形技能。广泛实验表明，HuMI的数据收集效率比遥操作提高3倍，并在未知环境中取得了70%的成功率，有效提升了人形机器人的泛化操作能力。|Yang Gao Team|[2602.06643](http://arxiv.org/abs/2602.06643)|**[link](https://humanoid-manipulation-interface.github.io)**|
|**2026-02-11**|**ExtremControl: Low-Latency Humanoid Teleoperation with Direct Extremity Control**|现有类人机器人低延迟遥操作系统因依赖重度预处理和仅基于位置的PD控制，导致延迟高（超过200ms），严重限制了响应速度。本文提出ExtremControl低延迟全身控制框架，其通过直接操作选定刚性链节（主要是末端）的SE(3)姿态，避免全身重定向；利用笛卡尔空间映射将人类运动直接转换为机器人链节目标；并在低层引入速度前馈控制以支持快速响应行为。该框架实现了低至50ms的端到端延迟，显著超越了以往工作200ms的限制，从而使类人机器人能够执行乒乓球平衡、杂耍和实时回击等高响应性任务。|Chuang Gan Team|[2602.11321](http://arxiv.org/abs/2602.11321)|**[link](https://owenowl.github.io/extremcontrol)**|
|**2026-02-11**|**APEX: Learning Adaptive High-Platform Traversal for Humanoid Robots**|尽管深度强化学习显著提升了类人机器人在不平坦地形上的足部移动能力，但攀爬超过腿长的平台仍是难题，因现有训练范式常收敛于不安全的跳跃式解决方案。本文提出APEX系统，通过结合地形条件行为（攀爬上下、行走、姿态重配置）实现感知驱动的攀爬式高平台穿越。核心方法是引入通用棘轮进度奖励，为接触丰富的目标达成机动提供密集且无速度的监督，同时强正则化以确保安全探索。通过训练基于LiDAR的全身机动策略，并利用双重策略（训练时建模映射伪影，部署时过滤和修补高程图）弥合Sim-to-Real感知差距。最终，将所有六种技能整合到一个策略中，在29自由度Unitree G1机器人上实现了0.8米（约腿长114%）平台的零样本Sim-to-Real穿越，并展现出对平台高度和初始姿态的鲁棒适应以及平稳稳定的多技能转换。|Ding Zhao Team|[2602.11143](http://arxiv.org/abs/2602.11143)|**[link](https://apex-humanoid.github.io/)**|
|**2026-02-11**|**Towards Learning a Generalizable 3D Scene Representation from 2D Observations**|现有神经辐射场（NeRF）方法通常在相机坐标系中操作，难以直接应用于需要全局工作空间理解的机器人操作任务。本文提出一种可泛化的神经辐射场方法，能从机器人自我中心观测预测三维工作空间占据。该模型在全局工作空间框架中构建占据表示，可直接应用于机器人操纵任务，并能够整合灵活的源视图且无需场景特定微调即可泛化到未见过的物体排列。在类人机器人上进行验证，模型在40个真实场景训练后实现了26mm的重建误差（包括被遮挡区域），证明了其超越传统立体视觉方法推断完整三维占据的能力及其在机器人操纵中的直接适用性。|Stefan Wermter Team|[2602.10943](http://arxiv.org/abs/2602.10943)|null|
|**2026-02-11**|**MOSAIC: Bridging the Sim-to-Real Gap in Generalist Humanoid Motion Tracking and Teleoperation with Rapid Residual Adaptation**|鉴于通用人形运动追踪器在模拟中表现优异，但在实际硬件持续遥操作时易受接口和动力学误差影响，本文提出了开源全栈系统MOSAIC。该系统首先通过强化学习在多源运动库上训练面向遥操作的通用运动追踪器，采用自适应重采样和强调世界坐标系运动一致性的奖励。为弥合模拟到真实世界的接口差距，MOSAIC通过快速残差适应，使用少量接口特定数据训练一个接口特定策略，并通过加性残差模块将其蒸馏到通用追踪器中，优于传统微调方法。实验结果（包括系统消融、分布外基准测试和真实机器人实验）证明，MOSAIC在实际延迟和噪声下能实现稳健的离线运动回放和在线长周期遥操作。|Alois Knoll Team|[2602.08594](http://arxiv.org/abs/2602.08594)|null|
|**2026-02-11**|**MotionWeaver: Holistic 4D-Anchored Framework for Multi-Humanoid Image Animation**|鉴于现有角色图像动画方法难以泛化到涉及多样人形形式、复杂交互和频繁遮挡的多人场景，本文提出了MotionWeaver框架。该框架引入统一的运动表示，提取与身份无关的运动并明确绑定到角色，以泛化到多样人形并扩展到多人场景。同时，提出了整体4D锚定范式，构建共享4D空间融合运动与视频潜空间，并通过分层4D级别监督强化交互和遮挡处理。为支持此研究，构建了46小时多人视频数据集和300视频基准。定量和定性实验结果表明，MotionWeaver在自建基准上达到SOTA，并能有效泛化至复杂多人场景。|Weizhan Zhang Team|[2602.13326](http://arxiv.org/abs/2602.13326)|null|
|**2026-02-10**|**EgoHumanoid: Unlocking In-the-Wild Loco-Manipulation with Robot-Free Egocentric Demonstration**|针对人形机器人动作操作对数据需求高，而现有方法未能充分利用人类演示数据，且存在人机体现差异的问题，本文提出了EgoHumanoid框架。该框架首次利用大量自我中心人类演示和少量机器人数据共同训练视觉-语言-动作策略，使人形机器人能在多样真实世界环境中执行动作操作。通过硬件设计到数据处理的系统对齐流水线，包括视图对齐和动作对齐，成功弥合了人机之间的形态和视角差异。广泛的真实世界实验表明，整合无机器人自我中心数据相比仅机器人基线性能显著提升51%，尤其是在未见过的环境中，且分析揭示了行为有效迁移及扩展人类数据的潜力。|Li Chen Team|[2602.10106](http://arxiv.org/abs/2602.10106)|**[link](https://opendrivelab.com/EgoHumanoid)**|
|**2026-02-10**|**Humanoid Factors: Design Principles for AI Humanoids in Human Worlds**|随着人形机器人开始与人类共享空间，传统人因工程需要扩展，不仅考虑人类因素，也要考虑人形机器人因素。当前人形机器人带来了人类行为、沟通和社会存在的期望，重塑了可用性、信任和安全。本文引入“人形机器人因素”框架，围绕物理、认知、社会和伦理四大支柱，指导人形机器人开发，使其能有效与人类共存和协作，并表征了人机能力间的重叠与差异。通过评估真实人形机器人控制算法，该框架揭示了传统机器人任务指标如何忽视关键人类认知和交互原则，为设计、评估和管理持续人机共存提供了基础性框架。|Lixiao Huang Team|[2602.10069](http://arxiv.org/abs/2602.10069)|null|
|**2026-02-10**|**TeleGate: Whole-Body Humanoid Teleoperation via Gated Expert Selection with Motion Prior**|针对人形机器人实时全身遥操作中，现有方法通过知识蒸馏将多专家策略整合，常导致高动态运动性能下降的挑战，本文提出了TeleGate统一遥操作框架。其核心思想是训练一个轻量级门控网络，根据本体感知状态和参考轨迹实时动态激活领域特定专家策略，从而保留其完整能力，避免知识蒸馏的性能损失。此外，引入基于VAE的运动先验模块，从历史观测中提取未来运动意图，实现预期控制。在模拟和Unitree G1机器人上的实验表明，TeleGate仅需2.5小时训练数据，即在跑步、跌倒恢复和跳跃等多样动态运动中实现了高精度实时遥操作，显著优于基线方法。|Rongyun Cao Team|[2602.09628](http://arxiv.org/abs/2602.09628)|null|
|**2026-02-09**|**Characteristics, Management, and Utilization of Muscles in Musculoskeletal Humanoids: Empirical Study on Kengoro and Musashi**|当前肌肉骨骼人形机器人研究中，对其生物仿生结构固有的多样属性及其管理利用方式缺乏统一讨论。本研究基于作者团队开发的Kengoro和Musashi机器人，将肌肉骨骼结构特征归纳为冗余性、独立性、各向异性、可变力臂和非线性弹性五大属性。文章进一步探讨了这些属性组合所带来的优势与劣势，并重点讨论了身体图式学习、反射控制、肌肉分组及身体图式适应等机制。最后，研究阐述了通过集成系统实现运动的实践，并展望了未来的研究挑战。|Masayuki Inaba Team|[2602.08518](http://arxiv.org/abs/2602.08518)|null|
|**2026-02-09**|**Learning Human-Like Badminton Skills for Humanoid Robots**|人形机器人实现羽毛球等高强度运动的类人表现面临巨大挑战，尤其是在运动学模仿与功能性、物理感知击打之间难以兼顾自然风格。为解决此问题，本文提出了Imitation-to-Interaction渐进式强化学习框架，旨在使机器人从“模仿者”进化为“击球手”。该方法通过人类数据建立运动先验，蒸馏到模型化状态表示中，并利用对抗性先验稳定动力学，同时引入流形扩展策略以应对稀疏的专家演示。实验结果显示，该框架在仿真中掌握了多样羽毛球技能，并首次实现了类人羽毛球技能从仿真到真实机器人的零样本迁移，展示了物理世界中的优雅和精准打击。|Peng Lu Team|[2602.08370](http://arxiv.org/abs/2602.08370)|null|
|**2026-02-07**|**VividFace: Real-Time and Realistic Facial Expression Shadowing for Humanoid Robots**|人形机器人面部表情实时模仿对于实现逼真、情感丰富的人机交互至关重要，但现有方法常因离线推理和细节捕捉不足而难以同时达到实时性和逼真性。为解决这些局限，本文提出了VividFace，一个实时且逼真的人形机器人面部表情阴影系统。该系统通过优化模仿框架X2CNet++，并引入特征适应训练策略，显著增强了表情表现力；同时，通过视频流兼容推理管线和基于异步I/O的工作流，实现了高效的实时模仿。广泛的真实世界演示验证了VividFace在0.05秒内模仿人类表情并生成生动人形面部的实用能力，且能泛化至多种面部配置。|Yang Zhang Team|[2602.07506](http://arxiv.org/abs/2602.07506)|null|
|**2026-02-07**|**TextOp: Real-time Interactive Text-Driven Humanoid Robot Motion Generation and Control**|现有的人形机器人全身控制器在灵活性和自主性方面存在局限，难以实现实时和交互式驱动。为解决这一问题，本文提出了TextOp，一个实时文本驱动的人形运动生成与控制框架，支持流式语言指令和即时修改。TextOp采用两级架构：高级运动扩散模型根据文本生成短时域轨迹，低级运动跟踪策略则在机器人上执行这些轨迹。广泛的真实机器人实验和离线评估表明，TextOp实现了即时响应、平滑全身运动和精确控制，在舞蹈、跳跃等复杂行为中展现出自由形式的意图表达和流畅过渡。|Xuelong Li Team|[2602.07439](http://arxiv.org/abs/2602.07439)|**[link](https://text-op.github.io/)**|
|**2026-02-07**|**Bridging Speech, Emotion, and Motion: a VLM-based Multimodal Edge-deployable Framework for Humanoid Robots**|为解决多数人形机器人缺乏协调的语音、面部表情和手势，以及在设备上自主运行的需求，本文提出了SeM²，一个基于视觉语言模型的框架。SeM²通过多模态感知模块捕捉用户上下文，结合思维链推理规划响应，并利用语义序列对齐机制确保言语内容与物理表达的精确时间协调，从而实现情感一致的多模态交互。研究实现了云端及边缘部署版本，其中边缘版本通过知识蒸馏高效运行。综合评估显示，SeM²在自然度、情感清晰度和模态一致性方面显著优于单模态基线，推动了社交表达性人形机器人在多样现实环境中的应用。|Miao Li Team|[2602.07434](http://arxiv.org/abs/2602.07434)|null|
|**2026-02-06**|**Cerebellar-Inspired Residual Control for Fault Recovery: From Inference-Time Adaptation to Structural Consolidation**|针对机器人策略在真实世界部署中常遇到的训练后故障，且不便重新训练的问题，本文提出了一种推理时、受小脑启发的残差控制框架。该框架通过在线校正动作增强冻结的强化学习策略，无需修改基础策略参数即可实现故障恢复。它实例化了小脑核心原理，如高维模式分离、并行残差路径和局部误差驱动可塑性，并通过保守的元适应调节残差权限。实验结果表明，在MuJoCo基准测试中，该框架在执行器、动力学和环境扰动下，对HalfCheetah-v5和Humanoid-v5在适度故障下性能显著提升，并在严重故障下表现出优雅的性能下降。|Amit Ranjan Trivedi Team|[2602.07227](http://arxiv.org/abs/2602.07227)|null|
|**2026-02-06**|**DynaRetarget: Dynamically-Feasible Retargeting using Sampling-Based Trajectory Optimization**|将人类运动重定向到人形机器人控制策略并确保其动态可行性是一项挑战。本文介绍了DynaRetarget，一个将人类运动重定向到人形控制策略的完整流程。其核心是新颖的基于采样的轨迹优化（SBTO）框架，该框架能将不完善的运动学轨迹细化为动态可行的运动，并通过逐步推进优化范围来处理长时域任务。DynaRetarget在重定向数百个人形-物体演示中取得了比现有技术更高的成功率，并能泛化到不同物体属性的场景，为生成大规模人形局部操作轨迹合成数据集提供了可能。|Majid Khadiv Team|[2602.06827](http://arxiv.org/abs/2602.06827)|null|
|**2026-02-06**|**ECO: Energy-Constrained Optimization with Reinforcement Learning for Humanoid Walking**|为解决仿人机器人节能稳定运动中现有方法需大量超参数调优且易导致次优策略的问题，本研究提出了ECO（Energy-Constrained Optimization）约束强化学习框架。该框架将能量指标明确定义为不等式约束，通过拉格朗日法强制执行能量消耗和参考运动约束，从而实现稳定、对称且节能的行走。ECO在仿真和真实机器人（BRUCE）上的实验结果表明，在保持鲁棒行走性能的同时，其能耗显著低于MPC、标准RL及其他SOTA约束RL方法，实现了仿人机器人节能运动的实质性进步。|Yao Su Team|[2602.06445](http://arxiv.org/abs/2602.06445)|null|
|**2026-02-06**|**Now You See That: Learning End-to-End Humanoid Locomotion from Raw Pixels**|为解决基于视觉的仿人机器人鲁棒运动中模拟到真实差距产生的感知噪声和多样地形下学习目标冲突的挑战，本研究提出了一个端到端的视觉驱动运动框架。为实现鲁棒的sim-to-real迁移，该框架开发了高保真深度传感器模拟，并提出了视觉感知行为蒸馏方法。为适应多样地形，引入了地形特定奖励塑形，并结合多评论家和多判别器学习。在配备不同立体深度摄像头的两个仿人机器人平台上，该策略展现出在多样环境中的鲁棒性能，能处理极端挑战和精细任务，包括双向长期楼梯遍历。|Zongwu Xie Team|[2602.06382](http://arxiv.org/abs/2602.06382)|null|

<p align=right>(<a href=#updated-on-20260220>back to top</a>)</p>
