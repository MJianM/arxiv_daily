---
layout: default
---

## Updated on 2026.02.17

## Categories

- [Manipulation](#manipulation)
- [World Model](#world-model)
- [VLM](#vlm)
- [VLA](#vla)
- [Humanoid](#humanoid)

## Manipulation

| Publish Date | Title | Chinese Summary | Authors | PDF | Code |
|:---------|:-----------------------|:------------------------|:---------|:------|:------|
|**2026-02-16**|**BPP: Long-Context Robot Imitation Learning by Focusing on Key History Frames**|针对机器人任务中对历史观测的依赖性，传统策略因训练时历史空间覆盖不足，易受虚假关联影响而泛化性差。本文提出“宏观策略”（Big Picture Policies, BPP），通过视觉-语言模型检测的关键帧来条件化策略，将多样化轨迹投射到紧凑的任务相关事件集，从而显著减少训练与部署间的分布偏移。实验结果表明，BPP在四项真实世界操作任务和三项仿真任务中，真实世界成功率比现有最佳方法高70%。|Aviral Kumar Team|[2602.15010](http://arxiv.org/abs/2602.15010)|null|
|**2026-02-16**|**PhyScensis: Physics-Augmented LLM Agents for Complex Physical Scene Arrangement**|鉴于自动生成交互式3D环境对机器人仿真数据收集的关键性，以及现有方法忽略物体间物理关系的局限性，本文提出PhyScensis框架。该框架结合大型语言模型（LLM）智能体和物理引擎，迭代提出包含空间和物理谓词的资产，并通过物理引擎求解器实现3D场景配置，同时利用反馈机制优化配置。实验证明，PhyScensis在场景复杂度、视觉质量和物理准确性方面均优于现有方法，为复杂物理场景生成提供了一套统一解决方案。|Chuang Gan Team|[2602.14968](http://arxiv.org/abs/2602.14968)|null|
|**2026-02-16**|**Affordance Transfer Across Object Instances via Semantically Anchored Functional Map**|面对传统示教学习（LfD）收集成本高且难以泛化到几何差异大对象的挑战，本文提出了“语义锚定功能图”（Semantic Anchored Functional Maps, SemFM）框架。该方法从单个视觉演示出发，通过识别语义功能区域、选择语义锚点并利用功能图传播约束，实现了跨几何差异对象的稠密、语义一致功能属性迁移。实验结果表明，SemFM能以较低计算成本实现准确的功能属性迁移，适用于实用的机器人感知-动作流水线。|Weiming Zhi Team|[2602.14874](http://arxiv.org/abs/2602.14874)|null|
|**2026-02-16**|**DriveFine: Refining-Augmented Masked Diffusion VLA for Precise and Robust Driving**|针对自动驾驶VLA模型中扩散和Token规划器存在的模态对齐、训练效率低、泛化受限及累积误差等问题，本文提出DriveFine模型。该模型是一种掩码扩散VLA模型，引入了新型即插即用block-MoE模块，将生成专家与细化专家解耦，实现灵活解码和自校正。结合混合强化学习策略，DriveFine在NAVSIM v1、v2和Navhard基准测试中展现出卓越的有效性和鲁棒性。|Yan Wang Team|[2602.14577](http://arxiv.org/abs/2602.14577)|null|
|**2026-02-16**|**RoboSolver: A Multi-Agent Large Language Model Framework for Solving Robotic Arm Problems**|为解决机器人操作器相关问题，本文提出了一个基于LLMs和VLMs的智能多智能体框架。该框架能够接收文本和视觉输入，并自动执行正逆运动学、速度加速度计算、3D仿真及运动控制。在三项基准测试中，与GPT-4o等大型模型结合后，框架在正运动学计算、视觉输入任务以及多功能机器人任务中均显著提升了准确率，最高达到0.97，远超原始模型的性能。|Alireza Taheri Team|[2602.14438](http://arxiv.org/abs/2602.14438)|null|
|**2026-02-16**|**A Soft Wrist with Anisotropic and Selectable Stiffness for Robust Robot Learning in Contact-rich Manipulation**|针对非结构化环境中接触式操作任务对机器人学习鲁棒性的挑战以及现有软末端执行器的局限性，本文提出了一种新型柔性腕部机构CLAW（Compliant Leaf-spring Anisotropic soft Wrist）。该设计利用正交板簧和带锁定机构的旋转关节，实现了大范围6自由度变形、可调各向异性刚度，且具备轻量化、低成本特性。实验结果表明，CLAW在插销任务中成功率达76%，优于其他主流夹具，展现了在接触密集场景中实现鲁棒机器人学习的潜力。|Masashi Hamaya Team|[2602.14434](http://arxiv.org/abs/2602.14434)|null|
|**2026-02-16**|**AdaptManip: Learning Adaptive Whole-Body Object Lifting and Delivery with Online Recurrent State Estimation**|针对人形机器人集成导航、物体举升和交付任务中模仿学习策略对干扰不鲁棒的问题，本文提出AdaptManip框架。该框架通过强化学习自主训练，无需人工演示，包含循环对象状态估计器、全身基础运动策略与残差操作控制，以及基于LiDAR的抗漂移全局定位。所有组件在仿真中训练后零样本部署到真实硬件，实验证明AdaptManip在适应性和成功率上显著优于基线方法，并实现了人形机器人在真实世界中的完全自主导航和操作。|Sehoon Ha Team|[2602.14363](http://arxiv.org/abs/2602.14363)|**[link](https://morganbyrd03.github.io/adaptmanip/)**|
|**2026-02-15**|**GRAIL: Goal Recognition Alignment through Imitation Learning**|为解决现有目标识别方法依赖最优策略表示而无法准确识别次优行为者目标的问题，本文提出通过模仿学习进行目标识别对齐（GRAIL）。该方法利用模仿学习和逆强化学习，直接从（可能次优的）演示轨迹中为每个候选目标学习一个目标导向策略，并能一次性推理。实验结果表明，GRAIL在系统性偏差、次优和噪声轨迹条件下，F1分数均有显著提升，同时在最优设置下仍保持竞争力，有助于在不确定环境中稳健解释智能体目标。|Reuth Mirsky Team|[2602.14252](http://arxiv.org/abs/2602.14252)|null|
|**2026-02-15**|**Learning Part-Aware Dense 3D Feature Field for Generalizable Articulated Object Manipulation**|针对关节式物体操作中泛化性差及2D特征提升到3D空间时的挑战，本文提出Part-Aware 3D Feature Field (PA3FF)，一种部件感知密集3D特征。PA3FF通过对比学习在大规模数据集上训练，能从点云输入中预测连续3D特征场，使得点特征距离反映功能部件接近程度。在此基础上，引入Part-Aware Diffusion Policy (PADP)模仿学习框架，实验证明PA3FF在操作场景中持续优于多种2D和3D表示，并能支持多种下游任务，为机器人操作提供了通用基础。|Hao Dong Team|[2602.14193](http://arxiv.org/abs/2602.14193)|**[link](https://pa3ff.github.io)**|
|**2026-02-15**|**RoboAug: One Annotation to Hundreds of Scenes via Region-Contrastive Data Augmentation for Robotic Manipulation**|为增强机器人学习在多样、未见场景中的泛化能力，同时减少对大规模预训练和完美物体检测的依赖，本文提出RoboAug生成式数据增强框架。RoboAug仅需单图像边界框标注，利用预训练生成模型进行语义数据增强，并集成区域对比损失以聚焦任务相关区域。在三款机器人上进行的逾3.5万次实验表明，RoboAug显著优于现有基线，在背景、干扰物和光照条件多样化的未见场景中，成功率获得大幅提升，展现出卓越的泛化性和有效性。|Jian Tang Team|[2602.14032](http://arxiv.org/abs/2602.14032)|null|
|**2026-02-15**|**WoVR: World Models as Reliable Simulators for Post-Training VLA Policies with RL**|强化学习(RL)在视觉-语言-动作(VLA)模型中具有超越模仿学习的潜力，但其大规模真实世界交互的需求限制了其在物理机器人上的部署。现有方法利用学习到的世界模型作为模拟器，但闭环的想象轨迹存在幻觉和长期误差累积问题。为解决此问题，本文提出了WoVR框架，旨在调控RL与不完美想象动态的交互。该框架通过可控的动作条件视频世界模型提高轨迹稳定性，通过关键帧初始化轨迹减少有效误差深度，并通过世界模型-策略协同演化保持策略与模拟器的对齐。在LIBERO基准和真实机器人操作实验中，WoVR将平均成功率从39.95%提高到69.2%（+29.3点），真实机器人成功率从61.7%提高到91.7%（+30.0点），证明了在有效控制幻觉的情况下，学习到的世界模型可作为实用的RL模拟器。|Dongbin Zhao Team|[2602.13977](http://arxiv.org/abs/2602.13977)|null|
|**2026-02-14**|**Enabling Option Learning in Sparse Rewards with Hindsight Experience Replay**|现有分层强化学习（HRL）框架在稀疏奖励的多目标环境中表现不佳，尤其是在物体操作任务中，奖励依赖于物体的状态而非智能体的直接交互。为解决此问题，本文首先提出了MOC-HER，将Hindsight Experience Replay (HER)机制整合到Multi-updates Option Critic (MOC)框架中。进一步，为应对物体操作的挑战，本文引入了Dual Objectives Hindsight Experience Replay (2HER)，该方法创建两组虚拟目标：除了基于物体最终状态的目标，还根据智能体执行器位置生成目标，同时奖励智能体与物体的交互和任务的完成。实验结果表明，MOC-2HER在机器人操作环境中实现了高达90%的成功率，远超MOC和MOC-HER的不到11%的成功率，突显了双目标重标签策略在稀疏奖励、多目标任务中的有效性。|Gabriel de Oliveira Ramos Team|[2602.13865](http://arxiv.org/abs/2602.13865)|null|
|**2026-02-14**|**Mean Flow Policy with Instantaneous Velocity Constraint for One-step Action Generation**|在强化学习中，学习富有表达力且高效的策略函数是一个重要的研究方向。尽管流基策略在建模复杂动作分布方面表现出色且采样快速，但其表达能力与计算开销（流步数）之间存在权衡。为解决这一问题，本文提出了一种名为平均速度策略（MVP）的新型生成策略函数，通过建模平均速度场实现最快的一步动作生成。为确保其高表达能力，训练过程中引入了即时速度约束（IVC）。理论证明该设计作为关键边界条件显著提高了学习精度和策略表达力。实验结果表明，MVP在Robomimic和OGBench等多个具有挑战性的机器人操作任务中取得了最先进的成功率，并大幅提升了训练和推理速度。|Shengbo Eben Li Team|[2602.13810](http://arxiv.org/abs/2602.13810)|null|
|**2026-02-14**|**Gaussian Sequences with Multi-Scale Dynamics for 4D Reconstruction from Monocular Casual Videos**|从普通视频中理解动态场景对于可扩展的机器人学习至关重要，然而在严格单目设置下进行四维（4D）重建是一个高度病态的问题。为解决此挑战，本文提出利用真实世界动态在物体到粒子层面的多尺度规律。在此基础上，本文设计了多尺度动态机制以分解复杂的运动场，并提出了高斯序列与多尺度动态（Gaussian sequences with multi-scale dynamics），这是一种通过多层级运动组合推导出的新型动态3D高斯表示。这种分层结构显著减轻了重建歧义并促进了物理上合理的动态。此外，本文融合了来自视觉基础模型的多模态先验以提供互补监督，从而约束了解决方案空间并提高了重建保真度。实验结果表明，该方法能够从单目普通视频中实现准确且全局一致的4D重建，并在动态新视图合成基准和真实世界操作数据集中显著优于现有方法。|Lei Sun Team|[2602.13806](http://arxiv.org/abs/2602.13806)|null|
|**2026-02-14**|**MOTIF: Learning Action Motifs for Few-shot Cross-Embodiment Transfer**|尽管视觉-语言-动作（VLA）模型推动了通用机器人学习，但由于运动学异构性以及收集足够真实世界示范数据进行微调的高成本，跨具身（cross-embodiment）迁移仍然充满挑战。现有跨具身策略通常依赖共享-私有架构，其私有参数容量有限且缺乏明确的适应机制。为解决这些局限性，本文提出了MOTIF框架，旨在实现高效的小样本跨具身迁移，它将具身无关的时空模式（称为动作基序）与异构动作数据解耦。具体而言，MOTIF首先通过带有进度感知对齐和具身对抗约束的矢量量化学习统一的基序，以确保时间和跨具身一致性。然后，设计一个轻量级预测器从实时输入预测这些基序，并将其与机器人特定状态融合，以指导流匹配策略在新的具身上生成动作。模拟和真实世界环境的评估均验证了MOTIF的优越性，在小样本迁移场景中显著优于强基线，模拟中提升6.5%，真实世界中提升43.7%。|Heng Tao Shen Team|[2602.13764](http://arxiv.org/abs/2602.13764)|null|
|**2026-02-14**|**HybridFlow: A Two-Step Generative Policy for Robotic Manipulation**|现有的机器人操作策略受推理延迟限制，缺乏足够的实时环境交互能力。尽管流匹配等更快的生成方法正逐步取代扩散方法，但其精度仍难以满足机器人操作的严格要求。本文关注MeanFlow作为流匹配的单步变体虽然快速但在动作生成精度上的不足。为平衡推理速度和生成质量，本文提出了HybridFlow，这是一种具有2-NFE（函数评估次数）的三阶段方法，包括MeanFlow模式下的全局跳转、用于分布对齐的ReNoise以及ReFlow模式下的局部细化。该方法利用MeanFlow单步生成的快速优势，同时以最少的生成步骤确保动作精度。真实世界实验表明，HybridFlow在成功率上比16步扩散策略高出15-25%，并将推理时间从152毫秒缩短到19毫秒（8倍加速，约52赫兹）；在未见颜色OOD抓取和可变形物体折叠任务上分别达到了70.0%和66.3%的成功率。这些结果表明HybridFlow是一种实用的低延迟方法，能增强机器人操作策略的真实世界交互能力。|Yide Liu Team|[2602.13718](http://arxiv.org/abs/2602.13718)|null|
|**2026-02-14**|**Symmetry-Aware Fusion of Vision and Tactile Sensing via Bilateral Force Priors for Robotic Manipulation**|机器人插入任务需要精密的、富接触的交互，仅凭视觉难以解决。尽管触觉反馈具有直观价值，但现有研究表明，朴素的视觉-触觉融合往往未能持续提供改进。为解决此问题，本文提出了一种用于视觉-触觉融合的跨模态Transformer（CMT），它通过结构化的自注意力与交叉注意力机制整合腕部摄像头观测和触觉信号。为稳定触觉嵌入，本文进一步引入了物理信息正则化，鼓励双边力平衡，反映了人类运动控制的原理。在TacSL基准上的实验表明，带有对称正则化的CMT实现了96.59%的插入成功率，超越了朴素和门控融合基线，并与“腕部+接触力”的优越配置（96.09%）非常接近。这些结果突出表明：触觉感知对于精确对齐不可或缺，以及经过物理信息正则化强化的原则性多模态融合，能够充分发挥视觉和触觉的互补优势，在现实感知条件下接近最优性能。|Tao Yu Team|[2602.13689](http://arxiv.org/abs/2602.13689)|null|
|**2026-02-14**|**Hierarchical Audio-Visual-Proprioceptive Fusion for Precise Robotic Manipulation**|现有机器人操作方法主要依赖视觉和本体感受，在部分可观测的真实世界环境中难以推断接触相关的交互状态。而声学线索能自然编码丰富的接触动态，但在多模态融合中却未被充分利用，且大多数融合方法错误地假设模态作用均一。为实现基于声学信息的精确机器人操作，本文提出一种分层表示融合框架，逐步整合音频、视觉和本体感受。该方法首先将视觉和本体感受表示条件化于声学线索，然后明确建模高阶跨模态交互以捕捉模态间的互补依赖。融合后的表示被扩散策略用于直接从多模态观测生成连续机器人动作。在真实世界机器人操作任务（如倒液体和开柜门）上的广泛实验表明，该方法持续优于现有最先进的多模态融合框架，尤其是在声学线索提供视觉无法轻易获得的任务相关信息时。此外，通过互信息分析解释了音频线索在机器人操作中的作用。|Peng Liu Team|[2602.13640](http://arxiv.org/abs/2602.13640)|null|
|**2026-02-13**|**Imitating What Works: Simulation-Filtered Modular Policy Learning from Human Videos**|通过观看人类视频学习操作技能，有望为机器人学习提供大规模数据的新来源。然而，人类视频在学习抓取后动作方面提供了强信号，但在学习抓取行为方面用处较小，特别是对于没有类似人手的机器人而言，任意稳定的抓取通常不兼容任务。为解决这一挑战，本文提出了Perceive-Simulate-Imitate (PSI) 框架，用于使用经过模拟中成对抓取-轨迹过滤处理的人类视频运动数据训练模块化操作策略。这一模拟步骤通过抓取适用性标签扩展了轨迹数据，从而能够监督学习面向任务的抓取能力。真实世界实验表明，该框架可以无需任何机器人数据高效学习精确操作技能，与简单使用抓取生成器相比，性能显著更鲁棒。|Wei-Chiu Ma Team|[2602.13197](http://arxiv.org/abs/2602.13197)|null|
|**2026-02-13**|**UniManip: General-Purpose Zero-Shot Robotic Manipulation with Agentic Operational Graph**|针对现有机器人操作方法在零样本泛化方面的不足，即端到端VLA模型缺乏精度而传统规划器语义刚性问题，本文提出了UniManip框架。该框架基于双层Agentic Operational Graph (AOG)，通过高层Agentic层进行任务编排和低层Scene层表示动态状态，实现语义推理与物理接地的统一，并以动态智能体循环方式主动实例化场景图、规划无碰撞轨迹并自主恢复失败。实验结果表明，UniManip在未见对象和任务上展现出鲁棒的零样本能力，成功率显著高于现有VLA和分层基线，且支持从固定基座到移动操作的零样本迁移。|Ziwei Wang Team|[2602.13086](http://arxiv.org/abs/2602.13086)|**[link](https://henryhcliu.github.io/unimanip)**|
|**2026-02-13**|**How Swarms Differ: Challenges in Collective Behaviour Comparison**|针对群体行为分析中数值特征集通常缺乏通用性且难以定量衡量行为相似性的问题，本研究深入探讨了特征集对集体行为的影响。我们从现有群体机器人学工作中筛选出特征集和相似性度量，并评估了它们在特定行为背景外的鲁棒性。研究发现，特征集和相似性度量的相互作用决定了区分相似行为群体的有效性，并提出了一种基于自组织图的方法来识别特征空间中行为难以区分的区域。|Jonas Kuckling Team|[2602.13016](http://arxiv.org/abs/2602.13016)|null|
|**2026-02-13**|**SafeFlowMPC: Predictive and Safe Trajectory Planning for Robot Manipulators with Learning-based Policies**|面对机器人日益融入日常生活对灵活性和实时反应能力的需求，以及学习方法缺乏安全保证和优化方法泛化能力不足的挑战，本文提出了SafeFlowMPC框架。该框架结合了流匹配与在线优化，旨在融合学习和优化方法的优势，并通过次优模型预测控制公式，实时确保操作安全性。在KUKA 7自由度机械臂上的真实世界实验（包括抓取和人机交接任务）中，SafeFlowMPC展现了强大的性能。|Andreas Kugi Team|[2602.12794](http://arxiv.org/abs/2602.12794)|null|
|**2026-02-13**|**Scaling Single Human Demonstrations for Imitation Learning using Generative Foundational Models**|鉴于模仿学习中收集机器人演示数据的困难以及人类演示到机器人转移的挑战，本文提出了Real2Gen框架，仅通过单个人类演示来训练操作策略。Real2Gen从人类演示中提取关键信息并传输到模拟环境，利用可编程专家智能体生成无限量的训练数据来学习流匹配策略。实验结果表明，Real2Gen平均成功率提高了26.6%，并且由于训练数据的丰富性，训练出的策略具有更强的泛化能力，实现了纯模拟训练策略的零样本真实世界部署。|Abhinav Valada Team|[2602.12734](http://arxiv.org/abs/2602.12734)|null|
|**2026-02-13**|**$\mathcal{X}$-KD: General Experiential Knowledge Distillation for Large Language Models**|针对大语言模型知识蒸馏（KD）中，现有方法常忽视教师模型原始学习环境的问题，本文提出了Experiential Knowledge Distillation ($\mathcal{X}$-KD) 框架。受经验学习理论和逆强化学习启发，$\mathcal{X}$-KD采用Approximated Variational Reward Imitation Learning (AVRIL) 框架，联合建模教师的原始奖励函数并执行策略蒸馏，使学生模型能在教师的原始学习环境中学习。实验证明，$\mathcal{X}$ -KD在抽象摘要、机器翻译和算术推理任务上均优于基线方法，并实现了更好的性能-多样性权衡和数据效率。|Yuyu Yuan Team|[2602.12674](http://arxiv.org/abs/2602.12674)|null|
|**2026-02-13**|**PMG: Parameterized Motion Generator for Human-like Locomotion Control**|为解决人形机器人运动中，现有全身体参考引导方法对高级命令接口适应性差、对数据和校准敏感等实际挑战，本文提出了Parameterized Motion Generator (PMG)。PMG是一种基于人类运动结构分析的实时运动生成器，通过紧凑的参数化运动数据和高维控制命令合成参考轨迹，并结合模仿学习流水线和仿真到现实电机参数识别模块。实验证明，该集成系统能生成自然、类人运动，精确响应高维控制输入（如VR远程操作），并实现高效、可验证的仿真到现实迁移。|Houde Liu Team|[2602.12656](http://arxiv.org/abs/2602.12656)|null|
|**2026-02-13**|**Real-to-Sim for Highly Cluttered Environments via Physics-Consistent Inter-Object Reasoning**|为解决从单视图观测重建物理有效3D场景时，现有方法常忽略物理约束导致无效状态，进而影响下游模拟可靠性的问题，本文提出了一种新颖的物理约束Real-to-Sim管道。该管道能够从单视图RGB-D数据重建物理一致的3D场景，其核心是一个可微分优化管道，通过接触图建模空间依赖，并利用可微分刚体模拟联合优化物体姿态和物理属性。实验结果表明，重建场景具有高物理保真度，能忠实复现真实世界接触动力学，从而实现稳定可靠的接触密集型操作。|Jun Ma Team|[2602.12633](http://arxiv.org/abs/2602.12633)|**[link](https://physics-constrained-real2sim.github.io)**|
|**2026-02-13**|**FlowHOI: Flow-based Semantics-Grounded Generation of Hand-Object Interactions for Dexterous Robot Manipulation**|近期视觉-语言-动作（VLA）模型能够生成看似合理的末端执行器运动，但在长程、富接触的任务中常常失败，因为缺乏对手-物体交互（HOI）结构的显式表示。为解决此问题，本文提出了FlowHOI，一个两阶段流匹配框架，它能根据第一人称观测、语言指令和3D高斯飞溅（3DGS）场景重建，生成语义接地、时间连贯的HOI序列，包括手部姿态、物体姿态和手-物体接触状态。该框架将以几何为中心的抓取与以语义为中心的操作解耦，后者通过紧凑的3D场景令牌进行条件化，并采用运动-文本对齐损失来语义化生成的交互。为解决高保真HOI监督数据稀缺的问题，本文引入了一个重建流水线，从大规模第一人称视频中恢复对齐的手-物体轨迹和网格，为鲁棒生成提供了HOI先验。FlowHOI在GRAB和HOT3D基准上实现了最高的动作识别精度和比最强扩散基线高1.7倍的物理模拟成功率，同时推理速度提升了40倍。此外，通过将生成的HOI表示重定向到真实机器人执行流程，本文在四个灵巧操作任务上验证了真实机器人执行的可行性。|Xingxing Zuo Team|[2602.13444](http://arxiv.org/abs/2602.13444)|**[link](https://huajian-zeng.github.io/projects/flowhoi/)**|
|**2026-02-12**|**FAIL: Flow Matching Adversarial Imitation Learning for Image Generation**|鉴于流匹配模型后训练与模仿学习的数学等同性，以及监督微调无法纠正策略漂移而偏好优化成本高昂的问题，本文提出了Flow Matching Adversarial Imitation Learning (FAIL) 框架。该框架通过对抗训练最小化策略与专家之间的散度，无需明确奖励或成对比较，并推导出了FAIL-PD和FAIL-PG两种算法。实验证明，FAIL在仅使用少量演示数据的情况下，能在提示遵循和美学基准上取得竞争性性能，并能有效泛化至离散图像和视频生成，同时作为鲁棒正则化器减轻奖励欺骗。|Weidi Xie Team|[2602.12155](http://arxiv.org/abs/2602.12155)|null|
|**2026-02-12**|**GigaBrain-0.5M*: a VLA That Learns From World Model-Based Reinforcement Learning**|针对传统VLA模型在场景理解和未来预测上的局限性，本研究提出了GigaBrain-0.5M*，一个基于世界模型强化学习的VLA模型。该模型在预训练的GigaBrain-0.5基础上，通过RAMP（Reinforcement leArning via world Model-conditioned Policy）进一步整合了世界模型强化学习，以实现鲁棒的跨任务适应性。实验结果表明，RAMP在洗衣折叠、箱子包装和意式浓缩咖啡制作等复杂任务中，相较于RECAP基线性能提升了约30%，并且在实际部署中展示了可靠的长期执行能力，能够无故障完成复杂操作任务。|Zheng Zhu Team|[2602.12099](http://arxiv.org/abs/2602.12099)|**[link](https://gigabrain05m.github.io/)**|
|**2026-02-12**|**Affordance-Graphed Task Worlds: Self-Evolving Task Generation for Scalable Embodied Learning**|鉴于在真实世界中训练机器人策略成本高昂且难以扩展，而现有生成模拟方法难以生成逻辑连贯的长时任务并处理动态物理不确定性，本研究提出了Affordance-Graphed Task Worlds (AGT-World) 框架。该框架能够根据真实世界观测自主构建交互式模拟环境和相应的机器人任务策略，通过将任务空间形式化为结构化图实现复杂目标的精确分层分解，并引入结合视觉-语言模型推理和几何验证的混合反馈自进化机制来完善策略。广泛实验证明，AGT-World在成功率和泛化能力上显著优于现有方法，实现了提议、执行和修正的自改进循环，以支持可扩展的机器人学习。|Changshui Zhang Team|[2602.12065](http://arxiv.org/abs/2602.12065)|null|
|**2026-02-12**|**HoloBrain-0 Technical Report**|为了弥合基础模型研究与可靠的真实世界机器人部署之间的差距，本研究引入了HoloBrain-0，一个全面的视觉-语言-动作 (VLA) 框架。该框架的核心是一个新颖的VLA架构，明确融入了机器人本体先验信息（如多视图相机参数和运动学描述），以增强3D空间推理并支持多样化的本体。通过“预训练-后训练”范式进行验证，该系统在RoboTwin 2.0、LIBERO和GenieSim等模拟基准测试中取得了最先进的成果，并在长时程真实世界操作任务中表现出色。值得注意的是，其高效的0.2B参数变体能与大得多的基线媲美，并支持低延迟的设备部署。为加速研究和实际应用，HoloBrain生态系统已完全开源。|Zhizhong Su Team|[2602.12062](http://arxiv.org/abs/2602.12062)|null|
|**2026-02-12**|**When would Vision-Proprioception Policies Fail in Robotic Manipulation?**|针对视觉-本体感受策略在复杂任务中泛化能力不一致的问题，本研究发现，在机器人运动转换的子阶段，视觉模态的作用有限，策略倾向于更简洁的本体感受信号，抑制了视觉学习。为此，我们提出了梯度调整与阶段引导 (GAP) 算法，通过利用本体感受估计运动转换阶段的概率，并据此自适应地调节本体感受梯度的幅度，从而实现视觉与本体感受的动态协作。综合实验表明，GAP算法在模拟和真实世界环境、单臂和双臂设置以及不同模型类型中均适用，并能形成鲁棒且可泛化的视觉-本体感受策略。|Di Hu Team|[2602.12032](http://arxiv.org/abs/2602.12032)|null|
|**2026-02-12**|**Accelerating Robotic Reinforcement Learning with Agent Guidance**|强化学习在机器人操作中面临严重的样本效率问题，而现有人机协作 (HIL) 方法虽能加速训练，但受限于可扩展性、操作员疲劳和不一致的人类专业知识。为解决此问题，本研究提出了Agent-guided Policy Search (AGPS) 框架，通过多模态智能体取代人工监督者，实现训练流程自动化。其核心思想是将智能体视为语义世界模型，注入内在价值先验来结构化物理探索，并利用可执行工具通过纠正性路点和空间约束提供精确指导。实验证明，AGPS在样本效率方面优于HIL方法，从而实现了无劳动力的可扩展机器人学习路径。|Yaodong Yang Team|[2602.11978](http://arxiv.org/abs/2602.11978)|null|
|**2026-02-12**|**Robot-DIFT: Distilling Diffusion Features for Geometrically Consistent Visuomotor Control**|本研究认为机器人操作泛化性的瓶颈在于当前视觉骨干网络与闭环控制物理需求之间的结构性不匹配，尤其在于现有模型缺乏精细的几何敏感性。鉴于生成扩散模型内在地编码了几何依赖性，但其随机性和延迟阻碍了直接应用，我们提出了Robot-DIFT框架。该框架通过流形蒸馏 (Manifold Distillation) 将冻结的扩散教师模型蒸馏到确定性空间-语义特征金字塔网络 (S2-FPN) 中，从而在保持生成模型丰富几何先验的同时，确保了时间稳定性、实时执行和抗漂移鲁棒性。在DROID数据集上的预训练结果表明，Robot-DIFT在几何一致性和控制性能上均优于领先的判别式基线，验证了视觉学习方式对机器人行为能力的关键影响。|Georgia Chalvatzaki Team|[2602.11934](http://arxiv.org/abs/2602.11934)|null|
|**2026-02-12**|**JEPA-VLA: Video Predictive Embedding is Needed for VLA Models**|现有VLA模型在机器人操作中仍面临样本效率低和泛化能力有限的问题，本研究认为这与预训练视觉表示在环境理解和策略先验方面知识不足有关。通过深入分析，我们发现现有VLA中常用的视觉表示未能有效捕获关键任务相关信息及诱导有效策略先验，而通过视频预训练的预测嵌入，特别是V-JEPA 2，能够灵活地处理不可预测因素并编码任务相关的时间动态。基于此，我们提出了JEPA-VLA，一种将预测嵌入自适应集成到现有VLA的简单有效方法。实验证明，JEPA-VLA在LIBERO、LIBERO-plus、RoboTwin2.0和真实机器人任务等基准测试中均取得了显著的性能提升。|Mingsheng Long Team|[2602.11832](http://arxiv.org/abs/2602.11832)|null|
|**2026-02-12**|**Clutt3R-Seg: Sparse-view 3D Instance Segmentation for Language-grounded Grasping in Cluttered Scenes**|针对杂乱环境中3D实例分割在遮挡、有限视角和噪声掩码下的挑战，本研究提出了Clutt3R-Seg，一种用于语言引导抓取的零样本鲁棒3D实例分割流水线。该方法的核心思想是引入语义线索的分层实例树，通过跨视图分组和条件替换，将噪声掩码作为信息线索，从而抑制过分割和欠分割，产生视图一致的掩码和鲁棒的3D实例。为应对多阶段任务中的场景变化，该方法还引入了一致性感知更新机制。在合成和真实世界数据集及真实机器人上的验证表明，Clutt3R-Seg在杂乱和稀疏视图场景中持续优于现有最先进基线，尤其在重度杂乱序列中表现出超过2.2倍的性能提升。|Ayoung Kim Team|[2602.11660](http://arxiv.org/abs/2602.11660)|null|
|**2026-02-12**|**ViTaS: Visual Tactile Soft Fusion Contrastive Learning for Visuomotor Learning**|针对现有机器人操作中视觉与触觉信息融合方法在遮挡场景下效果不佳、未能充分利用两种模态互补性且集成机制多为直接拼接的问题，本研究提出了ViTaS框架。该框架旨在结合视觉和触觉信息指导智能体行为，并引入了软融合对比学习（Soft Fusion Contrastive Learning）以及一个CVAE模块，以更好地利用视觉-触觉表示中的对齐和互补性。在12个模拟环境和3个真实世界环境中的实验验证表明，ViTaS显著优于现有基线，证明了其在利用多模态信息方面的有效性。|Huazhe Xu Team|[2602.11643](http://arxiv.org/abs/2602.11643)|null|
|**2026-02-12**|**EasyMimic: A Low-Cost Framework for Robot Imitation Learning from Human Videos**|鉴于大规模真实世界数据采集成本高昂阻碍了机器人模仿学习，尤其对低成本家用机器人而言，本研究提出了EasyMimic框架。该框架是一个低成本、可复制的解决方案，使机器人能通过标准RGB相机捕获的人类视频演示快速学习操作策略。其方法首先从视频中提取3D手部轨迹，并利用动作对齐模块将其映射到低成本机器人的夹持器控制空间；为弥合人机领域差距，引入了简单的手部视觉增强策略，并通过协同训练方法在处理过的人类数据和少量机器人数据上微调模型。实验证明，EasyMimic在LeRobot平台上在多种操作任务中取得了高性能，显著减少了对昂贵机器人数据采集的依赖，为智能机器人进入家庭提供了实用途径。|Qin Jin Team|[2602.11464](http://arxiv.org/abs/2602.11464)|null|
|**2026-02-12**|**Scaling World Model for Hierarchical Manipulation Policies**|视觉-语言-动作（VLA）模型在通用机器人操作中虽有前景，但在有限真实机器人数据下，其在分布外（OOD）场景中的泛化能力仍脆弱。本研究提出VISTA，一个分层视觉-语言-动作框架，利用大规模预训练世界模型的泛化能力实现鲁棒且可泛化的视觉子目标任务分解。该框架将世界模型作为高层规划器来生成带有目标图像的子任务序列，VLA作为低层执行器遵循指导生成动作。实验结果表明，与原始文本目标相比，合成的目标图像提供了更具视觉和物理细节的指导，使低层策略能泛化到新物体和场景，将VLA在OOD场景中的性能从14%提升至69%。|Xinghang Li Team|[2602.10983](http://arxiv.org/abs/2602.10983)|null|
|**2026-02-12**|**Self-Refining Vision Language Model for Robotic Failure Detection and Reasoning**|针对机器人故障推理中，真实世界故障的复杂性及丰富推理标签获取成本高昂的问题，本文提出了ARMOR框架。该框架将故障检测和推理建模为一个多任务自细化过程，模型通过迭代预测检测结果和自然语言推理，并从大规模稀疏二元标签和少量丰富推理标注的异构监督中学习。实验结果表明，ARMOR在故障检测率上比现有方法提升高达30%，在LLM模糊匹配分数测量的推理能力上提升高达100%，展现了对异构监督和开放式推理的鲁棒性。|Yesh Dattatreya Team|[2602.12405](http://arxiv.org/abs/2602.12405)|null|
|**2026-02-11**|**Human Preference Modeling Using Visual Motion Prediction Improves Robot Skill Learning from Egocentric Human Video**|当前从第一视角人类视频中学习机器人的方法，在度量视觉状态长期价值时存在假设限制，且在跨实体和环境时需要迁移学习到的价值函数。本研究提出一种新方法，通过预测后续图像中跟踪点的运动来建模人类偏好，并将奖励函数定义为机器人行为中预测与观察到的物体运动的一致性。随后，利用改进的Soft Actor Critic (SAC) 算法（通过10次机器人演示初始化）来估计此奖励的价值函数，并在机器人上优化策略。结果显示，该方法在真实机器人上有效，并且在模拟和真实机器人上的多项任务中，其学习到的策略与现有工作相当或超越。|Christopher G. Atkeson Team|[2602.11393](http://arxiv.org/abs/2602.11393)|null|
|**2026-02-11**|**MolmoSpaces: A Large-Scale Open Ecosystem for Robot Navigation and Manipulation**|机器人大规模部署要求对日常情境具备鲁棒性，但现有基准测试缺乏足够多样化的场景。本研究引入了MolmoSpaces，一个支持机器人策略大规模基准测试的开放生态系统，包含超过23万个多样化室内环境和13万个丰富注释的物体资产，且与模拟器无关。同时设计了MolmoSpaces-Bench基准套件。实验证明，MolmoSpaces-Bench具有强大的模拟到真实关联性，验证了新型零样本策略的优越性，并揭示了对提示措辞、初始关节位置和相机遮挡的关键敏感性，为机器人学习研究提供了可扩展的基础。|Ranjay Krishna Team|[2602.11337](http://arxiv.org/abs/2602.11337)|null|
|**2026-02-11**|**YOR: Your Own Mobile Manipulator for Generalizable Robotics**|随着机器人学习的发展，低成本机器人平台日益增多，但移动操作的最佳外形仍然是待解问题。本研究推出YOR，一个开源、低成本的移动操纵器，集成了全向基座、伸缩式垂直升降器和带抓手的双臂，以实现全身移动和操作。其设计强调模块化、易于组装和经济性（物料清单成本低于1万美元）。YOR在需要协调全身控制、双臂操作和自主导航的任务中展现出其能力，以远低于现有平台的成本提供了具有竞争力的移动操作研究功能。|Zichen Jeff Cui Team|[2602.11150](http://arxiv.org/abs/2602.11150)|null|
|**2026-02-11**|**ABot-M0: VLA Foundation Model for Robotic Manipulation with Action Manifold Learning**|在异构硬件上构建通用具身智能体是机器人学的一大挑战，面临数据碎片化、表示不一致和训练目标不匹配等问题。本研究提出了ABot-M0框架，它通过建立系统的数据整理管道，同时优化模型架构和训练策略，将异构原始数据转换为统一、高效的表示。通过构建大规模UniACT数据集，统一预训练提高了跨平台和任务的知识迁移和泛化能力。为提升动作预测效率和稳定性，引入了动作流形学习（AML）。此外，框架通过双流机制实现模块化感知，整合VLM语义与几何先验和多视角输入。实验结果表明各组件独立且具有累加效益。|Mu Xu Team|[2602.11236](http://arxiv.org/abs/2602.11236)|**[link](https://amap-cvlab.github.io/ABot-Manipulation/)**|
|**2026-02-11**|**OSIL: Learning Offline Safe Imitation Policies with Safety Inferred from Non-preferred Trajectories**|离线安全模仿学习面临在缺乏每时间步安全成本或奖励信息的演示数据中学习安全和奖励最大化策略的难题。本研究针对此问题提出了一种新颖的离线安全模仿学习算法OSIL，它通过非首选轨迹推断安全性。OSIL将安全策略学习表述为约束马尔可夫决策过程（CMDP），并通过推导奖励最大化目标的下界并学习一个估计非首选行为可能性的成本模型来重新构建CMDP问题，从而无需明确的安全成本和奖励标注。实验证明，OSIL能学习到更安全的策略，满足成本约束而不降低奖励性能，优于现有基线方法。|Balaraman Ravindran Team|[2602.11018](http://arxiv.org/abs/2602.11018)|null|
|**2026-02-11**|**Towards Learning a Generalizable 3D Scene Representation from 2D Observations**|现有方法在相机坐标系中构建表示，限制了其在机器人操作中的直接应用。本研究引入了一种可泛化的神经辐射场方法，用于根据以机器人自我为中心的观察来预测3D工作空间占用。该模型在全局工作空间框架中构建占用表示，使其直接适用于机器人操作，并能集成灵活的源视图，无需场景特定微调即可泛化到未见物体布局。在人形机器人上的实验证明，该模型在40个真实场景上训练后，实现了26毫米的重建误差（包括遮挡区域），验证了其推断完整3D占用（超越传统立体视觉方法）的能力。|Stefan Wermter Team|[2602.10943](http://arxiv.org/abs/2602.10943)|null|
|**2026-02-11**|**Semi-Supervised Cross-Domain Imitation Learning**|跨域模仿学习（CDIL）通过领域知识迁移加速策略学习，但在专家数据收集成本高昂时尤为重要。现有CDIL方法或依赖监督（不稳定），或无监督（不稳健）。本研究提出了半监督CDIL（SS-CDIL）设置及其首个具有理论依据的算法。该方法仅使用少量目标专家演示和未标记的不完善轨迹等离线数据。为解决域差异，提出一种新颖的跨域损失函数来学习域间状态-动作映射，并设计自适应权重函数以平衡源域和目标域知识。实验表明，该方法在MuJoCo和Robosuite上始终优于基线，实现了最小监督下稳定且数据高效的策略学习。|Ping-Chun Hsieh Team|[2602.10793](http://arxiv.org/abs/2602.10793)|null|
|**2026-02-11**|**Say, Dream, and Act: Learning Video World Models for Instruction-Driven Robot Manipulation**|机器人操作通常缺乏预测环境响应动作的能力，导致错误和低效，且现有视觉-语言模型（VLM）和世界模型在预测未来状态或生成空间一致帧方面存在局限。本研究提出一种用于快速且可预测的视频条件动作框架。该方法首先选择并适应一个鲁棒的视频生成模型以确保可靠的未来预测，然后应用对抗蒸馏进行快速、少步骤的视频生成，最后训练一个动作模型，利用生成的视频和真实观察来纠正空间误差。大量实验表明，该方法生成的时间连贯、空间准确的视频预测直接支持精确操作，在具身一致性、空间指代能力和任务完成度方面显著优于现有基线。|Yanwei Fu Team|[2602.10717](http://arxiv.org/abs/2602.10717)|null|

<p align=right>(<a href=#updated-on-20260217>back to top</a>)</p>

## World Model

| Publish Date | Title | Chinese Summary | Authors | PDF | Code |
|:---------|:-----------------------|:------------------------|:---------|:------|:------|
|**2026-02-16**|**EditCtrl: Disentangled Local and Global Control for Real-Time Generative Video Editing**|背景：高保真生成式视频编辑依赖预训练视频基础模型，但其计算成本高昂，即使是局部编辑也需处理整个视频上下文。方法：本文提出EditCtrl，一个高效的视频修复控制框架。它引入了新颖的局部视频上下文模块，仅对掩码标记进行操作，使计算成本与编辑区域大小成正比。同时，一个轻量级的时间全局上下文嵌入器确保视频整体上下文的一致性。结果：EditCtrl比现有先进方法计算效率高10倍，并提升了编辑质量。该方法还支持多区域文本提示编辑和自回归内容传播。|Caleb Leak Team|[2602.15031](http://arxiv.org/abs/2602.15031)|**[link](https://yehonathanlitman.github.io/edit_ctrl)**|
|**2026-02-16**|**Generalization from Low- to Moderate-Resolution Spectra with Neural Networks for Stellar Parameter Estimation: A Case Study with DESI**|背景：在恒星光谱分析中，跨巡天泛化能力（特别是从低分辨率到中分辨率光谱的迁移）是一个关键挑战。方法：本文研究了使用预训练多层感知机（MLPs）解决此问题，以LAMOST低分辨率光谱到DESI中分辨率光谱的迁移为例。作者在LAMOST低分辨率光谱或其嵌入上预训练MLPs，并在DESI光谱上进行微调，比较了直接在光谱上训练的MLPs与基于Transformer模型嵌入训练的MLPs，并评估了不同的微调策略。结果：预训练在LAMOST低分辨率光谱上的MLPs表现出色，即便不微调也能获得良好性能，适度微调可进一步提升。研究表明，简单预训练MLPs能提供有竞争力的跨巡天泛化能力，但光谱基础模型在跨巡天恒星参数估计中的作用仍需深入探索。|Viska Wei Team|[2602.15021](http://arxiv.org/abs/2602.15021)|null|
|**2026-02-16**|**Cold-Start Personalization via Training-Free Priors from Structured World Models**|背景：冷启动个性化（即在无用户历史数据时推断用户偏好）是一个挑战，因为用户只关心少数偏好维度，且关键维度因人而异。现有强化学习方法在多轮交互中难以有效利用偏好数据的分因子结构。方法：本文提出Pep（Preference Elicitation with Priors）框架，将冷启动偏好获取分解为离线结构学习和在线贝叶斯推理。Pep离线从完整用户档案中学习偏好相关性的结构化世界模型，然后在线进行免训练的贝叶斯推理，以选择信息丰富的提问并预测完整的偏好档案。结果：Pep在生成响应与用户偏好的一致性方面达到80.8%，远高于强化学习的68.5%，且交互次数减少3-5倍。它仅用约1万参数就实现此效果，而强化学习需80亿参数，突显了利用偏好数据分因子结构的重要性。|Asli Celikyilmaz Team|[2602.15012](http://arxiv.org/abs/2602.15012)|null|
|**2026-02-16**|**PDE foundation models are skillful AI weather emulators for the Martian atmosphere**|背景：为火星大气层构建熟练的预测天气模拟器，面临训练数据和计算资源不足的挑战。方法：本文展示了如何将预训练在多源偏微分方程数值解上的AI基础模型（Poseidon PDE基础模型）适配并微调，以构建火星大气的预测天气模拟器。研究扩展了Poseidon模型从二维到三维的方法，同时保留了预训练信息，并探讨了在稀疏初始条件下的模型性能。结果：通过预训练与模型扩展的结合，模型在独立验证年份的性能提升了34.4%。这表明偏微分方程基础模型不仅能近似其他偏微分方程的解，还能作为解决实际世界复杂交互问题的锚定模型，尤其是在训练数据或计算预算有限的情况下。|Juan Bernabe-Moreno Team|[2602.15004](http://arxiv.org/abs/2602.15004)|null|
|**2026-02-16**|**Use What You Know: Causal Foundation Models with Partial Graphs**|背景：传统的因果量估计依赖于为特定假设定制的估计器。新兴的因果基础模型（CFMs）提供统一方法，但目前无法融入领域知识，导致预测次优。方法：本文提出将因果信息（如完整因果图或部分祖先信息）条件化到CFMs中的方法。研究系统评估了不同的条件化策略，发现将可学习偏差注入注意力机制是利用完整和部分因果信息最有效的方法。结果：通过条件化，通用CFM的性能可以与针对特定因果结构训练的专用模型相媲美。这一方法克服了构建一体化因果基础模型的核心障碍，使其能够以数据驱动的方式回答因果查询，同时有效利用任何程度的领域专业知识。|Bernhard Schölkopf Team|[2602.14972](http://arxiv.org/abs/2602.14972)|null|
|**2026-02-16**|**Model Context Protocol (MCP) Tool Descriptions Are Smelly! Towards Improving AI Agent Efficiency with Augmented MCP Tool Descriptions**|背景：基于基础模型（FM）的智能体通过工具描述与外部系统交互，但这些自然语言描述中的缺陷可能误导FM，其普遍性和影响尚不明确。方法：本文对103个MCP服务器上的856个工具进行了大规模实证研究。通过识别工具描述的六个组成部分，开发了评分标准并据此形式化了“工具描述异味”。利用FM-based扫描器进行操作化评估，并增强描述以评估其对智能体性能的影响。结果：97.1%的工具描述至少含有一种“异味”，其中56%未能清晰阐明目的。尽管增强所有组件的描述可使任务成功率中位数提升5.85个百分点，但执行步骤也增加了67.46%，并导致性能下降，揭示了性能与成本之间的权衡。|Ahmed E. Hassan Team|[2602.14878](http://arxiv.org/abs/2602.14878)|null|
|**2026-02-16**|**World Models for Policy Refinement in StarCraft II**|背景：尽管大型语言模型（LLMs）展现出强大的推理能力，但现有基于LLM的《星际争霸II》（SC2）智能体主要关注策略优化，缺乏可学习的、动作条件化的转移模型来辅助决策。方法：本文提出StarWM，这是首个在部分可观测环境下预测SC2未来观测的世界模型。为学习SC2的混合动态，作者引入了一种将观测分解为五个语义模块的结构化文本表示，并构建了首个用于SC2动态预测的指令微调数据集SC2-Dynamics-50k。StarWM被集成到“生成-模拟-细化”决策循环中，形成StarWM-Agent。结果：StarWM在资源预测准确性等指标上比零样本基线提升近60%。在线评估中，StarWM-Agent对不同难度级别（Hard、Harder、VeryHard）的胜率分别提升了30%、15%和30%，同时改善了宏观管理稳定性和战术风险评估。|Bo Xu Team|[2602.14857](http://arxiv.org/abs/2602.14857)|null|
|**2026-02-16**|**SAILS: Segment Anything with Incrementally Learned Semantics for Task-Invariant and Training-Free Continual Learning**|背景：持续学习在类增量语义分割（CISS）中面临重复训练、高计算成本和灾难性遗忘的限制，制约了其实际应用。方法：本文提出SAILS（Segment Anything with Incrementally Learned Semantics），一个免训练的CISS框架，它将CISS解耦为两个阶段：首先利用Segment Anything Model (SAM)进行零样本区域提取，然后通过固定特征空间中的原型进行语义关联。SAILS通过选择性类内聚类，为每个类生成多个原型以更好地建模类内变异性。结果：SAILS无需增量训练，但在标准CISS数据集上通常超越了现有的基于训练的方法，特别是在遗忘问题严重的长期和挑战性任务序列中。SAILS完全消除了遗忘，保持了任务不变的一致性能，并展现出正向反向迁移。|René Schuster Team|[2602.14767](http://arxiv.org/abs/2602.14767)|null|
|**2026-02-16**|**Depth Completion as Parameter-Efficient Test-Time Adaptation**|背景：现有深度补全方法通常通过训练任务特定编码器来利用辅助输入，但易过拟合且泛化性差。而3D基础模型可提供更强的几何先验。方法：本文提出CAPA，一个参数高效的测试时优化框架，用于利用稀疏几何线索对预训练3D基础模型进行深度补全。CAPA冻结基础模型骨干，仅通过参数高效微调（如LoRA或VPT）更新少量参数，并利用推理时稀疏观测直接计算梯度进行指导。对于视频，CAPA引入序列级参数共享以利用时间相关性并强制多帧一致性。结果：CAPA与任何基于ViT的基础模型兼容，并在室内外数据集的各种条件下取得了最先进的结果。它有效地将基础模型的几何先验与场景特定测量相结合，修正了畸变。|Shengyu Huang Team|[2602.14751](http://arxiv.org/abs/2602.14751)|null|
|**2026-02-16**|**WebWorld: A Large-Scale World Model for Web Agent Training**|背景：网页智能体需要大量轨迹以实现泛化，但真实世界训练受网络延迟、速率限制和安全风险制约。现有模拟器局限于封闭环境。方法：本文推出WebWorld系列，首个大规模训练的开放网络模拟器。它利用可扩展数据管道在超过100万次开放网络交互中进行训练，支持推理、多格式数据和超过30步的长周期模拟。结果：WebWorld在WebWorld-Bench上实现了与Gemini-3-Pro相当的模拟性能。在WebWorld合成轨迹上训练的Qwen3-14B在WebArena上性能提升9.2%，达到与GPT-4o相当的水平。WebWorld作为世界模型，在推理时搜索方面超越了GPT-5。此外，它还展现出跨领域泛化能力，为构建世界模型提供了可复现的方法。|Zuozhu Liu Team|[2602.14721](http://arxiv.org/abs/2602.14721)|null|
|**2026-02-16**|**Arbor: A Framework for Reliable Navigation of Critical Conversation Flows**|针对大型语言模型在医疗分诊等高风险领域难以遵循结构化工作流程的问题，以及单一提示方法在长提示下易导致指令依从性下降的挑战，本文提出了Arbor框架。该框架将决策树导航分解为节点级任务，通过基于DAG的编排机制动态检索和评估转换，并将响应生成解耦。实验结果表明，与单一提示基线相比，Arbor在真实临床分诊对话中将平均轮次准确率提高了29.4个百分点，同时显著降低了延迟和成本，证明了架构分解能有效提升模型性能并降低对模型固有能力的依赖。|Luís Ungaro Team|[2602.14643](http://arxiv.org/abs/2602.14643)|null|
|**2026-02-16**|**Tabular Foundation Models Can Learn Association Rules**|传统的关联规则挖掘（ARM）方法存在规则爆炸和可扩展性差的问题，而现有神经方法在低数据量下性能不佳，但表格基础模型（TFMs）为解决这些局限性提供了基础。为此，本文提出了一个模型无关的关联规则学习框架，能够利用TFMs从任何条件概率模型中提取关联规则，并实例化了TabProbe。实验结果显示，TabProbe利用TFMs作为条件概率估计器，无需频繁项集挖掘即可生成简洁、高质量的关联规则，在低数据设置下仍保持强大的预测性能和鲁棒性。|Victoria Degeler Team|[2602.14622](http://arxiv.org/abs/2602.14622)|null|
|**2026-02-16**|**MATEO: A Multimodal Benchmark for Temporal Reasoning and Planning in LVLMs**|AI智能体在实现复杂目标时需要规划，但现有研究对基础模型时间执行顺序（TEO）的理解有限，多局限于线性近似或纯文本输入。为解决此问题，本文引入了MATEO（MultimodAl Temporal Execution Order）基准，旨在评估和提升大型视觉语言模型（LVLMs）的时间推理能力。通过收集高质量多模态食谱语料库及相应的TEO图注释，作者使用MATEO评估了六个最先进的LVLM，考察了不同模型规模、语言上下文、多模态输入结构和微调策略对时间推理能力的影响。|Giuseppe Riccardi Team|[2602.14589](http://arxiv.org/abs/2602.14589)|null|
|**2026-02-16**|**MedVAR: Towards Scalable and Efficient Medical Image Generation via Next-scale Autoregressive Prediction**|医学图像生成在数据增强和隐私保护中至关重要，但现有方法在架构效率、多器官数据和原则性评估方面存在不足。为此，本文提出了MedVAR，首个基于自回归的医学基础模型，采用“下一尺度预测”范式，实现快速可扩展的医学图像合成。MedVAR以粗到精的方式生成图像，并构建了一个包含约44万张CT和MRI图像的协调数据集。综合实验表明，MedVAR在图像保真度、多样性和可扩展性方面均达到最先进水平，为未来的医学生成基础模型提供了 promising 的架构方向。|Yueming Jin Team|[2602.14512](http://arxiv.org/abs/2602.14512)|null|
|**2026-02-16**|**Covariance-Aware Transformers for Quadratic Programming and Decision Making**|针对Transformer在涉及协方差矩阵的决策问题中的应用潜力，本文首先证明线性注意力机制可通过模拟梯度下降求解无约束二次规划（QP），并扩展至求解L1惩罚/约束QP。在此基础上，本文提出了Time2Decide，一种通过显式输入协方差矩阵来增强时间序列基础模型（TSFM）的通用方法。实验结果表明，Time2Decide在经典的投资组合优化问题上，其性能普遍优于基础TSFM模型，并在特定条件下甚至超越了传统的“预测-优化”流程，证明Transformer通过显式利用二阶统计量能有效解决复杂决策问题。|Samet Oymak Team|[2602.14506](http://arxiv.org/abs/2602.14506)|null|
|**2026-02-16**|**A Soft Wrist with Anisotropic and Selectable Stiffness for Robust Robot Learning in Contact-rich Manipulation**|在非结构化环境中，机器人进行接触密集型操作任务时，现有软末端执行器因形变范围有限、缺乏定向刚度控制或系统复杂而面临挑战。本文介绍了一种名为CLAW（Compliant Leaf-spring Anisotropic soft Wrist）的新型软腕机构，它通过简单的板簧和锁定旋转关节设计，实现了大范围6自由度形变和可调的各向异性刚度，同时保持轻量和低成本。在模仿学习实验中，CLAW在插销任务中实现了76%的成功率，显著优于其他夹具，并在处理高精度装配和精细物体操作等接触密集型场景中表现出强大潜力，预示其能增强机器人学习的鲁棒性。|Masashi Hamaya Team|[2602.14434](http://arxiv.org/abs/2602.14434)|null|
|**2026-02-15**|**WIMLE: Uncertainty-Aware World Models with IMLE for Sample-Efficient Continuous Control**|基于模型的强化学习（MBRL）常因模型误差累积、世界模型处理多模态动力学不佳及预测过度自信而表现受限。本文提出了WIMLE，一种将隐式最大似然估计（IMLE）扩展到MBRL框架的方法，旨在学习随机、多模态的世界模型，并利用集成和潜在采样估计预测不确定性。WIMLE在训练中根据预测置信度加权合成转换，以稳定学习。在40个连续控制任务上的实验结果表明，WIMLE实现了卓越的样本效率和有竞争力的渐近性能，尤其在挑战性任务上显著提升了样本效率，凸显了IMLE基多模态和不确定性感知加权对稳定MBRL的价值。|Ke Li Team|[2602.14351](http://arxiv.org/abs/2602.14351)|**[link](https://openreview.net/forum?id=mzLOnTb3WH)**|
|**2026-02-15**|**Multi-Agent Debate: A Unified Agentic Framework for Tabular Anomaly Detection**|表格异常检测常依赖单一或静态集成检测器，但异构模型在分布漂移、缺失数据和稀有异常下常出现分歧。本文提出了MAD（Multi-Agent Debating）框架，将这种分歧作为核心信号，通过数学协调层解决。框架中每个代理是一个ML检测器，提供异常分数、置信度和证据，并由LLM评论员增强。协调器将消息转换为损失并更新代理影响力，生成最终异常分数和可审计的辩论轨迹。实验表明，MAD在各种表格异常基准测试上提高了鲁棒性，并提供了更清晰的模型分歧追踪。|Sheng Li Team|[2602.14251](http://arxiv.org/abs/2602.14251)|null|
|**2026-02-15**|**Towards Spatial Transcriptomics-driven Pathology Foundation Models**|空间转录组学（ST）能够提供超越组织学评估的分子景观，多模态基础模型也显示了形态分子耦合提升组织学表征的潜力。为整合局部分子信息到病理视觉编码器，本文提出了Spatial Expression-Aligned Learning (SEAL) 框架，作为一种参数高效的视觉-组学自监督微调方法，可应用于现有病理学基础模型。SEAL通过在涵盖14个器官的70多万个配对基因表达点-组织区域示例上进行训练，在38项幻灯片级和15项补丁级下游任务上，持续优于纯视觉和ST预测基线，并展示了强大的域泛化能力和基因到图像检索等跨模态能力，为病理学基础模型的ST引导微调提供了通用且实用的框架。|Faisal Mahmood Team|[2602.14177](http://arxiv.org/abs/2602.14177)|null|
|**2026-02-15**|**ARport: An Augmented Reality System for Markerless Image-Guided Port Placement in Robotic Surgery**|精确的端口放置是机器人辅助手术的关键步骤，但术前规划与术中执行之间存在差距。本文提出了ARport，一个增强现实（AR）系统，旨在自动将预规划的套管布局映射到患者体表，提供直观的术中空间指导。ARport在光学透视头戴式显示器（OST-HMD）上实现，无需外部传感器或标记，通过基础模型提取患者体表并进行无标记配准，实现术前解剖模型与患者体表的对齐，从而现场可视化套管布局。全尺寸人体模型实验表明，ARport能够准确叠加预规划的套管位置，实现虚拟规划与真实解剖之间的一致空间对应，为临床工作流程的无缝集成提供了高效且极简的解决方案。|Qi Dou Team|[2602.14153](http://arxiv.org/abs/2602.14153)|null|
|**2026-02-13**|**Imitating What Works: Simulation-Filtered Modular Policy Learning from Human Videos**|针对机器人通过观察人类视频学习抓取操作技能时，传统方法难以有效学习与任务兼容的抓取行为的问题，本研究提出了Perceive-Simulate-Imitate (PSI) 框架。该框架利用仿真中的抓取轨迹过滤技术，对人类视频数据进行处理，并生成带有抓取适用性标签的扩展轨迹数据，从而实现面向任务的抓取能力监督学习。真实世界实验表明，PSI无需任何机器人数据即可高效学习精确的操纵技能，并且相比简单使用抓取生成器的方法，性能显著提升，鲁棒性更强。|Wei-Chiu Ma Team|[2602.13197](http://arxiv.org/abs/2602.13197)|null|
|**2026-02-13**|**Order Matters in Retrosynthesis: Structure-aware Generation via Reaction-Center-Guided Discrete Flow Matching**|为解决现有免模板逆合成方法学习效率低和半模板方法泛化受限的问题，本研究提出了一种结构感知的免模板框架，核心在于利用原子排序信息。该方法将反应中心原子置于序列头部，通过位置归纳偏差编码化学反应的两阶段特性，并采用RetroDiT骨干网络与离散流匹配相结合。实验结果表明，该方法在USPTO-50k和USPTO-Full数据集上取得了SOTA性能，且在预测反应中心下，性能超越了使用更多数据训练的基础模型，并验证了结构先验的重要性。|Tianshu Yu Team|[2602.13136](http://arxiv.org/abs/2602.13136)|null|
|**2026-02-13**|**A Calibrated Memorization Index (MI) for Detecting Training Data Leakage in Generative MRI Models**|鉴于图像生成模型可能复制训练数据，尤其在医学图像生成中引发隐私问题，本研究提出了一种校准的逐样本度量方法来检测训练数据的记忆化和重复。该方法利用MRI基础模型提取图像特征，聚合多层白化最近邻相似度，并映射为有界的“过拟合/新颖性指数”（ONI）和“记忆化指数”（MI）分数。在多个MRI数据集上的实验结果表明，该度量能稳健检测重复数据，并提供一致的度量值，在样本级别实现了近乎完美的重复项检测。|Ibrahim Habli Team|[2602.13066](http://arxiv.org/abs/2602.13066)|null|
|**2026-02-13**|**INHerit-SG: Incremental Hierarchical Semantic Scene Graphs with RAG-Style Retrieval**|针对现有语义场景图在机器人导航中难以支持可解释的人类意图推理的问题，本研究提出了INHerit-SG框架。该框架将地图定义为RAG-ready的知识库，通过引入自然语言描述作为语义锚点对齐人类意图，并采用异步双进程架构和分层结构解耦几何分割与语义推理，通过事件触发机制保持地图长期一致性。实验在新建数据集和真实世界环境中进行，结果表明INHerit-SG在复杂查询上达到了最先进性能，并提高了检索成功率和可靠性，展现了其在下游导航任务中的可扩展性。|Yang Gao Team|[2602.12971](http://arxiv.org/abs/2602.12971)|null|
|**2026-02-13**|**Information-theoretic analysis of world models in optimal reward maximizers**|为量化最优行为对世界内部表示的需求，本研究考虑了一个具有n个状态和m个动作的受控马尔可夫过程，并假设转移动态存在均匀先验。研究证明，观察一个对任何非恒定奖励函数最优的确定性策略，可以精确地传达n log m比特关于环境的信息。具体来说，环境与最优策略之间的互信息为n log m比特。这些发现为实现最优性所需的“隐式世界模型”提供了精确的信息理论下限，适用于多种奖励最大化目标。|Alex Altair Team|[2602.12963](http://arxiv.org/abs/2602.12963)|null|
|**2026-02-13**|**Scaling Single Human Demonstrations for Imitation Learning using Generative Foundational Models**|鉴于机器人模仿学习中收集演示数据耗时且不易从人类演示直接迁移，本研究提出Real2Gen框架，旨在从单个“人类”演示中训练机器人操纵策略。该方法从人类演示中提取必要信息并传输至仿真环境，在仿真中利用可编程专家智能体生成无限数据来训练流匹配策略。实验结果显示，Real2Gen在三个真实世界任务上成功率平均提升26.6%，并且由于训练数据丰富多样，训练策略的泛化能力显著提高，纯仿真训练的策略还能零样本部署到真实世界。|Abhinav Valada Team|[2602.12734](http://arxiv.org/abs/2602.12734)|null|
|**2026-02-13**|**MedXIAOHE: A Comprehensive Recipe for Building Medical MLLMs**|为提升真实世界临床应用中的通用医学理解和推理能力，本研究提出了医疗视觉-语言基础模型MedXIAOHE。该模型采用实体感知持续预训练框架，组织异构医学语料以拓宽知识覆盖并减少长尾问题；通过强化学习和工具增强的代理训练，整合多样化医学推理模式以支持带可验证决策轨迹的多步骤诊断推理；并融合用户偏好规则、证据推理和低幻觉长文本报告生成，提高真实世界使用的可靠性。MedXIAOHE在多项医学基准测试中取得了最先进的性能，并超越了领先的闭源多模态系统。|Zhixiong Yang Team|[2602.12705](http://arxiv.org/abs/2602.12705)|null|
|**2026-02-13**|**RelBench v2: A Large-Scale Benchmark and Repository for Relational Data**|为推动关系深度学习（RDL）的发展，并应对日益增长的模型规模需求，本研究引入了RelBench v2，一个大规模、真实的关系数据库基准扩展。RelBench v2新增了四个大型数据集和“自动完成任务”，旨在直接推理关系表中缺失的属性值，并整合了外部基准和评估框架以实现统一的关系-时间评估。实验结果表明，RDL模型在自动完成、预测和推荐任务中始终优于单表基线，突出了显式建模关系结构的重要性。|Jure Leskovec Team|[2602.12606](http://arxiv.org/abs/2602.12606)|**[link](https://relbench.stanford.edu)**|
|**2026-02-13**|**The Constant Eye: Benchmarking and Bridging Appearance Robustness in Autonomous Driving**|针对自动驾驶算法在OOD条件下易受外观变化影响，且难以区分外观与结构场景变化导致规划器失效的问题，本研究建立了Navdream，一个高保真鲁棒性基准。该基准利用生成式像素对齐风格迁移，隔离外观变化对驾驶性能的影响。为弥合这一差距，研究提出了一种通用感知接口，利用冻结的视觉基础模型（DINOv3）提取外观不变特征作为规划器的稳定接口。实验表明，现有规划算法在OOD外观下性能显著下降，而该即插即用解决方案在各种规划范式中实现了卓越的零样本泛化，在极端外观变化下仍保持一致性能。|Yiyi Liao Team|[2602.12563](http://arxiv.org/abs/2602.12563)|null|
|**2026-02-13**|**Self-Supervised JEPA-based World Models for LiDAR Occupancy Completion and Forecasting**|鉴于自动驾驶需要世界模型来支持长期规划，且模型学习需具备自监督的可扩展性，本研究提出AD-LiST-JEPA，一个基于联合嵌入预测架构（JEPA）的自监督世界模型。该模型旨在利用JEPA框架从激光雷达数据预测未来时空演变。通过下游基于激光雷达的占用完成和预测（OCF）任务评估学习到的表示质量，概念验证实验表明，经过JEPA世界模型学习预训练后的编码器在OCF性能上有所提升，证明了该方法在感知和预测联合任务中的潜力。|Anna Choromanska Team|[2602.12540](http://arxiv.org/abs/2602.12540)|null|
|**2026-02-13**|**Multi-Agent Model-Based Reinforcement Learning with Joint State-Action Learned Embeddings**|在部分可观察和高度动态环境中，多智能体协调学习面临表示学习和数据效率挑战。为此，本文提出了一种新颖的基于模型的强化学习框架，该框架将联合状态-动作表示学习与想象式展开相结合。作者设计了一个使用变分自编码器训练的世界模型，并利用学习到的状态-动作嵌入（SALE）进行增强，将其注入到预测未来展开的想象模块和估计联合动作值函数的联合智能体网络中。在星际争霸II微管理、多智能体MuJoCo和基于级别的觅食挑战等基准测试中，该方法在有限真实环境交互下，通过将想象轨迹与基于SALE的动作值相结合，显著优于基线算法，验证了其在多智能体模型范式中学习联合状态-动作嵌入的有效性。|David Meger Team|[2602.12520](http://arxiv.org/abs/2602.12520)|null|
|**2026-02-12**|**The Observer Effect in World Models: Invasive Adaptation Corrupts Latent Physics**|判断神经网络模型是内化了物理定律还是仅利用统计捷径，尤其是在分布外（OOD）变化下，仍是一个难题。传统的适应性评估方法（如微调或高容量探针）可能改变被测量的表示，从而混淆自监督学习（SSL）期间的真实学习内容。为解决此问题，本文提出了一种非侵入性评估协议PhyIP，该协议基于线性表示假设，通过测试物理量能否从冻结表示中线性解码来评估。在流体动力学和轨道力学任务中，实验发现当SSL错误率较低时，潜在结构变得线性可访问，PhyIP在OOD测试中成功恢复了内能和牛顿反平方标度（ρ>0.90）。相比之下，基于适应性的评估可能使这种结构崩溃（ρ≈0.05）。这表明低容量探针能更准确地评估物理世界模型，而适应性评估可能掩盖潜在结构。|Barbara Hammer Team|[2602.12218](http://arxiv.org/abs/2602.12218)|null|
|**2026-02-12**|**LDA-1B: Scaling Latent Dynamics Action Model via Universal Embodied Data Ingestion**|当前机器人基础模型多依赖大规模行为克隆，忽视了异构具身数据中可迁移的动力学知识，而现有统一世界模型（UWM）因粗糙数据使用和碎片化数据集难以扩展。为此，本文提出了LDA-1B，一个通过通用具身数据摄取进行扩展的机器人基础模型，它联合学习动力学、策略和视觉预测，并为不同质量数据分配不同角色。为支持大规模训练，作者构建并标准化了EI-30k数据集（超过3万小时的人类和机器人轨迹）。通过在结构化的DINO潜在空间中进行预测，实现了异构数据的可扩展动力学学习，避免了冗余的像素空间外观建模，并采用多模态扩散Transformer处理异步视觉和动作流，实现了10亿参数规模的稳定训练。实验结果表明，LDA-1B在接触密集型、灵巧型和长程任务上分别比现有方法（如π0.5）提高了21%、48%和23%，并能通过利用30%通常有害且被丢弃的低质量轨迹，实现数据高效微调，性能提升10%。|He Wang Team|[2602.12215](http://arxiv.org/abs/2602.12215)|**[link](https://pku-epic.github.io/LDA)**|
|**2026-02-12**|**DreamID-Omni: Unified Framework for Controllable Human-Centric Audio-Video Generation**|尽管基础模型在音视频生成方面取得进展，但以人物为中心的多任务（如参考音视频生成、视频编辑、音频驱动动画）仍被孤立处理，且难以在单一框架内实现对多角色身份和音色的精确解耦控制。本文提出了DreamID-Omni，一个统一的可控人物中心音视频生成框架。作者设计了一个对称条件扩散Transformer，通过对称条件注入方案整合异构条件信号。为解决多人物场景中普遍存在的身份-音色绑定失败和说话人混淆问题，提出了双层解耦策略：在信号层面采用同步RoPE确保严格的注意力空间绑定，在语义层面采用结构化字幕建立显式属性-主题映射。此外，还设计了多任务渐进训练方案，利用弱约束生成先验来正则化强约束任务。大量实验证明，DreamID-Omni在视频、音频和音视频一致性方面均达到了全面的最先进性能，甚至超越了领先的商业模型。|Xiangwang Hou Team|[2602.12160](http://arxiv.org/abs/2602.12160)|**[link](https://guoxu1233.github.io/DreamID-Omni/)**|
|**2026-02-12**|**It's TIME: Towards the Next Generation of Time Series Forecasting Benchmarks**|时间序列基础模型（TSFMs）正在革新预测领域，但现有基准在数据构成、数据完整性、任务制定和分析视角上存在局限。为解决这些问题，本文提出了TIME，一个新一代任务中心基准，包含50个全新数据集和98个预测任务，专为严格的零样本TSFM评估而设计，避免数据泄露。通过整合大型语言模型和人类专业知识，建立了严格的人机协作基准构建流程，确保高数据完整性，并根据真实操作需求和变量可预测性重新定义任务。此外，作者提出了一种新颖的模式级评估视角，超越了基于静态元标签的传统数据集级评估，通过利用结构化时间序列特征表征内在时间属性，为模型能力提供了更具普适性的见解。对12个代表性TSFMs进行评估，并建立了一个多粒度排行榜，以促进深入分析和可视化检查。|Chenghao Liu Team|[2602.12147](http://arxiv.org/abs/2602.12147)|null|
|**2026-02-12**|**Commencing-Student Enrolment Forecasting Under Data Sparsity with Time Series Foundation Models**|许多大学面临日益增长的财政压力，亟需准确预测新生入学人数，然而高等教育入学预测通常数据稀疏，年度序列短且受报告变化和体制转变影响。流行的经典方法因短样本导致参数估计和模型选择不稳定，以及结构性中断导致外推能力下降而不可靠。近期，TSFMs在泄漏受限的协变量构建下，为年度、数据稀疏的机构预测提供了强大的零样本先验。本文在零样本设置下，对多种TSFM家族进行了基准测试，并测试了一组紧凑、防泄漏的协变量集。作者引入了“机构运营状况指数”（IOCI），这是一个从时间戳文件证据中提取的可转移的0-100区间状态协变量，并结合了具有稳定特征工程的Google Trends需求代理。使用严格对齐的回溯测试，结果表明，在没有机构特定训练的情况下，条件化TSFMs的表现与经典基准相当，具体表现差异因群体和模型而异。|Surangika Ranathunga Team|[2602.12120](http://arxiv.org/abs/2602.12120)|null|
|**2026-02-12**|**The Pensieve Paradigm: Stateful Language Models Mastering Their Own Context**|当前的大型语言模型（LLMs）虽有强大的数据库和检索系统，但缺乏主动管理自身状态和记忆的机制，被动接受手动提供的上下文，受限于固定窗口大小。本研究引入了StateLM，一种新型基础模型，它具备内部推理循环来管理自身状态。该模型配备了一系列记忆工具（如上下文剪枝、文档索引、笔记）并被训练主动管理这些工具，从而摆脱固定窗口的架构限制。实验结果表明，StateLM在所有模型规模的长文档问答任务中持续优于标准LLMs，在聊天记忆任务中绝对准确率提升10%至20%，在深度研究任务BrowseComp-Plus上更是达到52%的准确率，远超标准LLM的5%左右。这使得LLMs从被动预测器转变为状态感知代理，实现了状态化和可管理的推理过程。|Yan Wang Team|[2602.12108](http://arxiv.org/abs/2602.12108)|null|
|**2026-02-12**|**GigaBrain-0.5M*: a VLA That Learns From World Model-Based Reinforcement Learning**|直接从当前观察预测多步动作块的视觉-语言-动作（VLA）模型，因场景理解和未来预测能力受限而存在不足。相比之下，预训练于网络规模视频语料库的视频世界模型具有强大的时空推理和准确的未来预测能力，为增强VLA学习提供了基础。本研究提出了GigaBrain-0.5M*，一个通过世界模型驱动强化学习训练的VLA模型，其基于在超过1万小时机器人操作数据上预训练的GigaBrain-0.5。GigaBrain-0.5M*通过RAMP（通过世界模型条件策略进行强化学习）整合强化学习，以实现鲁棒的跨任务适应。实验结果表明，RAMP相对于RECAP基线取得了显著性能提升，在“叠衣服”、“装箱”和“制作浓缩咖啡”等挑战性任务上提升了约30%，并且在真实世界部署中展示了可靠的长时序复杂操作能力。|Zheng Zhu Team|[2602.12099](http://arxiv.org/abs/2602.12099)|**[link](https://gigabrain05m.github.io/)**|
|**2026-02-12**|**Affordance-Graphed Task Worlds: Self-Evolving Task Generation for Scalable Embodied Learning**|在现实世界中直接训练机器人策略成本高昂且难以扩展，而生成式仿真虽能合成大规模数据，但现有方法常难以生成逻辑连贯的长时序任务，且因开环执行难以应对动态物理不确定性。本研究提出了Affordance-Graphed Task Worlds (AGT-World)，一个统一框架，能够根据现实世界观察自主构建交互式仿真环境及相应的机器人任务策略。AGT-World通过将任务空间形式化为结构化图，实现复杂目标的精确分层分解为原子原语，并引入结合视觉-语言模型推理和几何验证的混合反馈自进化机制来自主优化策略。大量实验证明，AGT-World在成功率和泛化能力上显著优于现有方法，实现了提议、执行和纠错的自改进循环，从而促进可扩展的机器人学习。|Changshui Zhang Team|[2602.12065](http://arxiv.org/abs/2602.12065)|null|
|**2026-02-12**|**VLAW: Iterative Co-Improvement of Vision-Language-Action Policy and World Model**|视觉-语言-动作（VLA）模型的性能和可靠性可通过迭代在线交互提升，但现实世界策略回放数据收集昂贵。虽然学习到的模拟器（动作条件视频生成模型）有望生成额外回放数据，但现有世界模型缺乏物理保真度，难以精确建模接触密集物体操纵中的关键物理细节。本研究提出一种简单的迭代改进算法，利用真实世界回放数据提升世界模型的保真度，然后该世界模型再用于生成补充合成数据以改进VLA模型。在真实机器人实验中，该方法显著提升了最先进VLA模型在多个下游任务上的性能，相比基础策略，成功率绝对提升39.2%，相比使用生成合成回放数据进行训练，提升了11.6%。|Chelsea Finn Team|[2602.12063](http://arxiv.org/abs/2602.12063)|null|
|**2026-02-12**|**HoloBrain-0 Technical Report**|基础模型研究与可靠的真实世界机器人部署之间存在差距。为弥合此差距，本研究引入了HoloBrain-0，一个全面的视觉-语言-动作（VLA）框架。其核心是一个新颖的VLA架构，明确融入了机器人具身先验（如多视角相机参数和运动学描述），以增强3D空间推理并支持多样化的具身形态。该设计通过可扩展的“预训练-后训练”范式进行验证，在RoboTwin 2.0、LIBERO和GenieSim等仿真基准测试中取得了最先进的结果，并在具有挑战性的长时序真实世界操作任务中表现出色。值得一提的是，其高效的0.2B参数变体能与更大的基线模型相媲美，支持低延迟的设备端部署。为加速研究，HoloBrain生态系统已完全开源，包括预训练VLA基础模型、后训练检查点和全栈VLA基础设施RoboOrchard。|Zhizhong Su Team|[2602.12062](http://arxiv.org/abs/2602.12062)|null|
|**2026-02-12**|**FedGRPO: Privately Optimizing Foundation Models with Group-Relative Rewards from Domain Client**|针对联邦基础模型(FedFMs)中现有知识迁移方法存在的训练成本高、通信开销大及隐私风险问题，本研究将其重构为强化学习式评估过程。提出了FedGRPO框架，通过构建轻量级置信图进行能力专家选择，并利用"组相对"概念将问题与解决方案打包为候选策略，仅聚合客户端返回的标量奖励信号，而非直接交换数据或模型更新。实验结果表明，FedGRPO在保证隐私和降低通信开销的同时，实现了优于传统FedFMs基线的下游任务精度和通信效率。|Yuxing Han Team|[2602.12014](http://arxiv.org/abs/2602.12014)|null|
|**2026-02-12**|**Accelerating Robotic Reinforcement Learning with Agent Guidance**|强化学习(RL)在机器人操作技能学习中面临样本效率低下，而现有的人在环(HIL)方法受限于可扩展性与人类监督的低效性。本研究提出了Agent-guided Policy Search (AGPS)框架，用多模态智能体取代人类监督者，将智能体视为语义世界模型，注入内在价值先验来指导物理探索。通过可执行工具，智能体提供精确的纠正航点和空间约束来修剪探索。实验结果表明，AGPS在精细插入和变形物体操作任务中，样本效率优于HIL方法，从而实现了自动化监督，为可扩展的机器人学习提供了新途径。|Yaodong Yang Team|[2602.11978](http://arxiv.org/abs/2602.11978)|null|
|**2026-02-12**|**Where Bits Matter in World Model Planning: A Paired Mixed-Bit Study for Efficient Spatial Reasoning**|为实现高效空间推理，本研究探讨了低比特规划行为在有限精度预算下，是否主要由总比特位宽决定，或由比特位在模型模块间的分配方式决定。通过在DINO-WM上进行混合位评估，观察到8比特和6比特设置接近FP16，3比特设置性能崩溃，而4比特设置对位宽分配敏感。在4比特过渡区，保留编码器精度可提升规划性能。这些发现表明，模块感知、预算感知的量化策略对于高效空间推理具有重要研究意义。|Vaishak Menon Team|[2602.11882](http://arxiv.org/abs/2602.11882)|null|
|**2026-02-12**|**PuYun-LDM: A Latent Diffusion Model for High-Resolution Ensemble Weather Forecasts**|潜在扩散模型(LDMs)在 <=0.25° 的高分辨率集合天气预报中存在扩散性受限问题，且现有频率方法因缺乏对变量间频谱异质性的考虑而导致正则化强度不均。本研究提出了PuYun-LDM模型，结合3D掩码自编码器(3D-MAE)编码天气状态演化特征作为扩散模型的额外条件，并采用变量感知掩码频率建模(VA-MFM)策略自适应选择阈值。实验证明，PuYun-LDM增强了潜在扩散性，在短时效预报中表现优于ENS，长时效预报中与ENS相当，并能在短时间内高效生成全球预报。|Bin Wang Team|[2602.11807](http://arxiv.org/abs/2602.11807)|null|
|**2026-02-12**|**HAIC: Humanoid Agile Object Interaction Control via Dynamics-Aware World Model**|类人机器人在处理非结构化环境中复杂全身任务时，现有方法难以应对具有独立动力学和非完整约束的欠驱动物体，且缺乏外部状态估计。本研究提出了HAIC统一框架，通过仅利用本体感受历史数据估计高阶物体状态的动力学预测器，并将预测投影至静态几何先验形成空间接地动态占用图，使策略能在盲区推断碰撞边界和接触可供性。通过非对称微调，世界模型持续适应学生策略。实验表明，HAIC在敏捷任务和多物体长时序任务中均取得了高成功率，有效应对惯性扰动并预测多物体动力学。|Renjing Xu Team|[2602.11758](http://arxiv.org/abs/2602.11758)|**[link](https://haic-humanoid.github.io/)**|
|**2026-02-12**|**ABot-N0: Technical Report on the VLA Foundation Model for Versatile Embodied Navigation**|具身导航领域长期受限于任务特定的架构。本研究引入了ABot-N0，一个统一的视觉-语言-动作(VLA)基础模型，旨在实现点目标、物体目标、指令跟随、POI目标和人物跟随五项核心导航任务的“大一统”。模型采用分层“大脑-行动”架构，将基于大型语言模型的认知大脑与基于流匹配的动作专家结合。为支持大规模学习，研究团队开发了ABot-N0数据引擎， curating 16.9M专家轨迹和5.0M推理样本。ABot-N0在7个基准测试中取得了新的SOTA性能，并整合规划器与分层拓扑记忆，实现动态真实世界环境中的鲁棒长时序任务。|Mu Xu Team|[2602.11598](http://arxiv.org/abs/2602.11598)|**[link](https://amap-cvlab.github.io/ABot-Navigation/ABot-N0/)**|
|**2026-02-12**|**Brain4FMs: A Benchmark of Foundation Models for Electrical Brain Signal**|脑基础模型(BFMs)在神经科学领域快速发展，但缺乏统一的方法理解和标准化评估框架。本研究旨在填补这一空白，通过自监督学习(SSL)分类法组织BFMs，并从数据集角度总结常见下游任务，整理临床和以人为中心神经技术应用的代表性公共数据集。在此基础上，推出了Brain4FMs开放评估平台，集成了15个代表性BFM和18个公共数据集。该平台支持标准化比较和分析预训练数据、SSL策略和架构对泛化和下游性能的影响，以指导更准确、可迁移的BFMs开发。|Yang Yang Team|[2602.11558](http://arxiv.org/abs/2602.11558)|null|
|**2026-02-12**|**TS-Memory: Plug-and-Play Memory for Time Series Foundation Models**|时间序列基础模型(TSFMs)在分布偏移下的下游领域适应面临挑战，现有参数化适应易导致灾难性遗忘，而非参数化检索则引入高推理延迟。本研究提出了参数化记忆蒸馏方法并实现为TS-Memory，一个轻量级记忆适配器。TS-Memory分两阶段训练：首先构建离线、无信息泄露的kNN教师模型合成置信度感知的量化目标；其次通过置信度门控监督将检索诱导的分布校正蒸馏到记忆适配器中。实验表明，TS-Memory在多种TSFMs和基准测试上持续改进点预测和概率预测性能，且效率与冻结骨干网络相当，实现了无检索部署。|Yuxuan Liang Team|[2602.11550](http://arxiv.org/abs/2602.11550)|null|
|**2026-02-12**|**Budget-Constrained Agentic Large Language Models: Intention-Based Planning for Costly Tool Use**|在预算约束下，大型语言模型调用外部工具解决多步骤任务的工具增强型智能体面临巨大状态-动作空间、高结果方差和过高探索成本导致直接规划难以处理的问题。本研究提出了INTENT，一个推理时规划框架，利用意图感知的层次世界模型来预测未来的工具使用和风险校准成本，并在线指导决策。实验证明，INTENT在成本增强型StableToolBench上严格遵循硬预算可行性，显著提高了任务成功率，并在工具价格和预算动态变化下表现出鲁棒性。|Qi Qi Team|[2602.11541](http://arxiv.org/abs/2602.11541)|null|
|**2026-02-12**|**Vascular anatomy-aware self-supervised pre-training for X-ray angiogram analysis**|X射线血管造影图像分析受限于带标注数据稀缺，大规模自监督学习(SSL)在该领域潜力未被充分挖掘。本研究引入了VasoMIM框架，通过解剖引导的掩码策略强制模型学习血管语义，并利用解剖一致性损失保留血管结构一致性，从而增强学习表征的判别力。同时，策展了迄今最大的X射线血管造影预训练数据集XA-170K。VasoMIM在多个下游任务中展现出卓越的可迁移性和领先的性能，凸显了其作为X射线血管造影分析基础模型的巨大潜力。|Zeng-Guang Hou Team|[2602.11536](http://arxiv.org/abs/2602.11536)|null|
|**2026-02-12**|**Semantic-aware Adversarial Fine-tuning for CLIP**|当前研究表明，通过对抗性微调CLIP图像编码器可增强其零样本分类的对抗鲁棒性，但生成对抗样本（AEs）时仅依赖图像与单一手动模板的余弦相似度，不足以衡量图文对的语义相似性，导致微调后的模型鲁棒性不足。为解决此问题，本文提出了一种语义集成攻击方法，通过最小化原始图像与一组精炼文本描述（由基础模型生成并去除了幻觉）之间的平均相似度来生成语义感知的AEs。在此基础上，作者提出了语义感知对抗微调（SAFT）框架。实验结果表明，SAFT在16个数据集上的零样本对抗鲁棒性方面显著优于现有方法，实现了实质性提升。|Feng Liu Team|[2602.12461](http://arxiv.org/abs/2602.12461)|null|
|**2026-02-12**|**Stabilizing Native Low-Rank LLM Pretraining**|基础模型日益增长的参数量带来了巨大的计算和内存挑战，而低秩分解是降低成本的潜在途径，但从头开始仅使用低秩权重训练模型且性能匹配全秩模型仍缺乏稳定的方法。本文研究表明，无需先验方法的“全秩”辅助指导，大型语言模型（LLMs）可以从头开始仅使用低秩分解权重训练所有非嵌入矩阵。作者发现权重矩阵更新中谱范数（最大奇异值）的失控增长是导致原生低秩训练不稳定和损失尖峰的主要因素，并提出Spectron方法：通过正交化进行谱重归一化，根据因子当前的谱范数动态限制所得权重更新。实验证明，Spectron实现了稳定、端到端的低秩训练，开销可忽略不计，并为原生低秩Transformer建立了计算最优的缩放定律，展示了可预测的幂律行为和相对于全秩模型改进的推理效率。|Eugene Belilovsky Team|[2602.12429](http://arxiv.org/abs/2602.12429)|null|
|**2026-02-12**|**Policy4OOD: A Knowledge-Guided World Model for Policy Intervention Simulation against the Opioid Overdose Crisis**|阿片类药物危机是美国严重的公共卫生问题，但由于政策互动复杂且系统动态，评估干预措施极具挑战。本文提出Policy4OOD，一个知识引导的时空世界模型，旨在整合预测、反事实推理和优化三种关键能力来有效评估阿片类政策。该模型通过策略知识图谱、州级空间依赖性及社会经济时间序列的联合编码，构建一个策略条件化的Transformer来预测阿片类药物相关结果。训练完成后，世界模型可作为模拟器，通过前向传播进行预测，通过替换历史策略编码进行反事实分析，并通过蒙特卡洛树搜索进行策略优化。实验结果表明，空间依赖性和结构化策略知识显著提高了预测准确性，验证了该模型在数据驱动的公共卫生决策支持中的潜力。|Yanfang Ye Team|[2602.12373](http://arxiv.org/abs/2602.12373)|null|
|**2026-02-12**|**Free Lunch in Medical Image Foundation Model Pre-training via Randomized Synthesis and Disentanglement**|医学图像基础模型（MIFMs）在临床任务中展现巨大潜力，但其发展受限于大规模标注数据集的稀缺、异质性和高成本。本文提出RaSD（Randomized Synthesis and Disentanglement），一个可扩展的框架，可完全利用合成数据预训练MIFMs。RaSD通过随机高斯分布模拟解剖结构和外观变异，使模型接触足够的多尺度结构和外观扰动，从而迫使其依赖不变和任务相关的解剖线索而非数据集特有纹理，实现鲁棒和可迁移的表示学习。在120万3D体和960万2D图像上进行预训练后，RaSD模型在6种成像模态、48个数据集和56个下游任务中，持续优于从零开始训练的模型，在17个任务上取得了最佳性能，并在大多数其他任务上与使用大型真实数据集预训练的模型表现相当。这些结果证明了仅合成数据即可驱动鲁棒表示学习的能力，为医学AI领域带来了范式转变。|Hao Chen Team|[2602.12317](http://arxiv.org/abs/2602.12317)|null|

<p align=right>(<a href=#updated-on-20260217>back to top</a>)</p>

## VLM

| Publish Date | Title | Chinese Summary | Authors | PDF | Code |
|:---------|:-----------------------|:------------------------|:---------|:------|:------|
|**2026-02-16**|**BPP: Long-Context Robot Imitation Learning by Focusing on Key History Frames**|在机器人任务中，传统策略因仅依赖当前观测而无法有效利用历史信息，导致在需要记忆的任务中泛化性差，尤其是在部署时容易受训练中虚假关联的影响。为解决此问题，本研究提出大图策略（BPP），通过视觉-语言模型识别并利用一组最小的关键帧作为历史观测条件。实验结果表明，BPP显著减少了训练与部署间的分布偏移，并在四项真实世界操作任务和三项仿真任务中，成功率比现有最佳方法提高了70%。|Aviral Kumar Team|[2602.15010](http://arxiv.org/abs/2602.15010)|null|
|**2026-02-16**|**ThermEval: A Structured Benchmark for Evaluation of Vision-Language Models on Thermal Imagery**|当前视觉语言模型（VLMs）在RGB图像上表现优异，但无法泛化至夜间监控、搜救等关键场景的热成像图像，且现有基准无法评估其对温度感知和推理能力。本研究引入ThermEval-B，一个包含5.5万个热视觉问答对的结构化基准，并整合了ThermEval-D数据集，首次提供带有语义身体部位标注的密集逐像素温度图。实验评估25个VLM后发现，模型在温度推理上普遍失败，在色图变换下性能下降，且容易依赖语言先验，提示或微调仅带来微弱提升，证实了热图像理解需专门评估。 |Nipun Batra Team|[2602.14989](http://arxiv.org/abs/2602.14989)|null|
|**2026-02-16**|**DM0: An Embodied-Native Vision-Language-Action Model towards Physical AI**|针对传统方法将互联网预训练模型适配到物理任务的局限性，本研究提出了DM0，一个具身原生（Embodied-Native）的视觉-语言-动作（VLA）框架，旨在为物理AI提供统一的具身操作和导航能力。该框架采用三阶段训练流水线：首先对VLM进行大规模统一预训练，整合网络文本、自动驾驶和具身交互日志数据，随后构建流匹配动作专家，并通过混合训练策略及具身空间支架策略实现高层推理与低层控制的协调。DM0在RoboChallenge基准测试中，于专业和通用设置下均取得了最先进的性能。|Tiancai Wang Team|[2602.14974](http://arxiv.org/abs/2602.14974)|**[link](https://github.com/Dexmal/dexbotic)**|
|**2026-02-16**|**MATEO: A Multimodal Benchmark for Temporal Reasoning and Planning in LVLMs**|AI智能体实现复杂目标需要精细规划，其中时间执行顺序（TEO）至关重要，但现有基础模型对TEO的理解研究不足，多限于线性近似或纯文本输入。为弥补这一空白，本研究引入MATEO基准，旨在评估和提升大型视觉语言模型（LVLMs）的时间推理能力。通过收集高质量多模态菜谱语料并利用众包构建TEO图谱，MATEO提供了丰富的标注。对六个主流LVLMs的评估揭示了语言上下文、多模态输入结构和微调策略对时间推理能力的关键影响，突显了当前LVLMs在该领域的局限性。|Giuseppe Riccardi Team|[2602.14589](http://arxiv.org/abs/2602.14589)|null|
|**2026-02-16**|**Replanning Human-Robot Collaborative Tasks with Vision-Language Models via Semantic and Physical Dual-Correction**|人机协作（HRC）在装配任务中面临人类指令模糊导致机器人行为不可靠的问题，现有基于VLM的方法虽能解释指令，但易产生幻觉推理和物理执行失败。为此，本研究提出了一个HRC框架，通过引入双重校正机制增强VLM的推理能力。该机制包含一个内部校正模型在执行前验证逻辑和可行性，以及一个外部校正模型通过事后反馈纠正物理失败。仿真和真实世界实验均表明，该框架显著提高了任务成功率，并能有效支持机器人根据人类指令进行交互式重规划。|Kensuke Harada Team|[2602.14551](http://arxiv.org/abs/2602.14551)|null|
|**2026-02-16**|**Error Patterns in Historical OCR: A Comparative Analysis of TrOCR and a Vision-Language Model**|18世纪印刷文本的OCR因降级打印、古体字和非标准化拼写而充满挑战，现有基于Transformer和VLM的OCR系统虽总体准确率高，但CER和WER等指标对学术使用可靠性洞察有限。本研究通过比较专用OCR Transformer（TrOCR）和通用VLM（Qwen）在历史英文文本线级识别上的表现，发现Qwen虽在总体准确率和鲁棒性方面占优，但存在选择性语言正则化和拼写规范化，可能改变历史原貌。TrOCR则能更好地保持拼写忠实度，但易出现级联错误。研究强调了架构归纳偏差对OCR错误结构的影响，以及在历史文献数字化中进行架构感知评估的必要性。|Mikko Tolonen Team|[2602.14524](http://arxiv.org/abs/2602.14524)|null|
|**2026-02-16**|**S2D: Selective Spectral Decay for Quantization-Friendly Conditioning of Neural Activations**|大规模Transformer模型中的激活异常值对模型量化构成严重挑战，导致量化精度显著下降，且随预训练规模的增加而加剧。本研究通过理论分析和实证观察，揭示了激活异常值与权重的主奇异值之间的直接联系。在此基础上，提出选择性谱衰减（S²D）方法，在微调阶段仅对最大奇异值对应的权重分量进行正则化。实验证明S²D显著减少了激活异常值，使得模型在W4A4量化下PTQ精度提升高达7%，结合QAT时提升4%，且泛化至下游任务和视觉-语言模型，提高了大规模模型的部署效率。|Deepak Gupta Team|[2602.14432](http://arxiv.org/abs/2602.14432)|null|
|**2026-02-16**|**Multi-Turn Adaptive Prompting Attack on Large Vision-Language Models**|针对多轮越狱攻击在大型视觉-语言模型（LVLMs）中因视觉输入过于恶意而易被防御的问题，本研究提出了MAPA（Multi-turn Adaptive Prompting Attack）方法。MAPA在每个回合中交替使用文本和视觉攻击动作以引发最恶意响应，并在跨回合中通过迭代式来回调整攻击轨迹，逐步放大响应的恶意性。这种双层设计使MAPA持续优于现有最先进方法，在对抗Llava-V1.6-Mistral-7B、Qwen2.5-VL-7B-Instruct、Llama-3.2-Vision-11B-Instruct和GPT-4o-mini等模型的最新基准测试中，攻击成功率提高了11-35%。|Yiliao Song Team|[2602.14399](http://arxiv.org/abs/2602.14399)|null|
|**2026-02-15**|**Moving Beyond Sparse Grounding with Complete Screen Parsing Supervision**|现代计算机使用智能体（CUA）需要对屏幕进行结构化感知才能可靠地理解指令和执行操作，但现有标注数据集稀疏且多样性不足，限制了其泛化能力，且实际部署需要高效率。本研究引入ScreenParse，一个用于完整屏幕解析的大规模数据集，包含771K个网页截图的密集标注。通过Webshot自动化流程及VLM辅助，ScreenParse提供了详尽的UI元素信息。基于此数据集，我们训练了紧凑型ScreenVLM，其在密集解析上显著优于更大规模的基础VLM，并在公共基准上展现了强大的迁移能力，证明了密集屏幕监督能为UI理解提供可迁移的结构先验。|Peter Staar Team|[2602.14276](http://arxiv.org/abs/2602.14276)|null|
|**2026-02-15**|**Dual-Signal Adaptive KV-Cache Optimization for Long-Form Video Understanding in Vision-Language Models**|视觉-语言模型（VLMs）在处理长视频时面临内存瓶颈，因KV缓存随序列长度线性增长，而现有驱逐策略先计算完整注意力矩阵再丢弃token，导致计算浪费。本研究提出Sali-Cache，一个先验优化框架，通过主动内存管理实现双信号自适应缓存。该方法结合光流分析的时间滤波器和显著性检测的空间滤波器，在注意力操作前智能管理内存。实验表明，Sali-Cache在LLaVA 1.6架构上实现了2.20倍的内存压缩比，同时保持100%的准确率，并在相同内存预算下能更长时间保留上下文特征，实现了在消费级硬件上高效处理长视频内容。|Priyesh Shukla Team|[2602.14236](http://arxiv.org/abs/2602.14236)|null|
|**2026-02-15**|**Index Light, Reason Deep: Deferred Visual Ingestion for Visual-Dense Document Question Answering**|当前多模态文档问答系统采用“供给侧摄取”策略，即在索引阶段全面生成视觉描述，导致成本高昂且不可靠。本文提出了“延迟视觉摄取（DVI）”框架，该框架采用“需求侧摄取”策略，仅在索引阶段进行轻量级元数据提取以实现页面定位，将视觉理解延迟到用户提问时，再将原始图像与特定问题发送给视觉语言模型进行分析。实验结果表明，DVI在零摄取视觉语言模型成本下，取得了与现有方法相当的整体准确率（46.7% vs 48.9%），在视觉必要查询上的有效率达50%，并实现了100%的页面定位率，有效将问答准确率问题转化为页面定位问题。|Tao Xu Team|[2602.14162](http://arxiv.org/abs/2602.14162)|null|
|**2026-02-15**|**Annotation-Efficient Vision-Language Model Adaptation to the Polish Language Using the LLaVA Framework**|现有视觉-语言模型（VLMs）多基于英语数据训练，限制了其在其他语言和文化背景下的应用。为解决此问题，本研究复现并调整了LLaVA-Next方法，通过全自动化流程翻译、过滤现有数据集并补充合成数据，构建了一套波兰语VLM。实验结果显示，该方法在波兰语改编的MMBench上相较于LLaVA-1.6-Vicuna-13B实现了9.5%的性能提升，并在生成性评估中，其生成的标题在语言正确性方面获得了更高评价，证明大规模自动化翻译结合轻量级过滤能有效为低资源语言引导高质量多模态模型。|Wojciech Kusa Team|[2602.14073](http://arxiv.org/abs/2602.14073)|null|
|**2026-02-15**|**MarsRetrieval: Benchmarking Vision-Language Models for Planetary-Scale Geospatial Retrieval on Mars**|行星科学中，现有深度学习基准多局限于监督视觉任务，不支持文本引导的地理空间发现。为此，本研究引入了MarsRetrieval，一个用于评估视觉-语言模型在火星地理空间发现能力的检索基准，包含图像-文本检索、地貌检索和全球地理定位等任务。研究提出统一的检索协议以评估多模态嵌入架构。实验结果表明MarsRetrieval极具挑战性，即使是强大的基础模型也难以捕捉领域特定的地貌区别，且领域特定微调对于行星环境中的可泛化地理空间发现至关重要。|Hongxin Wei Team|[2602.13961](http://arxiv.org/abs/2602.13961)|null|
|**2026-02-14**|**RMPL: Relation-aware Multi-task Progressive Learning with Stage-wise Training for Multimedia Event Extraction**|多媒体事件抽取（MEE）受标注数据缺乏限制，现有基准M2E2仅提供评估标注，导致直接监督训练困难，现有方法未能有效学习结构化事件表示。为解决这些局限，本文提出了RMPL（Relation-aware Multi-task Progressive Learning）框架，用于低资源条件下的MEE。RMPL通过阶段式训练，整合了来自单模态事件抽取和多媒体关系抽取的异构监督，先学习事件中心表示，再进行微调。实验结果表明，在M2E2基准上，结合多个视觉语言模型的RMPL在不同模态设置下均显示出持续的性能改进。|Yu Hong Team|[2602.13748](http://arxiv.org/abs/2602.13748)|null|
|**2026-02-14**|**Fine-tuned Vision Language Model for Localization of Parasitic Eggs in Microscopic Images**|土壤传播性蠕虫（STH）感染在全球范围广泛，但诊断专业知识有限，手动显微镜诊断耗时且易错。为实现自动化诊断，本文旨在利用经过微调的视觉语言模型（VLM），例如Microsoft Florence，来定位显微图像中的所有寄生虫卵。初步实验结果显示，该定位VLM的mIOU达到了0.94，优于其他目标检测方法，表明其有望成为自动化框架的核心组件，为智能寄生虫诊断提供可扩展的工程解决方案。|Nouar AlDahoul Team|[2602.13712](http://arxiv.org/abs/2602.13712)|null|
|**2026-02-14**|**LeafNet: A Large-Scale Dataset and Comprehensive Benchmark for Foundational Vision-Language Understanding of Plant Diseases**|基础模型和视觉-语言预训练在VLM领域取得显著进展，但在植物病理学等农业特定领域的应用受限于缺乏大规模、全面的多模态数据集和基准。为弥补此空白，本研究引入了LeafNet数据集和LeafBench视觉问答基准，涵盖97种病害的18.6万张图像和13,950个问答对。对12个先进VLM的基准测试揭示了其疾病理解能力的显著差异，二元分类准确率超90%，但细粒度识别低于65%。研究证实，多模态架构整合语言表示显著增强了诊断精度，凸显了LeafBench对VLM在植物病理学应用中方法学进展和评估的重要性。|Luyl-Da Quach Team|[2602.13662](http://arxiv.org/abs/2602.13662)|null|
|**2026-02-14**|**KorMedMCQA-V: A Multimodal Benchmark for Evaluating Vision-Language Models on the Korean Medical Licensing Examination**|当前视觉-语言模型（VLM）评估基准多为英语或通用领域，缺乏针对韩语医疗领域的多模态问答基准。为此，本研究引入了KorMedMCQA-V，一个韩语医学执照考试风格的多模态多项选择问答基准，包含1534个问题和2043张图像，涵盖多种临床模态，且约30%的问题需整合多图像证据。在统一零样本评估协议下，对50多个VLM的基准测试显示，最佳专有模型准确率达96.9%，最佳开源模型为83.7%，而最佳韩语专用模型仅为43.2%。研究还发现推理导向模型性能显著提升，医学领域专业化收益不一，所有模型在多图像问题上表现下降，且性能因成像模态而异。|Edward Choi Team|[2602.13650](http://arxiv.org/abs/2602.13650)|null|
|**2026-02-14**|**Towards Sparse Video Understanding and Reasoning**|现有视频问答（VQA）方法通常均匀采样视频帧，效率低下且未能有效捕捉关键信息，在多轮VQA中存在挑战。本研究提出了REVISE（Reasoning with Video Sparsity），一个多轮视频问答智能体，它选择少量信息帧，在多轮中维护摘要状态，并在有信心时提前停止。为微调开源模型，引入了EAGER（Evidence-Adjusted Gain for Efficient Reasoning）这一无标注奖励机制，包含置信度增益、摘要充分性和正确且提前停止三项。在多个VQA基准测试中，REVISE在提高准确率的同时，显著减少了帧数、轮次和提示token，展现了实用的稀疏视频推理能力。|Han Liu Team|[2602.13602](http://arxiv.org/abs/2602.13602)|null|
|**2026-02-14**|**AdaVBoost: Mitigating Hallucinations in LVLMs via Token-Level Adaptive Visual Attention Boosting**|大规模视觉-语言模型（LVLMs）存在幻觉问题，现有视觉注意力增强方法通过预定义缩放缓解，但固定缩放因子可能在不同生成步骤中表现出过弱或过强的局限性。为解决此问题，本文提出了AdaVBoost，一个token级别的自适应视觉注意力增强框架，旨在每个生成步骤动态确定注意力增强的程度。该框架引入视觉接地熵（VGE）来估计幻觉风险，并根据VGE对高风险token施加强视觉注意力增强，对低风险token施加弱增强。实验结果表明，AdaVBoost在多个LVLMs和幻觉基准测试中显著优于基线方法。|Tianyu Pang Team|[2602.13600](http://arxiv.org/abs/2602.13600)|null|
|**2026-02-14**|**OpAgent: Operator Agent for Web Navigation**|自主网络智能体在复杂且不稳定的真实网站环境中，面临现有SFT或离线RL方法因分布漂移而导致的性能局限。本文提出了一个强大的在线强化学习WebAgent，通过与非受限广域网站的直接迭代交互来优化策略。该方法包含分层多任务微调，建立了强大的VLM；开发了在线交互环境和RL管道，引入混合奖励机制缓解长期导航中的信用分配挑战；并提出了OpAgent模块化框架，整合规划器、接地器、反射器和总结器以实现错误恢复和自校正。实验结果显示，RL增强模型在WebArena上成功率达38.1%，而OpAgent框架进一步提升至71.6%，达到新的SOTA水平。|Peng Di Team|[2602.13559](http://arxiv.org/abs/2602.13559)|null|
|**2026-02-13**|**Steerable Vision-Language-Action Policies for Embodied Reasoning and Hierarchical Control**|预训练视觉语言模型(VLMs)拥有丰富的常识先验知识，但在机器人控制中有效落地仍面临挑战，现有分层方法中VLM对低层行为的引导受限于自然语言接口。为此，研究者提出了“可操控策略”(Steerable Policies)，通过在不同抽象级别（如子任务、动作、像素坐标）的丰富合成指令上训练视觉语言动作模型(VLA)，以提升低层可控性并释放VLM的预训练知识。实验结果表明，无论是通过学习型高层具身推理器还是即插即用VLM控制，Steerable Policies在真实的机器人操作实验中均优于现有基线，尤其在泛化和长任务方面表现出色。|Sergey Levine Team|[2602.13193](http://arxiv.org/abs/2602.13193)|null|
|**2026-02-13**|**Implicit-Scale 3D Reconstruction for Multi-Food Volume Estimation from Monocular Images**|现有膳食评估方法多依赖单图像分析或基于外观的推断，缺乏明确几何推理且对尺度模糊敏感，难以在真实用餐场景中准确估计食物份量。本研究提出了Implicit-Scale 3D Reconstruction from Monocular Multi-Food Images基准数据集，将食物份量估计重构为单目观测下的隐式尺度3D重建问题，通过餐盘、餐具等上下文线索而非显式度量来推断尺度，并着重于复杂的多食物场景。实验结果显示，几何重建方法相比强大的视觉语言基线，在准确性和鲁棒性上均有提升，最佳方法在体积估计上达到了0.21 MAPE，几何精度为5.7 L1 Chamfer Distance。|Jiangpeng He Team|[2602.13041](http://arxiv.org/abs/2602.13041)|**[link](https://www.kaggle.com/competitions/3d-reconstruction-from-monocular-multi-food-images/data)**|
|**2026-02-13**|**Training-Free Acceleration for Document Parsing Vision-Language Model with Hierarchical Speculative Decoding**|文档解析是多模态理解中的核心任务，但基于视觉语言模型(VLM)的端到端方法在处理长文档时，常因自回归生成长序列而导致显著的推理延迟。针对这一问题，本研究提出了一种免训练且高效的加速方法，借鉴推测解码的思想，使用轻量级文档解析流水线作为草稿模型预测未来批次token，并由更精确的VLM并行验证。此外，该方法还利用文档的布局结构将页面划分为独立区域进行并行解码。实验结果表明，该方法在通用OmniDocBench上为dots.ocr模型提供了2.42倍的无损加速，在长文档解析任务上加速高达4.89倍。|Lianwen Jin Team|[2602.12957](http://arxiv.org/abs/2602.12957)|null|
|**2026-02-13**|**HoRAMA: Holistic Reconstruction with Automated Material Assignment for Ray Tracing using NYURay**|下一代无线网络需要精确的特定站点确定性信道传播预测，而无线射线追踪(RT)依赖高精度3D环境模型和材料属性，手动建模耗时且传统视觉3D重建方法缺乏RT兼容性。为此，本研究提出了HoRAMA（Holistic Reconstruction with Automated Material Assignment）系统，该系统能利用智能手机捕获的RGB视频，结合MASt3R-SLAM的密集点云生成和视觉语言模型辅助的材料分配，自动生成RT兼容的3D模型。实验结果表明，HoRAMA的射线追踪预测在匹配多径分量功率预测方面与手动创建的3D模型基线表现相当（2.28 dB RMSE vs 2.18 dB），同时将3D重建时间从两个月缩短到16小时，显著提高了效率。|Theodore S. Rappaport Team|[2602.12942](http://arxiv.org/abs/2602.12942)|null|
|**2026-02-13**|**RoadscapesQA: A Multitask, Multimodal Dataset for Visual Question Answering on Indian Roads**|理解道路场景对自动驾驶至关重要，但现有数据集可能无法充分覆盖印度复杂多样的驾驶环境。本研究推出了Roadscapes，一个多任务多模态数据集，包含多达9,000张在印度不同驾驶环境中拍摄的图像，并附带手动验证的边界框。该数据集利用基于规则的启发式方法推断场景属性，并生成用于对象定位、推理和场景理解的问答对，涵盖了印度城市和乡村的多种昼夜场景。Roadscapes旨在推动非结构化环境中视觉场景理解的研究，并提供了使用视觉语言模型进行图像问答任务的初步基线。|Jyothikamalesh S Team|[2602.12877](http://arxiv.org/abs/2602.12877)|null|
|**2026-02-13**|**Thinking Like a Radiologist: A Dataset for Anatomy-Guided Interleaved Vision Language Reasoning in Chest X-ray Interpretation**|放射学诊断涉及视觉检查与语言推理的反复交织，但现有医学大型视觉语言模型(LVLMs)多依赖纯文本思维链推理，易产生幻觉，且现有伪视觉解决方案仍缺乏丰富的视觉细节。为此，本研究提出了MMRad-IVL-22K，这是首个专为胸部X射线解读中原生交织的视觉语言推理设计的大规模数据集，反映了放射科医生反复推理和视觉检查的工作流程。实验结果表明，多模态思维链引导的报告生成在临床准确性和报告质量方面显著优于纯文本思维链（RadGraph指标提高6%），证实高保真交织视觉语言证据是可靠医疗AI不可替代的组成部分。在MMRad-IVL-22K上微调的模型在推理一致性和报告质量方面也优于通用和医学专用LVLMs。|Wei Shen Team|[2602.12843](http://arxiv.org/abs/2602.12843)|null|
|**2026-02-13**|**X-SYS: A Reference Architecture for Interactive Explanation Systems**|可解释AI (XAI) 方法虽多，但将其部署为交互式系统仍面临挑战，因其需要兼顾算法与系统能力以维持解释可用性。本研究将可解释性视为一个信息系统问题，提出了X-SYS，一个交互式解释系统的参考架构。X-SYS围绕STAR（可伸缩性、可追溯性、响应性、适应性）四个质量属性，并指定了包含XUI服务、解释服务等五个组件的分解结构，将交互模式映射到系统能力以解耦用户界面和后端计算。通过SemanticLens系统实现X-SYS，展示了其如何通过契约化服务边界实现独立演进、通过离线/在线分离确保响应性以及通过持久状态管理支持可追溯性，为在操作约束下设计交互式解释系统提供了可重用蓝图和具体实例。|Sebastian Lapuschkin Team|[2602.12748](http://arxiv.org/abs/2602.12748)|null|
|**2026-02-13**|**SignScene: Visual Sign Grounding for Mapless Navigation**|导航标志能帮助人类在陌生环境中无地图导航，但机器人如何利用标志进行无地图导航是一个挑战，核心在于如何解释复杂多样的标志及其抽象语义内容，并将其与局部3D场景匹配。本研究将此形式化为“标志接地”(sign grounding)问题，即把标志上的语义指令映射到对应的场景元素和导航动作。研究者利用视觉语言模型(VLMs)的语义常识和推理能力，并提出了SignScene，一种以标志为中心的空-语义表示，旨在以利于VLM有效推理的形式呈现导航相关场景元素和标志信息。在包含九种环境类型、114个查询的数据集上，该方法实现了88%的接地准确率，显著优于基线，并成功使Spot机器人在真实世界中仅依赖标志进行无地图导航。|David Hsu Team|[2602.12686](http://arxiv.org/abs/2602.12686)|null|
|**2026-02-13**|**IndicFairFace: Balanced Indian Face Dataset for Auditing and Mitigating Geographical Bias in Vision-Language Models**|视觉语言模型(VLMs)常从训练数据中继承并放大社会偏见，印度群体尤其被错误代表，现有公平性数据集将印度视为单一类别，忽视了其内部地理多样性。为解决这一局限，本研究提出了IndicFairFace，一个包含14,400张图像的新颖且平衡的人脸数据集，旨在代表印度的地理多样性，图像伦理获取并在各邦和性别间均匀平衡。通过IndicFairFace，研究量化了基于CLIP的VLM中存在的印度国内地理偏见，并利用后验迭代零空间投影去偏方法成功减少了这种偏见。实验证明，该去偏方法对现有嵌入空间的影响很小，基准数据集上的检索准确率平均下降不到1.5%，确立了IndicFairFace作为研究印度背景下VLM地理偏见的第一个基准。|Jiechao Gao Team|[2602.12659](http://arxiv.org/abs/2602.12659)|null|
|**2026-02-13**|**PISHYAR: A Socially Intelligent Smart Cane for Indoor Social Navigation and Multimodal Human-Robot Interaction for Visually Impaired People**|现有智能助行设备多侧重物理导航，但缺乏社交智能和多模态人机交互能力。本研究提出了PISHYAR，一款结合社交感知导航和多模态人机交互的智能拐杖。该系统包含社交导航框架（集成RGB-D感知、对象检测、活动识别、路径规划和触觉反馈）和代理式多模态LLM-VLM交互框架（集成语音识别、VLM、LLM和文本转语音，并支持动态模式路由）。通过仿真、真实世界实验和用户研究，PISHYAR在避障和社会顺从性导航中表现可靠，整体准确率约80%，集体活动识别稳健。初步用户研究显示，视障用户对其交互框架的可用性、信任度和感知社交性给予高度评价，突显了PISHYAR作为多模态辅助移动设备在提供社交互动支持方面的潜力。|Alireza Taheri Team|[2602.12597](http://arxiv.org/abs/2602.12597)|null|
|**2026-02-13**|**On Robustness and Chain-of-Thought Consistency of RL-Finetuned VLMs**|在视觉语言模型（VLM）中，尽管强化学习（RL）微调能提升推理任务性能，但仍面临视觉基础薄弱、幻觉和过度依赖文本线索的问题。研究发现，简单的文本扰动（如误导性描述）会显著降低模型的鲁棒性和置信度，并揭示了RL微调中存在的准确性与忠实性之间的权衡。具体来说，微调提高了基准准确性，却可能损害推理链的可靠性和模型对上下文变化的鲁棒性。这些结果强调了仅凭准确性评估的局限性，并呼吁在训练和评估中同时关注正确性、鲁棒性以及视觉基础推理的忠实性。|Arnab Mondal Team|[2602.12506](http://arxiv.org/abs/2602.12506)|null|
|**2026-02-13**|**Layer-Specific Fine-Tuning for Improved Negation Handling in Medical Vision-Language Models**|由于视觉语言模型（VLM）在区分肯定和否定医疗陈述方面存在局限性，本研究引入了一个放射学诊断基准来系统评估VLM对极性的敏感性。为解决此问题，我们构建了一个包含结构化声明和属性级否定上下文临床否定数据集，并提出了一种名为否定感知选择性训练（NAST）的自适应方法。NAST利用因果追踪效应（CTE）根据各层对否定处理的因果贡献来调整梯度更新。实验结果表明，NAST在不损害通用视觉-语言对齐的情况下，显著提高了VLM对肯定和否定临床陈述的辨别能力，凸显了因果可解释性在安全关键医疗领域中进行有针对性模型适应的价值。|Rahmatollah Beheshti Team|[2602.12498](http://arxiv.org/abs/2602.12498)|null|
|**2026-02-12**|**Scaling Verification Can Be More Effective than Scaling Policy Learning for Vision-Language-Action Alignment**|针对视觉-语言-动作（VLA）模型在执行自然语言指令时存在的"意图-动作差距"问题，本研究探索了测试时验证方法。我们首先揭示了具身指令遵循的测试时标度定律，发现联合扩展复述指令和生成动作数量能更高效地恢复正确动作。为利用这些规律，我们提出了CoVer，一个对比验证器，并引入了“启动时计算”和分层验证推理管道。在部署时，该框架预先计算VLM生成的复述指令，生成动作候选项，然后使用验证器选择最优提示和动作块。实验结果表明，CoVer在SIMPLER基准上取得了显著提升（分布内22%，分布外13%），并在真实世界实验中进一步提升45%，在PolaRiS基准上任务进展提升14%，成功率提升9%。|Marco Pavone Team|[2602.12281](http://arxiv.org/abs/2602.12281)|null|
|**2026-02-12**|**ExStrucTiny: A Benchmark for Schema-Variable Structured Information Extraction from Document Images**|尽管通用视觉语言模型（VLM）在传统文档理解基准上表现良好，但它们在多样化文档类型和灵活模式下进行整体、细粒度结构化信息提取的能力仍未充分研究。现有数据集在实体本体、查询复杂度或文档类型上存在局限。为解决这些不足，本研究引入了ExStrucTiny，一个新的文档图像结构化信息提取（IE）基准数据集，它统一了关键实体提取、关系提取和视觉问答的特性。该数据集通过结合人工和合成并经过人工验证的样本构建，涵盖了更广泛的文档类型和提取场景。对开放和封闭式VLM在该基准上的分析揭示了模式适应、查询欠规范和答案定位等挑战，为未来改进通用模型在文档结构化信息提取方面的能力奠定了基础。|Manuela Veloso Team|[2602.12203](http://arxiv.org/abs/2602.12203)|null|
|**2026-02-12**|**3DGSNav: Enhancing Vision-Language Model Reasoning for Object Navigation via Active 3D Gaussian Splatting**|针对零样本对象导航（ZSON）中视觉语言模型（VLM）决策受低级感知准确性限制的问题，本研究提出了3DGSNav框架。3DGSNav利用3D高斯泼溅（3DGS）作为VLM的持久记忆来增强空间推理能力。通过主动感知，该框架逐步构建环境的3DGS表示，从而实现轨迹引导的、边界感知的第一人称视角的自由视点渲染。此外，研究设计了结构化视觉提示并结合思维链（CoT）提示来进一步提升VLM的推理能力。在导航过程中，实时对象检测器用于过滤潜在目标，而VLM驱动的主动视点切换则进行目标再验证，确保高效可靠的识别。在多个基准测试和真实世界四足机器人实验中，3DGSNav展现出鲁棒且具竞争力的性能。|Xinyi Yu Team|[2602.12159](http://arxiv.org/abs/2602.12159)|null|
|**2026-02-12**|**Affordance-Graphed Task Worlds: Self-Evolving Task Generation for Scalable Embodied Learning**|鉴于在真实世界中训练机器人策略成本高昂且现有生成模拟难以生成逻辑连贯的长周期任务并处理动态物理不确定性，本研究提出了Affordance-Graphed Task Worlds (AGT-World) 统一框架。该框架基于真实世界观测自主构建交互式模拟环境和机器人任务策略，通过将任务空间形式化为结构化图，实现复杂目标的精确分层分解，并引入了结合VLM推理和几何验证的自演化机制来优化策略。实验结果表明，该方法在成功率和泛化能力上显著优于现有方法，实现了提议、执行和修正的自我改进循环，以支持可扩展的机器人学习。|Changshui Zhang Team|[2602.12065](http://arxiv.org/abs/2602.12065)|null|
|**2026-02-12**|**Can Local Vision-Language Models improve Activity Recognition over Vision Transformers? -- Case Study on Newborn Resuscitation**|鉴于新生儿复苏活动准确记录的重要性及其在实践中的不足，以及现有方法在识别细粒度活动上的挑战，本研究探索了生成式AI（GenAI）方法，特别是结合大型语言模型（LLM）的局部视觉-语言模型（VLM），以改进新生儿复苏视频中的活动识别。研究将几种零样本VLM策略和微调VLM（包括LoRA）与分类头进行评估，并与监督式TimeSFormer基线进行比较。结果表明，小型VLM虽然容易产生幻觉，但经过LoRA微调后，F1分数可达0.91，显著超越TimeSFormer的0.70。|Øyvind Meinich-Bache Team|[2602.12002](http://arxiv.org/abs/2602.12002)|null|
|**2026-02-12**|**Benchmarking Vision-Language Models for French PDF-to-Markdown Conversion**|针对PDF到Markdown转换作为RAG管道关键步骤，以及现有基准多侧重英文或中文并可能过度惩罚无关格式差异的问题，本研究引入了一个专门针对法语文档、通过模型不一致采样挑选的挑战性新基准。该基准涵盖手写表格、复杂布局、密集表格和富含图形的页面，并采用单元测试式检查和类别特定归一化进行评估，以减少无关的呈现差异。实验结果显示，在15个模型中，最强的专有模型在手写和表格处理上表现出更高的鲁棒性，同时一些开源系统在标准打印布局上仍具竞争力。|Nicolas Mery Team|[2602.11960](http://arxiv.org/abs/2602.11960)|null|
|**2026-02-12**|**Are Two LLMs Better Than One? A Student-Teacher Dual-Head LLMs Architecture for Pharmaceutical Content Optimization**|针对大型语言模型（LLM）在制药等受监管领域内容创作中面临的科学准确性和合规性挑战，以及手动质量控制（QC）低效易错的问题，本研究引入了LRBTC，一个模块化的LLM和VLM驱动的QC架构。该架构涵盖语言、法规、品牌、技术和内容结构检查，并结合了师生双模型架构、人工干预（HITL）工作流和瀑布式规则过滤机制。实验结果表明，LRBTC在AIReg-Bench上取得了83.0%的F1和97.5%的召回率，相比Gemini 2.5 Pro，遗漏违规减少5倍；在CSpelling上平均准确率提高26.7%。误差分析揭示当前模型擅长检测拼写错误，但在复杂医学语法和标点错误识别方面仍有提升空间。|Anubhav Girdhar Team|[2602.11957](http://arxiv.org/abs/2602.11957)|null|
|**2026-02-12**|**LAMP: Implicit Language Map for Robot Navigation**|为解决现有视觉-语言模型（VLM）零样本导航方法在大环境中因显式存储语言向量而导致的内存需求过高和细粒度规划分辨率有限的问题，本研究提出了LAMP（Language Map）框架。该框架通过学习连续的、语言驱动的隐式神经语言场来编码语言特征，并结合稀疏图实现高效的粗略路径规划，进而通过梯度优化在学习场中细化目标姿态。该方法还采用贝叶斯框架建模嵌入不确定性以增强泛化能力，并通过图采样策略扩展到大环境。实验证明，LAMP在内存效率和细粒度目标到达精度方面均优于现有显式方法。|Sunwook Choi Team|[2602.11862](http://arxiv.org/abs/2602.11862)|**[link](https://lab-of-ai-and-robotics.github.io/LAMP/)**|
|**2026-02-12**|**JEPA-VLA: Video Predictive Embedding is Needed for VLA Models**|针对现有视觉-语言-动作（VLA）模型在机器人操作中存在的样本效率低和泛化能力有限的问题，本研究认为这与被忽视的预训练视觉表征在环境理解和策略先验方面知识不足有关。研究发现，视频上预训练的预测嵌入（特别是V-JEPA 2）能有效捕捉任务相关的时态动态并过滤不可预测的环境因素，从而弥补现有视觉表征的不足。基于此，论文提出了JEPA-VLA，一种将预测嵌入自适应集成到现有VLA中的简单而有效的方法。实验结果表明，JEPA-VLA在一系列基准和真实机器人任务中均取得了显著的性能提升。|Mingsheng Long Team|[2602.11832](http://arxiv.org/abs/2602.11832)|null|
|**2026-02-12**|**Revis: Sparse Latent Steering to Mitigate Object Hallucination in Large Vision-Language Models**|为解决大型视觉-语言模型（LVLMs）普遍存在的物体幻觉问题，该问题源于视觉特征和预训练文本表示在网络深层中的纠缠，本研究提出了REVIS，一个无需训练的框架。REVIS基于潜在空间几何原理，通过正交投影精确提取纯视觉信息向量，并采用校准策略，仅在视觉信息被抑制的精确深度进行稀疏干预，从而有效恢复视觉信息。经验评估结果显示，REVIS与最先进的基线相比，将目标幻觉率降低了约19%，同时保持了模型原有的通用推理能力。|Zhou Yang Team|[2602.11824](http://arxiv.org/abs/2602.11824)|null|
|**2026-02-12**|**Adaptive Debiasing Tsallis Entropy for Test-Time Adaptation**|现有主流的视觉-语言模型测试时自适应（TTA）方法依赖香农熵（SE）衡量预测不确定性，但由于预训练数据偏差，SE会产生有偏的不确定性估计。为解决此问题，研究发现并证明Tsallis熵（TE）通过引入非广延参数q，天然适合表征有偏分布。在此基础上，本文提出了自适应去偏Tsallis熵（ADTE），为每个类别定制类特定的q^l参数，该参数通过对持续传入的测试实例估计标签偏差进行归一化得到。实验结果表明，ADTE在ImageNet及其五种变体上超越了最先进的方法，并在10个跨域基准测试中取得了最高的平均性能，证实了其有效性。|Jianfeng Lu Team|[2602.11743](http://arxiv.org/abs/2602.11743)|null|
|**2026-02-12**|**Adapting Vision-Language Models for E-commerce Understanding at Scale**|电商产品理解需要文本、图像和结构化属性等多模态的强大理解能力。通用视觉-语言模型（VLMs）虽能提供泛化能力，但如何将其有效地适应电商数据中以属性为中心、多图像和噪声多的特性，同时不牺牲通用性能，是一个未被充分探索的问题。本文通过大规模实验研究，展示了对通用VLM进行有针对性的适配，可以在大幅提升电商任务性能的同时，保持其广泛的多模态能力。此外，本文还提出了一个新颖且全面的评估套件，涵盖了深度产品理解、严格指令遵循和动态属性提取等任务。|Shahram Khadivi Team|[2602.11733](http://arxiv.org/abs/2602.11733)|null|
|**2026-02-12**|**STVG-R1: Incentivizing Instance-Level Reasoning and Grounding in Videos via Reinforcement Learning**|在视觉-语言模型（VLMs）中，文本描述与视觉坐标之间的不匹配常导致幻觉问题，在时空视频定位（STVG）等密集预测任务中尤为突出。现有方法通常增强视觉-文本对齐或添加辅助解码器，但这会引入额外的可训练模块，导致高昂的标注和计算成本。本文提出了一种新颖的视觉提示范式，通过将每帧坐标预测重新表述为实例级识别问题，为每个对象分配唯一且时间一致的ID，并将其嵌入视频作为视觉提示。此外，研究引入了STVG-R1，这是首个用于STVG的强化学习框架，通过任务驱动奖励联合优化时间精度、空间一致性和结构化格式正则化。实验结果表明，STVG-R1在六个基准测试中表现出色，在HCSTVG-v2上m_IoU比基线Qwen2.5-VL-7B高出20.9%，达到SOTA，并展现出强大的零样本泛化能力。|Qing Li Team|[2602.11730](http://arxiv.org/abs/2602.11730)|null|
|**2026-02-12**|**ScalSelect: Scalable Training-Free Multimodal Data Selection for Efficient Visual Instruction Tuning**|大规模视觉指令微调（VIT）是提升视觉-语言模型（VLMs）在多模态任务中性能的关键范式，但其在大型数据集上的训练因数据冗余而计算昂贵且低效。现有的数据选择方法要么需要昂贵的训练或梯度计算，要么依赖代理模型或指令无关表示且复杂度高，限制了可扩展性。本文提出了ScalSelect，一种可扩展的无训练多模态数据选择方法，其复杂度与样本数量呈线性关系，无需外部模型或辅助数据集。ScalSelect通过提取指令令牌在目标VLM中最受关注的视觉特征来构建样本表示，然后识别其表示最能近似整个数据集主导子空间的样本，从而实现无需成对比较的可扩展重要性评分。广泛的实验表明，ScalSelect仅使用16%的数据即可达到全数据集训练97.5%以上的性能，在某些设置下甚至超越了全数据训练。|Kai Chen Team|[2602.11636](http://arxiv.org/abs/2602.11636)|**[link](https://github.com/ChangtiWu/ScalSelect}{ScalSelect})**|
|**2026-02-12**|**SkillRater: Untangling Capabilities in Multimodal Data**|传统数据筛选方法通常只给样本分配一个单一的质量分数，但这在模型需要学习多种不同能力时显得局限。本文认为质量应是多维的，每个维度对应模型需掌握的一种能力。为此，我们提出了SkillRater框架，将数据过滤分解为多个专业的评估器，每个评估器负责一种能力，并通过元学习在独立验证目标上进行训练。这些评估器的分数通过渐进式选择规则组合：在每个训练阶段，只要任一评估器将样本排名高于随时间收紧的阈值，该样本即被保留，以在早期保持多样性，后期聚焦高价值样本。在视觉语言模型上的验证表明，SkillRater在视觉理解、OCR和STEM推理能力上均显著优于未过滤基线，且学习到的评估器信号几乎正交，证实了多维度分解的有效性。|Akshat Shrivastava Team|[2602.11615](http://arxiv.org/abs/2602.11615)|null|
|**2026-02-12**|**Chatting with Images for Introspective Visual Thinking**|当前大型视觉-语言模型（LVLMs）通常依赖基于单次视觉编码的纯文本推理，这常导致细粒度视觉信息丢失，尤其是在处理跨区域或多图像的视觉语义或几何关系推理时。为解决这些挑战，本文提出了“与图像对话”这一新框架，将视觉操作重新定义为语言引导的特征调制。在该框架下，模型在富有表现力的语言提示指导下，动态地对多个图像区域进行联合再编码，从而实现语言推理与视觉状态更新之间更紧密的耦合。我们通过ViLaVT实例化了这一范式，它是一个配备动态视觉编码器的新型LVLM，并采用两阶段课程（监督微调和强化学习）进行训练。广泛的实验表明，ViLaVT在八个基准测试中取得了显著且一致的改进，尤其在复杂的多图像和基于视频的空间推理任务上表现突出。|Tieniu Tan Team|[2602.11073](http://arxiv.org/abs/2602.11073)|null|
|**2026-02-12**|**Self-Refining Vision Language Model for Robotic Failure Detection and Reasoning**|针对机器人系统故障推理面临的挑战，即真实世界故障的复杂性及丰富推理标签获取成本高昂，本研究提出了ARMOR模型。ARMOR将故障检测与自然语言推理视为一个多任务自完善过程，通过迭代预测和基于历史输出的条件推理进行训练。它利用大规模稀疏二元标签和少量丰富推理标注的异构监督，并通过离线和在线模仿学习进行优化。在推理阶段，ARMOR生成多条完善轨迹并利用自确定性指标选择最可靠的预测。实验结果显示，ARMOR在故障检测率上比现有方法提升了30%，在LLM模糊匹配分数衡量的推理能力上提升了100%，证明了其在异构监督下的鲁棒性及超越预定义故障模式的开放式推理能力。|Yesh Dattatreya Team|[2602.12405](http://arxiv.org/abs/2602.12405)|null|
|**2026-02-12**|**What does RL improve for Visual Reasoning? A Frankenstein-Style Analysis**|为了理解强化学习（RL）在视觉语言模型（VLM）视觉推理中相比监督微调（IN）的具体贡献，本研究提出了一种“弗兰肯斯坦式”分析框架。该框架通过因果探测定位功能、参数比较表征更新以及模型合并测试可迁移性。研究发现，RL主要通过在模型的中间到后期层诱导一致的推理时段转移来提升性能，并且这些中间到后期的优化对于RL的性能增益是可迁移和必要的。这些结果表明，RL对视觉推理的可靠贡献并非统一增强视觉感知，而是系统地细化了Transformer中间到后期层的计算，从而改善了视觉到推理的对齐和推理性能，揭示了仅通过基准评估来理解多模态推理改进的局限性。|Tianyi Zhou Team|[2602.12395](http://arxiv.org/abs/2602.12395)|null|
|**2026-02-12**|**Synthetic Image Detection with CLIP: Understanding and Assessing Predictive Cues**|鉴于现代生成模型能产生近乎真实的照片，合成图像检测（SID）的泛化能力及其在实际应用中的表现面临挑战，特别是在新生成模型面前。本研究旨在探究CLIP作为SID基础模型的有效性及其内在线索。为此，我们构建了SynthCLIC数据集以减少语义偏差，并利用可解释的线性分类器和文本概念模型分析CLIP特征。结果显示，CLIP检测器在GAN基准上表现优异，但在高质量扩散数据集SynthCLIC上性能略有下降，且跨不同生成器家族的泛化能力显著受限。研究发现，检测器主要依赖高层摄影属性而非明显的生成器伪影。这些发现强调了持续模型更新和更广泛训练暴露的必要性，并肯定了CLIP作为更通用、鲁棒SID方法的强大基础。|Michael Graber Team|[2602.12381](http://arxiv.org/abs/2602.12381)|null|
|**2026-02-12**|**ForeAct: Steering Your VLA with Efficient Visual Foresight Planning**|在开放世界环境中，视觉-语言-动作（VLA）模型将高级语言指令转换为具体可执行动作面临挑战。本研究提出了视觉预见规划器（ForeAct），一个通用高效的规划器，它通过想象未来观察和子任务描述来逐步指导VLA。ForeAct包含一个高效的预见图像生成模块，能从当前视觉输入和语言指令在0.33秒内预测高质量的未来观察，并结合一个视觉-语言模型生成子任务描述。最先进的VLA模型无需修改架构，只需通过增强视觉输入即可无缝集成ForeAct。预见生成器在超过100万个多任务、跨具身情景中进行预训练，学习了鲁棒的具身动力学。在包含11个真实世界任务的基准测试中，ForeAct取得了87.4%的平均成功率，比基线模型有显著提升。|Song Han Team|[2602.12322](http://arxiv.org/abs/2602.12322)|null|
|**2026-02-12**|**LatentAM: Real-Time, Large-Scale Latent Gaussian Attention Mapping via Online Dictionary Learning**|为解决开放词汇机器人感知中从流式RGB-D观测构建可扩展潜在特征地图的挑战，并克服传统VLM嵌入方法缺乏通用性和依赖预训练的问题，本研究提出了LatentAM框架。LatentAM是一种在线3D高斯泼溅（3DGS）映射框架，它采用模型无关且无需预训练的在线字典学习方法，实现了与不同VLM的即插即用集成。该方法将高斯基元与紧凑查询向量关联，通过带有可学习字典的注意力机制转换为近似VLM嵌入。字典在流式观测中高效初始化并在线优化，同时结合基于体素哈希的地图管理策略以实现大规模环境下的GPU内存有界使用。实验结果表明，LatentAM在特征重建保真度上显著优于现有方法，并在评估数据集上实现了接近实时的速度（12-35 FPS）。|Yulun Tian Team|[2602.12314](http://arxiv.org/abs/2602.12314)|null|
|**2026-02-11**|**Hierarchical Concept Embedding & Pursuit for Interpretable Image Classification**|可解释设计模型在计算机视觉中日益受到关注，因为它们能为其预测提供忠实的解释。在图像分类中，这类模型通常从图像中提取人类可解释的概念并用于分类。稀疏概念恢复方法利用视觉-语言模型的潜在空间将图像嵌入表示为概念嵌入的稀疏组合，但这类方法忽视了概念的层次结构，可能导致预测正确但解释与层次结构不一致。为解决此问题，本文提出了分层概念嵌入与追踪（HCEP）框架，它在潜在空间中诱导概念嵌入的层次结构，并利用分层稀疏编码来恢复图像中存在的概念。实验结果表明，HCEP在概念精度和召回率上均优于基线，同时保持了有竞争力的分类准确率，并且在样本数量有限时展现出卓越的分类和概念恢复性能，证明了将层次结构融入稀疏编码能产生更可靠和可解释的图像分类模型。|René Vidal Team|[2602.11448](http://arxiv.org/abs/2602.11448)|null|
|**2026-02-11**|**Beyond VLM-Based Rewards: Diffusion-Native Latent Reward Modeling**|扩散模型和流匹配模型的偏好优化需要既具有判别鲁棒性又计算高效的奖励函数。视觉-语言模型（VLMs）已成为主要的奖励提供者，利用其丰富的多模态先验来指导对齐。然而，它们的计算和内存成本很高，并且通过像素空间奖励优化潜在扩散生成器会引入域不匹配，使对齐复杂化。本文提出了DiNa-LRM，一种扩散原生的潜在奖励模型，它直接在噪声扩散状态上构建偏好学习。DiNa-LRM引入了一种噪声校准的Thurstone似然，其不确定性依赖于扩散噪声，并利用预训练的潜在扩散骨干网络与时间步条件奖励头。实验结果显示，DiNa-LRM在图像对齐基准测试中显著优于现有基于扩散的奖励基线，并以极低的计算成本实现了与最先进VLM相当的性能，显著改善了偏好优化动态，实现了更快、更资源高效的模型对齐。|Wenhan Luo Team|[2602.11146](http://arxiv.org/abs/2602.11146)|**[link](https://github.com/HKUST-C4G/diffusion-rm)**|
|**2026-02-11**|**Active Zero: Self-Evolving Vision-Language Models through Active Environment Exploration**|自博弈已使大型语言模型（LLMs）通过自生成挑战实现自主提升。然而，现有视觉-语言模型（VLMs）的自博弈方法依赖与静态图像集的被动交互，导致对初始数据集的强依赖和学习效率低下。为解决这些局限性，本文提出了Active-Zero框架，将模型学习从被动交互转向主动探索视觉环境。Active-Zero采用三个共同进化的智能体：一个Searcher根据模型能力前沿从开放世界存储库检索图像；一个Questioner合成校准的推理任务；一个Solver通过准确性奖励进行优化。这种闭环机制实现了自我脚手架的自动课程学习。在Qwen2.5-VL-7B-Instruct的12个基准测试中，Active-Zero在推理任务上平均准确率达到53.97%（提升5.7%），在通用理解上达到59.77%（提升3.9%），持续优于现有自博弈基线，表明主动探索是可扩展和自适应自进化视觉-语言系统的关键要素。|Tat-Seng Chua Team|[2602.11241](http://arxiv.org/abs/2602.11241)|null|
|**2026-02-11**|**Safe mobility support system using crowd mapping and avoidance route planning using VLM**|自主移动机器人在动态、尤其是拥挤环境中安全有效导航仍面临挑战。本文提出了一种新颖的框架，该框架集成了视觉-语言模型（VLM）和高斯过程回归（GPR），用于生成动态人群密度图（“抽象图”），以实现自主机器人导航。我们的方法利用VLM识别抽象环境概念（如人群密度）的能力，并通过GPR以概率方式表示这些概念。在大学校园进行的真实世界试验结果表明，机器人成功地生成了避开静态障碍物和动态人群的路径，从而增强了导航的安全性和适应性。|Koichi Ozaki Team|[2602.10910](http://arxiv.org/abs/2602.10910)|null|

<p align=right>(<a href=#updated-on-20260217>back to top</a>)</p>

## VLA

| Publish Date | Title | Chinese Summary | Authors | PDF | Code |
|:---------|:-----------------------|:------------------------|:---------|:------|:------|
|**2026-02-16**|**Beyond Imitation: Reinforcement Learning-Based Sim-Real Co-Training for VLA Models**|模拟提供了可扩展且低成本的方式来丰富视觉-语言-动作（VLA）训练，减少对昂贵真实机器人演示的依赖，但多数模拟-现实协同训练方法依赖于监督微调（SFT），将模拟视为静态演示源，未充分利用大规模闭环交互，从而限制了真实世界收益和泛化。针对此问题，本文提出了RL-based sim-real Co-training (RL-Co) 框架，该框架利用交互式模拟同时保留真实世界能力。其设计分为两阶段：首先，通过对真实和模拟演示的混合数据进行SFT来预热策略，然后通过在模拟中进行强化学习来微调策略，并添加辅助的真实世界数据监督损失以锚定策略并缓解灾难性遗忘。在四项真实世界桌面操作任务上，RL-Co在两种VLA架构（OpenVLA和π₀.₅）上均持续优于仅真实世界微调和基于SFT的协同训练，例如OpenVLA的真实世界成功率提升24%，π₀.₅提升20%，同时在未见任务变体上表现出更强的泛化能力和显著提升的真实世界数据效率。|Yu Wang Team|[2602.12628](http://arxiv.org/abs/2602.12628)|null|
|**2026-02-16**|**DM0: An Embodied-Native Vision-Language-Action Model towards Physical AI**|针对传统方法将物理任务适应视为事后微调，未能统一具身操作和导航的问题，本文提出了DM0，一个具身原生视觉-语言-动作（VLA）框架。该框架采用三阶段流程，通过大规模统一预训练、流匹配动作专家构建以及混合训练策略，从异构数据源学习并整合语义知识与物理先验，同时引入具身空间支架策略以约束动作解决方案空间。实验结果表明，DM0在RoboChallenge基准测试的Specialist和Generalist设置上均达到了最先进的性能。|Tiancai Wang Team|[2602.14974](http://arxiv.org/abs/2602.14974)|**[link](https://github.com/Dexmal/dexbotic)**|
|**2026-02-16**|**DriveFine: Refining-Augmented Masked Diffusion VLA for Precise and Robust Driving**|鉴于自动驾驶VLA模型中扩散式规划器存在模态对齐、训练效率和泛化能力问题，而基于Token的规划器有累积因果误差和解码不可逆性，本文提出了DriveFine，一个结合灵活解码和自校正能力的掩码扩散VLA模型。该模型设计了一种新型即插即用block-MoE，通过解耦的生成专家和精修专家来保留预训练权重的基础能力，并辅以混合强化学习策略以促进有效探索和训练稳定性。在NAVSIM和Navhard基准测试中，DriveFine展现出强大的有效性和鲁棒性。|Yan Wang Team|[2602.14577](http://arxiv.org/abs/2602.14577)|null|
|**2026-02-16**|**Replanning Human-Robot Collaborative Tasks with Vision-Language Models via Semantic and Physical Dual-Correction**|针对人机协作中人类指令的语言歧义和不完整性，导致机器人难以生成可行行为，且现有VLM方法存在幻觉推理和无法预测物理执行失败的问题，本文提出了一个增强VLM推理的双重校正机制HRC框架。该框架包含内部校正模型在动作执行前验证逻辑一致性与任务可行性，以及外部校正模型通过执行后反馈检测和纠正物理失败。仿真和真实世界实验均表明，该方法有效提高了协作任务的成功率，并支持交互式重新规划。|Kensuke Harada Team|[2602.14551](http://arxiv.org/abs/2602.14551)|null|
|**2026-02-16**|**TikArt: Aperture-Guided Observation for Fine-Grained Visual Reasoning via Reinforcement Learning**|针对多模态大语言模型在细粒度视觉推理中，微小或复杂区域的关键信息容易丢失的问题，本文提出了TikArt（Thinking Aperture），一个光圈引导的智能体。TikArt将多步视觉-语言推理建模为对感兴趣区域的决策过程，遵循“思考-光圈-观察”循环，利用Zoom和Segment两种光圈动作提取局部视觉线索并将其转化为语言记忆。通过AGRPO强化学习算法和两阶段课程，TikArt优化推理策略。实验证明，该方法在多个细粒度视觉推理基准上均优于基线模型，并能生成可解释的光圈轨迹。|Lei Zhao Team|[2602.14482](http://arxiv.org/abs/2602.14482)|null|
|**2026-02-16**|**Hierarchical Vision-Language Interaction for Facial Action Unit Detection**|针对面部动作单元（AU）检测在有限标注数据下难以学习判别性和可泛化表示的挑战，本文提出HiVA（Hierarchical Vision-language Interaction for AU Understanding）方法。该方法利用大型语言模型生成的文本AU描述作为语义先验，并通过AU感知动态图模块捕获细粒度和整体视觉-语言关联。HiVA还引入了分层跨模态注意力架构，包含解耦双重交叉注意力和上下文双重交叉注意力机制，以整合多粒度视觉特征和精炼的语言细节。实验结果显示，HiVA持续超越现有最先进方法，并产生语义有意义的激活模式。|Cuntai Guan Team|[2602.14425](http://arxiv.org/abs/2602.14425)|null|
|**2026-02-16**|**Multi-Turn Adaptive Prompting Attack on Large Vision-Language Models**|针对多轮越狱攻击在多模态大语言模型（LVLMs）上因简单添加视觉输入而容易被防御机制识别的问题，本文提出了MAPA（Multi-turn Adaptive Prompting Attack）攻击框架。MAPA采用两级设计：在每轮中交替进行文本-视觉攻击动作以引出最恶意的响应，并跨轮迭代调整攻击轨迹以逐步放大响应的恶意程度。实验结果表明，MAPA在多个最新基准测试中，攻击成功率比现有最先进方法提高了11-35%，展现出显著的有效性。|Yiliao Song Team|[2602.14399](http://arxiv.org/abs/2602.14399)|null|
|**2026-02-15**|**WoVR: World Models as Reliable Simulators for Post-Training VLA Policies with RL**|针对强化学习在VLA模型中因需要大量真实世界交互而难以部署，且基于世界模型的模拟推演存在幻觉和长期误差累积，导致策略利用模型不准确性而非真实任务进展的问题，本文提出了WoVR框架。WoVR明确规范RL与不完美想象动力学的交互，通过可控动作条件视频世界模型提高推演稳定性，通过关键帧初始化推演减少有效误差深度，并通过世界模型-策略协同演化保持对齐。在LIBERO基准和真实机器人操作中，WoVR显著提升了平均成功率，证明了其在控制幻觉下作为实用RL模拟器的有效性。|Dongbin Zhao Team|[2602.13977](http://arxiv.org/abs/2602.13977)|null|
|**2026-02-14**|**Semantic-Contact Fields for Category-Level Generalizable Tactile Tool Manipulation**|针对通用机器人策略在接触密集型工具操作中缺乏高保真物理接地，且现有接触感知策略难以泛化到不同工具几何形状，以及大规模真实世界触觉数据难以获取的挑战，本文提出了Semantic-Contact Fields (SCFields)，一种融合视觉语义和密集接触估计的统一3D表示。该方法通过两阶段Sim-to-Real接触学习管道实现：先在大规模模拟数据上预训练通用接触物理，再在少量真实数据上微调以对齐传感器特性，从而实现对未知工具的物理泛化。实验表明，SCFields在刮擦、绘画和剥离任务上实现了鲁棒的类别级泛化，显著优于纯视觉和原始触觉基线。|Yan Wu Team|[2602.13833](http://arxiv.org/abs/2602.13833)|null|
|**2026-02-14**|**MOTIF: Learning Action Motifs for Few-shot Cross-Embodiment Transfer**|针对视觉-语言-动作（VLA）模型在跨具身转移中面临的运动学异构性和高昂数据收集成本挑战，且现有共享-私有架构容量有限的问题，本文提出了MOTIF框架。MOTIF通过向量量化、进度感知对齐和具身对抗约束，从异构动作数据中解耦与具身无关的时空模式（即动作基序）。随后，设计轻量级预测器从实时输入中预测这些基序，并将其与机器人特定状态融合，以指导流匹配策略在新具身中生成动作。实验证明，MOTIF在少样本跨具身转移场景中显著优于强基线，在仿真和真实世界中分别提升了6.5%和43.7%的性能。|Heng Tao Shen Team|[2602.13764](http://arxiv.org/abs/2602.13764)|null|
|**2026-02-14**|**HBVLA: Pushing 1-Bit Post-Training Quantization for Vision-Language-Action Models**|针对VLA模型因计算和内存占用大而难以在资源受限机器人上部署，且现有二值化方法无法弥合全精度与二值化权重间的分布差距，导致长期闭环执行中量化误差累积的问题，本文提出了HBVLA框架。该框架利用策略感知增强Hessian识别关键权重，对非显著权重采用稀疏正交变换诱导低熵中间状态，并在Harr域中对所有权重进行组式1比特量化。实验结果表明，HBVLA在LIBERO和SimplerEnv上量化后仍能保留90%以上的全精度性能，且在真实世界评估中仅带来微小成功率下降，验证了其在硬件受限平台上的鲁棒部署能力。|Ivor Tsang Team|[2602.13710](http://arxiv.org/abs/2602.13710)|null|
|**2026-02-13**|**Steerable Vision-Language-Action Policies for Embodied Reasoning and Hierarchical Control**|预训练的视觉-语言模型（VLMs）虽能提供丰富的常识先验，但将其有效落地到机器人行为仍具挑战，现有分层方法通过自然语言指令连接VLM与VLA，限制了VLM对低层行为的引导。为增强VLM对低层行为的控制，本文引入“可控策略”（Steerable Policies），即在多抽象层次（如子任务、运动、像素坐标）的丰富合成指令上训练VLA模型。这使得VLM能够通过上下文学习引导这些策略，从而解锁其预训练知识并提升任务泛化能力。广泛的真实世界操作实验表明，与现有基于VLM的VLA和分层基线相比，新方法在泛化和长周期任务中表现更优。|Sergey Levine Team|[2602.13193](http://arxiv.org/abs/2602.13193)|null|
|**2026-02-13**|**UniManip: General-Purpose Zero-Shot Robotic Manipulation with Agentic Operational Graph**|通用机器人操作需要机器人无缝连接高层语义意图与低层物理交互，但现有方法在零样本泛化方面存在不足。针对此问题，本文提出了UniManip框架，其核心是双层代理操作图（AOG），旨在统一语义推理与物理接地。该框架通过高层代理层进行任务编排，低层场景层表示动态状态，实现抽象规划与几何约束的持续对齐，从而支持鲁棒的零样本执行。作为一个动态代理循环，UniManip能从非结构化感知中实例化以物体为中心的场景图，通过安全感知局部规划器参数化无碰撞轨迹，并利用结构化记忆自主诊断和从执行失败中恢复。广泛实验证明，该系统在未见过物体和任务上的零样本成功率分别比VLA和分层基线高22.5%和25.0%，并能直接零样本迁移到移动操作任务。|Ziwei Wang Team|[2602.13086](http://arxiv.org/abs/2602.13086)|**[link](https://henryhcliu.github.io/unimanip)**|
|**2026-02-13**|**Learning Native Continuation for Action Chunking Flow Policies**|动作分块能使视觉-语言-动作（VLA）模型实时运行，但简单的分块执行常导致块边界处的不连续性。现有的实时分块（RTC）方法虽能缓解此问题，但由于其在策略外部，会引起虚假的多模态切换和非内在平滑的轨迹。为此，本文提出了Legato，一种针对基于动作分块流的VLA策略的训练时序延续方法。Legato通过将去噪初始化为已知动作和噪声的调度形混合物，使模型接触部分动作信息，并重塑学习到的流动力学以确保训练和推理在每步引导下保持一致性，同时使用随机调度条件训练以支持不同推理延迟和实现可控平滑性。实验证明，Legato能生成更平滑的轨迹，减少执行时的虚假多模态切换，从而减少犹豫和缩短任务完成时间，在五项操作任务中均比RTC表现更优，轨迹平滑度和任务完成时间均提升约10%。|Yang Gao Team|[2602.12978](http://arxiv.org/abs/2602.12978)|**[link](https://lyfeng001.github.io/Legato/)**|
|**2026-02-13**|**ALOE: Action-Level Off-Policy Evaluation for Vision-Language-Action Model Post-Training**|在真实世界中通过在线强化学习（RL）改进大型视觉-语言-动作（VLA）系统时，价值函数估计是关键，但其通常从混合数据源中收集的轨迹片段进行，这本质上是一个离策略评估问题，而现有工作常采用保守的同策略估计，限制了学习效果。为解决这一问题，本文提出了ALOE（Action-Level Off-Policy Evaluation）框架，用于VLA的后期训练。ALOE采用基于分块的时间差分自举法来评估单个动作序列而非预测最终任务结果，这在稀疏奖励下能更好地将信用归因于关键动作分块，并支持稳定的策略改进。在三项真实世界操作任务上的评估表明，ALOE在不影响执行速度的前提下提高了学习效率，验证了离策略RL在VLA后期训练中可被可靠地重新引入。|Maoqing Yao Team|[2602.12691](http://arxiv.org/abs/2602.12691)|null|
|**2026-02-13**|**SignScene: Visual Sign Grounding for Mapless Navigation**|人类可利用导航标识在陌生环境中无需地图进行导航，本研究旨在使机器人也能利用标识实现开放世界中的无图导航。核心挑战在于解释标识：真实世界标识多样复杂，其抽象语义内容需与局部3D场景进行接地。本文将此形式化为标识接地问题，即把标识上的语义指令映射到对应的场景元素和导航动作。考虑到视觉-语言模型（VLMs）具备所需的语义常识和推理能力但对空间表示敏感，我们提出了SignScene，一种以标识为中心的空间-语义表示方法，它捕获导航相关的场景元素和标识信息，并以有利于VLM有效推理的形式呈现。在包含9种不同环境类型、114个查询的数据集上评估，SignScene达到了88%的接地准确率，显著优于基线方法，并能驱动Spot机器人在真实世界中仅依赖标识进行无图导航。|David Hsu Team|[2602.12686](http://arxiv.org/abs/2602.12686)|null|
|**2026-02-13**|**Xiaomi-Robotics-0: An Open-Sourced Vision-Language-Action Model with Real-Time Execution**|本文介绍了小米机器人0号（Xiaomi-Robotics-0），一个针对高性能、快速且平滑实时执行优化的先进视觉-语言-动作（VLA）模型。该方法的核心在于精心设计的训练配方和部署策略。模型首先在大规模跨实体机器人轨迹和视觉-语言数据上进行预训练，赋予其广泛且泛化的动作生成能力，同时避免灾难性遗忘底层VLM的视觉语义知识。在后期训练中，作者提出了多种技术用于异步执行训练，以解决真实机器人执行时的推理延迟问题。部署时，仔细对齐连续预测动作块的时间步，确保实时执行的连续性和无缝性。广泛的模拟基准测试和两个需要精确灵巧双臂操作的真实机器人任务评估表明，该方法在所有模拟基准上均达到了最先进的性能，并在真实机器人任务上使用消费级GPU实现了高成功率和吞吐量，代码和模型检查点已开源。|Quanyun Zhou Team|[2602.12684](http://arxiv.org/abs/2602.12684)|**[link](https://xiaomi-robotics-0.github.io)**|
|**2026-02-13**|**CRAFT: Adapting VLA Models to Contact-rich Manipulation via Force-aware Curriculum Fine-tuning**|视觉-语言-动作（VLA）模型在处理接触密集型操作任务时面临挑战，因为成功需要精确对齐、稳定接触和处理变形物体，而高熵视觉语言输入与低熵但关键的力信号之间存在不平衡，导致模型过度依赖感知并产生不稳定控制。针对此问题，本文引入CRAFT，一个力感知课程微调框架，该框架集成了一个变分信息瓶颈模块，以在早期训练中调节视觉和语言嵌入，鼓励模型优先处理力信号，然后逐步恢复完整的多模态信息。为实现力感知学习，作者设计了一个同源主从遥操作系统，用于收集各种接触密集型任务中同步的视觉、语言和力数据。真实世界实验表明，CRAFT持续提高了任务成功率，泛化到未见物体和新任务变体，并能有效适应不同VLA架构，从而实现鲁棒且可泛化的接触密集型操作。|Jingtao Sun Team|[2602.12532](http://arxiv.org/abs/2602.12532)|null|
|**2026-02-13**|**AsyncVLA: An Asynchronous VLA for Fast and Robust Navigation on the Edge**|当前机器人基础模型虽泛化能力强，但推理延迟高，导致在动态环境中不安全。针对此问题，本文提出了AsyncVLA异步控制框架，将语义推理与反应性执行解耦。该框架通过远程工作站上的大型基础模型提供高层指导，同时由轻量级板载Edge Adapter高频精细化动作，并通过端到端微调协议和轨迹重加权策略弥合异步流间的域间隙。在面对高达6秒通信延迟的真实视觉导航任务中，AsyncVLA的成功率比现有最佳基线高出40%，成功连接了大型模型的语义智能与边缘机器人所需的实时反应能力。|Sergey Levine Team|[2602.13476](http://arxiv.org/abs/2602.13476)|null|
|**2026-02-13**|**FlowHOI: Flow-based Semantics-Grounded Generation of Hand-Object Interactions for Dexterous Robot Manipulation**|现有视觉-语言-动作（VLA）模型在长周期、接触密集型任务中表现不佳，原因在于缺乏对手-物体交互（HOI）结构的明确表示。为解决此问题，本文提出FlowHOI，一个两阶段流匹配框架，可根据自我中心观察、语言指令和3D高斯泼溅场景重建生成语义明确、时间连贯的HOI序列。该方法将以几何为中心的抓取与以语义为中心的操控解耦，并利用3D场景令牌和运动-文本对齐损失进行语义接地，同时通过从大规模自我中心视频重建HOI轨迹的方法弥补高保真HOI监督的稀缺性。实验结果显示，FlowHOI在GRAB和HOT3D基准测试中实现了最高的动作识别准确率，物理模拟成功率比扩散基线高1.7倍，推理速度提升40倍，并成功在真实机器人上执行了灵巧操作任务。|Xingxing Zuo Team|[2602.13444](http://arxiv.org/abs/2602.13444)|**[link](https://huajian-zeng.github.io/projects/flowhoi/)**|
|**2026-02-12**|**Scaling Verification Can Be More Effective than Scaling Policy Learning for Vision-Language-Action Alignment**|通用机器人理解并执行自然语言指令是长期愿景，尽管视觉-语言-动作（VLA）模型已取得显著进展，但其生成动作仍可能与指令不符。为缩小“意图-动作差距”，本文研究了测试时验证方法。通过分析具身指令遵循的测试时缩放定律，发现联合缩放复述指令和生成动作的数量能更有效地增加测试时样本多样性。在此基础上，提出了CoVer，一种用于VLA对齐的对比验证器，该架构能随计算资源和数据的增加而良好扩展。进一步引入“启动时计算”和分层验证推理流程：在部署时预计算多样化的复述指令，为每条指令重复生成动作候选，然后使用验证器选择最优高层提示和低层动作块。实验表明，相比扩展策略预训练，CoVer在SIMPLER基准上实现了22%的分布内和13%的分布外增益，并在真实世界实验中进一步提高了45%，在PolaRiS基准上任务进度和成功率分别提升了14%和9%。|Marco Pavone Team|[2602.12281](http://arxiv.org/abs/2602.12281)|null|
|**2026-02-12**|**GigaBrain-0.5M*: a VLA That Learns From World Model-Based Reinforcement Learning**|针对现有视觉-语言-动作（VLA）模型在场景理解和未来预测方面的局限性，本研究提出GigaBrain-0.5M*，一个通过世界模型强化学习训练的VLA模型。该模型基于已在大量机器人操作数据上预训练的GigaBrain-0.5，并整合了RAMP（Reinforcement learning via world Model-conditioned Policy）以实现鲁棒的跨任务适应。实验结果表明，RAMP在RECAP基线之上取得了显著的性能提升，在洗衣折叠、箱子包装和咖啡制作等挑战性任务上提升约30%，并在真实部署中展示了可靠的长期执行能力，能够无故障完成复杂的操纵任务。|Zheng Zhu Team|[2602.12099](http://arxiv.org/abs/2602.12099)|**[link](https://gigabrain05m.github.io/)**|
|**2026-02-12**|**VLAW: Iterative Co-Improvement of Vision-Language-Action Policy and World Model**|为提升视觉-语言-动作（VLA）模型性能和可靠性，并解决真实世界数据收集成本高及现有世界模型物理保真度不足的问题，本研究提出一个迭代改进算法。该算法利用少量真实世界试错数据提高世界模型的保真度，然后世界模型生成补充合成数据以改进VLA模型。在真实机器人上的实验表明，该方法使先进VLA模型的成功率相较于基础策略绝对提升39.2%，并且通过生成的合成试错数据训练，又额外提升了11.6%。|Chelsea Finn Team|[2602.12063](http://arxiv.org/abs/2602.12063)|null|
|**2026-02-12**|**HoloBrain-0 Technical Report**|针对基础模型研究与可靠机器人真实部署之间的差距，本研究提出了HoloBrain-0，一个全面的视觉-语言-动作（VLA）框架。其核心是一个新颖的VLA架构，通过明确整合机器人具身先验（如多视角相机参数和运动学描述）来增强3D空间推理并支持多样化具身形态。该设计通过“预训练后微调”范式验证，在多个仿真基准和长时程真实世界操作任务中取得领先结果，且一个高效的0.2B参数变体能支持低延迟部署。研究还全面开源了HoloBrain生态系统，旨在加速研究和实际应用。|Zhizhong Su Team|[2602.12062](http://arxiv.org/abs/2602.12062)|null|
|**2026-02-12**|**When would Vision-Proprioception Policies Fail in Robotic Manipulation?**|针对视觉-本体感受策略在机器人运动过渡阶段视觉模态作用受限、策略倾向于利用本体感受信号导致视觉学习受抑制的问题，本研究提出了梯度调整与阶段引导（GAP）算法。该算法通过利用本体感受信息估计运动过渡阶段的概率，并自适应地调整本体感受梯度的幅值，以实现视觉和本体感受模态的动态协同。综合实验表明，GAP算法能够提升视觉-本体感受策略的鲁棒性和泛化性，适用于模拟和真实环境、单臂和双臂设置，并兼容多种VLA模型。|Di Hu Team|[2602.12032](http://arxiv.org/abs/2602.12032)|null|
|**2026-02-12**|**JEPA-VLA: Video Predictive Embedding is Needed for VLA Models**|针对现有视觉-语言-动作（VLA）模型存在的样本效率低和泛化能力有限问题，本研究发现其根源在于预训练视觉表示在环境理解和策略先验方面的知识不足。通过深入分析，研究指出在视频上预训练的预测嵌入，特别是V-JEPA 2，能更有效地捕捉任务相关时态动态并忽略不可预测因素，从而弥补了现有视觉表示的缺陷。在此基础上，提出了JEPA-VLA，一种将预测嵌入自适应整合到现有VLA中的方法。实验证明，JEPA-VLA在LIBERO、LIBERO-plus、RoboTwin2.0和真实机器人任务等一系列基准测试中均取得了显著性能提升。|Mingsheng Long Team|[2602.11832](http://arxiv.org/abs/2602.11832)|null|
|**2026-02-12**|**ABot-N0: Technical Report on the VLA Foundation Model for Versatile Embodied Navigation**|针对具身导航任务碎片化的问题，本研究引入了ABot-N0，一个统一的视觉-语言-动作（VLA）基础模型，旨在实现点目标、物体目标、指令遵循、兴趣点目标和人员跟踪五大核心任务的“大一统”。该模型采用分层“大脑-动作”架构，利用大型语言模型进行语义推理，并结合流匹配专家生成精确轨迹。为支持大规模学习，研究构建了ABot-N0数据引擎，整理了海量专家轨迹和推理样本。实验结果表明，ABot-N0在7个基准测试中取得了最先进的性能，并能通过集成的Agentic导航系统实现动态真实世界环境中的鲁棒、长时程任务执行。|Mu Xu Team|[2602.11598](http://arxiv.org/abs/2602.11598)|**[link](https://amap-cvlab.github.io/ABot-Navigation/ABot-N0/)**|
|**2026-02-12**|**Scaling World Model for Hierarchical Manipulation Policies**|针对视觉-语言-动作（VLA）模型在域外（OOD）设置下泛化能力不足的问题，本研究引入了一个分层VLA框架VISTA。VISTA利用大规模预训练世界模型的泛化能力实现鲁棒且可泛化的视觉子目标任务分解，其中高层世界模型规划任务分解为带有目标图像的子任务序列，低层VLA策略遵循文本和视觉指导生成动作。这些合成的目标图像为低层策略提供了视觉和物理上的详细指导，使其能够泛化到未见过的物体和新场景。实验结果表明，在世界模型生成的指导下，VISTA在大量OOD场景中显著提升了VLA性能，在novel场景中性能从14%提高到69%。|Xinghang Li Team|[2602.10983](http://arxiv.org/abs/2602.10983)|null|
|**2026-02-12**|**ForeAct: Steering Your VLA with Efficient Visual Foresight Planning**|视觉-语言-动作（VLA）模型将高级语言指令转换为具体可执行动作，在开放世界环境中尤具挑战性。本文提出了Visual Foresight Planning (ForeAct)，这是一种通用高效的规划器，通过想象的未来观察和子任务描述逐步指导VLA。该规划器包含一个高效的预测图像生成模块（在0.33秒内预测高质量未来观察）和一个视觉-语言模型，后者负责推理任务并生成子任务描述。重要的是，ForeAct能够无缝集成到现有VLA中，只需扩充视觉输入而无需修改架构。经过百万级跨具身任务的预训练，预测生成器学习了鲁棒的具身动力学。在包含11项多样化、多步骤真实世界任务的基准测试中，ForeAct实现了87.4%的平均成功率，比基线 $π_0$提高了40.9%，比带有文本子任务指导的$π_0$ 提高了30.3%。|Song Han Team|[2602.12322](http://arxiv.org/abs/2602.12322)|null|
|**2026-02-11**|**H-WM: Robotic Task and Motion Planning Guided by Hierarchical World Model**|针对现有世界模型在长时程机器人规划中累积误差的问题，以及传统符号逻辑世界模型缺乏视觉感知接地的局限性，本研究提出分层世界模型（H-WM）。H-WM在一个统一的双层框架内联合预测逻辑和视觉状态转换，将符号推理的鲁棒性与视觉观察的感知基础相结合。为训练H-WM，研究引入了一个对齐机器人运动与符号状态、动作和视觉观察的机器人数据集。实验证明，分层输出为长时程任务提供了稳定一致的中间指导，有效减轻了误差积累，并在VLA控制策略上展示了该方法的有效性和通用性。|Yingxue Zhang Team|[2602.11291](http://arxiv.org/abs/2602.11291)|null|
|**2026-02-11**|**RISE: Self-Improving Robot Policy with Compositional World Model**|针对视觉-语言-动作（VLA）模型在接触密集和动态操作任务中易受执行偏差影响的脆弱性，以及物理世界中在线强化学习（RL）的限制，本研究提出了RISE，一个通过想象进行机器人强化学习的可扩展框架。其核心是一个组合世界模型，该模型能预测多视角未来并通过进度价值模型评估想象结果，从而为策略改进提供信息丰富的优势。这些组件被整合到一个闭环自改进流水线中，在想象空间中持续生成试错并更新策略。在三个真实世界任务中，RISE相对于现有技术取得了显著性能提升，如在动态砖块分类、背包包装和盒子关闭任务中，绝对性能分别提升超过35%、45%和35%。|Hongyang Li Team|[2602.11075](http://arxiv.org/abs/2602.11075)|**[link](https://opendrivelab.com/kai0-rl/)**|
|**2026-02-11**|**RADAR: Benchmarking Vision-Language-Action Generalization via Real-World Dynamics, Spatial-Physical Intelligence, and Autonomous Evaluation**|针对现有视觉-语言-动作（VLA）模型评估主要局限于仿真或高度受限的真实世界，导致现实差距大、泛化能力差的问题，本研究提出RADAR（Real-world Autonomous Dynamics And Reasoning）基准。RADAR旨在系统评估VLA在真实条件下的泛化能力，集成了物理动力学套件、专门测试空间推理和物理理解的任务，以及基于3D指标的全自主评估流程。通过RADAR对多个先进VLA模型进行审计，发现模型在适度物理动态下性能急剧下降，例如在传感器噪声下3D IoU从0.261下降到0.068，且空间推理能力有限，揭示了模型在真实世界条件下的严重脆弱性，强调了RADAR作为可靠泛化评估基准的必要性。|Guangrun Wang Team|[2602.10980](http://arxiv.org/abs/2602.10980)|null|
|**2026-02-11**|**From Representational Complementarity to Dual Systems: Synergizing VLM and Vision-Only Backbones for End-to-End Driving**|VLA驾驶将语言功能引入端到端规划，但其核心变化除准确性-成本权衡外尚不明确。本文通过RecogDrive进行3-RQ分析，比较VLM和纯视觉骨干网络在相同规划器下的表现。研究发现VLM能引入独特子空间，导致在长尾场景下行为差异，VLM更激进而ViT更保守。基于此，本文提出HybridDriveVLA，通过学习评分器融合两者的轨迹，将PDMS提升至92.10。进一步，DualDriveVLA实现了快慢策略，仅在低置信度时调用VLM，在15%场景中调用VLM即可达到91.00 PDMS，并使吞吐量提高3.2倍。|Yan Wang Team|[2602.10719](http://arxiv.org/abs/2602.10719)|null|
|**2026-02-11**|**Say, Dream, and Act: Learning Video World Models for Instruction-Driven Robot Manipulation**|机器人操作需要预测环境对动作的响应，但现有系统常缺乏此能力，导致效率低下。鉴于VLMs无法明确预测未来状态且现有世界模型存在预测短期或空间不一致帧的问题，本文提出了一种快速且预测性的视频条件动作框架。该方法首先适配鲁棒的视频生成模型以确保未来预测的可靠性，接着通过对抗蒸馏实现快速视频生成，最后训练一个动作模型利用生成视频和真实观测纠正空间错误。实验结果表明，该方法生成的视频预测在时间连贯性和空间准确性方面表现优异，显著提升了具身一致性、空间指代能力和任务完成度。|Yanwei Fu Team|[2602.10717](http://arxiv.org/abs/2602.10717)|null|
|**2026-02-11**|**AugVLA-3D: Depth-Driven Feature Augmentation for Vision-Language-Action Models**|当前VLA模型主要依赖2D图像训练，限制了其在复杂3D环境中的空间理解和动作落地。为解决此问题，本文提出了一个将深度估计集成到VLA模型中的新框架，以丰富3D特征表示。该方法利用VGGT从RGB输入中提取3D几何线索，并引入“动作助手”模块，通过动作先验约束3D表示以确保与下游控制任务的一致性。实验结果表明，所提出的方法不仅增强了几何模糊场景中的感知，还显著提高了动作预测准确性，强调了深度驱动数据增强对弥合2D观测与3D决策差距的潜力。|F. Richard Yu Team|[2602.10698](http://arxiv.org/abs/2602.10698)|null|
|**2026-02-11**|**Improving Medical Visual Reinforcement Fine-Tuning via Perception and Reasoning Augmentation**|强化微调（RFT）在语言模型后训练中展现潜力，但在跨模态、以视觉为中心的医学影像领域应用不足，而该领域需要强大的视觉感知和结构化推理。为此，本文提出了VRFT-Aug，一个专为医学领域设计的视觉强化微调框架。VRFT-Aug通过引入先验知识注入、感知驱动策略优化、医学知情奖励塑造和行为模仿等训练策略来增强感知和推理。实验结果表明，该方法在多个医学数据集上持续优于标准监督微调和RFT基线，并提供了可推广到其他医学图像任务的实用训练启发。|Qicheng Lao Team|[2602.10619](http://arxiv.org/abs/2602.10619)|null|
|**2026-02-11**|**LAP: Language-Action Pre-Training Enables Zero-shot Cross-Embodiment Transfer**|机器人学长期目标是实现零样本部署的通用策略，但现有VLA模型与训练实体紧密耦合。针对此问题，本文提出了语言-动作预训练（LAP），一种将低级机器人动作直接用自然语言表示的方法，使动作监督与预训练VLM的输入-输出分布对齐，无需定制化设计或昂贵标注。基于LAP，本文提出了LAP-3B，实现了在未见机器人上的显著零样本迁移，平均成功率超过50%，比现有VLA模型提升约2倍。此外，LAP还支持高效适应、有利扩展，并通过统一的语言-动作格式在协同训练中获得额外收益。|Anirudha Majumdar Team|[2602.10556](http://arxiv.org/abs/2602.10556)|**[link](https://lap-vla.github.io)**|
|**2026-02-11**|**Found-RL: foundation model-enhanced reinforcement learning for autonomous driving**|强化学习在自动驾驶中面临样本效率低和语义可解释性不足问题，而基础模型（特别是VLM）虽能提供丰富知识，但高推理延迟阻碍其在RL训练中的实时部署。为此，本文提出了Found-RL平台，通过异步批量推理框架将VLM推理与仿真循环解耦，解决了延迟瓶颈。该平台引入了价值边际正则化和优势加权动作指导等监督机制，将VLM建议有效提炼到RL策略中。此外，通过条件对比动作对齐解决CLIP的动态盲区问题，实现密集奖励塑造。Found-RL使得轻量级RL模型在保持实时推理（500 FPS）的同时，达到接近十亿参数VLM的性能。|Sikai Chen Team|[2602.10458](http://arxiv.org/abs/2602.10458)|null|
|**2026-02-10**|**Hardware Co-Design Scaling Laws via Roofline Modelling for On-Device LLMs**|VLA模型在资源受限设备部署中面临LLM骨干网络选择挑战，需平衡准确性与推理延迟及硬件效率。本文提出了硬件协同设计法则，将模型训练损失建模为架构超参数的函数，并通过屋脊线模型表征推理延迟，从而建立了准确性-延迟的直接对应关系。通过对1,942个候选架构进行经验评估和训练，拟合出架构与训练损失的缩放定律。将该定律与延迟建模结合，本文确定了硬件协同设计LLM的帕累托前沿，并将架构搜索公式化为精度与性能的联合优化。结果显示，该方法将架构选择时间从数月缩短到数天，并在相同延迟下，其协同设计的架构在WikiText-2上的困惑度降低了19.42%。|Cheng Deng Team|[2602.10377](http://arxiv.org/abs/2602.10377)|null|
|**2026-02-10**|**ST4VLA: Spatially Guided Training for Vision-Language-Action Models**|大型VLM虽擅长多模态理解，但在具身任务中将指令转化为低级运动动作时表现不足。为此，本文引入了ST4VLA，一个双系统VLA框架，通过空间引导训练使动作学习与VLM中的空间先验对齐。ST4VLA包含两阶段：首先是空间基础预训练，利用大规模和机器人数据通过点、框、轨迹预测为VLM注入可迁移先验；其次是空间引导动作后训练，通过空间提示促使模型生成更丰富的空间先验以指导动作生成。实验表明，ST4VLA显著提升了Google Robot和WidowX Robot的性能，在SimplerEnv上创造了SOTA，并展现出对未见物体、转述指令的更强泛化能力及在真实世界中对扰动的鲁棒性。|Jiangmiao Pang Team|[2602.10109](http://arxiv.org/abs/2602.10109)|null|
|**2026-02-10**|**EgoHumanoid: Unlocking In-the-Wild Loco-Manipulation with Robot-Free Egocentric Demonstration**|人形机器人运动-操作任务面临数据需求高和具身差距挑战。针对此，本文提出了EgoHumanoid框架，首次利用大量以自我为中心的人类演示数据与有限机器人数据共同训练视觉-语言-动作策略，使人形机器人在多样真实环境中执行运动-操作。为弥合人类与机器人间的形态和视点差异，本文引入了包含硬件设计、数据处理的系统对齐流程，并开发了便携式人类数据收集系统。其核心是视点对齐和动作对齐，分别减少视觉域差异和将人类动作映射到机器人动作空间。真实世界实验表明，整合人类演示数据显著优于仅用机器人数据的基线（51%），尤其是在未见环境中。|Li Chen Team|[2602.10106](http://arxiv.org/abs/2602.10106)|**[link](https://opendrivelab.com/EgoHumanoid)**|

<p align=right>(<a href=#updated-on-20260217>back to top</a>)</p>

## Humanoid

| Publish Date | Title | Chinese Summary | Authors | PDF | Code |
|:---------|:-----------------------|:------------------------|:---------|:------|:------|
|**2026-02-16**|**Replanning Human-Robot Collaborative Tasks with Vision-Language Models via Semantic and Physical Dual-Correction**|针对人机协作中视觉语言模型(VLM)在解释模糊指令时易产生幻觉推理和物理执行失败的问题，本文提出一个增强VLM推理的HRC框架。该框架采用双重校正机制，包括执行前验证逻辑一致性和任务可行性的内部校正模型，以及执行后通过反馈检测和纠正物理失败的外部校正模型。仿真和真实世界实验表明，该方法有效提高了任务成功率，并实现了人形机器人根据人类指令进行交互式重新规划，验证了其在协作任务中的实用性。|Kensuke Harada Team|[2602.14551](http://arxiv.org/abs/2602.14551)|null|
|**2026-02-16**|**AdaptManip: Learning Adaptive Whole-Body Object Lifting and Delivery with Online Recurrent State Estimation**|针对人形机器人在集成导航、物体抓取和递送任务中，现有模仿学习方法对干扰鲁棒性差的挑战，本文提出AdaptManip框架。该框架通过强化学习训练，无需人类演示，集成了实时对象状态估计器、用于稳定定位操作的全身基础策略和基于LiDAR的全局位置估计器。实验结果表明，AdaptManip在适应性和总成功率方面显著优于基线方法，且在真实世界中成功演示了人形机器人的全自主导航、物体提升和递送。|Sehoon Ha Team|[2602.14363](http://arxiv.org/abs/2602.14363)|**[link](https://morganbyrd03.github.io/adaptmanip/)**|
|**2026-02-15**|**WIMLE: Uncertainty-Aware World Models with IMLE for Sample-Efficient Continuous Control**|鉴于基于模型的强化学习(MBRL)常因模型误差累积、多模态动力学处理不足及预测过自信而性能受限，本文引入WIMLE方法。该方法将隐式最大似然估计扩展至MBRL框架，以学习随机、多模态的世界模型，并通过集成和潜在采样估计预测不确定性，同时利用预测置信度加权合成转换以稳定学习。实验结果表明，WIMLE在多项连续控制任务中实现了卓越的样本效率，并取得了与先进基线相当或更优的渐近性能。|Ke Li Team|[2602.14351](http://arxiv.org/abs/2602.14351)|**[link](https://openreview.net/forum?id=mzLOnTb3WH)**|
|**2026-02-15**|**ProAct: A Dual-System Framework for Proactive Embodied Social Agents**|针对具身社交代理在实现低延迟交互与高层预见性社交行为之间的时间尺度冲突，本文提出了ProAct双系统框架。该框架解耦了低延迟的“行为系统”和处理长时程社会推理的“认知系统”，并引入通过ControlNet条件化的流匹配模型，以支持异步意图注入并实现反应性和预见性手势的无缝转换。用户研究表明，ProAct显著提升了机器人交互的预见性、社交存在感和用户参与度。|Libin Liu Team|[2602.14048](http://arxiv.org/abs/2602.14048)|**[link](https://proactrobot.github.io/)**|
|**2026-02-14**|**Humanoid Hanoi: Investigating Shared Whole-Body Control for Skill-Based Box Rearrangement**|针对人形机器人在长时程箱子重新排列任务中，共享全身控制器在不同技能组合下鲁棒性下降的问题，本文提出一个基于技能的框架。该框架所有技能通过一个共享、任务无关的全身控制器执行，并通过简单的数据聚合程序，利用域随机化下的闭环技能执行来增强训练。在“Humanoid Hanoi”基准测试中，该方法在模拟和真实Digit V3人形机器人上均实现了长时程的全自主箱子重新排列，并量化了共享WBC方法相对于非共享基线的优势。|Alan Fern Team|[2602.13850](http://arxiv.org/abs/2602.13850)|null|
|**2026-02-14**|**Impact-Robust Posture Optimization for Aerial Manipulation**|针对力矩控制冗余机器人在碰撞时鲁棒性不足，状态和指令易出现尖峰的问题，本文提出一种优化机器人姿态的新方法。该方法基于刚体碰撞模型，定义一个量化碰撞前后速度变化的配置依赖指标，并通过梯度下降运动任务将其嵌入任务空间逆动力学全身控制器中，以实时寻找最小化该指标的姿态。实验结果表明，该方法显著减少了碰撞后机器人状态和指令的尖峰，有效避免了执行器饱和，并突出了运动学冗余对碰撞鲁棒性的重要性。|Antonio Franchi Team|[2602.13762](http://arxiv.org/abs/2602.13762)|null|
|**2026-02-14**|**A Kung Fu Athlete Bot That Can Do It All Day: Highly Dynamic, Balance-Challenging Motion Dataset and Autonomous Fall-Resilient Tracking**|针对人形机器人在高动态运动跟踪中存在的鲁棒性挑战及缺乏跌倒恢复策略的问题，本文首先构建了KungFuAthlete高动态武术运动数据集。在此基础上，提出一种新颖的训练范式，使单个策略能够联合学习高动态运动跟踪和跌倒恢复，从而将敏捷执行与稳定化统一在一个框架内。该框架扩展了机器人的能力，使其从纯粹的运动跟踪扩展到具有恢复能力的执行，显著提升了人形机器人在真实世界高动态场景中的自主性和鲁棒性。|Xuesong Li Team|[2602.13656](http://arxiv.org/abs/2602.13656)|null|
|**2026-02-13**|**Adding internal audio sensing to internal vision enables human-like in-hand fabric recognition with soft robotic fingertips**|针对机器人难以同时捕捉高分辨率时空力模式和高频纹理振动以实现类似人类的织物触觉感知问题，本文提出了一个集成Minsight形变力传感器和Minsound振动传感器的系统。该机器人通过主动夹持摩擦织物，利用多模态信息，并结合基于Transformer的分类方法，在20种织物分类任务上达到了97%的准确率。研究证实了音频传感器的重要性，并展现了该方法在学习织物物理属性方面的泛化能力。|Katherine J. Kuchenbecker Team|[2602.12918](http://arxiv.org/abs/2602.12918)|null|
|**2026-02-13**|**PMG: Parameterized Motion Generator for Human-like Locomotion Control**|针对人形机器人现有低级运动控制器在适应高级命令接口、多样任务上下文以及对数据、速度和校准敏感性方面的局限，本文提出了参数化运动生成器（PMG）。该生成器基于人类运动结构分析，利用紧凑的参数化运动数据和高维控制命令合成参考轨迹，并结合模仿学习和仿真到真实参数识别。在人形机器人ZERITH Z1上的实验证明，PMG能产生自然类人步态，精确响应高维控制，并实现高效的仿真到真实迁移，为实际部署提供了有效途径。|Houde Liu Team|[2602.12656](http://arxiv.org/abs/2602.12656)|null|
|**2026-02-12**|**General Humanoid Whole-Body Control via Pretraining and Fast Adaptation**|针对人形机器人在高动态场景下难以实现通用全身控制、快速适应新运动及保持鲁棒平衡的问题，本文提出了FAST框架。该框架引入Parseval引导残差策略适应机制，通过学习轻量级delta动作策略实现高效的分布外运动适应并减轻灾难性遗忘。同时，通过质心感知控制增强物理鲁棒性。实验证明，FAST在模拟和真实世界中均在鲁棒性、适应效率和泛化能力方面超越了现有先进方法。|Zongqing Lu Team|[2602.11929](http://arxiv.org/abs/2602.11929)|null|
|**2026-02-12**|**HAIC: Humanoid Agile Object Interaction Control via Dynamics-Aware World Model**|针对人形机器人与欠驱动物体交互的挑战，即其独立动力学、非完整约束、耦合力及遮挡等问题，本文提出了HAIC框架。该框架通过一个动力学预测器，仅利用本体感知历史来估计物体的高阶状态，并将其投影到静态几何先验上形成动态占用图，使机器人能在盲区推断交互边界。同时，采用不对称微调机制，使世界模型持续适应策略探索，确保鲁棒状态估计。实验结果表明，HAIC在滑板、推拉购物车等敏捷任务中成功率高，能主动补偿惯性扰动，并在多物体长周期任务中表现出色。|Renjing Xu Team|[2602.11758](http://arxiv.org/abs/2602.11758)|**[link](https://haic-humanoid.github.io/)**|
|**2026-02-12**|**Future Mining: Learning for Safety and Security**|鉴于采矿环境恶劣（照明差、GPS受限、连接中断）对感知精度、态势感知及分布式学习系统的影响，以及网络物理威胁对作业安全的危害，本文提出了一个统一的智能安全与安保架构愿景。该架构整合了多模态感知、安全联邦学习、强化学习、DTN通信和能源感知传感。通过Miner Finder、Multimodal Situational Awareness、Backdoor Attack Monitor、TrustFed LFD和IoT driven Equipment Health Monitoring等五个核心模块，该系统旨在解决矿工定位、危害理解、联邦学习鲁棒性和预测性维护等问题。研究表明，该框架能够指导矿工、识别受损模型或传感器，并保障关键设备可靠性，为构建弹性可信的智能采矿系统提供了全面构想。|Sanjay Madria Team|[2602.11472](http://arxiv.org/abs/2602.11472)|null|
|**2026-02-12**|**Humanoid Manipulation Interface: Humanoid Whole-Body Manipulation from Robot-Free Demonstrations**|当前人形机器人全身操作方法受限于硬件物流和复杂奖励工程，导致自主技能有限且通常仅限于受控环境。为解决这些问题，本文提出了Humanoid Manipulation Interface (HuMI)，一个便携高效的框架，用于在各种环境中学习多样化的全身操作任务。HuMI通过便携硬件捕捉丰富的全身运动，实现无机器人数据收集，并利用分层学习流程将人类运动转化为灵巧且可行的人形技能。广泛实验表明，HuMI的数据收集效率比遥操作提高3倍，并在未知环境中取得了70%的成功率，有效提升了人形机器人的泛化操作能力。|Yang Gao Team|[2602.06643](http://arxiv.org/abs/2602.06643)|**[link](https://humanoid-manipulation-interface.github.io)**|
|**2026-02-11**|**ExtremControl: Low-Latency Humanoid Teleoperation with Direct Extremity Control**|针对现有机器人遥操作系统存在的延迟高、响应性差，无法支持快速反馈任务的问题，本文提出了ExtremControl低延迟全身控制框架。该方法通过直接操作人形机器人末端等选定刚性链节的SE(3)姿态来避免全身重定向，利用笛卡尔空间映射直接转换人体运动为机器人目标，并在低层引入速度前馈控制以提升响应性。理论分析和仿真及实际机器人实验验证了其有效性。基于ExtremControl实现的遥操作系统，端到端延迟低至50ms，显著低于现有方法的200ms，成功实现了乒乓球平衡、杂耍等高响应性任务。|Chuang Gan Team|[2602.11321](http://arxiv.org/abs/2602.11321)|**[link](https://owenowl.github.io/extremcontrol)**|
|**2026-02-11**|**APEX: Learning Adaptive High-Platform Traversal for Humanoid Robots**|当前深度强化学习在人形机器人步态控制上取得进展，但难以应对超出腿长的高平台穿越，因传统方法常导致高冲击、不安全的跳跃式解决方案。本文提出了APEX系统，实现了基于感知和攀爬的高平台穿越，它组合了攀爬、行走、姿态重配置等地形适应行为。核心创新是泛化的棘轮进度奖励机制，提供密集但无速度的监督，实现高效探索和安全正则化。通过基于LiDAR的全身操作策略和双重模拟到真实感知间隙弥合策略，将六种技能提炼成单一策略。实验表明，Unitree G1人形机器人实现了0.8米平台（腿长114%）的零样本穿越，并展现出鲁棒的适应性和平滑的多技能转换。|Ding Zhao Team|[2602.11143](http://arxiv.org/abs/2602.11143)|**[link](https://apex-humanoid.github.io/)**|
|**2026-02-11**|**Towards Learning a Generalizable 3D Scene Representation from 2D Observations**|鉴于现有神经辐射场（NeRF）方法多在相机坐标系下操作，难以直接应用于机器人操作，本文提出了一种可泛化的神经辐射场方法，用于从机器人自我中心观测预测3D工作空间占用。该模型在全局工作空间框架中构建占用表示，可以直接应用于机器人操作，并能集成灵活的源视图，泛化到未见过的物体排列而无需特定场景微调。在人形机器人上的实验表明，该模型在40个真实场景中训练后，重建误差为26mm，包括被遮挡区域，验证了其超越传统立体视觉方法推断完整3D 3D占用能力。|Stefan Wermter Team|[2602.10943](http://arxiv.org/abs/2602.10943)|null|
|**2026-02-11**|**MOSAIC: Bridging the Sim-to-Real Gap in Generalist Humanoid Motion Tracking and Teleoperation with Rapid Residual Adaptation**|鉴于通用人形运动追踪器在模拟中表现优异，但在实际硬件持续遥操作时易受接口和动力学误差影响，本文提出了开源全栈系统MOSAIC。该系统首先通过强化学习在多源运动库上训练面向遥操作的通用运动追踪器，采用自适应重采样和强调世界坐标系运动一致性的奖励。为弥合模拟到真实世界的接口差距，MOSAIC通过快速残差适应，使用少量接口特定数据训练一个接口特定策略，并通过加性残差模块将其蒸馏到通用追踪器中，优于传统微调方法。实验结果（包括系统消融、分布外基准测试和真实机器人实验）证明，MOSAIC在实际延迟和噪声下能实现稳健的离线运动回放和在线长周期遥操作。|Alois Knoll Team|[2602.08594](http://arxiv.org/abs/2602.08594)|null|
|**2026-02-11**|**MotionWeaver: Holistic 4D-Anchored Framework for Multi-Humanoid Image Animation**|鉴于现有角色图像动画方法难以泛化到涉及多样人形形式、复杂交互和频繁遮挡的多人场景，本文提出了MotionWeaver框架。该框架引入统一的运动表示，提取与身份无关的运动并明确绑定到角色，以泛化到多样人形并扩展到多人场景。同时，提出了整体4D锚定范式，构建共享4D空间融合运动与视频潜空间，并通过分层4D级别监督强化交互和遮挡处理。为支持此研究，构建了46小时多人视频数据集和300视频基准。定量和定性实验结果表明，MotionWeaver在自建基准上达到SOTA，并能有效泛化至复杂多人场景。|Weizhan Zhang Team|[2602.13326](http://arxiv.org/abs/2602.13326)|null|
|**2026-02-10**|**EgoHumanoid: Unlocking In-the-Wild Loco-Manipulation with Robot-Free Egocentric Demonstration**|针对人形机器人动作操作对数据需求高，而现有方法未能充分利用人类演示数据，且存在人机体现差异的问题，本文提出了EgoHumanoid框架。该框架首次利用大量自我中心人类演示和少量机器人数据共同训练视觉-语言-动作策略，使人形机器人能在多样真实世界环境中执行动作操作。通过硬件设计到数据处理的系统对齐流水线，包括视图对齐和动作对齐，成功弥合了人机之间的形态和视角差异。广泛的真实世界实验表明，整合无机器人自我中心数据相比仅机器人基线性能显著提升51%，尤其是在未见过的环境中，且分析揭示了行为有效迁移及扩展人类数据的潜力。|Li Chen Team|[2602.10106](http://arxiv.org/abs/2602.10106)|**[link](https://opendrivelab.com/EgoHumanoid)**|
|**2026-02-10**|**Humanoid Factors: Design Principles for AI Humanoids in Human Worlds**|随着人形机器人开始与人类共享空间，传统人因工程需要扩展，不仅考虑人类因素，也要考虑人形机器人因素。当前人形机器人带来了人类行为、沟通和社会存在的期望，重塑了可用性、信任和安全。本文引入“人形机器人因素”框架，围绕物理、认知、社会和伦理四大支柱，指导人形机器人开发，使其能有效与人类共存和协作，并表征了人机能力间的重叠与差异。通过评估真实人形机器人控制算法，该框架揭示了传统机器人任务指标如何忽视关键人类认知和交互原则，为设计、评估和管理持续人机共存提供了基础性框架。|Lixiao Huang Team|[2602.10069](http://arxiv.org/abs/2602.10069)|null|
|**2026-02-10**|**TeleGate: Whole-Body Humanoid Teleoperation via Gated Expert Selection with Motion Prior**|针对人形机器人实时全身遥操作中，现有方法通过知识蒸馏将多专家策略整合，常导致高动态运动性能下降的挑战，本文提出了TeleGate统一遥操作框架。其核心思想是训练一个轻量级门控网络，根据本体感知状态和参考轨迹实时动态激活领域特定专家策略，从而保留其完整能力，避免知识蒸馏的性能损失。此外，引入基于VAE的运动先验模块，从历史观测中提取未来运动意图，实现预期控制。在模拟和Unitree G1机器人上的实验表明，TeleGate仅需2.5小时训练数据，即在跑步、跌倒恢复和跳跃等多样动态运动中实现了高精度实时遥操作，显著优于基线方法。|Rongyun Cao Team|[2602.09628](http://arxiv.org/abs/2602.09628)|null|
|**2026-02-09**|**Characteristics, Management, and Utilization of Muscles in Musculoskeletal Humanoids: Empirical Study on Kengoro and Musashi**|当前肌肉骨骼人形机器人研究中，对其生物仿生结构固有的多样属性及其管理利用方式缺乏统一讨论。本研究基于作者团队开发的Kengoro和Musashi机器人，将肌肉骨骼结构特征归纳为冗余性、独立性、各向异性、可变力臂和非线性弹性五大属性。文章进一步探讨了这些属性组合所带来的优势与劣势，并重点讨论了身体图式学习、反射控制、肌肉分组及身体图式适应等机制。最后，研究阐述了通过集成系统实现运动的实践，并展望了未来的研究挑战。|Masayuki Inaba Team|[2602.08518](http://arxiv.org/abs/2602.08518)|null|
|**2026-02-09**|**Learning Human-Like Badminton Skills for Humanoid Robots**|人形机器人实现羽毛球等高强度运动的类人表现面临巨大挑战，尤其是在运动学模仿与功能性、物理感知击打之间难以兼顾自然风格。为解决此问题，本文提出了Imitation-to-Interaction渐进式强化学习框架，旨在使机器人从“模仿者”进化为“击球手”。该方法通过人类数据建立运动先验，蒸馏到模型化状态表示中，并利用对抗性先验稳定动力学，同时引入流形扩展策略以应对稀疏的专家演示。实验结果显示，该框架在仿真中掌握了多样羽毛球技能，并首次实现了类人羽毛球技能从仿真到真实机器人的零样本迁移，展示了物理世界中的优雅和精准打击。|Peng Lu Team|[2602.08370](http://arxiv.org/abs/2602.08370)|null|
|**2026-02-07**|**VividFace: Real-Time and Realistic Facial Expression Shadowing for Humanoid Robots**|人形机器人面部表情实时模仿对于实现逼真、情感丰富的人机交互至关重要，但现有方法常因离线推理和细节捕捉不足而难以同时达到实时性和逼真性。为解决这些局限，本文提出了VividFace，一个实时且逼真的人形机器人面部表情阴影系统。该系统通过优化模仿框架X2CNet++，并引入特征适应训练策略，显著增强了表情表现力；同时，通过视频流兼容推理管线和基于异步I/O的工作流，实现了高效的实时模仿。广泛的真实世界演示验证了VividFace在0.05秒内模仿人类表情并生成生动人形面部的实用能力，且能泛化至多种面部配置。|Yang Zhang Team|[2602.07506](http://arxiv.org/abs/2602.07506)|null|
|**2026-02-07**|**TextOp: Real-time Interactive Text-Driven Humanoid Robot Motion Generation and Control**|现有的人形机器人全身控制器在灵活性和自主性方面存在局限，难以实现实时和交互式驱动。为解决这一问题，本文提出了TextOp，一个实时文本驱动的人形运动生成与控制框架，支持流式语言指令和即时修改。TextOp采用两级架构：高级运动扩散模型根据文本生成短时域轨迹，低级运动跟踪策略则在机器人上执行这些轨迹。广泛的真实机器人实验和离线评估表明，TextOp实现了即时响应、平滑全身运动和精确控制，在舞蹈、跳跃等复杂行为中展现出自由形式的意图表达和流畅过渡。|Xuelong Li Team|[2602.07439](http://arxiv.org/abs/2602.07439)|**[link](https://text-op.github.io/)**|
|**2026-02-07**|**Bridging Speech, Emotion, and Motion: a VLM-based Multimodal Edge-deployable Framework for Humanoid Robots**|为解决多数人形机器人缺乏协调的语音、面部表情和手势，以及在设备上自主运行的需求，本文提出了SeM²，一个基于视觉语言模型的框架。SeM²通过多模态感知模块捕捉用户上下文，结合思维链推理规划响应，并利用语义序列对齐机制确保言语内容与物理表达的精确时间协调，从而实现情感一致的多模态交互。研究实现了云端及边缘部署版本，其中边缘版本通过知识蒸馏高效运行。综合评估显示，SeM²在自然度、情感清晰度和模态一致性方面显著优于单模态基线，推动了社交表达性人形机器人在多样现实环境中的应用。|Miao Li Team|[2602.07434](http://arxiv.org/abs/2602.07434)|null|
|**2026-02-06**|**Cerebellar-Inspired Residual Control for Fault Recovery: From Inference-Time Adaptation to Structural Consolidation**|针对机器人策略在真实世界部署中常遇到的训练后故障，且不便重新训练的问题，本文提出了一种推理时、受小脑启发的残差控制框架。该框架通过在线校正动作增强冻结的强化学习策略，无需修改基础策略参数即可实现故障恢复。它实例化了小脑核心原理，如高维模式分离、并行残差路径和局部误差驱动可塑性，并通过保守的元适应调节残差权限。实验结果表明，在MuJoCo基准测试中，该框架在执行器、动力学和环境扰动下，对HalfCheetah-v5和Humanoid-v5在适度故障下性能显著提升，并在严重故障下表现出优雅的性能下降。|Amit Ranjan Trivedi Team|[2602.07227](http://arxiv.org/abs/2602.07227)|null|
|**2026-02-06**|**DynaRetarget: Dynamically-Feasible Retargeting using Sampling-Based Trajectory Optimization**|将人类运动重定向到人形机器人控制策略并确保其动态可行性是一项挑战。本文介绍了DynaRetarget，一个将人类运动重定向到人形控制策略的完整流程。其核心是新颖的基于采样的轨迹优化（SBTO）框架，该框架能将不完善的运动学轨迹细化为动态可行的运动，并通过逐步推进优化范围来处理长时域任务。DynaRetarget在重定向数百个人形-物体演示中取得了比现有技术更高的成功率，并能泛化到不同物体属性的场景，为生成大规模人形局部操作轨迹合成数据集提供了可能。|Majid Khadiv Team|[2602.06827](http://arxiv.org/abs/2602.06827)|null|
|**2026-02-06**|**ECO: Energy-Constrained Optimization with Reinforcement Learning for Humanoid Walking**|为解决仿人机器人节能稳定运动中现有方法需大量超参数调优且易导致次优策略的问题，本研究提出了ECO（Energy-Constrained Optimization）约束强化学习框架。该框架将能量指标明确定义为不等式约束，通过拉格朗日法强制执行能量消耗和参考运动约束，从而实现稳定、对称且节能的行走。ECO在仿真和真实机器人（BRUCE）上的实验结果表明，在保持鲁棒行走性能的同时，其能耗显著低于MPC、标准RL及其他SOTA约束RL方法，实现了仿人机器人节能运动的实质性进步。|Yao Su Team|[2602.06445](http://arxiv.org/abs/2602.06445)|null|
|**2026-02-06**|**Now You See That: Learning End-to-End Humanoid Locomotion from Raw Pixels**|为解决基于视觉的仿人机器人鲁棒运动中模拟到真实差距产生的感知噪声和多样地形下学习目标冲突的挑战，本研究提出了一个端到端的视觉驱动运动框架。为实现鲁棒的sim-to-real迁移，该框架开发了高保真深度传感器模拟，并提出了视觉感知行为蒸馏方法。为适应多样地形，引入了地形特定奖励塑形，并结合多评论家和多判别器学习。在配备不同立体深度摄像头的两个仿人机器人平台上，该策略展现出在多样环境中的鲁棒性能，能处理极端挑战和精细任务，包括双向长期楼梯遍历。|Zongwu Xie Team|[2602.06382](http://arxiv.org/abs/2602.06382)|null|

<p align=right>(<a href=#updated-on-20260217>back to top</a>)</p>
