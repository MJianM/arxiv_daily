{
  "meta": {
    "updated": "2026-02-13",
    "max_papers_per_category": 500
  },
  "categories": {
    "Manipulation": [
      {
        "paper_id": "2602.12155",
        "title": "FAIL: Flow Matching Adversarial Imitation Learning for Image Generation",
        "abstract": "Post-training of flow matching models-aligning the output distribution with a high-quality target-is mathematically equivalent to imitation learning. While Supervised Fine-Tuning mimics expert demonstrations effectively, it cannot correct policy drift in unseen states. Preference optimization methods address this but require costly preference pairs or reward modeling. We propose Flow Matching Adversarial Imitation Learning (FAIL), which minimizes policy-expert divergence through adversarial training without explicit rewards or pairwise comparisons. We derive two algorithms: FAIL-PD exploits differentiable ODE solvers for low-variance pathwise gradients, while FAIL-PG provides a black-box alternative for discrete or computationally constrained settings. Fine-tuning FLUX with only 13,000 demonstrations from Nano Banana pro, FAIL achieves competitive performance on prompt following and aesthetic benchmarks. Furthermore, the framework generalizes effectively to discrete image and video generation, and functions as a robust regularizer to mitigate reward hacking in reward-based optimization. Code and data are available at https://github.com/HansPolo113/FAIL.",
        "authors_display": "Weidi Xie Team",
        "pdf_url": "http://arxiv.org/abs/2602.12155",
        "code_url": null,
        "date": "2026-02-12",
        "primary_category": "cs.CV",
        "chinese_summary": "流匹配模型在后训练中与高质量目标对齐的问题，数学上等同于模仿学习，但现有方法如SFT难以纠正未知状态下的策略漂移，偏好优化成本高。为此，本文提出Flow Matching Adversarial Imitation Learning (FAIL)，通过对抗训练最小化策略与专家之间的差异，且无需显式奖励或成对比较。FAIL包含两种算法：FAIL-PD利用可微分ODE求解器获取低方差路径梯度，FAIL-PG则提供黑盒替代方案。实验表明，仅用13,000个演示数据微调FLUX，FAIL在提示遵循和美学基准上达到了具有竞争力的性能，并能有效泛化至离散图像和视频生成，同时作为鲁棒正则化器缓解奖励欺骗问题。"
      },
      {
        "paper_id": "2602.12099",
        "title": "GigaBrain-0.5M*: a VLA That Learns From World Model-Based Reinforcement Learning",
        "abstract": "Vision-language-action (VLA) models that directly predict multi-step action chunks from current observations face inherent limitations due to constrained scene understanding and weak future anticipation capabilities. In contrast, video world models pre-trained on web-scale video corpora exhibit robust spatiotemporal reasoning and accurate future prediction, making them a natural foundation for enhancing VLA learning. Therefore, we propose \\textit{GigaBrain-0.5M*}, a VLA model trained via world model-based reinforcement learning. Built upon \\textit{GigaBrain-0.5}, which is pre-trained on over 10,000 hours of robotic manipulation data, whose intermediate version currently ranks first on the international RoboChallenge benchmark. \\textit{GigaBrain-0.5M*} further integrates world model-based reinforcement learning via \\textit{RAMP} (Reinforcement leArning via world Model-conditioned Policy) to enable robust cross-task adaptation. Empirical results demonstrate that \\textit{RAMP} achieves substantial performance gains over the RECAP baseline, yielding improvements of approximately 30\\% on challenging tasks including \\texttt{Laundry Folding}, \\texttt{Box Packing}, and \\texttt{Espresso Preparation}. Critically, \\textit{GigaBrain-0.5M$^*$} exhibits reliable long-horizon execution, consistently accomplishing complex manipulation tasks without failure as validated by real-world deployment videos on our \\href{https://gigabrain05m.github.io}{project page}.",
        "authors_display": "Zheng Zhu Team",
        "pdf_url": "http://arxiv.org/abs/2602.12099",
        "code_url": "https://gigabrain05m.github.io/",
        "date": "2026-02-12",
        "primary_category": "cs.CV",
        "chinese_summary": "现有直接从观测预测多步动作块的视觉-语言-动作（VLA）模型，因场景理解和未来预测能力受限而表现出固有限制。针对此问题，本文提出GigaBrain-0.5M*，一个基于世界模型强化学习（world model-based RL）训练的VLA模型，旨在利用视频世界模型强大的时空推理和预测能力。该模型构建于已在万小时机器人操作数据上预训练的GigaBrain-0.5之上，并通过RAMP（Reinforcement leArning via world Model-conditioned Policy）进一步整合世界模型强化学习，以实现鲁棒的跨任务适应。实验结果表明，RAMP在Laundry Folding、Box Packing、Espresso Preparation等挑战性任务上比RECAP基线性能提升约30%，并且GigaBrain-0.5M*展示了可靠的长周期执行能力，能持续完成复杂操作任务。"
      },
      {
        "paper_id": "2602.12065",
        "title": "Affordance-Graphed Task Worlds: Self-Evolving Task Generation for Scalable Embodied Learning",
        "abstract": "Training robotic policies directly in the real world is expensive and unscalable. Although generative simulation enables large-scale data synthesis, current approaches often fail to generate logically coherent long-horizon tasks and struggle with dynamic physical uncertainties due to open-loop execution. To address these challenges, we propose Affordance-Graphed Task Worlds (AGT-World), a unified framework that autonomously constructs interactive simulated environments and corresponding robot task policies based on real-world observations. Unlike methods relying on random proposals or static replication, AGT-World formalizes the task space as a structured graph, enabling the precise, hierarchical decomposition of complex goals into theoretically grounded atomic primitives. Furthermore, we introduce a Self-Evolution mechanism with hybrid feedback to autonomously refine policies, combining Vision-Language Model reasoning and geometric verification. Extensive experiments demonstrate that our method significantly outperforms in success rates and generalization, achieving a self-improving cycle of proposal, execution, and correction for scalable robot learning.",
        "authors_display": "Changshui Zhang Team",
        "pdf_url": "http://arxiv.org/abs/2602.12065",
        "code_url": null,
        "date": "2026-02-12",
        "primary_category": "cs.RO",
        "chinese_summary": "在真实世界中直接训练机器人策略成本高昂且难以扩展，而生成式仿真虽能合成大规模数据，却常因开环执行而无法生成逻辑连贯的长周期任务，并难以应对动态物理不确定性。为解决这些挑战，本文提出Affordance-Graphed Task Worlds (AGT-World) 统一框架，该框架能根据真实世界观测自主构建交互式模拟环境和相应的机器人任务策略。AGT-World通过将任务空间形式化为结构化图，实现复杂目标的精确分层分解，并引入结合视觉-语言模型推理和几何验证的混合反馈自进化机制来自主优化策略。广泛实验表明，AGT-World在成功率和泛化能力上显著优于现有方法，实现了提案、执行和纠正的自改进循环，为可扩展的机器人学习提供了有效途径。"
      },
      {
        "paper_id": "2602.12062",
        "title": "HoloBrain-0 Technical Report",
        "abstract": "In this work, we introduce HoloBrain-0, a comprehensive Vision-Language-Action (VLA) framework that bridges the gap between foundation model research and reliable real-world robot deployment. The core of our system is a novel VLA architecture that explicitly incorporates robot embodiment priors, including multi-view camera parameters and kinematic descriptions (URDF), to enhance 3D spatial reasoning and support diverse embodiments. We validate this design through a scalable ``pre-train then post-train\" paradigm, achieving state-of-the-art results on simulation benchmarks such as RoboTwin 2.0, LIBERO, and GenieSim, as well as strong results on challenging long-horizon real-world manipulation tasks. Notably, our efficient 0.2B-parameter variant rivals significantly larger baselines, enabling low-latency on-device deployment. To further accelerate research and practical adoption, we fully open-source the entire HoloBrain ecosystem, which includes: (1) powerful pre-trained VLA foundations; (2) post-trained checkpoints for multiple simulation suites and real-world tasks; and (3) RoboOrchard, a full-stack VLA infrastructure for data curation, model training and deployment. Together with standardized data collection protocols, this release provides the community with a complete, reproducible path toward high-performance robotic manipulation.",
        "authors_display": "Zhizhong Su Team",
        "pdf_url": "http://arxiv.org/abs/2602.12062",
        "code_url": null,
        "date": "2026-02-12",
        "primary_category": "cs.RO",
        "chinese_summary": "当前基础模型研究与可靠的真实世界机器人部署之间存在差距。为此，本文引入HoloBrain-0，一个全面的视觉-语言-动作（VLA）框架，其核心是融合机器人本体先验（如多视角相机参数和运动学描述URDF）的VLA架构，以增强3D空间推理并支持多样化的机器人本体。该系统采用“预训练-后训练”范式进行验证，并在RoboTwin 2.0、LIBERO和GenieSim等仿真基准以及挑战性的长周期真实世界操作任务中取得了领先或出色的表现。值得注意的是，其高效的0.2B参数版本性能与显著更大的基线模型相当，支持低延迟设备部署。为了加速研究和实际应用，作者完整开源了HoloBrain生态系统，包括强大的预训练VLA基础、用于多个仿真套件和真实世界任务的后训练检查点，以及用于数据整理、模型训练和部署的全栈VLA基础设施RoboOrchard。"
      },
      {
        "paper_id": "2602.12032",
        "title": "When would Vision-Proprioception Policies Fail in Robotic Manipulation?",
        "abstract": "Proprioceptive information is critical for precise servo control by providing real-time robotic states. Its collaboration with vision is highly expected to enhance performances of the manipulation policy in complex tasks. However, recent studies have reported inconsistent observations on the generalization of vision-proprioception policies. In this work, we investigate this by conducting temporally controlled experiments. We found that during task sub-phases that robot's motion transitions, which require target localization, the vision modality of the vision-proprioception policy plays a limited role. Further analysis reveals that the policy naturally gravitates toward concise proprioceptive signals that offer faster loss reduction when training, thereby dominating the optimization and suppressing the learning of the visual modality during motion-transition phases. To alleviate this, we propose the Gradient Adjustment with Phase-guidance (GAP) algorithm that adaptively modulates the optimization of proprioception, enabling dynamic collaboration within the vision-proprioception policy. Specifically, we leverage proprioception to capture robotic states and estimate the probability of each timestep in the trajectory belonging to motion-transition phases. During policy learning, we apply fine-grained adjustment that reduces the magnitude of proprioception's gradient based on estimated probabilities, leading to robust and generalizable vision-proprioception policies. The comprehensive experiments demonstrate GAP is applicable in both simulated and real-world environments, across one-arm and dual-arm setups, and compatible with both conventional and Vision-Language-Action models. We believe this work can offer valuable insights into the development of vision-proprioception policies in robotic manipulation.",
        "authors_display": "Di Hu Team",
        "pdf_url": "http://arxiv.org/abs/2602.12032",
        "code_url": null,
        "date": "2026-02-12",
        "primary_category": "cs.RO",
        "chinese_summary": "本体感知信息对于机器人精确伺服控制至关重要，但视觉-本体策略在泛化性上表现不一致。本文研究发现，在需要目标定位的机器人运动转换子阶段，视觉模态的作用有限，策略在训练过程中自然倾向于能更快降低损失的简洁本体信号，从而抑制了运动转换阶段视觉模态的学习。为缓解此问题，本文提出Gradient Adjustment with Phase-guidance (GAP) 算法，该算法自适应地调节本体感知的优化过程，促进视觉-本体策略的动态协作。具体而言，GAP利用本体感知来估计轨迹中每个时间步属于运动转换阶段的概率，并据此对本体感知的梯度幅度进行精细调整。综合实验证明，GAP适用于仿真和真实世界环境，支持单臂和双臂配置，并兼容传统和视觉-语言-动作模型，有效提升了视觉-本体策略的鲁棒性和泛化能力。"
      },
      {
        "paper_id": "2602.11978",
        "title": "Accelerating Robotic Reinforcement Learning with Agent Guidance",
        "abstract": "Reinforcement Learning (RL) offers a powerful paradigm for autonomous robots to master generalist manipulation skills through trial-and-error. However, its real-world application is stifled by severe sample inefficiency. Recent Human-in-the-Loop (HIL) methods accelerate training by using human corrections, yet this approach faces a scalability barrier. Reliance on human supervisors imposes a 1:1 supervision ratio that limits fleet expansion, suffers from operator fatigue over extended sessions, and introduces high variance due to inconsistent human proficiency. We present Agent-guided Policy Search (AGPS), a framework that automates the training pipeline by replacing human supervisors with a multimodal agent. Our key insight is that the agent can be viewed as a semantic world model, injecting intrinsic value priors to structure physical exploration. By using executable tools, the agent provides precise guidance via corrective waypoints and spatial constraints for exploration pruning. We validate our approach on two tasks, ranging from precision insertion to deformable object manipulation. Results demonstrate that AGPS outperforms HIL methods in sample efficiency. This automates the supervision pipeline, unlocking the path to labor-free and scalable robot learning. Project website: https://agps-rl.github.io/agps.",
        "authors_display": "Yaodong Yang Team",
        "pdf_url": "http://arxiv.org/abs/2602.11978",
        "code_url": null,
        "date": "2026-02-12",
        "primary_category": "cs.RO",
        "chinese_summary": "强化学习在赋予自主机器人掌握通用操作技能方面具有巨大潜力，但在实际应用中因样本效率低下而受阻。尽管现有的人机协作（HIL）方法通过人类纠正加速训练，但其受制于1:1的监督比例、操作员疲劳以及人类技能不一致导致的训练方差大，难以扩展。针对此问题，本文提出Agent-guided Policy Search (AGPS) 框架，通过引入多模态智能体替代人类监督员，实现训练流程的自动化。AGPS的核心思想是将智能体视为语义世界模型，它注入内在价值先验来构造物理探索，并通过可执行工具提供精确的纠正航路点和空间约束以剪枝探索空间。在精确插入和变形物体操作等任务上的验证结果表明，AGPS在样本效率方面优于HIL方法，从而实现了监督流水线的自动化，为无劳动力和可扩展的机器人学习开辟了道路。"
      },
      {
        "paper_id": "2602.11934",
        "title": "Robot-DIFT: Distilling Diffusion Features for Geometrically Consistent Visuomotor Control",
        "abstract": "We hypothesize that a key bottleneck in generalizable robot manipulation is not solely data scale or policy capacity, but a structural mismatch between current visual backbones and the physical requirements of closed-loop control. While state-of-the-art vision encoders (including those used in VLAs) optimize for semantic invariance to stabilize classification, manipulation typically demands geometric sensitivity the ability to map millimeter-level pose shifts to predictable feature changes. Their discriminative objective creates a \"blind spot\" for fine-grained control, whereas generative diffusion models inherently encode geometric dependencies within their latent manifolds, encouraging the preservation of dense multi-scale spatial structure. However, directly deploying stochastic diffusion features for control is hindered by stochastic instability, inference latency, and representation drift during fine-tuning. To bridge this gap, we propose Robot-DIFT, a framework that decouples the source of geometric information from the process of inference via Manifold Distillation. By distilling a frozen diffusion teacher into a deterministic Spatial-Semantic Feature Pyramid Network (S2-FPN), we retain the rich geometric priors of the generative model while ensuring temporal stability, real-time execution, and robustness against drift. Pretrained on the large-scale DROID dataset, Robot-DIFT demonstrates superior geometric consistency and control performance compared to leading discriminative baselines, supporting the view that how a model learns to see dictates how well it can learn to act.",
        "authors_display": "Georgia Chalvatzaki Team",
        "pdf_url": "http://arxiv.org/abs/2602.11934",
        "code_url": null,
        "date": "2026-02-12",
        "primary_category": "cs.RO",
        "chinese_summary": "通用机器人操作的关键瓶颈不仅限于数据规模或策略容量，更在于当前视觉骨干与闭环控制物理需求之间的结构性不匹配：SOTA视觉编码器为分类优化语义不变性，而操作则要求毫米级位姿变化与可预测特征映射的几何敏感性，判别性目标导致精细控制的“盲点”。尽管生成式扩散模型固有地编码了潜在流形中的几何依赖，但直接将其随机特征用于控制存在不稳定、推理延迟和微调时表示漂移等问题。为弥合这一差距，本文提出Robot-DIFT框架，通过流形蒸馏将几何信息来源与推理过程解耦。该方法将冻结的扩散教师模型蒸馏到确定性空间-语义特征金字塔网络（S2-FPN），从而在保留生成模型丰富几何先验的同时，确保时间稳定性、实时执行和抗漂移鲁棒性。在大型Droid数据集上的预训练结果表明，Robot-DIFT在几何一致性和控制性能上优于领先的判别性基线，证明了模型学习“如何看”决定了其“如何行动”的效果。"
      },
      {
        "paper_id": "2602.11832",
        "title": "JEPA-VLA: Video Predictive Embedding is Needed for VLA Models",
        "abstract": "Recent vision-language-action (VLA) models built upon pretrained vision-language models (VLMs) have achieved significant improvements in robotic manipulation. However, current VLAs still suffer from low sample efficiency and limited generalization. This paper argues that these limitations are closely tied to an overlooked component, pretrained visual representation, which offers insufficient knowledge on both aspects of environment understanding and policy prior. Through an in-depth analysis, we find that commonly used visual representations in VLAs, whether pretrained via language-image contrastive learning or image-based self-supervised learning, remain inadequate at capturing crucial, task-relevant environment information and at inducing effective policy priors, i.e., anticipatory knowledge of how the environment evolves under successful task execution. In contrast, we discover that predictive embeddings pretrained on videos, in particular V-JEPA 2, are adept at flexibly discarding unpredictable environment factors and encoding task-relevant temporal dynamics, thereby effectively compensating for key shortcomings of existing visual representations in VLAs. Building on these observations, we introduce JEPA-VLA, a simple yet effective approach that adaptively integrates predictive embeddings into existing VLAs. Our experiments demonstrate that JEPA-VLA yields substantial performance gains across a range of benchmarks, including LIBERO, LIBERO-plus, RoboTwin2.0, and real-robot tasks.",
        "authors_display": "Mingsheng Long Team",
        "pdf_url": "http://arxiv.org/abs/2602.11832",
        "code_url": null,
        "date": "2026-02-12",
        "primary_category": "cs.CV",
        "chinese_summary": "近年来，基于预训练视觉-语言模型（VLMs）构建的视觉-语言-动作（VLA）模型在机器人操作中取得了显著进展，但仍面临样本效率低下和泛化能力有限的挑战。本文认为这些限制与被忽视的预训练视觉表示有关，其在环境理解和策略先验知识方面提供的信息不足。通过深入分析，研究发现VLA中常用的视觉表示（无论是通过语言-图像对比学习还是基于图像的自监督学习预训练）在捕获关键、任务相关的环境信息以及诱导有效策略先验（即对成功任务执行下环境如何演变的预期知识）方面仍然不足。相反，本文发现通过视频预训练的预测嵌入，特别是V-JEPA 2，能够灵活地舍弃不可预测的环境因素并编码任务相关的时态动态，从而有效弥补现有VLA视觉表示的关键缺陷。基于这些观察，本文引入JEPA-VLA，一种简单而有效的方法，自适应地将预测嵌入整合到现有VLA中。实验证明，JEPA-VLA在LIBERO、LIBERO-plus、RoboTwin2.0和真实机器人任务等一系列基准上均取得了显著的性能提升。"
      },
      {
        "paper_id": "2602.11660",
        "title": "Clutt3R-Seg: Sparse-view 3D Instance Segmentation for Language-grounded Grasping in Cluttered Scenes",
        "abstract": "Reliable 3D instance segmentation is fundamental to language-grounded robotic manipulation. Its critical application lies in cluttered environments, where occlusions, limited viewpoints, and noisy masks degrade perception. To address these challenges, we present Clutt3R-Seg, a zero-shot pipeline for robust 3D instance segmentation for language-grounded grasping in cluttered scenes. Our key idea is to introduce a hierarchical instance tree of semantic cues. Unlike prior approaches that attempt to refine noisy masks, our method leverages them as informative cues: through cross-view grouping and conditional substitution, the tree suppresses over- and under-segmentation, yielding view-consistent masks and robust 3D instances. Each instance is enriched with open-vocabulary semantic embeddings, enabling accurate target selection from natural language instructions. To handle scene changes during multi-stage tasks, we further introduce a consistency-aware update that preserves instance correspondences from only a single post-interaction image, allowing efficient adaptation without rescanning. Clutt3R-Seg is evaluated on both synthetic and real-world datasets, and validated on a real robot. Across all settings, it consistently outperforms state-of-the-art baselines in cluttered and sparse-view scenarios. Even on the most challenging heavy-clutter sequences, Clutt3R-Seg achieves an AP@25 of 61.66, over 2.2x higher than baselines, and with only four input views it surpasses MaskClustering with eight views by more than 2x. The code is available at: https://github.com/jeonghonoh/clutt3r-seg.",
        "authors_display": "Ayoung Kim Team",
        "pdf_url": "http://arxiv.org/abs/2602.11660",
        "code_url": null,
        "date": "2026-02-12",
        "primary_category": "cs.CV",
        "chinese_summary": "可靠的3D实例分割是语言驱动机器人操作的基础，但在杂乱环境中，遮挡、有限视角和噪声掩码会严重降低感知性能。为解决这些挑战，本文提出Clutt3R-Seg，一个零样本、鲁棒的3D实例分割流水线，专为杂乱场景中的语言驱动抓取设计。核心思想是引入语义线索的层次实例树，该方法利用噪声掩码作为信息线索，通过跨视图分组和条件替换来抑制过分割和欠分割，从而生成视图一致的掩码和鲁棒的3D实例。每个实例都富含开放词汇语义嵌入，支持从自然语言指令中精确选择目标。此外，为处理多阶段任务中的场景变化，该方法引入了一致性感知更新机制，仅通过一次交互后的单个图像即可保持实例对应关系。Clutt3R-Seg在合成和真实世界数据集及真实机器人上均表现出色，在杂乱和稀疏视图场景中持续优于SOTA基线，在重度杂乱序列中AP@25达到61.66，比基线高2.2倍以上。"
      },
      {
        "paper_id": "2602.11643",
        "title": "ViTaS: Visual Tactile Soft Fusion Contrastive Learning for Visuomotor Learning",
        "abstract": "Tactile information plays a crucial role in human manipulation tasks and has recently garnered increasing attention in robotic manipulation. However, existing approaches mostly focus on the alignment of visual and tactile features and the integration mechanism tends to be direct concatenation. Consequently, they struggle to effectively cope with occluded scenarios due to neglecting the inherent complementary nature of both modalities and the alignment may not be exploited enough, limiting the potential of their real-world deployment. In this paper, we present ViTaS, a simple yet effective framework that incorporates both visual and tactile information to guide the behavior of an agent. We introduce Soft Fusion Contrastive Learning, an advanced version of conventional contrastive learning method and a CVAE module to utilize the alignment and complementarity within visuo-tactile representations. We demonstrate the effectiveness of our method in 12 simulated and 3 real-world environments, and our experiments show that ViTaS significantly outperforms existing baselines. Project page: https://skyrainwind.github.io/ViTaS/index.html.",
        "authors_display": "Huazhe Xu Team",
        "pdf_url": "http://arxiv.org/abs/2602.11643",
        "code_url": null,
        "date": "2026-02-12",
        "primary_category": "cs.RO",
        "chinese_summary": "触觉信息在机器人操作中日益受到关注，但现有方法大多侧重于视觉与触觉特征的对齐，且集成机制常为直接拼接，导致在遮挡场景下表现不佳，未能充分利用两种模态固有的互补性，从而限制了实际部署潜力。为此，本文提出ViTaS，一个简单而有效的框架，结合视觉和触觉信息来指导智能体的行为。该框架引入了Soft Fusion对比学习（一种改进的对比学习方法）和一个CVAE模块，以利用视触觉表示中的对齐性和互补性。实验结果表明，ViTaS在12个仿真环境和3个真实世界环境中均表现出显著优于现有基线的性能，证明了其有效性。"
      },
      {
        "paper_id": "2602.11464",
        "title": "EasyMimic: A Low-Cost Framework for Robot Imitation Learning from Human Videos",
        "abstract": "Robot imitation learning is often hindered by the high cost of collecting large-scale, real-world data. This challenge is especially significant for low-cost robots designed for home use, as they must be both user-friendly and affordable. To address this, we propose the EasyMimic framework, a low-cost and replicable solution that enables robots to quickly learn manipulation policies from human video demonstrations captured with standard RGB cameras. Our method first extracts 3D hand trajectories from the videos. An action alignment module then maps these trajectories to the gripper control space of a low-cost robot. To bridge the human-to-robot domain gap, we introduce a simple and user-friendly hand visual augmentation strategy. We then use a co-training method, fine-tuning a model on both the processed human data and a small amount of robot data, enabling rapid adaptation to new tasks. Experiments on the low-cost LeRobot platform demonstrate that EasyMimic achieves high performance across various manipulation tasks. It significantly reduces the reliance on expensive robot data collection, offering a practical path for bringing intelligent robots into homes. Project website: https://zt375356.github.io/EasyMimic-Project/.",
        "authors_display": "Qin Jin Team",
        "pdf_url": "http://arxiv.org/abs/2602.11464",
        "code_url": null,
        "date": "2026-02-12",
        "primary_category": "cs.RO",
        "chinese_summary": "针对机器人模仿学习中真实世界数据收集成本高昂的问题，尤其是在低成本家用机器人领域，本研究提出了EasyMimic框架。该方法通过标准RGB相机捕获人类视频，提取3D手部轨迹，并利用动作对齐模块映射到机器人夹持器控制空间。为弥合人-机域鸿沟，引入了手部视觉增强策略，并结合协同训练在处理后的人类数据和少量机器人数据上快速适应新任务。实验表明，EasyMimic在LeRobot平台上实现了高效的操作性能，显著降低了对昂贵机器人数据的依赖。"
      },
      {
        "paper_id": "2602.10983",
        "title": "Scaling World Model for Hierarchical Manipulation Policies",
        "abstract": "Vision-Language-Action (VLA) models are promising for generalist robot manipulation but remain brittle in out-of-distribution (OOD) settings, especially with limited real-robot data. To resolve the generalization bottleneck, we introduce a hierarchical Vision-Language-Action framework \\our{} that leverages the generalization of large-scale pre-trained world model for robust and generalizable VIsual Subgoal TAsk decomposition VISTA. Our hierarchical framework \\our{} consists of a world model as the high-level planner and a VLA as the low-level executor. The high-level world model first divides manipulation tasks into subtask sequences with goal images, and the low-level policy follows the textual and visual guidance to generate action sequences. Compared to raw textual goal specification, these synthesized goal images provide visually and physically grounded details for low-level policies, making it feasible to generalize across unseen objects and novel scenarios. We validate both visual goal synthesis and our hierarchical VLA policies in massive out-of-distribution scenarios, and the performance of the same-structured VLA in novel scenarios could boost from 14% to 69% with the guidance generated by the world model. Results demonstrate that our method outperforms previous baselines with a clear margin, particularly in out-of-distribution scenarios. Project page: \\href{https://vista-wm.github.io/}{https://vista-wm.github.io}",
        "authors_display": "Xinghang Li Team",
        "pdf_url": "http://arxiv.org/abs/2602.10983",
        "code_url": null,
        "date": "2026-02-12",
        "primary_category": "cs.RO",
        "chinese_summary": "视觉-语言-动作（VLA）模型在通用机器人操作中面临域外泛化能力不足的挑战。本研究提出了VISTA分层框架，利用大规模预训练世界模型的泛化能力实现鲁棒且可泛化的视觉子目标任务分解。该框架将世界模型作为高层规划器，负责将操作任务分解为带有目标图像的子任务序列，而低层VLA策略则依据文本和视觉指导生成动作。实验结果显示，通过世界模型生成的视觉目标指导，相同结构的VLA在域外场景中的性能显著提升，验证了其在未见物体和新场景下的强大泛化能力。"
      },
      {
        "paper_id": "2602.11393",
        "title": "Human Preference Modeling Using Visual Motion Prediction Improves Robot Skill Learning from Egocentric Human Video",
        "abstract": "We present an approach to robot learning from egocentric human videos by modeling human preferences in a reward function and optimizing robot behavior to maximize this reward. Prior work on reward learning from human videos attempts to measure the long-term value of a visual state as the temporal distance between it and the terminal state in a demonstration video. These approaches make assumptions that limit performance when learning from video. They must also transfer the learned value function across the embodiment and environment gap. Our method models human preferences by learning to predict the motion of tracked points between subsequent images and defines a reward function as the agreement between predicted and observed object motion in a robot's behavior at each step. We then use a modified Soft Actor Critic (SAC) algorithm initialized with 10 on-robot demonstrations to estimate a value function from this reward and optimize a policy that maximizes this value function, all on the robot. Our approach is capable of learning on a real robot, and we show that policies learned with our reward model match or outperform prior work across multiple tasks in both simulation and on the real robot.",
        "authors_display": "Christopher G. Atkeson Team",
        "pdf_url": "http://arxiv.org/abs/2602.11393",
        "code_url": null,
        "date": "2026-02-11",
        "primary_category": "cs.RO",
        "chinese_summary": "现有从第一视角人类视频中学习机器人奖励函数的方法存在假设限制和跨域迁移挑战。本研究提出一种新方法，通过预测图像中跟踪点的运动来建模人类偏好，并定义奖励函数为每一步预测与观察到的物体运动的一致性。随后，利用修改后的Soft Actor Critic (SAC) 算法，在少量真实机器人演示的初始化下，直接在机器人上估计值函数并优化策略。实验证明，该奖励模型学习的策略在仿真和真实机器人的多个任务中均达到或超越了现有性能。"
      },
      {
        "paper_id": "2602.11337",
        "title": "MolmoSpaces: A Large-Scale Open Ecosystem for Robot Navigation and Manipulation",
        "abstract": "Deploying robots at scale demands robustness to the long tail of everyday situations. The countless variations in scene layout, object geometry, and task specifications that characterize real environments are vast and underrepresented in existing robot benchmarks. Measuring this level of generalization requires infrastructure at a scale and diversity that physical evaluation alone cannot provide. We introduce MolmoSpaces, a fully open ecosystem to support large-scale benchmarking of robot policies. MolmoSpaces consists of over 230k diverse indoor environments, ranging from handcrafted household scenes to procedurally generated multiroom houses, populated with 130k richly annotated object assets, including 48k manipulable objects with 42M stable grasps. Crucially, these environments are simulator-agnostic, supporting popular options such as MuJoCo, Isaac, and ManiSkill. The ecosystem supports the full spectrum of embodied tasks: static and mobile manipulation, navigation, and multiroom long-horizon tasks requiring coordinated perception, planning, and interaction across entire indoor environments. We also design MolmoSpaces-Bench, a benchmark suite of 8 tasks in which robots interact with our diverse scenes and richly annotated objects. Our experiments show MolmoSpaces-Bench exhibits strong sim-to-real correlation (R = 0.96, \\r{ho} = 0.98), confirm newer and stronger zero-shot policies outperform earlier versions in our benchmarks, and identify key sensitivities to prompt phrasing, initial joint positions, and camera occlusion. Through MolmoSpaces and its open-source assets and tooling, we provide a foundation for scalable data generation, policy training, and benchmark creation for robot learning research.",
        "authors_display": "Ranjay Krishna Team",
        "pdf_url": "http://arxiv.org/abs/2602.11337",
        "code_url": null,
        "date": "2026-02-11",
        "primary_category": "cs.RO",
        "chinese_summary": "机器人大规模部署面临长尾场景和环境多样性的挑战，而现有基准测试缺乏所需规模。本研究推出了MolmoSpaces生态系统，包含超过23万个多样化室内环境和13万个丰富标注的物体资产，且与模拟器无关，支持各类具身任务。MolmoSpaces-Bench基准套件包含8个任务，用于评估机器人与多样化场景的交互。实验结果显示，该基准测试具有高模拟到现实相关性，并成功识别了新零样本策略的优越性以及对关键参数的敏感性，为可扩展的数据生成和策略训练提供了基础。"
      },
      {
        "paper_id": "2602.11150",
        "title": "YOR: Your Own Mobile Manipulator for Generalizable Robotics",
        "abstract": "Recent advances in robot learning have generated significant interest in capable platforms that may eventually approach human-level competence. This interest, combined with the commoditization of actuators, has propelled growth in low-cost robotic platforms. However, the optimal form factor for mobile manipulation, especially on a budget, remains an open question. We introduce YOR, an open-source, low-cost mobile manipulator that integrates an omnidirectional base, a telescopic vertical lift, and two arms with grippers to achieve whole-body mobility and manipulation. Our design emphasizes modularity, ease of assembly using off-the-shelf components, and affordability, with a bill-of-materials cost under 10,000 USD. We demonstrate YOR's capability by completing tasks that require coordinated whole-body control, bimanual manipulation, and autonomous navigation. Overall, YOR offers competitive functionality for mobile manipulation research at a fraction of the cost of existing platforms. Project website: https://www.yourownrobot.ai/",
        "authors_display": "Zichen Jeff Cui Team",
        "pdf_url": "http://arxiv.org/abs/2602.11150",
        "code_url": null,
        "date": "2026-02-11",
        "primary_category": "cs.RO",
        "chinese_summary": "鉴于低成本机器人平台日益增长的需求，本研究旨在探索移动操作器的最佳形式。本文介绍了YOR，一款开源、低成本的移动操纵器，其设计强调模块化、易于组装和经济性（物料清单成本低于1万美元）。YOR集成了全向底座、伸缩式升降装置和双臂夹持器，实现了全身移动和操作。实验证明，YOR能够完成需要协调全身控制、双手操作和自主导航的任务，以远低于现有平台的成本提供了有竞争力的移动操作功能。"
      },
      {
        "paper_id": "2602.11236",
        "title": "ABot-M0: VLA Foundation Model for Robotic Manipulation with Action Manifold Learning",
        "abstract": "Building general-purpose embodied agents across diverse hardware remains a central challenge in robotics, often framed as the ''one-brain, many-forms'' paradigm. Progress is hindered by fragmented data, inconsistent representations, and misaligned training objectives. We present ABot-M0, a framework that builds a systematic data curation pipeline while jointly optimizing model architecture and training strategies, enabling end-to-end transformation of heterogeneous raw data into unified, efficient representations. From six public datasets, we clean, standardize, and balance samples to construct UniACT-dataset, a large-scale dataset with over 6 million trajectories and 9,500 hours of data, covering diverse robot morphologies and task scenarios. Unified pre-training improves knowledge transfer and generalization across platforms and tasks, supporting general-purpose embodied intelligence. To improve action prediction efficiency and stability, we propose the Action Manifold Hypothesis: effective robot actions lie not in the full high-dimensional space but on a low-dimensional, smooth manifold governed by physical laws and task constraints. Based on this, we introduce Action Manifold Learning (AML), which uses a DiT backbone to predict clean, continuous action sequences directly. This shifts learning from denoising to projection onto feasible manifolds, improving decoding speed and policy stability. ABot-M0 supports modular perception via a dual-stream mechanism that integrates VLM semantics with geometric priors and multi-view inputs from plug-and-play 3D modules such as VGGT and Qwen-Image-Edit, enhancing spatial understanding without modifying the backbone and mitigating standard VLM limitations in 3D reasoning. Experiments show components operate independently with additive benefits. We will release all code and pipelines for reproducibility and future research.",
        "authors_display": "Mu Xu Team",
        "pdf_url": "http://arxiv.org/abs/2602.11236",
        "code_url": "https://amap-cvlab.github.io/ABot-Manipulation/",
        "date": "2026-02-11",
        "primary_category": "cs.CV",
        "chinese_summary": "为解决跨多样化硬件构建通用具身智能体面临的数据碎片化和表示不一致问题，本研究提出了ABot-M0框架。该框架通过系统化的数据整理流程和模型架构、训练策略的联合优化，将异构原始数据转换为统一高效的表示，并构建了UniACT-dataset大规模数据集。基于动作流形假设，引入Action Manifold Learning (AML) 直接预测连续动作序列，提高解码效率和策略稳定性。通过双流机制实现模块化感知，整合VLM语义与几何先验和多视图输入。实验证明各组件独立运行并带来累加效益，支持通用具身智能体的泛化和知识迁移。"
      },
      {
        "paper_id": "2602.11018",
        "title": "OSIL: Learning Offline Safe Imitation Policies with Safety Inferred from Non-preferred Trajectories",
        "abstract": "This work addresses the problem of offline safe imitation learning (IL), where the goal is to learn safe and reward-maximizing policies from demonstrations that do not have per-timestep safety cost or reward information. In many real-world domains, online learning in the environment can be risky, and specifying accurate safety costs can be difficult. However, it is often feasible to collect trajectories that reflect undesirable or unsafe behavior, implicitly conveying what the agent should avoid. We refer to these as non-preferred trajectories. We propose a novel offline safe IL algorithm, OSIL, that infers safety from non-preferred demonstrations. We formulate safe policy learning as a Constrained Markov Decision Process (CMDP). Instead of relying on explicit safety cost and reward annotations, OSIL reformulates the CMDP problem by deriving a lower bound on reward maximizing objective and learning a cost model that estimates the likelihood of non-preferred behavior. Our approach allows agents to learn safe and reward-maximizing behavior entirely from offline demonstrations. We empirically demonstrate that our approach can learn safer policies that satisfy cost constraints without degrading the reward performance, thus outperforming several baselines.",
        "authors_display": "Balaraman Ravindran Team",
        "pdf_url": "http://arxiv.org/abs/2602.11018",
        "code_url": null,
        "date": "2026-02-11",
        "primary_category": "cs.LG",
        "chinese_summary": "针对离线安全模仿学习中缺乏每一步安全成本或奖励信息的问题，本研究提出了OSIL算法。OSIL通过从“非偏好轨迹”中推断安全性，并将其表述为约束马尔可夫决策过程 (CMDP)。该方法通过推导奖励最大化目标的下界和学习成本模型来估计非偏好行为的可能性，从而无需显式安全成本或奖励标注。实验证明，OSIL能够在不损害奖励性能的情况下，完全从离线演示中学习出满足成本约束且更安全的策略，优于现有基线方法。"
      },
      {
        "paper_id": "2602.10943",
        "title": "Towards Learning a Generalizable 3D Scene Representation from 2D Observations",
        "abstract": "We introduce a Generalizable Neural Radiance Field approach for predicting 3D workspace occupancy from egocentric robot observations. Unlike prior methods operating in camera-centric coordinates, our model constructs occupancy representations in a global workspace frame, making it directly applicable to robotic manipulation. The model integrates flexible source views and generalizes to unseen object arrangements without scene-specific finetuning. We demonstrate the approach on a humanoid robot and evaluate predicted geometry against 3D sensor ground truth. Trained on 40 real scenes, our model achieves 26mm reconstruction error, including occluded regions, validating its ability to infer complete 3D occupancy beyond traditional stereo vision methods.",
        "authors_display": "Stefan Wermter Team",
        "pdf_url": "http://arxiv.org/abs/2602.10943",
        "code_url": null,
        "date": "2026-02-11",
        "primary_category": "cs.CV",
        "chinese_summary": "为了解决传统方法在相机中心坐标系中操作难以直接应用于机器人操纵的问题，本研究提出了一种可泛化的神经辐射场方法。该模型旨在从机器人第一视角观测中预测3D工作空间占用，并在全局工作空间框架中构建占用表示。该方法能够集成灵活的源视图，并泛化到未见对象布置而无需进行场景特定微调。在人形机器人上的实验结果表明，该模型在40个真实场景训练后，实现了26毫米的重建误差（包括遮挡区域），验证了其超越传统立体视觉方法推断完整3D占用信息的能力。"
      },
      {
        "paper_id": "2602.10793",
        "title": "Semi-Supervised Cross-Domain Imitation Learning",
        "abstract": "Cross-domain imitation learning (CDIL) accelerates policy learning by transferring expert knowledge across domains, which is valuable in applications where the collection of expert data is costly. Existing methods are either supervised, relying on proxy tasks and explicit alignment, or unsupervised, aligning distributions without paired data, but often unstable. We introduce the Semi-Supervised CDIL (SS-CDIL) setting and propose the first algorithm for SS-CDIL with theoretical justification. Our method uses only offline data, including a small number of target expert demonstrations and some unlabeled imperfect trajectories. To handle domain discrepancy, we propose a novel cross-domain loss function for learning inter-domain state-action mappings and design an adaptive weight function to balance the source and target knowledge. Experiments on MuJoCo and Robosuite show consistent gains over the baselines, demonstrating that our approach achieves stable and data-efficient policy learning with minimal supervision. Our code is available at~ https://github.com/NYCU-RL-Bandits-Lab/CDIL.",
        "authors_display": "Ping-Chun Hsieh Team",
        "pdf_url": "http://arxiv.org/abs/2602.10793",
        "code_url": null,
        "date": "2026-02-11",
        "primary_category": "cs.LG",
        "chinese_summary": "跨域模仿学习（CDIL）在专家数据收集昂贵时具有重要价值，但现有方法或依赖监督对齐，或无监督但不稳定。本研究引入了半监督CDIL（SS-CDIL）设置，并提出了首个具有理论依据的算法。该方法仅利用离线数据，包括少量目标专家演示和一些未标注的不完美轨迹。为处理域差异，该方法设计了新颖的跨域损失函数学习域间状态-动作映射，并引入自适应权重函数平衡源域和目标域知识。实验表明，该方法在最小监督下实现了稳定且数据高效的策略学习，性能优于现有基线。"
      },
      {
        "paper_id": "2602.10717",
        "title": "Say, Dream, and Act: Learning Video World Models for Instruction-Driven Robot Manipulation",
        "abstract": "Robotic manipulation requires anticipating how the environment evolves in response to actions, yet most existing systems lack this predictive capability, often resulting in errors and inefficiency. While Vision-Language Models (VLMs) provide high-level guidance, they cannot explicitly forecast future states, and existing world models either predict only short horizons or produce spatially inconsistent frames. To address these challenges, we propose a framework for fast and predictive video-conditioned action. Our approach first selects and adapts a robust video generation model to ensure reliable future predictions, then applies adversarial distillation for fast, few-step video generation, and finally trains an action model that leverages both generated videos and real observations to correct spatial errors. Extensive experiments show that our method produces temporally coherent, spatially accurate video predictions that directly support precise manipulation, achieving significant improvements in embodiment consistency, spatial referring ability, and task completion over existing baselines. Codes & Models will be released.",
        "authors_display": "Yanwei Fu Team",
        "pdf_url": "http://arxiv.org/abs/2602.10717",
        "code_url": null,
        "date": "2026-02-11",
        "primary_category": "cs.RO",
        "chinese_summary": "机器人操作中，现有系统普遍缺乏预测环境响应的能力，导致效率低下。针对视觉-语言模型无法显式预测未来状态以及现有世界模型预测范围有限或空间不一致的问题，本研究提出了一个快速预测视频条件动作的框架。该方法首先选择并调整鲁棒视频生成模型以确保可靠预测，然后应用对抗蒸馏实现快速少量步骤的视频生成，最后训练一个动作模型，利用生成视频和真实观测纠正空间误差。广泛实验表明，该方法生成了时间连贯、空间准确的视频预测，显著提升了具身一致性、空间指代能力和任务完成度。"
      }
    ],
    "World Model": [
      {
        "paper_id": "2602.12218",
        "title": "The Observer Effect in World Models: Invasive Adaptation Corrupts Latent Physics",
        "abstract": "Determining whether neural models internalize physical laws as world models, rather than exploiting statistical shortcuts, remains challenging, especially under out-of-distribution (OOD) shifts. Standard evaluations often test latent capability via downstream adaptation (e.g., fine-tuning or high-capacity probes), but such interventions can change the representations being measured and thus confound what was learned during self-supervised learning (SSL). We propose a non-invasive evaluation protocol, PhyIP. We test whether physical quantities are linearly decodable from frozen representations, motivated by the linear representation hypothesis. Across fluid dynamics and orbital mechanics, we find that when SSL achieves low error, latent structure becomes linearly accessible. PhyIP recovers internal energy and Newtonian inverse-square scaling on OOD tests (e.g., $ρ> 0.90$). In contrast, adaptation-based evaluations can collapse this structure ($ρ\\approx 0.05$). These findings suggest that adaptation-based evaluation can obscure latent structures and that low-capacity probes offer a more accurate evaluation of physical world models.",
        "authors_display": "Barbara Hammer Team",
        "pdf_url": "http://arxiv.org/abs/2602.12218",
        "code_url": null,
        "date": "2026-02-12",
        "primary_category": "cs.LG",
        "chinese_summary": "在评估神经网络模型是否真正内化物理定律而非利用统计捷径时，特别是在分布外（OOD）测试下，现有方法存在挑战，因为下游适应性评估会改变表征，混淆模型的学习内容。本文提出了非侵入式评估协议PhyIP，通过检测物理量是否可从冻结的表征中线性解码来验证物理世界模型。实验结果表明，在流体动力学和轨道力学任务中，当自监督学习（SSL）误差较低时，潜在结构变得线性可访问，PhyIP在OOD测试上成功恢复了内能和牛顿平方反比律（ρ>0.90），而基于适应性的评估则可能掩盖这些结构（ρ≈0.05）。这表明低容量探测器能更准确地评估物理世界模型。"
      },
      {
        "paper_id": "2602.12215",
        "title": "LDA-1B: Scaling Latent Dynamics Action Model via Universal Embodied Data Ingestion",
        "abstract": "Recent robot foundation models largely rely on large-scale behavior cloning, which imitates expert actions but discards transferable dynamics knowledge embedded in heterogeneous embodied data. While the Unified World Model (UWM) formulation has the potential to leverage such diverse data, existing instantiations struggle to scale to foundation-level due to coarse data usage and fragmented datasets. We introduce LDA-1B, a robot foundation model that scales through universal embodied data ingestion by jointly learning dynamics, policy, and visual forecasting, assigning distinct roles to data of varying quality. To support this regime at scale, we assemble and standardize EI-30k, an embodied interaction dataset comprising over 30k hours of human and robot trajectories in a unified format. Scalable dynamics learning over such heterogeneous data is enabled by prediction in a structured DINO latent space, which avoids redundant pixel-space appearance modeling. Complementing this representation, LDA-1B employs a multi-modal diffusion transformer to handle asynchronous vision and action streams, enabling stable training at the 1B-parameter scale. Experiments in simulation and the real world show LDA-1B outperforms prior methods (e.g., $π_{0.5}$) by up to 21\\%, 48\\%, and 23\\% on contact-rich, dexterous, and long-horizon tasks, respectively. Notably, LDA-1B enables data-efficient fine-tuning, gaining 10\\% by leveraging 30\\% low-quality trajectories typically harmful and discarded.",
        "authors_display": "He Wang Team",
        "pdf_url": "http://arxiv.org/abs/2602.12215",
        "code_url": "https://pku-epic.github.io/LDA",
        "date": "2026-02-12",
        "primary_category": "cs.RO",
        "chinese_summary": "当前机器人基础模型多依赖行为克隆，忽略了异构具身数据中蕴含的可迁移动力学知识，而统一世界模型（UWM）因数据使用粗糙和数据集碎片化难以扩展。本文提出了LDA-1B，一个通过通用具身数据摄取进行扩展的机器人基础模型，它联合学习动力学、策略和视觉预测，并为不同质量的数据分配不同角色。为支持大规模学习，研究构建了EI-30k数据集，并利用DINO潜在空间进行结构化预测以避免像素级建模。LDA-1B采用多模态扩散Transformer处理异步视觉和动作流，实现了10亿参数规模的稳定训练。实验证明，LDA-1B在接触密集、灵巧和长时序任务上分别比现有方法（如π0.5）性能提升高达21%、48%和23%，并能有效利用低质量轨迹进行数据高效微调，性能提升10%。"
      },
      {
        "paper_id": "2602.12160",
        "title": "DreamID-Omni: Unified Framework for Controllable Human-Centric Audio-Video Generation",
        "abstract": "Recent advancements in foundation models have revolutionized joint audio-video generation. However, existing approaches typically treat human-centric tasks including reference-based audio-video generation (R2AV), video editing (RV2AV) and audio-driven video animation (RA2V) as isolated objectives. Furthermore, achieving precise, disentangled control over multiple character identities and voice timbres within a single framework remains an open challenge. In this paper, we propose DreamID-Omni, a unified framework for controllable human-centric audio-video generation. Specifically, we design a Symmetric Conditional Diffusion Transformer that integrates heterogeneous conditioning signals via a symmetric conditional injection scheme. To resolve the pervasive identity-timbre binding failures and speaker confusion in multi-person scenarios, we introduce a Dual-Level Disentanglement strategy: Synchronized RoPE at the signal level to ensure rigid attention-space binding, and Structured Captions at the semantic level to establish explicit attribute-subject mappings. Furthermore, we devise a Multi-Task Progressive Training scheme that leverages weakly-constrained generative priors to regularize strongly-constrained tasks, preventing overfitting and harmonizing disparate objectives. Extensive experiments demonstrate that DreamID-Omni achieves comprehensive state-of-the-art performance across video, audio, and audio-visual consistency, even outperforming leading proprietary commercial models. We will release our code to bridge the gap between academic research and commercial-grade applications.",
        "authors_display": "Xiangwang Hou Team",
        "pdf_url": "http://arxiv.org/abs/2602.12160",
        "code_url": "https://guoxu1233.github.io/DreamID-Omni/",
        "date": "2026-02-12",
        "primary_category": "cs.CV",
        "chinese_summary": "现有音视频生成模型将以人为中心的任务（如参考音视频生成、视频编辑、音频驱动视频动画）视为孤立目标，且难以在单一框架内实现对多角色身份和音色的精确解耦控制。本文提出了DreamID-Omni，一个可控的以人为中心音视频生成统一框架。该框架设计了一个对称条件扩散Transformer，通过对称条件注入方案整合异构条件信号，并引入了双层解耦策略（信号级的同步RoPE和语义级的结构化字幕）以解决身份-音色绑定失败和说话人混淆问题。此外，还设计了多任务渐进式训练方案，利用弱约束生成先验来规范强约束任务。广泛实验表明，DreamID-Omni在视频、音频和视听一致性方面取得了全面领先的SOTA表现，甚至超越了顶级的商业模型。"
      },
      {
        "paper_id": "2602.12147",
        "title": "It's TIME: Towards the Next Generation of Time Series Forecasting Benchmarks",
        "abstract": "Time series foundation models (TSFMs) are revolutionizing the forecasting landscape from specific dataset modeling to generalizable task evaluation. However, we contend that existing benchmarks exhibit common limitations in four dimensions: constrained data composition dominated by reused legacy sources, compromised data integrity lacking rigorous quality assurance, misaligned task formulations detached from real-world contexts, and rigid analysis perspectives that obscure generalizable insights. To bridge these gaps, we introduce TIME, a next-generation task-centric benchmark comprising 50 fresh datasets and 98 forecasting tasks, tailored for strict zero-shot TSFM evaluation free from data leakage. Integrating large language models and human expertise, we establish a rigorous human-in-the-loop benchmark construction pipeline to ensure high data integrity and redefine task formulation by aligning forecasting configurations with real-world operational requirements and variate predictability. Furthermore, we propose a novel pattern-level evaluation perspective that moves beyond traditional dataset-level evaluations based on static meta labels. By leveraging structural time series features to characterize intrinsic temporal properties, this approach offers generalizable insights into model capabilities across diverse patterns. We evaluate 12 representative TSFMs and establish a multi-granular leaderboard to facilitate in-depth analysis and visualized inspection. The leaderboard is available at https://huggingface.co/spaces/Real-TSF/TIME-leaderboard.",
        "authors_display": "Chenghao Liu Team",
        "pdf_url": "http://arxiv.org/abs/2602.12147",
        "code_url": null,
        "date": "2026-02-12",
        "primary_category": "cs.LG",
        "chinese_summary": "现有时间序列基础模型（TSFM）基准在数据构成、数据完整性、任务制定和分析视角上存在局限。为解决这些问题，本文引入了TIME，一个新一代以任务为中心的基准，包含50个全新数据集和98个预测任务，专为严格的零样本TSFM评估设计，避免数据泄露。研究整合了大型语言模型和人类专业知识，建立了严谨的人机协作基准构建流程，以确保数据完整性，并通过将预测配置与实际操作需求和变量可预测性对齐来重新定义任务制定。此外，提出了新颖的模式级评估视角，超越传统数据集级评估，通过利用结构化时间序列特征表征内在时间属性，为模型在不同模式下的能力提供通用洞察。实验评估了12个代表性TSFM并建立了多粒度排行榜以促进深入分析。"
      },
      {
        "paper_id": "2602.12120",
        "title": "Commencing-Student Enrolment Forecasting Under Data Sparsity with Time Series Foundation Models",
        "abstract": "Many universities face increasing financial pressure and rely on accurate forecasts of commencing enrolments. However, enrolment forecasting in higher education is often data-sparse; annual series are short and affected by reporting changes and regime shifts. Popular classical approaches can be unreliable, as parameter estimation and model selection are unstable with short samples, and structural breaks degrade extrapolation. Recently, TSFMs have provided zero-shot priors, delivering strong gains in annual, data-sparse institutional forecasting under leakage-disciplined covariate construction. We benchmark multiple TSFM families in a zero-shot setting and test a compact, leakage-safe covariate set and introduce the Institutional Operating Conditions Index (IOCI), a transferable 0-100 regime covariate derived from time-stamped documentary evidence available at each forecast origin, alongside Google Trends demand proxies with stabilising feature engineering. Using an expanding-window backtest with strict vintage alignment, covariate-conditioned TSFMs perform on par with classical benchmarks without institution-specific training, with performance differences varying by cohort and model.",
        "authors_display": "Surangika Ranathunga Team",
        "pdf_url": "http://arxiv.org/abs/2602.12120",
        "code_url": null,
        "date": "2026-02-12",
        "primary_category": "cs.AI",
        "chinese_summary": "许多大学面临日益增长的财政压力，并依赖对入学人数的准确预测，但高等教育中的入学预测通常数据稀疏，且年度序列短，易受报告变化和制度转变影响，导致经典方法不可靠。本文在零样本设置下，基准测试了多种时间序列基础模型（TSFM）家族，并测试了一组紧凑、防泄露的协变量集。研究引入了“机构运营条件指数”（IOCI），一个可转移的0-100区间制度协变量，以及结合稳定特征工程的Google趋势需求代理。通过严格的时间窗口扩展回溯测试，结果显示，在不进行机构特定训练的情况下，条件协变量的TSFMs表现与经典基准相当，性能差异因队列和模型而异。"
      },
      {
        "paper_id": "2602.12108",
        "title": "The Pensieve Paradigm: Stateful Language Models Mastering Their Own Context",
        "abstract": "In the world of Harry Potter, when Dumbledore's mind is overburdened, he extracts memories into a Pensieve to be revisited later. In the world of AI, while we possess the Pensieve-mature databases and retrieval systems, our models inexplicably lack the \"wand\" to operate it. They remain like a Dumbledore without agency, passively accepting a manually engineered context as their entire memory. This work finally places the wand in the model's hand. We introduce StateLM, a new class of foundation models endowed with an internal reasoning loop to manage their own state. We equip our model with a suite of memory tools, such as context pruning, document indexing, and note-taking, and train it to actively manage these tools. By learning to dynamically engineering its own context, our model breaks free from the architectural prison of a fixed window. Experiments across various model sizes demonstrate StateLM's effectiveness across diverse scenarios. On long-document QA tasks, StateLMs consistently outperform standard LLMs across all model scales; on the chat memory task, they achieve absolute accuracy improvements of 10% to 20% over standard LLMs. On the deep research task BrowseComp-Plus, the performance gap becomes even more pronounced: StateLM achieves up to 52% accuracy, whereas standard LLM counterparts struggle around 5%. Ultimately, our approach shifts LLMs from passive predictors to state-aware agents where reasoning becomes a stateful and manageable process.",
        "authors_display": "Yan Wang Team",
        "pdf_url": "http://arxiv.org/abs/2602.12108",
        "code_url": null,
        "date": "2026-02-12",
        "primary_category": "cs.AI",
        "chinese_summary": "当前AI模型，特别是大型语言模型（LLMs），缺乏主动管理自身记忆的能力，仅被动接受手动设计的上下文，受限于固定窗口的架构。本文提出了StateLM，一类新型基础模型，其核心是赋予模型内部推理循环以管理自身状态的能力。我们为模型配备了一套记忆工具，包括上下文剪枝、文档索引和笔记功能，并训练其主动管理这些工具，从而动态构建自己的上下文。实验证明，StateLM在所有模型规模的长文档问答任务上始终优于标准LLMs；在聊天记忆任务上，绝对准确率提升了10%到20%；在深度研究任务BrowseComp-Plus上，StateLM准确率高达52%，而标准LLMs仅为5%左右。该方法将LLMs从被动预测器转变为具备状态感知的智能体，使推理成为一个有状态且可管理的过程。"
      },
      {
        "paper_id": "2602.12099",
        "title": "GigaBrain-0.5M*: a VLA That Learns From World Model-Based Reinforcement Learning",
        "abstract": "Vision-language-action (VLA) models that directly predict multi-step action chunks from current observations face inherent limitations due to constrained scene understanding and weak future anticipation capabilities. In contrast, video world models pre-trained on web-scale video corpora exhibit robust spatiotemporal reasoning and accurate future prediction, making them a natural foundation for enhancing VLA learning. Therefore, we propose \\textit{GigaBrain-0.5M*}, a VLA model trained via world model-based reinforcement learning. Built upon \\textit{GigaBrain-0.5}, which is pre-trained on over 10,000 hours of robotic manipulation data, whose intermediate version currently ranks first on the international RoboChallenge benchmark. \\textit{GigaBrain-0.5M*} further integrates world model-based reinforcement learning via \\textit{RAMP} (Reinforcement leArning via world Model-conditioned Policy) to enable robust cross-task adaptation. Empirical results demonstrate that \\textit{RAMP} achieves substantial performance gains over the RECAP baseline, yielding improvements of approximately 30\\% on challenging tasks including \\texttt{Laundry Folding}, \\texttt{Box Packing}, and \\texttt{Espresso Preparation}. Critically, \\textit{GigaBrain-0.5M$^*$} exhibits reliable long-horizon execution, consistently accomplishing complex manipulation tasks without failure as validated by real-world deployment videos on our \\href{https://gigabrain05m.github.io}{project page}.",
        "authors_display": "Zheng Zhu Team",
        "pdf_url": "http://arxiv.org/abs/2602.12099",
        "code_url": "https://gigabrain05m.github.io/",
        "date": "2026-02-12",
        "primary_category": "cs.CV",
        "chinese_summary": "直接预测多步动作块的视觉-语言-动作（VLA）模型受限于场景理解和未来预测能力。本文提出GigaBrain-0.5M*，一个基于世界模型强化学习训练的VLA模型，旨在利用预训练于海量视频语料库的世界模型强大的时空推理和预测能力。该模型以在超过10000小时机器人操作数据上预训练的GigaBrain-0.5为基础，并通过RAMP（Reinforcement leArning via world Model-conditioned Policy）进一步整合世界模型强化学习，以实现强大的跨任务适应性。实验结果表明，RAMP在Laundry Folding、Box Packing和Espresso Preparation等挑战性任务上比RECAP基线提高了约30%。GigaBrain-0.5M*展现了可靠的长时序执行能力，在实际部署中能持续无故障地完成复杂操作任务。"
      },
      {
        "paper_id": "2602.12065",
        "title": "Affordance-Graphed Task Worlds: Self-Evolving Task Generation for Scalable Embodied Learning",
        "abstract": "Training robotic policies directly in the real world is expensive and unscalable. Although generative simulation enables large-scale data synthesis, current approaches often fail to generate logically coherent long-horizon tasks and struggle with dynamic physical uncertainties due to open-loop execution. To address these challenges, we propose Affordance-Graphed Task Worlds (AGT-World), a unified framework that autonomously constructs interactive simulated environments and corresponding robot task policies based on real-world observations. Unlike methods relying on random proposals or static replication, AGT-World formalizes the task space as a structured graph, enabling the precise, hierarchical decomposition of complex goals into theoretically grounded atomic primitives. Furthermore, we introduce a Self-Evolution mechanism with hybrid feedback to autonomously refine policies, combining Vision-Language Model reasoning and geometric verification. Extensive experiments demonstrate that our method significantly outperforms in success rates and generalization, achieving a self-improving cycle of proposal, execution, and correction for scalable robot learning.",
        "authors_display": "Changshui Zhang Team",
        "pdf_url": "http://arxiv.org/abs/2602.12065",
        "code_url": null,
        "date": "2026-02-12",
        "primary_category": "cs.RO",
        "chinese_summary": "在真实世界中训练机器人策略成本高昂且难以扩展，而生成式仿真虽能合成大量数据，但现有方法难以生成逻辑连贯的长时序任务，且因开环执行而难以处理动态物理不确定性。为解决这些挑战，本文提出了Affordance-Graphed Task Worlds (AGT-World)，一个统一框架，可根据真实世界观察自主构建交互式仿真环境及相应的机器人任务策略。AGT-World将任务空间形式化为结构化图，实现复杂目标的精确分层分解。此外，引入了混合反馈的自进化机制（结合视觉-语言模型推理和几何验证）来自主优化策略。广泛实验表明，该方法在成功率和泛化能力上显著优于现有方法，实现了提案、执行和修正的自改进循环，以实现可扩展的机器人学习。"
      },
      {
        "paper_id": "2602.12063",
        "title": "VLAW: Iterative Co-Improvement of Vision-Language-Action Policy and World Model",
        "abstract": "The goal of this paper is to improve the performance and reliability of vision-language-action (VLA) models through iterative online interaction. Since collecting policy rollouts in the real world is expensive, we investigate whether a learned simulator-specifically, an action-conditioned video generation model-can be used to generate additional rollout data. Unfortunately, existing world models lack the physical fidelity necessary for policy improvement: they are predominantly trained on demonstration datasets that lack coverage of many different physical interactions (particularly failure cases) and struggle to accurately model small yet critical physical details in contact-rich object manipulation. We propose a simple iterative improvement algorithm that uses real-world roll-out data to improve the fidelity of the world model, which can then, in turn, be used to generate supplemental synthetic data for improving the VLA model. In our experiments on a real robot, we use this approach to improve the performance of a state-of-the-art VLA model on multiple downstream tasks. We achieve a 39.2% absolute success rate improvement over the base policy and 11.6% improvement from training with the generated synthetic rollouts. Videos can be found at this anonymous website: https://sites.google.com/view/vla-w",
        "authors_display": "Chelsea Finn Team",
        "pdf_url": "http://arxiv.org/abs/2602.12063",
        "code_url": null,
        "date": "2026-02-12",
        "primary_category": "cs.RO",
        "chinese_summary": "通过迭代在线交互提升视觉-语言-动作（VLA）模型性能和可靠性在真实世界中成本高昂。尽管学习型模拟器（动作条件视频生成模型）可用于生成额外训练数据，但现有世界模型缺乏物理保真度，特别是在许多物理交互（尤其是失败情况）的覆盖以及接触密集物体操作中微小而关键的物理细节建模方面表现不足。本文提出了一种简单的迭代改进算法，该算法利用真实世界的数据来提高世界模型的保真度，然后将改进后的世界模型用于生成补充合成数据以进一步提升VLA模型。在真实机器人上的实验表明，此方法使一个先进的VLA模型在多个下游任务上的成功率比基础策略绝对提升了39.2%，其中11.6%的提升来源于使用生成的合成数据进行训练。"
      },
      {
        "paper_id": "2602.12062",
        "title": "HoloBrain-0 Technical Report",
        "abstract": "In this work, we introduce HoloBrain-0, a comprehensive Vision-Language-Action (VLA) framework that bridges the gap between foundation model research and reliable real-world robot deployment. The core of our system is a novel VLA architecture that explicitly incorporates robot embodiment priors, including multi-view camera parameters and kinematic descriptions (URDF), to enhance 3D spatial reasoning and support diverse embodiments. We validate this design through a scalable ``pre-train then post-train\" paradigm, achieving state-of-the-art results on simulation benchmarks such as RoboTwin 2.0, LIBERO, and GenieSim, as well as strong results on challenging long-horizon real-world manipulation tasks. Notably, our efficient 0.2B-parameter variant rivals significantly larger baselines, enabling low-latency on-device deployment. To further accelerate research and practical adoption, we fully open-source the entire HoloBrain ecosystem, which includes: (1) powerful pre-trained VLA foundations; (2) post-trained checkpoints for multiple simulation suites and real-world tasks; and (3) RoboOrchard, a full-stack VLA infrastructure for data curation, model training and deployment. Together with standardized data collection protocols, this release provides the community with a complete, reproducible path toward high-performance robotic manipulation.",
        "authors_display": "Zhizhong Su Team",
        "pdf_url": "http://arxiv.org/abs/2602.12062",
        "code_url": null,
        "date": "2026-02-12",
        "primary_category": "cs.RO",
        "chinese_summary": "现有基础模型研究与可靠的真实世界机器人部署之间的差距仍然存在，尤其在视觉-语言-动作（VLA）模型中，机器人具身先验的融入不足。本文提出了HoloBrain-0，一个全面的VLA框架，其核心是一个新颖的VLA架构，明确融入了机器人具身先验（包括多视角相机参数和运动学描述URDF），以增强3D空间推理并支持多样化的具身平台。通过可扩展的“预训练-后训练”范式，该设计在RoboTwin 2.0、LIBERO和GenieSim等仿真基准上实现了最先进的结果，并在具有挑战性的长时序真实世界操作任务中表现出色。值得注意的是，其高效的0.2B参数变体能与更大的基线模型相媲美，实现低延迟的设备端部署。为了进一步加速研究和实际应用，HoloBrain生态系统已完全开源，包括强大的预训练VLA基础模型、多个仿真套件和真实世界任务的后训练检查点，以及用于数据整理、模型训练和部署的全栈VLA基础设施RoboOrchard。结合标准化的数据收集协议，这一发布为社区提供了实现高性能机器人操作的完整、可复现路径。"
      },
      {
        "paper_id": "2602.12014",
        "title": "FedGRPO: Privately Optimizing Foundation Models with Group-Relative Rewards from Domain Client",
        "abstract": "One important direction of Federated Foundation Models (FedFMs) is leveraging data from small client models to enhance the performance of a large server-side foundation model. Existing methods based on model level or representation level knowledge transfer either require expensive local training or incur high communication costs and introduce unavoidable privacy risks. We reformulate this problem as a reinforcement learning style evaluation process and propose FedGRPO, a privacy preserving framework comprising two modules. The first module performs competence-based expert selection by building a lightweight confidence graph from auxiliary data to identify the most suitable clients for each question. The second module leverages the \"Group Relative\" concept from the Group Relative Policy Optimization (GRPO) framework by packaging each question together with its solution rationale into candidate policies, dispatching these policies to a selected subset of expert clients, and aggregating solely the resulting scalar reward signals via a federated group-relative loss function. By exchanging reward values instead of data or model updates, FedGRPO reduces privacy risk and communication overhead while enabling parallel evaluation across heterogeneous devices. Empirical results on diverse domain tasks demonstrate that FedGRPO achieves superior downstream accuracy and communication efficiency compared to conventional FedFMs baselines.",
        "authors_display": "Yuxing Han Team",
        "pdf_url": "http://arxiv.org/abs/2602.12014",
        "code_url": null,
        "date": "2026-02-12",
        "primary_category": "cs.LG",
        "chinese_summary": "针对联邦基础模型(FedFMs)中现有知识迁移方法存在的昂贵训练、高通信成本和隐私风险问题，本研究提出FedGRPO框架。该框架将问题重新定义为强化学习评估，通过构建轻量级置信图进行基于能力的专家选择，并利用\"Group Relative\"概念将问题与解决方案打包为候选策略，仅聚合选定客户端的标量奖励信号。实验证明，FedGRPO通过交换奖励值而非数据或模型更新，有效降低了隐私风险和通信开销，并在下游任务中实现了卓越的准确性和通信效率。"
      },
      {
        "paper_id": "2602.11978",
        "title": "Accelerating Robotic Reinforcement Learning with Agent Guidance",
        "abstract": "Reinforcement Learning (RL) offers a powerful paradigm for autonomous robots to master generalist manipulation skills through trial-and-error. However, its real-world application is stifled by severe sample inefficiency. Recent Human-in-the-Loop (HIL) methods accelerate training by using human corrections, yet this approach faces a scalability barrier. Reliance on human supervisors imposes a 1:1 supervision ratio that limits fleet expansion, suffers from operator fatigue over extended sessions, and introduces high variance due to inconsistent human proficiency. We present Agent-guided Policy Search (AGPS), a framework that automates the training pipeline by replacing human supervisors with a multimodal agent. Our key insight is that the agent can be viewed as a semantic world model, injecting intrinsic value priors to structure physical exploration. By using executable tools, the agent provides precise guidance via corrective waypoints and spatial constraints for exploration pruning. We validate our approach on two tasks, ranging from precision insertion to deformable object manipulation. Results demonstrate that AGPS outperforms HIL methods in sample efficiency. This automates the supervision pipeline, unlocking the path to labor-free and scalable robot learning. Project website: https://agps-rl.github.io/agps.",
        "authors_display": "Yaodong Yang Team",
        "pdf_url": "http://arxiv.org/abs/2602.11978",
        "code_url": null,
        "date": "2026-02-12",
        "primary_category": "cs.RO",
        "chinese_summary": "强化学习在机器人通用操作技能习得上面临样本效率低和人机交互(HIL)方法可扩展性受限的挑战。本研究提出Agent-guided Policy Search (AGPS)框架，通过多模态智能体取代人类监督员，实现训练过程自动化。AGPS将智能体视为语义世界模型，利用可执行工具提供精确的校正路径点和空间约束来指导探索。实验结果表明，AGPS在样本效率方面优于HIL方法，为实现无需人工、可扩展的机器人学习提供了新途径。"
      },
      {
        "paper_id": "2602.11882",
        "title": "Where Bits Matter in World Model Planning: A Paired Mixed-Bit Study for Efficient Spatial Reasoning",
        "abstract": "Efficient spatial reasoning requires world models that remain reliable under tight precision budgets. We study whether low-bit planning behavior is determined mostly by total bitwidth or by where bits are allocated across modules. Using DINO-WM on the Wall planning task, we run a paired-goal mixed-bit evaluation across uniform, mixed, asymmetric, and layerwise variants under two planner budgets. We observe a consistent three-regime pattern: 8-bit and 6-bit settings remain close to FP16, 3-bit settings collapse, and 4-bit settings are allocation-sensitive. In that transition region, preserving encoder precision improves planning relative to uniform quantization, and near-size asymmetric variants show the same encoder-side direction. In a later strict 22-cell replication with smaller per-cell episode count, the mixed-versus-uniform INT4 sign becomes budget-conditioned, which further highlights the sensitivity of this transition regime. These findings motivate module-aware, budget-aware quantization policies as a broader research direction for efficient spatial reasoning. Code and run artifacts are available at https://github.com/suraj-ranganath/DINO-MBQuant.",
        "authors_display": "Vaishak Menon Team",
        "pdf_url": "http://arxiv.org/abs/2602.11882",
        "code_url": null,
        "date": "2026-02-12",
        "primary_category": "cs.LG",
        "chinese_summary": "为实现高效空间推理，本研究探讨了世界模型在低位宽预算下规划行为的影响，特别是位宽总数与模块间分配的相对重要性。通过在DINO-WM上进行Wall规划任务的混合位评估，发现存在8/6位接近FP16、3位崩溃、4位对分配敏感的三区域模式。在4位转换区，保留编码器精度可提升规划性能。这些结果表明，高效空间推理需引入模块感知和预算感知的量化策略。"
      },
      {
        "paper_id": "2602.11807",
        "title": "PuYun-LDM: A Latent Diffusion Model for High-Resolution Ensemble Weather Forecasts",
        "abstract": "Latent diffusion models (LDMs) suffer from limited diffusability in high-resolution (<=0.25°) ensemble weather forecasting, where diffusability characterizes how easily a latent data distribution can be modeled by a diffusion process. Unlike natural image fields, meteorological fields lack task-agnostic foundation models and explicit semantic structures, making VFM-based regularization inapplicable. Moreover, existing frequency-based approaches impose identical spectral regularization across channels under a homogeneity assumption, which leads to uneven regularization strength under the inter-variable spectral heterogeneity in multivariate meteorological data. To address these challenges, we propose a 3D Masked AutoEncoder (3D-MAE) that encodes weather-state evolution features as an additional conditioning for the diffusion model, together with a Variable-Aware Masked Frequency Modeling (VA-MFM) strategy that adaptively selects thresholds based on the spectral energy distribution of each variable. Together, we propose PuYun-LDM, which enhances latent diffusability and achieves superior performance to ENS at short lead times while remaining comparable to ENS at longer horizons. PuYun-LDM generates a 15-day global forecast with a 6-hour temporal resolution in five minutes on a single NVIDIA H200 GPU, while ensemble forecasts can be efficiently produced in parallel.",
        "authors_display": "Bin Wang Team",
        "pdf_url": "http://arxiv.org/abs/2602.11807",
        "code_url": null,
        "date": "2026-02-12",
        "primary_category": "cs.AI",
        "chinese_summary": "鉴于潜在扩散模型(LDMs)在 <=0.25°高分辨率集合天气预报中扩散性受限，且现有方法无法有效处理多变量气象数据中的光谱异质性问题，本研究提出PuYun-LDM。该模型结合3D Masked AutoEncoder编码天气状态演化特征作为额外条件，以及Variable-Aware Masked Frequency Modeling策略自适应选择阈值。实验结果显示，PuYun-LDM增强了潜在扩散性，在短时效内预测性能优于ENS，长时效内与之相当，并能在短时间内高效生成全球预报。"
      },
      {
        "paper_id": "2602.11758",
        "title": "HAIC: Humanoid Agile Object Interaction Control via Dynamics-Aware World Model",
        "abstract": "Humanoid robots show promise for complex whole-body tasks in unstructured environments. Although Human-Object Interaction (HOI) has advanced, most methods focus on fully actuated objects rigidly coupled to the robot, ignoring underactuated objects with independent dynamics and non-holonomic constraints. These introduce control challenges from coupling forces and occlusions. We present HAIC, a unified framework for robust interaction across diverse object dynamics without external state estimation. Our key contribution is a dynamics predictor that estimates high-order object states (velocity, acceleration) solely from proprioceptive history. These predictions are projected onto static geometric priors to form a spatially grounded dynamic occupancy map, enabling the policy to infer collision boundaries and contact affordances in blind spots. We use asymmetric fine-tuning, where a world model continuously adapts to the student policy's exploration, ensuring robust state estimation under distribution shifts. Experiments on a humanoid robot show HAIC achieves high success rates in agile tasks (skateboarding, cart pushing/pulling under various loads) by proactively compensating for inertial perturbations, and also masters multi-object long-horizon tasks like carrying a box across varied terrain by predicting the dynamics of multiple objects.",
        "authors_display": "Renjing Xu Team",
        "pdf_url": "http://arxiv.org/abs/2602.11758",
        "code_url": "https://haic-humanoid.github.io/",
        "date": "2026-02-12",
        "primary_category": "cs.RO",
        "chinese_summary": "针对类人机器人在非结构化环境中与具有独立动力学和非完整约束的欠驱动物体交互时的挑战，本研究提出HAIC框架。该框架通过一个动力学预测器，仅利用本体感受历史估计物体高阶状态，并将其投射到静态几何先验上形成动态占用图，从而使策略能在盲区推断碰撞和接触可供性。通过不对称微调，世界模型持续适应学生策略。实验表明，HAIC在敏捷任务和多物体长时任务中实现了高成功率，展现出对惯性扰动的主动补偿能力。"
      },
      {
        "paper_id": "2602.11598",
        "title": "ABot-N0: Technical Report on the VLA Foundation Model for Versatile Embodied Navigation",
        "abstract": "Embodied navigation has long been fragmented by task-specific architectures. We introduce ABot-N0, a unified Vision-Language-Action (VLA) foundation model that achieves a ``Grand Unification'' across 5 core tasks: Point-Goal, Object-Goal, Instruction-Following, POI-Goal, and Person-Following. ABot-N0 utilizes a hierarchical ``Brain-Action'' architecture, pairing an LLM-based Cognitive Brain for semantic reasoning with a Flow Matching-based Action Expert for precise, continuous trajectory generation.   To support large-scale learning, we developed the ABot-N0 Data Engine, curating 16.9M expert trajectories and 5.0M reasoning samples across 7,802 high-fidelity 3D scenes (10.7 $\\text{km}^2$). ABot-N0 achieves new SOTA performance across 7 benchmarks, significantly outperforming specialized models. Furthermore, our Agentic Navigation System integrates a planner with hierarchical topological memory, enabling robust, long-horizon missions in dynamic real-world environments.",
        "authors_display": "Mu Xu Team",
        "pdf_url": "http://arxiv.org/abs/2602.11598",
        "code_url": "https://amap-cvlab.github.io/ABot-Navigation/ABot-N0/",
        "date": "2026-02-12",
        "primary_category": "cs.RO",
        "chinese_summary": "为解决具身导航领域任务架构碎片化问题，本研究引入了ABot-N0，一个统一的视觉-语言-动作(VLA)基础模型。ABot-N0采用分层“大脑-行动”架构，结合基于LLM的认知大脑进行语义推理与基于流匹配的行动专家进行精确轨迹生成，并构建了大规模数据引擎。ABot-N0在5个核心任务上实现了“大一统”，并在7个基准测试中取得新的SOTA性能，显著优于专用模型。其集成的Agentic导航系统通过分层拓扑记忆，进一步支持了真实世界中鲁棒、长时程任务。"
      },
      {
        "paper_id": "2602.11558",
        "title": "Brain4FMs: A Benchmark of Foundation Models for Electrical Brain Signal",
        "abstract": "Brain Foundation Models (BFMs) are transforming neuroscience by enabling scalable and transferable learning from neural signals, advancing both clinical diagnostics and cutting-edge neuroscience exploration. Their emergence is powered by large-scale clinical recordings, particularly electroencephalography (EEG) and intracranial EEG, which provide rich temporal and spatial representations of brain dynamics. However, despite their rapid proliferation, the field lacks a unified understanding of existing methodologies and a standardized evaluation framework. To fill this gap, we map the benchmark design space along two axes: (i) from the model perspective, we organize BFMs under a self-supervised learning (SSL) taxonomy; and (ii) from the dataset perspective, we summarize common downstream tasks and curate representative public datasets across clinical and human-centric neurotechnology applications. Building on this consolidation, we introduce Brain4FMs, an open evaluation platform with plug-and-play interfaces that integrates 15 representative BFMs and 18 public datasets. It enables standardized comparisons and analysis of how pretraining data, SSL strategies, and architectures affect generalization and downstream performance, guiding more accurate and transferable BFMs. The code is available at https://anonymous.4open.science/r/Brain4FMs-85B8.",
        "authors_display": "Yang Yang Team",
        "pdf_url": "http://arxiv.org/abs/2602.11558",
        "code_url": null,
        "date": "2026-02-12",
        "primary_category": "cs.LG",
        "chinese_summary": "脑基础模型(BFMs)正推动神经科学发展，但缺乏统一方法论和标准化评估框架。为填补此空白，本研究绘制了BFM的基准设计空间，从模型角度基于自监督学习(SSL)分类法组织BFMs，并从数据集角度总结下游任务和整理公开数据集。在此基础上，开发了Brain4FMs开放评估平台，集成了15个BFMs和18个数据集，实现了标准化比较和分析预训练数据、SSL策略和架构对泛化性能的影响，以指导更准确、可迁移BFMs的开发。"
      },
      {
        "paper_id": "2602.11550",
        "title": "TS-Memory: Plug-and-Play Memory for Time Series Foundation Models",
        "abstract": "Time Series Foundation Models (TSFMs) achieve strong zero-shot forecasting through large-scale pre-training, but adapting them to downstream domains under distribution shift remains challenging. Existing solutions face a trade-off: Parametric Adaptation can cause catastrophic forgetting and requires costly multi-domain maintenance, while Non-Parametric Retrieval improves forecasts but incurs high inference latency due to datastore search. We propose Parametric Memory Distillation and implement it as TS-Memory, a lightweight memory adapter that augments frozen TSFMs. TS-Memory is trained in two stages. First, we construct an offline, leakage-safe kNN teacher that synthesizes confidence-aware quantile targets from retrieved futures. Second, we distill this retrieval-induced distributional correction into a lightweight memory adapter via confidence-gated supervision. During inference, TS-Memory fuses memory and backbone predictions with constant-time overhead, enabling retrieval-free deployment. Experiments across diverse TSFMs and benchmarks demonstrate consistent improvements in both point and probabilistic forecasting over representative adaptation methods, with efficiency comparable to the frozen backbone.",
        "authors_display": "Yuxuan Liang Team",
        "pdf_url": "http://arxiv.org/abs/2602.11550",
        "code_url": null,
        "date": "2026-02-12",
        "primary_category": "cs.LG",
        "chinese_summary": "时间序列基础模型(TSFMs)在零样本预测方面表现出色，但在分布偏移下适应下游领域时面临挑战。本研究提出Parametric Memory Distillation方法，并实现为TS-Memory，一个轻量级记忆适配器以增强冻结的TSFMs。TS-Memory分两阶段训练：首先构建离线kNN教师合成置信度感知的量化目标，其次通过置信度门控监督将其蒸馏到记忆适配器中。实验证明，TS-Memory在点预测和概率预测方面持续优于现有适应方法，且推理效率与冻结骨干网络相当。"
      },
      {
        "paper_id": "2602.11541",
        "title": "Budget-Constrained Agentic Large Language Models: Intention-Based Planning for Costly Tool Use",
        "abstract": "We study budget-constrained tool-augmented agents, where a large language model must solve multi-step tasks by invoking external tools under a strict monetary budget. We formalize this setting as sequential decision making in context space with priced and stochastic tool executions, making direct planning intractable due to massive state-action spaces, high variance of outcomes and prohibitive exploration cost. To address these challenges, we propose INTENT, an inference-time planning framework that leverages an intention-aware hierarchical world model to anticipate future tool usage, risk-calibrated cost, and guide decisions online. Across cost-augmented StableToolBench, INTENT strictly enforces hard budget feasibility while substantially improving task success over baselines, and remains robust under dynamic market shifts such as tool price changes and varying budgets.",
        "authors_display": "Qi Qi Team",
        "pdf_url": "http://arxiv.org/abs/2602.11541",
        "code_url": null,
        "date": "2026-02-12",
        "primary_category": "cs.AI",
        "chinese_summary": "针对预算受限的工具增强型智能体在解决多步骤任务时，因工具执行的定价与随机性导致直接规划不可行的问题，本研究提出INTENT框架。INTENT是一种推理时规划方法，它利用意图感知分层世界模型来预测未来的工具使用、风险校准成本，并在线指导决策。在成本增强型StableToolBench上的实验表明，INTENT严格遵循预算可行性，显著提高了任务成功率，并在动态市场变化下保持了鲁棒性。"
      },
      {
        "paper_id": "2602.11536",
        "title": "Vascular anatomy-aware self-supervised pre-training for X-ray angiogram analysis",
        "abstract": "X-ray angiography is the gold standard imaging modality for cardiovascular diseases. However, current deep learning approaches for X-ray angiogram analysis are severely constrained by the scarcity of annotated data. While large-scale self-supervised learning (SSL) has emerged as a promising solution, its potential in this domain remains largely unexplored, primarily due to the lack of effective SSL frameworks and large-scale datasets. To bridge this gap, we introduce a vascular anatomy-aware masked image modeling (VasoMIM) framework that explicitly integrates domain-specific anatomical knowledge. Specifically, VasoMIM comprises two key designs: an anatomy-guided masking strategy and an anatomical consistency loss. The former strategically masks vessel-containing patches to compel the model to learn robust vascular semantics, while the latter preserves structural consistency of vessels between original and reconstructed images, enhancing the discriminability of the learned representations. In conjunction with VasoMIM, we curate XA-170K, the largest X-ray angiogram pre-training dataset to date. We validate VasoMIM on four downstream tasks across six datasets, where it demonstrates superior transferability and achieves state-of-the-art performance compared to existing methods. These findings highlight the significant potential of VasoMIM as a foundation model for advancing a wide range of X-ray angiogram analysis tasks. VasoMIM and XA-170K will be available at https://github.com/Dxhuang-CASIA/XA-SSL.",
        "authors_display": "Zeng-Guang Hou Team",
        "pdf_url": "http://arxiv.org/abs/2602.11536",
        "code_url": null,
        "date": "2026-02-12",
        "primary_category": "cs.CV",
        "chinese_summary": "X射线血管造影在心血管疾病诊断中受限于注释数据稀缺和缺乏有效自监督学习(SSL)框架。本研究提出VasoMIM血管解剖结构感知掩码图像建模框架，通过解剖结构引导的掩码策略强制模型学习血管语义，并利用解剖结构一致性损失增强表示的判别力。同时，构建了目前最大的X射线血管造影预训练数据集XA-170K。实验表明，VasoMIM在多个下游任务上展现出优越的可迁移性和SOTA性能，凸显了其作为X射线血管造影分析基础模型的巨大潜力。"
      }
    ],
    "VLM": [
      {
        "paper_id": "2602.12281",
        "title": "Scaling Verification Can Be More Effective than Scaling Policy Learning for Vision-Language-Action Alignment",
        "abstract": "The long-standing vision of general-purpose robots hinges on their ability to understand and act upon natural language instructions. Vision-Language-Action (VLA) models have made remarkable progress toward this goal, yet their generated actions can still misalign with the given instructions. In this paper, we investigate test-time verification as a means to shrink the \"intention-action gap.'' We first characterize the test-time scaling law for embodied instruction following and demonstrate that jointly scaling the number of rephrased instructions and generated actions greatly increases test-time sample diversity, often recovering correct actions more efficiently than scaling each dimension independently. To capitalize on these scaling laws, we present CoVer, a contrastive verifier for vision-language-action alignment, and show that our architecture scales gracefully with additional computational resources and data. We then introduce \"boot-time compute\" and a hierarchical verification inference pipeline for VLAs. At deployment, our framework precomputes a diverse set of rephrased instructions from a Vision-Language-Model (VLM), repeatedly generates action candidates for each instruction, and then uses a verifier to select the optimal high-level prompt and low-level action chunks. Compared to scaling policy pre-training on the same data, our verification approach yields 22% gains in-distribution and 13% out-of-distribution on the SIMPLER benchmark, with a further 45% improvement in real-world experiments. On the PolaRiS benchmark, CoVer achieves 14% gains in task progress and 9% in success rate.",
        "authors_display": "Marco Pavone Team",
        "pdf_url": "http://arxiv.org/abs/2602.12281",
        "code_url": null,
        "date": "2026-02-12",
        "primary_category": "cs.RO",
        "chinese_summary": "现有视觉-语言-动作（VLA）模型在执行自然语言指令时常出现行动与意图不符的问题。为此，本研究提出一种测试时验证方法来缩小这一“意图-行动”差距。该方法通过联合扩展复述指令数量和生成动作候选来增加测试时样本多样性，并引入CoVer（一种视觉-语言-动作对齐的对比验证器）及分层验证推理流程，利用VLM预计算指令，生成动作候选，并由验证器选出最优方案。实验结果表明，该验证方法在SIMPLER基准上实现了分布内22%和分布外13%的性能提升，在真实世界实验中提升了45%，在PolaRiS基准上任务进度提高了14%，成功率提高了9%。"
      },
      {
        "paper_id": "2602.12203",
        "title": "ExStrucTiny: A Benchmark for Schema-Variable Structured Information Extraction from Document Images",
        "abstract": "Enterprise documents, such as forms and reports, embed critical information for downstream applications like data archiving, automated workflows, and analytics. Although generalist Vision Language Models (VLMs) perform well on established document understanding benchmarks, their ability to conduct holistic, fine-grained structured extraction across diverse document types and flexible schemas is not well studied. Existing Key Entity Extraction (KEE), Relation Extraction (RE), and Visual Question Answering (VQA) datasets are limited by narrow entity ontologies, simple queries, or homogeneous document types, often overlooking the need for adaptable and structured extraction. To address these gaps, we introduce ExStrucTiny, a new benchmark dataset for structured Information Extraction (IE) from document images, unifying aspects of KEE, RE, and VQA. Built through a novel pipeline combining manual and synthetic human-validated samples, ExStrucTiny covers more varied document types and extraction scenarios. We analyze open and closed VLMs on this benchmark, highlighting challenges such as schema adaptation, query under-specification, and answer localization. We hope our work provides a bedrock for improving generalist models for structured IE in documents.",
        "authors_display": "Manuela Veloso Team",
        "pdf_url": "http://arxiv.org/abs/2602.12203",
        "code_url": null,
        "date": "2026-02-12",
        "primary_category": "cs.CL",
        "chinese_summary": "通用视觉语言模型（VLMs）在文档理解方面表现出色，但其在多种文档类型和灵活模式下进行全面、细粒度结构化信息提取的能力尚待充分研究。为弥补现有关键实体提取、关系提取和视觉问答数据集的局限性，本研究引入了ExStrucTiny，一个用于从文档图像中进行结构化信息提取的新基准数据集。该数据集通过结合人工和合成样本的新颖流程构建，涵盖了更多样化的文档类型和提取场景。对开源和闭源VLM在该基准上的分析揭示了模式适应、查询欠规范和答案定位等挑战，为未来通用模型在文档结构化信息提取方面的改进奠定了基础。"
      },
      {
        "paper_id": "2602.12159",
        "title": "3DGSNav: Enhancing Vision-Language Model Reasoning for Object Navigation via Active 3D Gaussian Splatting",
        "abstract": "Object navigation is a core capability of embodied intelligence, enabling an agent to locate target objects in unknown environments. Recent advances in vision-language models (VLMs) have facilitated zero-shot object navigation (ZSON). However, existing methods often rely on scene abstractions that convert environments into semantic maps or textual representations, causing high-level decision making to be constrained by the accuracy of low-level perception. In this work, we present 3DGSNav, a novel ZSON framework that embeds 3D Gaussian Splatting (3DGS) as persistent memory for VLMs to enhance spatial reasoning. Through active perception, 3DGSNav incrementally constructs a 3DGS representation of the environment, enabling trajectory-guided free-viewpoint rendering of frontier-aware first-person views. Moreover, we design structured visual prompts and integrate them with Chain-of-Thought (CoT) prompting to further improve VLM reasoning. During navigation, a real-time object detector filters potential targets, while VLM-driven active viewpoint switching performs target re-verification, ensuring efficient and reliable recognition. Extensive evaluations across multiple benchmarks and real-world experiments on a quadruped robot demonstrate that our method achieves robust and competitive performance against state-of-the-art approaches.The Project Page:https://aczheng-cai.github.io/3dgsnav.github.io/",
        "authors_display": "Xinyi Yu Team",
        "pdf_url": "http://arxiv.org/abs/2602.12159",
        "code_url": null,
        "date": "2026-02-12",
        "primary_category": "cs.RO",
        "chinese_summary": "现有零样本物体导航（ZSON）方法依赖于将环境抽象为语义地图或文本表示，导致高级决策受限于低级感知的准确性。本研究提出3DGSNav，一个新颖的ZSON框架，将3D高斯辐射场（3DGS）作为VLM的持久记忆，以增强空间推理能力。该方法通过主动感知逐步构建环境的3DGS表示，实现轨迹引导的自由视点渲染，并结合结构化视觉提示和思维链（CoT）提示来改进VLM的推理。在导航过程中，实时物体检测器过滤潜在目标，并通过VLM驱动的主动视点切换进行目标再验证。大量基准测试和真实世界四足机器人实验证明，该方法在鲁棒性和性能上均优于现有先进方法。"
      },
      {
        "paper_id": "2602.12065",
        "title": "Affordance-Graphed Task Worlds: Self-Evolving Task Generation for Scalable Embodied Learning",
        "abstract": "Training robotic policies directly in the real world is expensive and unscalable. Although generative simulation enables large-scale data synthesis, current approaches often fail to generate logically coherent long-horizon tasks and struggle with dynamic physical uncertainties due to open-loop execution. To address these challenges, we propose Affordance-Graphed Task Worlds (AGT-World), a unified framework that autonomously constructs interactive simulated environments and corresponding robot task policies based on real-world observations. Unlike methods relying on random proposals or static replication, AGT-World formalizes the task space as a structured graph, enabling the precise, hierarchical decomposition of complex goals into theoretically grounded atomic primitives. Furthermore, we introduce a Self-Evolution mechanism with hybrid feedback to autonomously refine policies, combining Vision-Language Model reasoning and geometric verification. Extensive experiments demonstrate that our method significantly outperforms in success rates and generalization, achieving a self-improving cycle of proposal, execution, and correction for scalable robot learning.",
        "authors_display": "Changshui Zhang Team",
        "pdf_url": "http://arxiv.org/abs/2602.12065",
        "code_url": null,
        "date": "2026-02-12",
        "primary_category": "cs.RO",
        "chinese_summary": "在真实世界中训练机器人策略成本高昂且难以扩展。尽管生成式仿真能合成大规模数据，但现有方法常难以生成逻辑连贯的长期任务，且因开环执行而难以应对动态物理不确定性。为解决这些挑战，本研究提出了Affordance-Graphed Task Worlds (AGT-World)，一个统一的框架，能基于真实世界观察自主构建交互式仿真环境和相应的机器人任务策略。该框架将任务空间形式化为结构化图，实现复杂目标的精确分层分解，并引入了结合视觉语言模型推理和几何验证的混合反馈“自演化”机制来自主优化策略。实验结果表明，该方法在成功率和泛化性方面显著优于现有方法，实现了可扩展机器人学习的提出、执行和纠正的自改进循环。"
      },
      {
        "paper_id": "2602.12002",
        "title": "Can Local Vision-Language Models improve Activity Recognition over Vision Transformers? -- Case Study on Newborn Resuscitation",
        "abstract": "Accurate documentation of newborn resuscitation is essential for quality improvement and adherence to clinical guidelines, yet remains underutilized in practice. Previous work using 3D-CNNs and Vision Transformers (ViT) has shown promising results in detecting key activities from newborn resuscitation videos, but also highlighted the challenges in recognizing such fine-grained activities. This work investigates the potential of generative AI (GenAI) methods to improve activity recognition from such videos. Specifically, we explore the use of local vision-language models (VLMs), combined with large language models (LLMs), and compare them to a supervised TimeSFormer baseline. Using a simulated dataset comprising 13.26 hours of newborn resuscitation videos, we evaluate several zero-shot VLM-based strategies and fine-tuned VLMs with classification heads, including Low-Rank Adaptation (LoRA). Our results suggest that small (local) VLMs struggle with hallucinations, but when fine-tuned with LoRA, the results reach F1 score at 0.91, surpassing the TimeSformer results of 0.70.",
        "authors_display": "Øyvind Meinich-Bache Team",
        "pdf_url": "http://arxiv.org/abs/2602.12002",
        "code_url": null,
        "date": "2026-02-12",
        "primary_category": "cs.CV",
        "chinese_summary": "新生儿复苏的准确记录对于质量改进至关重要，但在实践中利用率不足，现有3D-CNN和ViT方法在识别精细活动时面临挑战。本研究旨在探索生成式AI（GenAI）方法，特别是结合大型语言模型（LLM）的局部视觉语言模型（VLM）在改进新生儿复苏视频活动识别方面的潜力，并与监督式TimeSFormer基线进行比较。利用一个包含13.26小时模拟视频的数据集，评估了多种零样本VLM策略和经过LoRA微调的VLM。结果显示，小型VLM虽然存在幻觉问题，但经过LoRA微调后，其F1分数达到0.91，显著超越了TimeSFormer的0.70分。"
      },
      {
        "paper_id": "2602.11960",
        "title": "Benchmarking Vision-Language Models for French PDF-to-Markdown Conversion",
        "abstract": "This report evaluates PDF-to-Markdown conversion using recent Vision-Language Models (VLMs) on challenging French documents. Document parsing is a critical step for Retrieval-Augmented Generation (RAG) pipelines, where transcription and layout errors propagate to downstream retrieval and grounding. Existing benchmarks often emphasize English or Chinese and can over-penalize benign formatting and linearization choices (e.g., line breaks, list segmentation, alternative table renderings) that are largely irrelevant for downstream use.   We introduce a French-focused benchmark of difficult pages selected via model-disagreement sampling from a corpus of 60{,}000 documents, covering handwritten forms, complex layouts, dense tables, and graphics-rich pages. Evaluation is performed with unit-test-style checks that target concrete failure modes (text presence, reading order, and local table constraints) combined with category-specific normalization designed to discount presentation-only variance. Across 15 models, we observe substantially higher robustness for the strongest proprietary models on handwriting and forms, while several open-weights systems remain competitive on standard printed layouts.",
        "authors_display": "Nicolas Mery Team",
        "pdf_url": "http://arxiv.org/abs/2602.11960",
        "code_url": null,
        "date": "2026-02-12",
        "primary_category": "cs.CV",
        "chinese_summary": "PDF到Markdown的转换是检索增强生成（RAG）流程的关键步骤，但转录和布局错误会影响后续检索和推理。现有基准多侧重于英语或中文，且常对下游应用无害的格式差异过度惩罚。为此，本研究提出了一个专注于法语的、针对性强的困难文档基准，通过模型分歧采样从6万份文档中选取，涵盖手写表单、复杂布局、密集表格和图文并茂页面。评估采用针对具体故障模式（文本存在、阅读顺序、局部表格约束）的单元测试式检查，并结合类别特定的归一化处理。对15个模型的评估显示，在手写和表单方面，最强的专有模型表现出显著更高的鲁棒性，而一些开源模型在标准打印布局上仍具竞争力。"
      },
      {
        "paper_id": "2602.11957",
        "title": "Are Two LLMs Better Than One? A Student-Teacher Dual-Head LLMs Architecture for Pharmaceutical Content Optimization",
        "abstract": "Large language models (LLMs) are increasingly used to create content in regulated domains such as pharmaceuticals, where outputs must be scientifically accurate and legally compliant. Manual quality control (QC) is slow, error prone, and can become a publication bottleneck. We introduce LRBTC, a modular LLM and vision language model (VLM) driven QC architecture covering Language, Regulatory, Brand, Technical, and Content Structure checks. LRBTC combines a Student-Teacher dual model architecture, human in the loop (HITL) workflow with waterfall rule filtering to enable scalable, verifiable content validation and optimization. On AIReg-Bench, our approach achieves 83.0% F1 and 97.5% recall, reducing missed violations by 5x compared with Gemini 2.5 Pro. On CSpelling, it improves mean accuracy by 26.7%. Error analysis further reveals that while current models are strong at detecting misspellings (92.5 recall), they fail to identify complex medical grammatical (25.0 recall) and punctuation (41.7 recall) errors, highlighting a key area for future work. This work provides a practical, plug and play solution for reliable, transparent quality control of content in high stakes, compliance critical industries. We also provide access to our Demo under MIT Licenses.",
        "authors_display": "Anubhav Girdhar Team",
        "pdf_url": "http://arxiv.org/abs/2602.11957",
        "code_url": null,
        "date": "2026-02-12",
        "primary_category": "cs.LG",
        "chinese_summary": "大型语言模型（LLMs）在制药等受监管领域的内容创作中应用日益广泛，但其输出必须科学准确且符合法规。人工质量控制（QC）效率低下且易出错。本研究引入了LRBTC，一个模块化的LLM和VLM驱动的质量控制架构，涵盖语言、法规、品牌、技术和内容结构检查。该架构结合了学生-教师双模型体系和人机协作工作流，通过瀑布规则过滤，实现了可扩展、可验证的内容验证和优化。在AIReg-Bench上的实验结果显示，LRBTC取得了83.0%的F1分数和97.5%的召回率，将未发现的违规行为减少了5倍。在CSpelling上，平均准确率提高了26.7%。误差分析表明，模型擅长检测拼写错误（92.5%召回率），但未能识别复杂的医学语法（25.0%召回率）和标点符号错误（41.7%召回率），揭示了未来研究的关键领域。"
      },
      {
        "paper_id": "2602.11862",
        "title": "LAMP: Implicit Language Map for Robot Navigation",
        "abstract": "Recent advances in vision-language models have made zero-shot navigation feasible, enabling robots to follow natural language instructions without requiring labeling. However, existing methods that explicitly store language vectors in grid or node-based maps struggle to scale to large environments due to excessive memory requirements and limited resolution for fine-grained planning. We introduce LAMP (Language Map), a novel neural language field-based navigation framework that learns a continuous, language-driven map and directly leverages it for fine-grained path generation. Unlike prior approaches, our method encodes language features as an implicit neural field rather than storing them explicitly at every location. By combining this implicit representation with a sparse graph, LAMP supports efficient coarse path planning and then performs gradient-based optimization in the learned field to refine poses near the goal. This coarse-to-fine pipeline, language-driven, gradient-guided optimization is the first application of an implicit language map for precise path generation. This refinement is particularly effective at selecting goal regions not directly observed by leveraging semantic similarities in the learned feature space. To further enhance robustness, we adopt a Bayesian framework that models embedding uncertainty via the von Mises-Fisher distribution, thereby improving generalization to unobserved regions. To scale to large environments, LAMP employs a graph sampling strategy that prioritizes spatial coverage and embedding confidence, retaining only the most informative nodes and substantially reducing computational overhead. Our experimental results, both in NVIDIA Isaac Sim and on a real multi-floor building, demonstrate that LAMP outperforms existing explicit methods in both memory efficiency and fine-grained goal-reaching accuracy.",
        "authors_display": "Sunwook Choi Team",
        "pdf_url": "http://arxiv.org/abs/2602.11862",
        "code_url": "https://lab-of-ai-and-robotics.github.io/LAMP/",
        "date": "2026-02-12",
        "primary_category": "cs.RO",
        "chinese_summary": "现有零样本导航方法通过在栅格或基于节点的地图中明确存储语言向量，在大规模环境中面临内存需求过高和分辨率有限的问题，阻碍了细粒度规划。本研究提出了LAMP（语言地图），一种新颖的基于神经语言场的导航框架。该框架将语言特征编码为隐式神经场，学习一个连续的、语言驱动的地图，并利用其进行细粒度路径生成。通过结合隐式表示和稀疏图进行高效粗粒度路径规划，并通过学习到的场进行基于梯度的优化以精确定位目标附近位姿。为增强鲁棒性，该方法采用贝叶斯框架对嵌入不确定性进行建模，并通过图采样策略提升在大环境中的可扩展性。在NVIDIA Isaac Sim和真实多层建筑中的实验结果表明，LAMP在内存效率和细粒度目标到达精度上均优于现有显式方法。"
      },
      {
        "paper_id": "2602.11832",
        "title": "JEPA-VLA: Video Predictive Embedding is Needed for VLA Models",
        "abstract": "Recent vision-language-action (VLA) models built upon pretrained vision-language models (VLMs) have achieved significant improvements in robotic manipulation. However, current VLAs still suffer from low sample efficiency and limited generalization. This paper argues that these limitations are closely tied to an overlooked component, pretrained visual representation, which offers insufficient knowledge on both aspects of environment understanding and policy prior. Through an in-depth analysis, we find that commonly used visual representations in VLAs, whether pretrained via language-image contrastive learning or image-based self-supervised learning, remain inadequate at capturing crucial, task-relevant environment information and at inducing effective policy priors, i.e., anticipatory knowledge of how the environment evolves under successful task execution. In contrast, we discover that predictive embeddings pretrained on videos, in particular V-JEPA 2, are adept at flexibly discarding unpredictable environment factors and encoding task-relevant temporal dynamics, thereby effectively compensating for key shortcomings of existing visual representations in VLAs. Building on these observations, we introduce JEPA-VLA, a simple yet effective approach that adaptively integrates predictive embeddings into existing VLAs. Our experiments demonstrate that JEPA-VLA yields substantial performance gains across a range of benchmarks, including LIBERO, LIBERO-plus, RoboTwin2.0, and real-robot tasks.",
        "authors_display": "Mingsheng Long Team",
        "pdf_url": "http://arxiv.org/abs/2602.11832",
        "code_url": null,
        "date": "2026-02-12",
        "primary_category": "cs.CV",
        "chinese_summary": "当前视觉-语言-动作（VLA）模型在机器人操作中样本效率低、泛化能力有限，这很大程度上源于预训练视觉表征在环境理解和策略先验方面知识不足。本研究通过深入分析发现，现有的视觉表征无法有效捕捉任务相关的环境信息和诱导有效的策略先验。相反，经视频预训练的预测性嵌入（特别是V-JEPA 2）能灵活地摒弃不可预测的环境因素并编码任务相关的时态动态，有效弥补了现有视觉表征的关键缺陷。基于这些观察，本研究提出了JEPA-VLA，一种简单而有效的方法，将预测性嵌入自适应地整合到现有VLA模型中。实验证明，JEPA-VLA在LIBERO、LIBERO-plus、RoboTwin2.0和真实机器人任务等一系列基准测试中均取得了显著的性能提升。"
      },
      {
        "paper_id": "2602.11824",
        "title": "Revis: Sparse Latent Steering to Mitigate Object Hallucination in Large Vision-Language Models",
        "abstract": "Despite the advanced capabilities of Large Vision-Language Models (LVLMs), they frequently suffer from object hallucination. One reason is that visual features and pretrained textual representations often become intertwined in the deeper network layers. To address this, we propose REVIS, a training-free framework designed to explicitly re-activate this suppressed visual information. Rooted in latent space geometry, REVIS extracts the pure visual information vector via orthogonal projection and employs a calibrated strategy to perform sparse intervention only at the precise depth where suppression occurs. This surgical approach effectively restores visual information with minimal computational cost. Empirical evaluations on standard benchmarks demonstrate that REVIS reduces object hallucination rates by approximately 19% compared to state-of-the-art baselines, while preserving general reasoning capabilities.",
        "authors_display": "Zhou Yang Team",
        "pdf_url": "http://arxiv.org/abs/2602.11824",
        "code_url": null,
        "date": "2026-02-12",
        "primary_category": "cs.AI",
        "chinese_summary": "尽管大型视觉-语言模型（LVLMs）能力先进，但常出现物体幻觉问题，原因在于视觉特征和预训练文本表示在深层网络中相互纠缠，抑制了视觉信息。本研究提出了一种免训练框架REVIS，旨在明确地重新激活被抑制的视觉信息。该方法基于潜在空间几何原理，通过正交投影提取纯视觉信息向量，并采用校准策略，仅在发生抑制的精确深度进行稀疏干预。实验评估表明，REVIS将物体幻觉率相比现有最佳基线降低了约19%，同时保留了通用的推理能力。"
      },
      {
        "paper_id": "2602.11743",
        "title": "Adaptive Debiasing Tsallis Entropy for Test-Time Adaptation",
        "abstract": "Mainstream Test-Time Adaptation (TTA) methods for adapting vision-language models, e.g., CLIP, typically rely on Shannon Entropy (SE) at test time to measure prediction uncertainty and inconsistency. However, since CLIP has a built-in bias from pretraining on highly imbalanced web-crawled data, SE inevitably results in producing biased estimates of uncertainty entropy. To address this issue, we notably find and demonstrate that Tsallis Entropy (TE), a generalized form of SE, is naturally suited for characterizing biased distributions by introducing a non-extensive parameter q, with the performance of SE serving as a lower bound for TE. Building upon this, we generalize TE into Adaptive Debiasing Tsallis Entropy (ADTE) for TTA, customizing a class-specific parameter q^l derived by normalizing the estimated label bias from continuously incoming test instances, for each category. This adaptive approach allows ADTE to accurately select high-confidence views and seamlessly integrate with a label adjustment strategy to enhance adaptation, without introducing distribution-specific hyperparameter tuning. Besides, our investigation reveals that both TE and ADTE can serve as direct, advanced alternatives to SE in TTA, without any other modifications. Experimental results show that ADTE outperforms state-of-the-art methods on ImageNet and its five variants, and achieves the highest average performance on 10 cross-domain benchmarks, regardless of the model architecture or text prompts used. Our code is available at https://github.com/Jinx630/ADTE.",
        "authors_display": "Jianfeng Lu Team",
        "pdf_url": "http://arxiv.org/abs/2602.11743",
        "code_url": null,
        "date": "2026-02-12",
        "primary_category": "cs.CV",
        "chinese_summary": "针对视觉-语言模型（如CLIP）的测试时适应（TTA）方法，由于预训练数据偏置导致香农熵（SE）在不确定性估计中存在偏差。本文提出广义形式的Tsallis熵（TE）及其自适应去偏版本ADTE，通过引入非广延参数q并定制类特定参数，有效表征有偏分布，从而更准确地选择高置信度视图并增强适应性。实验证明，ADTE在多个基准测试上超越了现有SOTA方法，展现出优异的性能和鲁棒性，且对模型架构和文本提示不敏感。"
      },
      {
        "paper_id": "2602.11733",
        "title": "Adapting Vision-Language Models for E-commerce Understanding at Scale",
        "abstract": "E-commerce product understanding demands by nature, strong multimodal comprehension from text, images, and structured attributes. General-purpose Vision-Language Models (VLMs) enable generalizable multimodal latent modelling, yet there is no documented, well-known strategy for adapting them to the attribute-centric, multi-image, and noisy nature of e-commerce data, without sacrificing general performance. In this work, we show through a large-scale experimental study, how targeted adaptation of general VLMs can substantially improve e-commerce performance while preserving broad multimodal capabilities. Furthermore, we propose a novel extensive evaluation suite covering deep product understanding, strict instruction following, and dynamic attribute extraction.",
        "authors_display": "Shahram Khadivi Team",
        "pdf_url": "http://arxiv.org/abs/2602.11733",
        "code_url": null,
        "date": "2026-02-12",
        "primary_category": "cs.CV",
        "chinese_summary": "电商产品理解需要强大的多模态（文本、图像、属性）能力，而通用视觉-语言模型（VLMs）虽具备通用建模潜力，但缺乏在不牺牲通用性能的前提下，针对电商数据特点（属性中心、多图像、高噪声）进行适应的策略。本研究通过大规模实验，证明了对通用VLM进行有针对性的适应能显著提升电商领域性能并保留广泛的多模态能力。此外，还提出了一个全面的评估套件，以深入衡量产品理解、指令遵循和动态属性提取能力。"
      },
      {
        "paper_id": "2602.11730",
        "title": "STVG-R1: Incentivizing Instance-Level Reasoning and Grounding in Videos via Reinforcement Learning",
        "abstract": "In vision-language models (VLMs), misalignment between textual descriptions and visual coordinates often induces hallucinations. This issue becomes particularly severe in dense prediction tasks such as spatial-temporal video grounding (STVG). Prior approaches typically focus on enhancing visual-textual alignment or attaching auxiliary decoders. However, these strategies inevitably introduce additional trainable modules, leading to significant annotation costs and computational overhead. In this work, we propose a novel visual prompting paradigm that avoids the difficult problem of aligning coordinates across modalities. Specifically, we reformulate per-frame coordinate prediction as a compact instance-level identification problem by assigning each object a unique, temporally consistent ID. These IDs are embedded into the video as visual prompts, providing explicit and interpretable inputs to the VLMs. Furthermore, we introduce STVG-R1, the first reinforcement learning framework for STVG, which employs a task-driven reward to jointly optimize temporal accuracy, spatial consistency, and structural format regularization. Extensive experiments on six benchmarks demonstrate the effectiveness of our approach. STVG-R1 surpasses the baseline Qwen2.5-VL-7B by a remarkable margin of 20.9% on m_IoU on the HCSTVG-v2 benchmark, establishing a new state of the art (SOTA). Surprisingly, STVG-R1 also exhibits strong zero-shot generalization to multi-object referring video object segmentation tasks, achieving a SOTA 47.3% J&F on MeViS.",
        "authors_display": "Qing Li Team",
        "pdf_url": "http://arxiv.org/abs/2602.11730",
        "code_url": null,
        "date": "2026-02-12",
        "primary_category": "cs.CV",
        "chinese_summary": "视觉-语言模型（VLMs）中文本与视觉坐标错位导致幻觉，尤其在时空视频定位（STVG）等密集预测任务中问题突出，且现有方法引入了额外的计算成本。本文提出一种新颖的视觉提示范式，将帧坐标预测重构为实例识别，通过为每个对象分配时间一致的ID并将其嵌入为视觉提示，为VLM提供显式输入。同时，引入首个用于STVG的强化学习框架STVG-R1，通过任务驱动奖励共同优化时空一致性。实验结果显示，STVG-R1在多个基准上取得SOTA性能，并在零样本多对象视频目标分割任务上展现出强大的泛化能力。"
      },
      {
        "paper_id": "2602.11636",
        "title": "ScalSelect: Scalable Training-Free Multimodal Data Selection for Efficient Visual Instruction Tuning",
        "abstract": "Large-scale Visual Instruction Tuning (VIT) has become a key paradigm for advancing the performance of vision-language models (VLMs) across various multimodal tasks. However, training on the large-scale datasets is computationally expensive and inefficient due to redundancy in the data, which motivates the need for multimodal data selection to improve training efficiency. Existing data selection methods for VIT either require costly training or gradient computation. Training-free alternatives often depend on proxy models or datasets, instruction-agnostic representations, and pairwise similarity with quadratic complexity, limiting scalability and representation fidelity. In this work, we propose ScalSelect, a scalable training-free multimodal data selection method with linear-time complexity with respect to the number of samples, eliminating the need for external models or auxiliary datasets. ScalSelect first constructs sample representations by extracting visual features most attended by instruction tokens in the target VLM, capturing instruction-relevant information. It then identifies samples whose representations best approximate the dominant subspace of the full dataset representations, enabling scalable importance scoring without pairwise comparisons. Extensive experiments across multiple VLMs, datasets, and selection budgets demonstrate that ScalSelect achieves over 97.5% of the performance of training on the full dataset using only 16% of the data, and even outperforms full-data training in some settings. The code is available at \\href{https://github.com/ChangtiWu/ScalSelect}{ScalSelect}.",
        "authors_display": "Kai Chen Team",
        "pdf_url": "http://arxiv.org/abs/2602.11636",
        "code_url": "https://github.com/ChangtiWu/ScalSelect}{ScalSelect}",
        "date": "2026-02-12",
        "primary_category": "cs.CV",
        "chinese_summary": "大规模视觉指令微调（VIT）虽然提升了视觉-语言模型（VLMs）性能，但数据冗余导致训练效率低下，现有数据选择方法又面临计算成本高或可扩展性受限的问题。本文提出ScalSelect，一种可扩展的无训练多模态数据选择方法，其复杂度与样本数量呈线性关系。该方法通过提取模型对指令关注的视觉特征构建样本表示，并识别最能近似整个数据集主导子空间的样本，从而无需成对比较即可进行高效重要性评分。实验证明，ScalSelect仅用16%的数据即可达到全数据集训练97.5%以上的性能，在某些情况下甚至超越全数据训练。"
      },
      {
        "paper_id": "2602.11615",
        "title": "SkillRater: Untangling Capabilities in Multimodal Data",
        "abstract": "Data curation methods typically assign samples a single quality score. We argue this scalar framing is fundamentally limited: when training requires multiple distinct capabilities, a monolithic scorer cannot maximize useful signals for all of them simultaneously. Quality is better understood as multidimensional, with each dimension corresponding to a capability the model must acquire. We introduce SkillRater, a framework that decomposes data filtering into specialized raters - one per capability, each trained via meta-learning on a disjoint validation objective - and composes their scores through a progressive selection rule: at each training stage, a sample is retained if any rater ranks it above a threshold that tightens over time, preserving diversity early while concentrating on high-value samples late. We validate this approach on vision language models, decomposing quality into three capability dimensions: visual understanding, OCR, and STEM reasoning. At 2B parameters, SkillRater improves over unfiltered baselines by 5.63% on visual understanding, 2.00% on OCR, and 3.53% on STEM on held out benchmarks. The learned rater signals are near orthogonal, confirming that the decomposition captures genuinely independent quality dimensions and explaining why it outperforms both unfiltered training and monolithic learned filtering.",
        "authors_display": "Akshat Shrivastava Team",
        "pdf_url": "http://arxiv.org/abs/2602.11615",
        "code_url": null,
        "date": "2026-02-12",
        "primary_category": "cs.LG",
        "chinese_summary": "传统数据筛选将样本质量视为单一标量，无法同时优化模型所需的多种能力。本文提出SkillRater框架，将数据过滤分解为多个专门的评分器（每个能力一个），通过元学习在独立验证目标上训练。该框架采用渐进式选择规则，确保在训练早期保留多样性，后期聚焦高价值样本。在视觉语言模型上验证，将质量分解为视觉理解、OCR和STEM推理三个维度，SkillRater在基准测试上显著提升了各项能力。学习到的评分器信号几乎正交，证实了多维度分解的有效性，并解释了其优于单一过滤器的原因。"
      },
      {
        "paper_id": "2602.11073",
        "title": "Chatting with Images for Introspective Visual Thinking",
        "abstract": "Current large vision-language models (LVLMs) typically rely on text-only reasoning based on a single-pass visual encoding, which often leads to loss of fine-grained visual information. Recently the proposal of ''thinking with images'' attempts to alleviate this limitation by manipulating images via external tools or code; however, the resulting visual states are often insufficiently grounded in linguistic semantics, impairing effective cross-modal alignment - particularly when visual semantics or geometric relationships must be reasoned over across distant regions or multiple images. To address these challenges, we propose ''chatting with images'', a new framework that reframes visual manipulation as language-guided feature modulation. Under the guidance of expressive language prompts, the model dynamically performs joint re-encoding over multiple image regions, enabling tighter coupling between linguistic reasoning and visual state updates. We instantiate this paradigm in ViLaVT, a novel LVLM equipped with a dynamic vision encoder explicitly designed for such interactive visual reasoning, and trained it with a two-stage curriculum combining supervised fine-tuning and reinforcement learning to promote effective reasoning behaviors. Extensive experiments across eight benchmarks demonstrate that ViLaVT achieves strong and consistent improvements, with particularly pronounced gains on complex multi-image and video-based spatial reasoning tasks.",
        "authors_display": "Tieniu Tan Team",
        "pdf_url": "http://arxiv.org/abs/2602.11073",
        "code_url": null,
        "date": "2026-02-12",
        "primary_category": "cs.CV",
        "chinese_summary": "当前大型视觉-语言模型（LVLMs）主要依赖单次视觉编码的纯文本推理，导致细粒度视觉信息丢失，且现有“图像思考”方法在语言语义上 grounding 不足。本文提出“图像对话”框架，将视觉操作重新定义为语言引导的特征调制，在语言提示下，模型动态地对多个图像区域进行联合再编码，实现语言推理与视觉状态更新的紧密耦合。该范式在ViLaVT中实现，其动态视觉编码器专为交互式视觉推理设计，并通过两阶段课程训练。实验证明，ViLaVT在八个基准上取得显著提升，尤其在复杂多图像和视频空间推理任务上表现突出。"
      },
      {
        "paper_id": "2602.11448",
        "title": "Hierarchical Concept Embedding & Pursuit for Interpretable Image Classification",
        "abstract": "Interpretable-by-design models are gaining traction in computer vision because they provide faithful explanations for their predictions. In image classification, these models typically recover human-interpretable concepts from an image and use them for classification. Sparse concept recovery methods leverage the latent space of vision-language models to represent image embeddings as a sparse combination of concept embeddings. However, because such methods ignore the hierarchical structure of concepts, they can produce correct predictions with explanations that are inconsistent with the hierarchy. In this work, we propose Hierarchical Concept Embedding \\& Pursuit (HCEP), a framework that induces a hierarchy of concept embeddings in the latent space and uses hierarchical sparse coding to recover the concepts present in an image. Given a hierarchy of semantic concepts, we construct a corresponding hierarchy of concept embeddings and, assuming the correct concepts for an image form a rooted path in the hierarchy, derive desirable conditions for identifying them in the embedded space. We show that hierarchical sparse coding reliably recovers hierarchical concept embeddings, whereas vanilla sparse coding fails. Our experiments on real-world datasets demonstrate that HCEP outperforms baselines in concept precision and recall while maintaining competitive classification accuracy. Moreover, when the number of samples is limited, HCEP achieves superior classification accuracy and concept recovery. These results show that incorporating hierarchical structures into sparse coding yields more reliable and interpretable image classification models.",
        "authors_display": "René Vidal Team",
        "pdf_url": "http://arxiv.org/abs/2602.11448",
        "code_url": null,
        "date": "2026-02-11",
        "primary_category": "cs.LG",
        "chinese_summary": "现有的稀疏概念恢复方法在图像分类中忽视了概念的层次结构，可能导致解释与实际层次不符。本文提出分层概念嵌入与追踪（HCEP）框架，在潜在空间中诱导概念嵌入的层次结构，并利用分层稀疏编码来恢复图像中的概念。通过构建相应的概念嵌入层次结构，并假设正确概念形成根路径，HCEP能可靠地识别它们。实验表明，HCEP在概念精度和召回率方面优于基线，同时保持竞争性分类准确度，尤其在样本有限时表现更优，证明引入层次结构能提升模型可靠性和可解释性。"
      },
      {
        "paper_id": "2602.11146",
        "title": "Beyond VLM-Based Rewards: Diffusion-Native Latent Reward Modeling",
        "abstract": "Preference optimization for diffusion and flow-matching models relies on reward functions that are both discriminatively robust and computationally efficient. Vision-Language Models (VLMs) have emerged as the primary reward provider, leveraging their rich multimodal priors to guide alignment. However, their computation and memory cost can be substantial, and optimizing a latent diffusion generator through a pixel-space reward introduces a domain mismatch that complicates alignment. In this paper, we propose DiNa-LRM, a diffusion-native latent reward model that formulates preference learning directly on noisy diffusion states. Our method introduces a noise-calibrated Thurstone likelihood with diffusion-noise-dependent uncertainty. DiNa-LRM leverages a pretrained latent diffusion backbone with a timestep-conditioned reward head, and supports inference-time noise ensembling, providing a diffusion-native mechanism for test-time scaling and robust rewarding. Across image alignment benchmarks, DiNa-LRM substantially outperforms existing diffusion-based reward baselines and achieves performance competitive with state-of-the-art VLMs at a fraction of the computational cost. In preference optimization, we demonstrate that DiNa-LRM improves preference optimization dynamics, enabling faster and more resource-efficient model alignment.",
        "authors_display": "Wenhan Luo Team",
        "pdf_url": "http://arxiv.org/abs/2602.11146",
        "code_url": "https://github.com/HKUST-C4G/diffusion-rm",
        "date": "2026-02-11",
        "primary_category": "cs.CV",
        "chinese_summary": "扩散和流匹配模型的偏好优化需要鲁棒且高效的奖励函数，现有视觉-语言模型（VLMs）作为奖励提供者成本高昂，且像素空间奖励与潜在扩散生成器之间存在域不匹配。本文提出DiNa-LRM，一种扩散原生的潜在奖励模型，直接在带噪声的扩散状态上进行偏好学习。该方法引入了噪声校准的Thurstone似然，并利用预训练的潜在扩散骨干网络与时间步条件奖励头，支持推理时噪声集成。实验表明，DiNa-LRM在图像对齐任务上显著优于现有扩散基线，以远低于SOTA VLM的计算成本实现了相当的性能，并改进了偏好优化动态。"
      },
      {
        "paper_id": "2602.11241",
        "title": "Active Zero: Self-Evolving Vision-Language Models through Active Environment Exploration",
        "abstract": "Self-play has enabled large language models to autonomously improve through self-generated challenges. However, existing self-play methods for vision-language models rely on passive interaction with static image collections, resulting in strong dependence on initial datasets and inefficient learning. Without the ability to actively seek visual data tailored to their evolving capabilities, agents waste computational effort on samples that are either trivial or beyond their current skill level. To address these limitations, we propose Active-Zero, a framework that shifts from passive interaction to active exploration of visual environments. Active-Zero employs three co-evolving agents: a Searcher that retrieves images from open-world repositories based on the model's capability frontier, a Questioner that synthesizes calibrated reasoning tasks, and a Solver refined through accuracy rewards. This closed loop enables self-scaffolding auto-curricula where the model autonomously constructs its learning trajectory. On Qwen2.5-VL-7B-Instruct across 12 benchmarks, Active-Zero achieves 53.97 average accuracy on reasoning tasks (5.7% improvement) and 59.77 on general understanding (3.9% improvement), consistently outperforming existing self-play baselines. These results highlight active exploration as a key ingredient for scalable and adaptive self-evolving vision-language systems.",
        "authors_display": "Tat-Seng Chua Team",
        "pdf_url": "http://arxiv.org/abs/2602.11241",
        "code_url": null,
        "date": "2026-02-11",
        "primary_category": "cs.CV",
        "chinese_summary": "现有视觉-语言模型（VLMs）的自我博弈方法依赖静态图像集的被动交互，导致学习效率低下且对初始数据集依赖性强。本文提出Active-Zero框架，将学习范式从被动交互转变为主动探索视觉环境。该框架包含三个协同进化的智能体：一个根据模型能力前沿检索图像的搜索器、一个合成校准推理任务的提问器、以及一个通过准确性奖励改进的求解器，形成自主构建学习轨迹的自适应课程。实验结果显示，Active-Zero在多个基准测试中显著提高了推理任务和通用理解的准确性，超越了现有自我博弈基线，证明主动探索是可扩展自进化VLM的关键。"
      },
      {
        "paper_id": "2602.10910",
        "title": "Safe mobility support system using crowd mapping and avoidance route planning using VLM",
        "abstract": "Autonomous mobile robots offer promising solutions for labor shortages and increased operational efficiency. However, navigating safely and effectively in dynamic environments, particularly crowded areas, remains challenging. This paper proposes a novel framework that integrates Vision-Language Models (VLM) and Gaussian Process Regression (GPR) to generate dynamic crowd-density maps (``Abstraction Maps'') for autonomous robot navigation. Our approach utilizes VLM's capability to recognize abstract environmental concepts, such as crowd densities, and represents them probabilistically via GPR. Experimental results from real-world trials on a university campus demonstrated that robots successfully generated routes avoiding both static obstacles and dynamic crowds, enhancing navigation safety and adaptability.",
        "authors_display": "Koichi Ozaki Team",
        "pdf_url": "http://arxiv.org/abs/2602.10910",
        "code_url": null,
        "date": "2026-02-11",
        "primary_category": "cs.RO",
        "chinese_summary": "自主移动机器人在动态、拥挤环境中导航面临挑战。本文提出一种新颖框架，将视觉-语言模型（VLM）与高斯过程回归（GPR）结合，生成动态人群密度图（“抽象图”）以辅助机器人导航。该方法利用VLM识别抽象环境概念（如人群密度），并通过GPR进行概率表示。真实世界试验结果表明，机器人成功规划了避开静态障碍物和动态人群的路线，有效提升了导航安全性和适应性。"
      }
    ],
    "VLA": [
      {
        "paper_id": "2602.12281",
        "title": "Scaling Verification Can Be More Effective than Scaling Policy Learning for Vision-Language-Action Alignment",
        "abstract": "The long-standing vision of general-purpose robots hinges on their ability to understand and act upon natural language instructions. Vision-Language-Action (VLA) models have made remarkable progress toward this goal, yet their generated actions can still misalign with the given instructions. In this paper, we investigate test-time verification as a means to shrink the \"intention-action gap.'' We first characterize the test-time scaling law for embodied instruction following and demonstrate that jointly scaling the number of rephrased instructions and generated actions greatly increases test-time sample diversity, often recovering correct actions more efficiently than scaling each dimension independently. To capitalize on these scaling laws, we present CoVer, a contrastive verifier for vision-language-action alignment, and show that our architecture scales gracefully with additional computational resources and data. We then introduce \"boot-time compute\" and a hierarchical verification inference pipeline for VLAs. At deployment, our framework precomputes a diverse set of rephrased instructions from a Vision-Language-Model (VLM), repeatedly generates action candidates for each instruction, and then uses a verifier to select the optimal high-level prompt and low-level action chunks. Compared to scaling policy pre-training on the same data, our verification approach yields 22% gains in-distribution and 13% out-of-distribution on the SIMPLER benchmark, with a further 45% improvement in real-world experiments. On the PolaRiS benchmark, CoVer achieves 14% gains in task progress and 9% in success rate.",
        "authors_display": "Marco Pavone Team",
        "pdf_url": "http://arxiv.org/abs/2602.12281",
        "code_url": null,
        "date": "2026-02-12",
        "primary_category": "cs.RO",
        "chinese_summary": "现有VLA模型在执行自然语言指令时存在“意图-动作”差距，导致生成的动作与指令不符。本文提出CoVer，一种测试时验证方法，通过联合扩展重述指令和生成动作数量来增加测试时样本多样性。该方法利用“启动时计算”和分层验证推理流程，预计算多样化指令并生成多个动作候选项，然后通过对比验证器选出最优动作。实验结果表明，CoVer在SIMPLER基准上取得了显著的性能提升，并在真实世界实验中实现了45%的额外改进，PolaRiS基准上任务进度和成功率亦有提高。"
      },
      {
        "paper_id": "2602.12099",
        "title": "GigaBrain-0.5M*: a VLA That Learns From World Model-Based Reinforcement Learning",
        "abstract": "Vision-language-action (VLA) models that directly predict multi-step action chunks from current observations face inherent limitations due to constrained scene understanding and weak future anticipation capabilities. In contrast, video world models pre-trained on web-scale video corpora exhibit robust spatiotemporal reasoning and accurate future prediction, making them a natural foundation for enhancing VLA learning. Therefore, we propose \\textit{GigaBrain-0.5M*}, a VLA model trained via world model-based reinforcement learning. Built upon \\textit{GigaBrain-0.5}, which is pre-trained on over 10,000 hours of robotic manipulation data, whose intermediate version currently ranks first on the international RoboChallenge benchmark. \\textit{GigaBrain-0.5M*} further integrates world model-based reinforcement learning via \\textit{RAMP} (Reinforcement leArning via world Model-conditioned Policy) to enable robust cross-task adaptation. Empirical results demonstrate that \\textit{RAMP} achieves substantial performance gains over the RECAP baseline, yielding improvements of approximately 30\\% on challenging tasks including \\texttt{Laundry Folding}, \\texttt{Box Packing}, and \\texttt{Espresso Preparation}. Critically, \\textit{GigaBrain-0.5M$^*$} exhibits reliable long-horizon execution, consistently accomplishing complex manipulation tasks without failure as validated by real-world deployment videos on our \\href{https://gigabrain05m.github.io}{project page}.",
        "authors_display": "Zheng Zhu Team",
        "pdf_url": "http://arxiv.org/abs/2602.12099",
        "code_url": "https://gigabrain05m.github.io/",
        "date": "2026-02-12",
        "primary_category": "cs.CV",
        "chinese_summary": "由于场景理解和未来预期能力的限制，直接预测多步动作块的VLA模型存在固有局限。为解决此问题，本文提出了GigaBrain-0.5M*，一个通过基于世界模型的强化学习（RAMP）训练的VLA模型，其基础是预训练在大量机器人操作数据上的GigaBrain-0.5。RAMP整合了世界模型，旨在实现鲁棒的跨任务适应性。实验证明，GigaBrain-0.5M*在挑战性任务上比RECAP基线性能提升约30%，并能可靠地完成长周期复杂操作任务，无失败记录。"
      },
      {
        "paper_id": "2602.12063",
        "title": "VLAW: Iterative Co-Improvement of Vision-Language-Action Policy and World Model",
        "abstract": "The goal of this paper is to improve the performance and reliability of vision-language-action (VLA) models through iterative online interaction. Since collecting policy rollouts in the real world is expensive, we investigate whether a learned simulator-specifically, an action-conditioned video generation model-can be used to generate additional rollout data. Unfortunately, existing world models lack the physical fidelity necessary for policy improvement: they are predominantly trained on demonstration datasets that lack coverage of many different physical interactions (particularly failure cases) and struggle to accurately model small yet critical physical details in contact-rich object manipulation. We propose a simple iterative improvement algorithm that uses real-world roll-out data to improve the fidelity of the world model, which can then, in turn, be used to generate supplemental synthetic data for improving the VLA model. In our experiments on a real robot, we use this approach to improve the performance of a state-of-the-art VLA model on multiple downstream tasks. We achieve a 39.2% absolute success rate improvement over the base policy and 11.6% improvement from training with the generated synthetic rollouts. Videos can be found at this anonymous website: https://sites.google.com/view/vla-w",
        "authors_display": "Chelsea Finn Team",
        "pdf_url": "http://arxiv.org/abs/2602.12063",
        "code_url": null,
        "date": "2026-02-12",
        "primary_category": "cs.RO",
        "chinese_summary": "提高VLA模型性能和可靠性面临真实世界策略轨迹数据收集成本高昂的问题。现有世界模型在物理保真度上不足，难以准确建模接触丰富的操作细节。本文提出一种迭代改进算法，利用少量真实世界轨迹数据来提升动作条件视频生成世界模型的保真度。进而，该改进后的世界模型被用于生成补充合成数据，以进一步优化VLA模型。在真实机器人上的实验表明，此方法使VLA模型在多个下游任务上的成功率绝对提高了39.2%，其中合成轨迹训练带来了11.6%的改进。"
      },
      {
        "paper_id": "2602.12062",
        "title": "HoloBrain-0 Technical Report",
        "abstract": "In this work, we introduce HoloBrain-0, a comprehensive Vision-Language-Action (VLA) framework that bridges the gap between foundation model research and reliable real-world robot deployment. The core of our system is a novel VLA architecture that explicitly incorporates robot embodiment priors, including multi-view camera parameters and kinematic descriptions (URDF), to enhance 3D spatial reasoning and support diverse embodiments. We validate this design through a scalable ``pre-train then post-train\" paradigm, achieving state-of-the-art results on simulation benchmarks such as RoboTwin 2.0, LIBERO, and GenieSim, as well as strong results on challenging long-horizon real-world manipulation tasks. Notably, our efficient 0.2B-parameter variant rivals significantly larger baselines, enabling low-latency on-device deployment. To further accelerate research and practical adoption, we fully open-source the entire HoloBrain ecosystem, which includes: (1) powerful pre-trained VLA foundations; (2) post-trained checkpoints for multiple simulation suites and real-world tasks; and (3) RoboOrchard, a full-stack VLA infrastructure for data curation, model training and deployment. Together with standardized data collection protocols, this release provides the community with a complete, reproducible path toward high-performance robotic manipulation.",
        "authors_display": "Zhizhong Su Team",
        "pdf_url": "http://arxiv.org/abs/2602.12062",
        "code_url": null,
        "date": "2026-02-12",
        "primary_category": "cs.RO",
        "chinese_summary": "为了弥合基础模型研究与可靠机器人部署之间的鸿沟，本文引入了HoloBrain-0，一个全面的VLA框架。其核心是一个新颖的VLA架构，通过显式融入多视角相机参数和运动学描述等机器人具身先验，增强3D空间推理并支持多样化具身。该框架采用“预训练-后训练”范式，并在模拟基准和真实世界长周期操作任务中实现了最先进的表现。HoloBrain生态系统已完全开源，包括预训练模型、后训练检查点以及用于数据管理、模型训练和部署的全栈基础设施RoboOrchard，以加速研究和实际应用。"
      },
      {
        "paper_id": "2602.12032",
        "title": "When would Vision-Proprioception Policies Fail in Robotic Manipulation?",
        "abstract": "Proprioceptive information is critical for precise servo control by providing real-time robotic states. Its collaboration with vision is highly expected to enhance performances of the manipulation policy in complex tasks. However, recent studies have reported inconsistent observations on the generalization of vision-proprioception policies. In this work, we investigate this by conducting temporally controlled experiments. We found that during task sub-phases that robot's motion transitions, which require target localization, the vision modality of the vision-proprioception policy plays a limited role. Further analysis reveals that the policy naturally gravitates toward concise proprioceptive signals that offer faster loss reduction when training, thereby dominating the optimization and suppressing the learning of the visual modality during motion-transition phases. To alleviate this, we propose the Gradient Adjustment with Phase-guidance (GAP) algorithm that adaptively modulates the optimization of proprioception, enabling dynamic collaboration within the vision-proprioception policy. Specifically, we leverage proprioception to capture robotic states and estimate the probability of each timestep in the trajectory belonging to motion-transition phases. During policy learning, we apply fine-grained adjustment that reduces the magnitude of proprioception's gradient based on estimated probabilities, leading to robust and generalizable vision-proprioception policies. The comprehensive experiments demonstrate GAP is applicable in both simulated and real-world environments, across one-arm and dual-arm setups, and compatible with both conventional and Vision-Language-Action models. We believe this work can offer valuable insights into the development of vision-proprioception policies in robotic manipulation.",
        "authors_display": "Di Hu Team",
        "pdf_url": "http://arxiv.org/abs/2602.12032",
        "code_url": null,
        "date": "2026-02-12",
        "primary_category": "cs.RO",
        "chinese_summary": "本体感觉信息对于机器人精确控制至关重要，但视觉-本体感觉策略在泛化性上存在不一致的观察。本文研究发现，在机器人运动过渡阶段，策略倾向于更快的本体感觉信号，抑制了视觉模态的学习。为解决此问题，提出梯度调整与阶段指导（GAP）算法，该算法根据本体感觉估计的运动过渡概率，自适应地调整本体感觉梯度的幅度，以实现视觉与本体感觉的动态协作。实验证明，GAP在模拟和真实世界环境中均能生成鲁棒且可泛化的视觉-本体感觉策略，并兼容多种设置和模型。"
      },
      {
        "paper_id": "2602.11832",
        "title": "JEPA-VLA: Video Predictive Embedding is Needed for VLA Models",
        "abstract": "Recent vision-language-action (VLA) models built upon pretrained vision-language models (VLMs) have achieved significant improvements in robotic manipulation. However, current VLAs still suffer from low sample efficiency and limited generalization. This paper argues that these limitations are closely tied to an overlooked component, pretrained visual representation, which offers insufficient knowledge on both aspects of environment understanding and policy prior. Through an in-depth analysis, we find that commonly used visual representations in VLAs, whether pretrained via language-image contrastive learning or image-based self-supervised learning, remain inadequate at capturing crucial, task-relevant environment information and at inducing effective policy priors, i.e., anticipatory knowledge of how the environment evolves under successful task execution. In contrast, we discover that predictive embeddings pretrained on videos, in particular V-JEPA 2, are adept at flexibly discarding unpredictable environment factors and encoding task-relevant temporal dynamics, thereby effectively compensating for key shortcomings of existing visual representations in VLAs. Building on these observations, we introduce JEPA-VLA, a simple yet effective approach that adaptively integrates predictive embeddings into existing VLAs. Our experiments demonstrate that JEPA-VLA yields substantial performance gains across a range of benchmarks, including LIBERO, LIBERO-plus, RoboTwin2.0, and real-robot tasks.",
        "authors_display": "Mingsheng Long Team",
        "pdf_url": "http://arxiv.org/abs/2602.11832",
        "code_url": null,
        "date": "2026-02-12",
        "primary_category": "cs.CV",
        "chinese_summary": "当前VLA模型在机器人操作中面临样本效率低和泛化能力有限的问题，这与预训练视觉表示在环境理解和策略先验方面的不足密切相关。本文分析指出，现有视觉表示未能充分捕捉任务相关的环境信息和有效的策略先验。研究发现，预训练在视频上的预测嵌入（尤其是V-JEPA 2）能灵活地过滤不可预测因素并编码任务相关的时态动态。基于此，本文提出了JEPA-VLA，一种简单有效的方法，将预测嵌入自适应地集成到现有VLA中。实验结果表明，JEPA-VLA在一系列基准测试和真实机器人任务中取得了显著的性能提升。"
      },
      {
        "paper_id": "2602.11598",
        "title": "ABot-N0: Technical Report on the VLA Foundation Model for Versatile Embodied Navigation",
        "abstract": "Embodied navigation has long been fragmented by task-specific architectures. We introduce ABot-N0, a unified Vision-Language-Action (VLA) foundation model that achieves a ``Grand Unification'' across 5 core tasks: Point-Goal, Object-Goal, Instruction-Following, POI-Goal, and Person-Following. ABot-N0 utilizes a hierarchical ``Brain-Action'' architecture, pairing an LLM-based Cognitive Brain for semantic reasoning with a Flow Matching-based Action Expert for precise, continuous trajectory generation.   To support large-scale learning, we developed the ABot-N0 Data Engine, curating 16.9M expert trajectories and 5.0M reasoning samples across 7,802 high-fidelity 3D scenes (10.7 $\\text{km}^2$). ABot-N0 achieves new SOTA performance across 7 benchmarks, significantly outperforming specialized models. Furthermore, our Agentic Navigation System integrates a planner with hierarchical topological memory, enabling robust, long-horizon missions in dynamic real-world environments.",
        "authors_display": "Mu Xu Team",
        "pdf_url": "http://arxiv.org/abs/2602.11598",
        "code_url": "https://amap-cvlab.github.io/ABot-Navigation/ABot-N0/",
        "date": "2026-02-12",
        "primary_category": "cs.RO",
        "chinese_summary": "具身导航任务长期被特定架构所割裂，缺乏统一性。本文提出了ABot-N0，一个统一的VLA基础模型，实现了点目标、物体目标、指令遵循、POI目标和人员跟随这五项核心任务的“大统一”。ABot-N0采用分层的“大脑-动作”架构，结合基于LLM的认知大脑进行语义推理和基于流匹配的动作专家进行精确轨迹生成。为支持大规模学习，研究团队构建了ABot-N0数据引擎，整理了海量专家轨迹和推理样本。实验结果显示，ABot-N0在7个基准测试中达到了新的SOTA性能，显著超越了专用模型，其代理导航系统也展现出在动态真实世界环境中执行长周期任务的鲁棒性。"
      },
      {
        "paper_id": "2602.10983",
        "title": "Scaling World Model for Hierarchical Manipulation Policies",
        "abstract": "Vision-Language-Action (VLA) models are promising for generalist robot manipulation but remain brittle in out-of-distribution (OOD) settings, especially with limited real-robot data. To resolve the generalization bottleneck, we introduce a hierarchical Vision-Language-Action framework \\our{} that leverages the generalization of large-scale pre-trained world model for robust and generalizable VIsual Subgoal TAsk decomposition VISTA. Our hierarchical framework \\our{} consists of a world model as the high-level planner and a VLA as the low-level executor. The high-level world model first divides manipulation tasks into subtask sequences with goal images, and the low-level policy follows the textual and visual guidance to generate action sequences. Compared to raw textual goal specification, these synthesized goal images provide visually and physically grounded details for low-level policies, making it feasible to generalize across unseen objects and novel scenarios. We validate both visual goal synthesis and our hierarchical VLA policies in massive out-of-distribution scenarios, and the performance of the same-structured VLA in novel scenarios could boost from 14% to 69% with the guidance generated by the world model. Results demonstrate that our method outperforms previous baselines with a clear margin, particularly in out-of-distribution scenarios. Project page: \\href{https://vista-wm.github.io/}{https://vista-wm.github.io}",
        "authors_display": "Xinghang Li Team",
        "pdf_url": "http://arxiv.org/abs/2602.10983",
        "code_url": null,
        "date": "2026-02-12",
        "primary_category": "cs.RO",
        "chinese_summary": "VLA模型在通用机器人操作中前景广阔，但在分布外（OOD）场景中表现脆弱，尤其在真实机器人数据有限时。为解决泛化瓶颈，本文引入了VISTA，一个分层VLA框架，利用大规模预训练世界模型的泛化能力实现鲁棒且可泛化的视觉子目标任务分解。该框架包含一个作为高级规划器的世界模型和一个作为低级执行器的VLA，世界模型将任务分解为带有目标图像的子任务序列，低级策略依据文本和视觉指导生成动作。这些合成的目标图像为低级策略提供了具体细节，使其能泛化到未见物体和新场景。实验表明，在世界模型指导下，VLA在新型OOD场景中的性能从14%提升至69%，显著优于现有基线。"
      },
      {
        "paper_id": "2602.11291",
        "title": "H-WM: Robotic Task and Motion Planning Guided by Hierarchical World Model",
        "abstract": "World models are becoming central to robotic planning and control, as they enable prediction of future state transitions. Existing approaches often emphasize video generation or natural language prediction, which are difficult to directly ground in robot actions and suffer from compounding errors over long horizons. Traditional task and motion planning relies on symbolic logic world models, such as planning domains, that are robot-executable and robust for long-horizon reasoning. However, these methods typically operate independently of visual perception, preventing synchronized symbolic and perceptual state prediction. We propose a Hierarchical World Model (H-WM) that jointly predicts logical and visual state transitions within a unified bilevel framework. H-WM combines a high-level logical world model with a low-level visual world model, integrating the robot-executable, long-horizon robustness of symbolic reasoning with perceptual grounding from visual observations. The hierarchical outputs provide stable and consistent intermediate guidance for long-horizon tasks, mitigating error accumulation and enabling robust execution across extended task sequences. To train H-WM, we introduce a robotic dataset that aligns robot motion with symbolic states, actions, and visual observations. Experiments across vision-language-action (VLA) control policies demonstrate the effectiveness and generality of the approach.",
        "authors_display": "Yingxue Zhang Team",
        "pdf_url": "http://arxiv.org/abs/2602.11291",
        "code_url": null,
        "date": "2026-02-11",
        "primary_category": "cs.RO",
        "chinese_summary": "世界模型在机器人规划和控制中能预测未来状态，但现有方法常难以直接关联机器人动作，并在长周期内累积误差。传统的符号逻辑世界模型虽然鲁棒，但缺乏视觉感知。本文提出了分层世界模型（H-WM），在一个统一的两级框架中联合预测逻辑和视觉状态转换。H-WM结合了高级逻辑世界模型和低级视觉世界模型，将符号推理的鲁棒性与视觉观察的感知基础相结合。为此，研究团队引入了一个对齐机器人运动、符号状态、动作和视觉观察的数据集进行训练。实验证明，分层输出为长周期任务提供了稳定一致的中间指导，有效减轻了误差积累，并在VLA控制策略上展现了有效性和通用性。"
      },
      {
        "paper_id": "2602.11075",
        "title": "RISE: Self-Improving Robot Policy with Compositional World Model",
        "abstract": "Despite the sustained scaling on model capacity and data acquisition, Vision-Language-Action (VLA) models remain brittle in contact-rich and dynamic manipulation tasks, where minor execution deviations can compound into failures. While reinforcement learning (RL) offers a principled path to robustness, on-policy RL in the physical world is constrained by safety risk, hardware cost, and environment reset. To bridge this gap, we present RISE, a scalable framework of robotic reinforcement learning via imagination. At its core is a Compositional World Model that (i) predicts multi-view future via a controllable dynamics model, and (ii) evaluates imagined outcomes with a progress value model, producing informative advantages for the policy improvement. Such compositional design allows state and value to be tailored by best-suited yet distinct architectures and objectives. These components are integrated into a closed-loop self-improving pipeline that continuously generates imaginary rollouts, estimates advantages, and updates the policy in imaginary space without costly physical interaction. Across three challenging real-world tasks, RISE yields significant improvement over prior art, with more than +35% absolute performance increase in dynamic brick sorting, +45% for backpack packing, and +35% for box closing, respectively.",
        "authors_display": "Hongyang Li Team",
        "pdf_url": "http://arxiv.org/abs/2602.11075",
        "code_url": "https://opendrivelab.com/kai0-rl/",
        "date": "2026-02-11",
        "primary_category": "cs.RO",
        "chinese_summary": "尽管VLA模型持续扩展，但在接触密集和动态操作任务中仍易失败。物理世界中的强化学习受限于安全风险和成本。为解决此问题，本文提出了RISE，一个通过想象进行机器人强化学习的可扩展框架。其核心是一个组合式世界模型，能够通过可控动态模型预测多视角未来，并利用进展价值模型评估想象结果。这些组件被整合到一个闭环自改进管道中，在想象空间中持续生成轨迹、估计优势并更新策略，无需昂贵的物理交互。RISE在三个挑战性的真实世界任务中取得了显著性能提升，例如在动态砖块分类中绝对性能提升超过35%。"
      },
      {
        "paper_id": "2602.10980",
        "title": "RADAR: Benchmarking Vision-Language-Action Generalization via Real-World Dynamics, Spatial-Physical Intelligence, and Autonomous Evaluation",
        "abstract": "VLA models have achieved remarkable progress in embodied intelligence; however, their evaluation remains largely confined to simulations or highly constrained real-world settings. This mismatch creates a substantial reality gap, where strong benchmark performance often masks poor generalization in diverse physical environments. We identify three systemic shortcomings in current benchmarking practices that hinder fair and reliable model comparison. (1) Existing benchmarks fail to model real-world dynamics, overlooking critical factors such as dynamic object configurations, robot initial states, lighting changes, and sensor noise. (2) Current protocols neglect spatial--physical intelligence, reducing evaluation to rote manipulation tasks that do not probe geometric reasoning. (3) The field lacks scalable fully autonomous evaluation, instead relying on simplistic 2D metrics that miss 3D spatial structure or on human-in-the-loop systems that are costly, biased, and unscalable. To address these limitations, we introduce RADAR (Real-world Autonomous Dynamics And Reasoning), a benchmark designed to systematically evaluate VLA generalization under realistic conditions. RADAR integrates three core components: (1) a principled suite of physical dynamics; (2) dedicated tasks that explicitly test spatial reasoning and physical understanding; and (3) a fully autonomous evaluation pipeline based on 3D metrics, eliminating the need for human supervision. We apply RADAR to audit multiple state-of-the-art VLA models and uncover severe fragility beneath their apparent competence. Performance drops precipitously under modest physical dynamics, with the expectation of 3D IoU declining from 0.261 to 0.068 under sensor noise. Moreover, models exhibit limited spatial reasoning capability. These findings position RADAR as a necessary bench toward reliable and generalizable real-world evaluation of VLA models.",
        "authors_display": "Guangrun Wang Team",
        "pdf_url": "http://arxiv.org/abs/2602.10980",
        "code_url": null,
        "date": "2026-02-11",
        "primary_category": "cs.RO",
        "chinese_summary": "现有视觉-语言-动作（VLA）模型在具身智能中取得进展，但其评估多限于仿真或受限真实环境，导致与真实世界存在巨大差距，泛化性差。针对这一问题，本文提出了RADAR基准，旨在系统评估VLA模型在真实条件下的泛化能力，其核心包含一套物理动态、明确测试空间推理和物理理解的任务，以及基于3D指标的完全自主评估管道。实验结果表明，RADAR能揭示SOTA VLA模型在物理动态下性能的严重下降和空间推理能力的局限性，证明其是评估VLA模型在真实世界中可靠性和泛化性的必要工具。"
      },
      {
        "paper_id": "2602.10719",
        "title": "From Representational Complementarity to Dual Systems: Synergizing VLM and Vision-Only Backbones for End-to-End Driving",
        "abstract": "Vision-Language-Action (VLA) driving augments end-to-end (E2E) planning with language-enabled backbones, yet it remains unclear what changes beyond the usual accuracy--cost trade-off. We revisit this question with 3--RQ analysis in RecogDrive by instantiating the system with a full VLM and vision-only backbones, all under an identical diffusion Transformer planner. RQ1: At the backbone level, the VLM can introduce additional subspaces upon the vision-only backbones. RQ2: This unique subspace leads to a different behavioral in some long-tail scenario: the VLM tends to be more aggressive whereas ViT is more conservative, and each decisively wins on about 2--3% of test scenarios; With an oracle that selects, per scenario, the better trajectory between the VLM and ViT branches, we obtain an upper bound of 93.58 PDMS. RQ3: To fully harness this observation, we propose HybridDriveVLA, which runs both ViT and VLM branches and selects between their endpoint trajectories using a learned scorer, improving PDMS to 92.10. Finally, DualDriveVLA implements a practical fast--slow policy: it runs ViT by default and invokes the VLM only when the scorer's confidence falls below a threshold; calling the VLM on 15% of scenarios achieves 91.00 PDMS while improving throughput by 3.2x. Code will be released.",
        "authors_display": "Yan Wang Team",
        "pdf_url": "http://arxiv.org/abs/2602.10719",
        "code_url": null,
        "date": "2026-02-11",
        "primary_category": "cs.RO",
        "chinese_summary": "视觉-语言-动作（VLA）驾驶通过语言骨干网增强端到端规划，但其核心影响尚不明确。本文通过RecogDrive系统，使用VLM和纯视觉骨干网（均采用扩散Transformer规划器）进行了3个研究问题分析，探究了VLM引入的额外子空间及其在长尾场景中的行为差异。结果显示VLM引入了独特的行为子空间，在长尾场景中VLM更激进而ViT更保守，各有2-3%的场景表现优异。基于此，提出了HybridDriveVLA和DualDriveVLA，前者通过学习得分器结合ViT和VLM分支将PDMS提升至92.10，后者作为实用快慢策略，在15%场景调用VLM，在吞吐量提高3.2倍的同时实现了91.00 PDMS。"
      },
      {
        "paper_id": "2602.10717",
        "title": "Say, Dream, and Act: Learning Video World Models for Instruction-Driven Robot Manipulation",
        "abstract": "Robotic manipulation requires anticipating how the environment evolves in response to actions, yet most existing systems lack this predictive capability, often resulting in errors and inefficiency. While Vision-Language Models (VLMs) provide high-level guidance, they cannot explicitly forecast future states, and existing world models either predict only short horizons or produce spatially inconsistent frames. To address these challenges, we propose a framework for fast and predictive video-conditioned action. Our approach first selects and adapts a robust video generation model to ensure reliable future predictions, then applies adversarial distillation for fast, few-step video generation, and finally trains an action model that leverages both generated videos and real observations to correct spatial errors. Extensive experiments show that our method produces temporally coherent, spatially accurate video predictions that directly support precise manipulation, achieving significant improvements in embodiment consistency, spatial referring ability, and task completion over existing baselines. Codes & Models will be released.",
        "authors_display": "Yanwei Fu Team",
        "pdf_url": "http://arxiv.org/abs/2602.10717",
        "code_url": null,
        "date": "2026-02-11",
        "primary_category": "cs.RO",
        "chinese_summary": "机器人操作中，现有系统普遍缺乏预测环境响应动作的演变能力，导致错误和低效，且现有世界模型预测范围短或生成空间不一致。为解决这些问题，本文提出了一个用于快速和预测性视频条件动作的框架。该方法首先选择并调整鲁棒的视频生成模型以进行可靠的未来预测，随后采用对抗蒸馏实现快速视频生成，最后训练一个动作模型，该模型结合生成视频和真实观测来纠正空间错误。实验结果表明，该方法生成的时间连贯、空间准确的视频预测能直接支持精确操作，并在具身一致性、空间指代能力和任务完成度方面显著优于现有基线。"
      },
      {
        "paper_id": "2602.10698",
        "title": "AugVLA-3D: Depth-Driven Feature Augmentation for Vision-Language-Action Models",
        "abstract": "Vision-Language-Action (VLA) models have recently achieved remarkable progress in robotic perception and control, yet most existing approaches primarily rely on VLM trained using 2D images, which limits their spatial understanding and action grounding in complex 3D environments. To address this limitation, we propose a novel framework that integrates depth estimation into VLA models to enrich 3D feature representations. Specifically, we employ a depth estimation baseline called VGGT to extract geometry-aware 3D cues from standard RGB inputs, enabling efficient utilization of existing large-scale 2D datasets while implicitly recovering 3D structural information. To further enhance the reliability of these depth-derived features, we introduce a new module called action assistant, which constrains the learned 3D representations with action priors and ensures their consistency with downstream control tasks. By fusing the enhanced 3D features with conventional 2D visual tokens, our approach significantly improves the generalization ability and robustness of VLA models. Experimental results demonstrate that the proposed method not only strengthens perception in geometrically ambiguous scenarios but also leads to superior action prediction accuracy. This work highlights the potential of depth-driven data augmentation and auxiliary expert supervision for bridging the gap between 2D observations and 3D-aware decision-making in robotic systems.",
        "authors_display": "F. Richard Yu Team",
        "pdf_url": "http://arxiv.org/abs/2602.10698",
        "code_url": null,
        "date": "2026-02-11",
        "primary_category": "cs.CV",
        "chinese_summary": "VLA模型在机器人感知与控制上取得进展，但多数依赖2D图像训练的VLM限制了其在复杂3D环境中的空间理解和动作接地。针对此，本文提出一种新框架，通过集成深度估计来丰富VLA模型的3D特征表示。具体方法是利用VGGT深度估计算法从RGB输入中提取几何感知的3D线索，并引入一个动作辅助模块，用动作先验约束3D表示以确保与下游控制任务的一致性。实验证明，该方法不仅增强了几何模糊场景的感知，还提升了动作预测准确性，凸显了深度驱动数据增强和辅助专家监督在弥合2D观测与3D决策之间鸿沟的潜力。"
      },
      {
        "paper_id": "2602.10619",
        "title": "Improving Medical Visual Reinforcement Fine-Tuning via Perception and Reasoning Augmentation",
        "abstract": "While recent advances in Reinforcement Fine-Tuning (RFT) have shown that rule-based reward schemes can enable effective post-training for large language models, their extension to cross-modal, vision-centric domains remains largely underexplored. This limitation is especially pronounced in the medical imaging domain, where effective performance requires both robust visual perception and structured reasoning. In this work, we address this gap by proposing VRFT-Aug, a visual reinforcement fine-tuning framework tailored for the medical domain. VRFT-Aug introduces a series of training strategies designed to augment both perception and reasoning, including prior knowledge injection, perception-driven policy refinement, medically informed reward shaping, and behavioral imitation. Together, these methods aim to stabilize and improve the RFT process.   Through extensive experiments across multiple medical datasets, we show that our approaches consistently outperform both standard supervised fine-tuning and RFT baselines. Moreover, we provide empirically grounded insights and practical training heuristics that can be generalized to other medical image tasks. We hope this work contributes actionable guidance and fresh inspiration for the ongoing effort to develop reliable, reasoning-capable models for high-stakes medical applications.",
        "authors_display": "Qicheng Lao Team",
        "pdf_url": "http://arxiv.org/abs/2602.10619",
        "code_url": null,
        "date": "2026-02-11",
        "primary_category": "cs.CV",
        "chinese_summary": "强化微调（RFT）在大型语言模型后训练中表现出色，但其在跨模态、以视觉为中心的医学影像领域的应用仍待探索。为弥补这一空白，本文提出了VRFT-Aug，一个专为医学领域设计的视觉强化微调框架。该框架引入了多项训练策略，包括先验知识注入、感知驱动策略优化、医学知情奖励塑造和行为模仿，旨在稳定并改进RFT过程。在多个医学数据集上进行的广泛实验表明，VRFT-Aug持续优于标准的监督微调和RFT基线，并提供了可推广到其他医学图像任务的实用训练经验。"
      },
      {
        "paper_id": "2602.10556",
        "title": "LAP: Language-Action Pre-Training Enables Zero-shot Cross-Embodiment Transfer",
        "abstract": "A long-standing goal in robotics is a generalist policy that can be deployed zero-shot on new robot embodiments without per-embodiment adaptation. Despite large-scale multi-embodiment pre-training, existing Vision-Language-Action models (VLAs) remain tightly coupled to their training embodiments and typically require costly fine-tuning. We introduce Language-Action Pre-training (LAP), a simple recipe that represents low-level robot actions directly in natural language, aligning action supervision with the pre-trained vision-language model's input-output distribution. LAP requires no learned tokenizer, no costly annotation, and no embodiment-specific architectural design. Based on LAP, we present LAP-3B, which to the best of our knowledge is the first VLA to achieve substantial zero-shot transfer to previously unseen robot embodiments without any embodiment-specific fine-tuning. Across multiple novel robots and manipulation tasks, LAP-3B attains over 50% average zero-shot success, delivering roughly a 2x improvement over the strongest prior VLAs. We further show that LAP enables efficient adaptation and favorable scaling, while unifying action prediction and VQA in a shared language-action format that yields additional gains through co-training.",
        "authors_display": "Anirudha Majumdar Team",
        "pdf_url": "http://arxiv.org/abs/2602.10556",
        "code_url": "https://lap-vla.github.io",
        "date": "2026-02-11",
        "primary_category": "cs.RO",
        "chinese_summary": "机器人领域长期目标是实现通用策略的零样本部署，但现有VLA模型与训练载体紧密耦合，需昂贵微调。本文提出了语言-动作预训练（LAP），一种通过自然语言直接表示低级机器人动作的方法，将动作监督与预训练视觉-语言模型的输入-输出分布对齐，无需学习tokenizer、昂贵标注或特定载体设计。基于LAP，提出LAP-3B，首次实现了向未见机器人载体的显著零样本迁移，平均成功率超过50%，较现有VLA提升约2倍。LAP还支持高效适应和扩展，并通过统一语言-动作格式实现协同训练增益。"
      },
      {
        "paper_id": "2602.10458",
        "title": "Found-RL: foundation model-enhanced reinforcement learning for autonomous driving",
        "abstract": "Reinforcement Learning (RL) has emerged as a dominant paradigm for end-to-end autonomous driving (AD). However, RL suffers from sample inefficiency and a lack of semantic interpretability in complex scenarios. Foundation Models, particularly Vision-Language Models (VLMs), can mitigate this by offering rich, context-aware knowledge, yet their high inference latency hinders deployment in high-frequency RL training loops. To bridge this gap, we present Found-RL, a platform tailored to efficiently enhance RL for AD using foundation models. A core innovation is the asynchronous batch inference framework, which decouples heavy VLM reasoning from the simulation loop, effectively resolving latency bottlenecks to support real-time learning. We introduce diverse supervision mechanisms: Value-Margin Regularization (VMR) and Advantage-Weighted Action Guidance (AWAG) to effectively distill expert-like VLM action suggestions into the RL policy. Additionally, we adopt high-throughput CLIP for dense reward shaping. We address CLIP's dynamic blindness via Conditional Contrastive Action Alignment, which conditions prompts on discretized speed/command and yields a normalized, margin-based bonus from context-specific action-anchor scoring. Found-RL provides an end-to-end pipeline for fine-tuned VLM integration and shows that a lightweight RL model can achieve near-VLM performance compared with billion-parameter VLMs while sustaining real-time inference (approx. 500 FPS). Code, data, and models will be publicly available at https://github.com/ys-qu/found-rl.",
        "authors_display": "Sikai Chen Team",
        "pdf_url": "http://arxiv.org/abs/2602.10458",
        "code_url": null,
        "date": "2026-02-11",
        "primary_category": "cs.AI",
        "chinese_summary": "强化学习（RL）是端到端自动驾驶（AD）的主导范式，但面临样本效率低下和语义可解释性不足的问题，而视觉-语言模型（VLM）可提供丰富的上下文感知知识。然而，VLM的高推理延迟阻碍了其在高速RL训练中的部署。为此，本文提出了Found-RL平台，通过异步批处理推理框架将VLM推理与仿真循环解耦，有效解决了延迟瓶颈。Found-RL引入了价值-裕度正则化和优势加权动作指导等监督机制，将VLM专家建议有效蒸馏到RL策略中，并通过条件对比动作对齐利用CLIP进行密集奖励塑造。实验表明，轻量级RL模型可实现接近十亿参数VLM的性能，同时保持实时推理（约500 FPS）。"
      },
      {
        "paper_id": "2602.10377",
        "title": "Hardware Co-Design Scaling Laws via Roofline Modelling for On-Device LLMs",
        "abstract": "Vision-Language-Action Models (VLAs) have emerged as a key paradigm of Physical AI and are increasingly deployed in autonomous vehicles, robots, and smart spaces. In these resource-constrained on-device settings, selecting an appropriate large language model (LLM) backbone is a critical challenge: models must balance accuracy with strict inference latency and hardware efficiency constraints. This makes hardware-software co-design a game-changing requirement for on-device LLM deployment, where each hardware platform demands a tailored architectural solution. We propose a hardware co-design law that jointly captures model accuracy and inference performance. Specifically, we model training loss as an explicit function of architectural hyperparameters and characterise inference latency via roofline modelling. We empirically evaluate 1,942 candidate architectures on NVIDIA Jetson Orin, training 170 selected models for 10B tokens each to fit a scaling law relating architecture to training loss. By coupling this scaling law with latency modelling, we establish a direct accuracy-latency correspondence and identify the Pareto frontier for hardware co-designed LLMs. We further formulate architecture search as a joint optimisation over precision and performance, deriving feasible design regions under industrial hardware and application budgets. Our approach reduces architecture selection from months to days. At the same latency as Qwen2.5-0.5B on the target hardware, our co-designed architecture achieves 19.42% lower perplexity on WikiText-2. To our knowledge, this is the first principled and operational framework for hardware co-design scaling laws in on-device LLM deployment. We will make the code and related checkpoints publicly available.",
        "authors_display": "Cheng Deng Team",
        "pdf_url": "http://arxiv.org/abs/2602.10377",
        "code_url": null,
        "date": "2026-02-10",
        "primary_category": "cs.LG",
        "chinese_summary": "视觉-语言-动作模型（VLA）在设备端部署时，需平衡模型准确性与严格的推理延迟及硬件效率，硬件-软件协同设计至关重要。本文提出了一种硬件协同设计法则，联合捕捉模型准确性和推理性能。通过将训练损失建模为架构超参数函数，并利用屋脊线模型刻画推理延迟，建立了直接的准确性-延迟对应关系，并识别了硬件协同设计LLM的帕累托前沿。该方法将架构选择时间从数月缩短到数天，在相同延迟下，协同设计的架构在WikiText-2上的困惑度比Qwen2.5-0.5B降低19.42%。该工作首次提供了设备上LLM部署中硬件协同设计缩放定律的原则性框架。"
      },
      {
        "paper_id": "2602.10109",
        "title": "ST4VLA: Spatially Guided Training for Vision-Language-Action Models",
        "abstract": "Large vision-language models (VLMs) excel at multimodal understanding but fall short when extended to embodied tasks, where instructions must be transformed into low-level motor actions. We introduce ST4VLA, a dual-system Vision-Language-Action framework that leverages Spatial Guided Training to align action learning with spatial priors in VLMs. ST4VLA includes two stages: (i) spatial grounding pre-training, which equips the VLM with transferable priors via scalable point, box, and trajectory prediction from both web-scale and robot-specific data, and (ii) spatially guided action post-training, which encourages the model to produce richer spatial priors to guide action generation via spatial prompting. This design preserves spatial grounding during policy learning and promotes consistent optimization across spatial and action objectives. Empirically, ST4VLA achieves substantial improvements over vanilla VLA, with performance increasing from 66.1 -> 84.6 on Google Robot and from 54.7 -> 73.2 on WidowX Robot, establishing new state-of-the-art results on SimplerEnv. It also demonstrates stronger generalization to unseen objects and paraphrased instructions, as well as robustness to long-horizon perturbations in real-world settings. These results highlight scalable spatially guided training as a promising direction for robust, generalizable robot learning. Source code, data and models are released at https://internrobotics.github.io/internvla-m1.github.io/",
        "authors_display": "Jiangmiao Pang Team",
        "pdf_url": "http://arxiv.org/abs/2602.10109",
        "code_url": null,
        "date": "2026-02-10",
        "primary_category": "cs.RO",
        "chinese_summary": "大型视觉-语言模型（VLM）虽擅长多模态理解，但在具身任务中将其指令转化为低级动作时表现不佳。为此，本文提出了ST4VLA，一个双系统视觉-语言-动作框架，利用空间引导训练将动作学习与VLM中的空间先验对齐。ST4VLA包含空间接地预训练和空间引导动作后训练两个阶段，前者通过点、框和轨迹预测为VLM提供可迁移先验，后者鼓励模型通过空间提示生成更丰富的空间先验以指导动作生成。实验结果表明，ST4VLA在Google Robot和WidowX Robot上性能显著提升，并在SimplerEnv上刷新SOTA，同时对未见物体、释义指令和真实世界扰动表现出更强的泛化性和鲁棒性。"
      },
      {
        "paper_id": "2602.10106",
        "title": "EgoHumanoid: Unlocking In-the-Wild Loco-Manipulation with Robot-Free Egocentric Demonstration",
        "abstract": "Human demonstrations offer rich environmental diversity and scale naturally, making them an appealing alternative to robot teleoperation. While this paradigm has advanced robot-arm manipulation, its potential for the more challenging, data-hungry problem of humanoid loco-manipulation remains largely unexplored. We present EgoHumanoid, the first framework to co-train a vision-language-action policy using abundant egocentric human demonstrations together with a limited amount of robot data, enabling humanoids to perform loco-manipulation across diverse real-world environments. To bridge the embodiment gap between humans and robots, including discrepancies in physical morphology and viewpoint, we introduce a systematic alignment pipeline spanning from hardware design to data processing. A portable system for scalable human data collection is developed, and we establish practical collection protocols to improve transferability. At the core of our human-to-humanoid alignment pipeline lies two key components. The view alignment reduces visual domain discrepancies caused by camera height and perspective variation. The action alignment maps human motions into a unified, kinematically feasible action space for humanoid control. Extensive real-world experiments demonstrate that incorporating robot-free egocentric data significantly outperforms robot-only baselines by 51\\%, particularly in unseen environments. Our analysis further reveals which behaviors transfer effectively and the potential for scaling human data.",
        "authors_display": "Li Chen Team",
        "pdf_url": "http://arxiv.org/abs/2602.10106",
        "code_url": "https://opendrivelab.com/EgoHumanoid",
        "date": "2026-02-10",
        "primary_category": "cs.RO",
        "chinese_summary": "人类演示数据作为机器人远程操作的替代方案潜力巨大，但其在人形机器人运动-操作这一数据密集型挑战中的应用仍未充分探索。本文提出了EgoHumanoid框架，首次将大量以自我为中心的人类演示与有限机器人数据共同训练视觉-语言-动作策略，使人形机器人能在多样真实环境中执行运动-操作任务。为弥合人类与机器人间的具身鸿沟，该框架引入了从硬件设计到数据处理的系统对齐管道，特别是视点对齐和动作对齐两个核心组件。广泛的真实世界实验表明，整合无机器人自我中心数据显著优于仅机器人基线51%，尤其是在未见环境中，并揭示了人类数据有效迁移和扩展的潜力。"
      }
    ],
    "Humanoid": [
      {
        "paper_id": "2602.11929",
        "title": "General Humanoid Whole-Body Control via Pretraining and Fast Adaptation",
        "abstract": "Learning a general whole-body controller for humanoid robots remains challenging due to the diversity of motion distributions, the difficulty of fast adaptation, and the need for robust balance in high-dynamic scenarios. Existing approaches often require task-specific training or suffer from performance degradation when adapting to new motions. In this paper, we present FAST, a general humanoid whole-body control framework that enables Fast Adaptation and Stable Motion Tracking. FAST introduces Parseval-Guided Residual Policy Adaptation, which learns a lightweight delta action policy under orthogonality and KL constraints, enabling efficient adaptation to out-of-distribution motions while mitigating catastrophic forgetting. To further improve physical robustness, we propose Center-of-Mass-Aware Control, which incorporates CoM-related observations and objectives to enhance balance when tracking challenging reference motions. Extensive experiments in simulation and real-world deployment demonstrate that FAST consistently outperforms state-of-the-art baselines in robustness, adaptation efficiency, and generalization.",
        "authors_display": "Zongqing Lu Team",
        "pdf_url": "http://arxiv.org/abs/2602.11929",
        "code_url": null,
        "date": "2026-02-12",
        "primary_category": "cs.RO",
        "chinese_summary": "针对人形机器人全身控制在运动多样性、快速适应和高动态平衡方面的挑战，本研究提出了FAST框架。该框架通过Parseval-Guided Residual Policy Adaptation学习轻量级策略以高效适应分布外运动并减轻灾难性遗忘，同时引入Center-of-Mass-Aware Control提升物理鲁棒性。仿真和真实世界实验表明，FAST在鲁棒性、适应效率和泛化能力上均优于现有基线。"
      },
      {
        "paper_id": "2602.11758",
        "title": "HAIC: Humanoid Agile Object Interaction Control via Dynamics-Aware World Model",
        "abstract": "Humanoid robots show promise for complex whole-body tasks in unstructured environments. Although Human-Object Interaction (HOI) has advanced, most methods focus on fully actuated objects rigidly coupled to the robot, ignoring underactuated objects with independent dynamics and non-holonomic constraints. These introduce control challenges from coupling forces and occlusions. We present HAIC, a unified framework for robust interaction across diverse object dynamics without external state estimation. Our key contribution is a dynamics predictor that estimates high-order object states (velocity, acceleration) solely from proprioceptive history. These predictions are projected onto static geometric priors to form a spatially grounded dynamic occupancy map, enabling the policy to infer collision boundaries and contact affordances in blind spots. We use asymmetric fine-tuning, where a world model continuously adapts to the student policy's exploration, ensuring robust state estimation under distribution shifts. Experiments on a humanoid robot show HAIC achieves high success rates in agile tasks (skateboarding, cart pushing/pulling under various loads) by proactively compensating for inertial perturbations, and also masters multi-object long-horizon tasks like carrying a box across varied terrain by predicting the dynamics of multiple objects.",
        "authors_display": "Renjing Xu Team",
        "pdf_url": "http://arxiv.org/abs/2602.11758",
        "code_url": "https://haic-humanoid.github.io/",
        "date": "2026-02-12",
        "primary_category": "cs.RO",
        "chinese_summary": "为解决人形机器人在人机交互中对具有独立动力学和非完整约束的欠驱动物体进行操作的挑战，本研究提出了HAIC框架。该框架通过一个仅利用本体感知历史的动力学预测器估计高阶物体状态，并将其投射到静态几何先验形成动态占用图，使机器人能在盲区推断接触信息。通过非对称微调，HAIC在人形机器人上实现了对欠驱动物体的高成功率敏捷任务（如滑板），并能完成多物体长时序任务。"
      },
      {
        "paper_id": "2602.11472",
        "title": "Future Mining: Learning for Safety and Security",
        "abstract": "Mining is rapidly evolving into an AI driven cyber physical ecosystem where safety and operational reliability depend on robust perception, trustworthy distributed intelligence, and continuous monitoring of miners and equipment. However, real world mining environments impose severe constraints, including poor illumination, GPS denied conditions, irregular underground topologies and intermittent connectivity. These factors degrade perception accuracy, disrupt situational awareness and weaken distributed learning systems. At the same time, emerging cyber physical threats such as backdoor triggers, sensor spoofing, label flipping attacks, and poisoned model updates further jeopardize operational safety as mines adopt autonomous vehicles, humanoid assistance, and federated learning for collaborative intelligence. Energy constrained sensors also experience uneven battery depletion, creating blind spots in safety coverage and disrupting hazard detection pipelines. This paper presents a vision for a Unified Smart Safety and Security Architecture that integrates multimodal perception, secure federated learning, reinforcement learning, DTN enabled communication, and energy aware sensing into a cohesive safety framework. We introduce five core modules: Miner Finder, Multimodal Situational Awareness, Backdoor Attack Monitor, TrustFed LFD, and IoT driven Equipment Health Monitoring. These modules collectively address miner localization, hazard understanding, federated robustness, and predictive maintenance. Together, they form an end to end framework capable of guiding miners through obstructed pathways, identifying compromised models or sensors, and ensuring mission critical equipment reliability. This work outlines a comprehensive research vision for building a resilient and trustworthy intelligent mining system capable of maintaining operational continuity under adversarial conditions.",
        "authors_display": "Sanjay Madria Team",
        "pdf_url": "http://arxiv.org/abs/2602.11472",
        "code_url": null,
        "date": "2026-02-12",
        "primary_category": "cs.CR",
        "chinese_summary": "鉴于恶劣采矿环境对感知、态势感知和分布式学习的严峻挑战，以及日益增长的网络物理安全威胁，本研究提出了一个统一的智能安全与安保架构愿景。该架构整合了多模态感知、安全联邦学习、强化学习、延迟容忍网络通信和能源感知传感，包含矿工定位、多模态态势感知等五个核心模块。此框架旨在构建一个弹性、可信的智能采矿系统，以应对对抗性条件下的运营挑战并确保任务关键设备的可靠性。"
      },
      {
        "paper_id": "2602.06643",
        "title": "Humanoid Manipulation Interface: Humanoid Whole-Body Manipulation from Robot-Free Demonstrations",
        "abstract": "Current approaches for humanoid whole-body manipulation, primarily relying on teleoperation or visual sim-to-real reinforcement learning, are hindered by hardware logistics and complex reward engineering. Consequently, demonstrated autonomous skills remain limited and are typically restricted to controlled environments. In this paper, we present the Humanoid Manipulation Interface (HuMI), a portable and efficient framework for learning diverse whole-body manipulation tasks across various environments. HuMI enables robot-free data collection by capturing rich whole-body motion using portable hardware. This data drives a hierarchical learning pipeline that translates human motions into dexterous and feasible humanoid skills. Extensive experiments across five whole-body tasks--including kneeling, squatting, tossing, walking, and bimanual manipulation--demonstrate that HuMI achieves a 3x increase in data collection efficiency compared to teleoperation and attains a 70% success rate in unseen environments.",
        "authors_display": "Yang Gao Team",
        "pdf_url": "http://arxiv.org/abs/2602.06643",
        "code_url": "https://humanoid-manipulation-interface.github.io",
        "date": "2026-02-12",
        "primary_category": "cs.RO",
        "chinese_summary": "当前人形机器人全身操作方法受限于硬件和奖励工程，导致自主技能有限且多在受控环境。本研究提出了Humanoid Manipulation Interface (HuMI)，一个便携高效的框架，旨在学习多种环境中的多样化全身操作任务。HuMI通过便携式硬件捕捉人类全身运动数据，实现了无机器人数据采集，并利用分层学习管道将人类动作转化为灵巧可行的人形机器人技能。实验表明，HuMI的数据采集效率比遥操作提高了3倍，并在未见环境中达到了70%的成功率。"
      },
      {
        "paper_id": "2602.11321",
        "title": "ExtremControl: Low-Latency Humanoid Teleoperation with Direct Extremity Control",
        "abstract": "Building a low-latency humanoid teleoperation system is essential for collecting diverse reactive and dynamic demonstrations. However, existing approaches rely on heavily pre-processed human-to-humanoid motion retargeting and position-only PD control, resulting in substantial latency that severely limits responsiveness and prevents tasks requiring rapid feedback and fast reactions. To address this problem, we propose ExtremControl, a low latency whole-body control framework that: (1) operates directly on SE(3) poses of selected rigid links, primarily humanoid extremities, to avoid full-body retargeting; (2) utilizes a Cartesian-space mapping to directly convert human motion to humanoid link targets; and (3) incorporates velocity feedforward control at low level to support highly responsive behavior under rapidly changing control interfaces. We further provide a unified theoretical formulation of ExtremControl and systematically validate its effectiveness through experiments in both simulation and real-world environments. Building on ExtremControl, we implement a low-latency humanoid teleoperation system that supports both optical motion capture and VR-based motion tracking, achieving end-to-end latency as low as 50ms and enabling highly responsive behaviors such as ping-pong ball balancing, juggling, and real-time return, thereby substantially surpassing the 200ms latency limit observed in prior work.",
        "authors_display": "Chuang Gan Team",
        "pdf_url": "http://arxiv.org/abs/2602.11321",
        "code_url": "https://owenowl.github.io/extremcontrol",
        "date": "2026-02-11",
        "primary_category": "cs.RO",
        "chinese_summary": "针对现有遥操作系统在人形机器人上存在高延迟、限制快速响应的问题，本研究提出了ExtremControl低延迟全身控制框架。该方法直接在选定肢端的SE(3)姿态上操作，避免了全身运动重定向，并通过笛卡尔空间映射和低层速度前馈控制实现高响应性。理论和实验验证表明，ExtremControl实现了低至50ms的端到端延迟，显著超越了以往工作，成功支持了如乒乓球平衡等需要快速反应的高动态任务。"
      },
      {
        "paper_id": "2602.11143",
        "title": "APEX: Learning Adaptive High-Platform Traversal for Humanoid Robots",
        "abstract": "Humanoid locomotion has advanced rapidly with deep reinforcement learning (DRL), enabling robust feet-based traversal over uneven terrain. Yet platforms beyond leg length remain largely out of reach because current RL training paradigms often converge to jumping-like solutions that are high-impact, torque-limited, and unsafe for real-world deployment. To address this gap, we propose APEX, a system for perceptive, climbing-based high-platform traversal that composes terrain-conditioned behaviors: climb-up and climb-down at vertical edges, walking or crawling on the platform, and stand-up and lie-down for posture reconfiguration. Central to our approach is a generalized ratchet progress reward for learning contact-rich, goal-reaching maneuvers. It tracks the best-so-far task progress and penalizes non-improving steps, providing dense yet velocity-free supervision that enables efficient exploration under strong safety regularization. Based on this formulation, we train LiDAR-based full-body maneuver policies and reduce the sim-to-real perception gap through a dual strategy: modeling mapping artifacts during training and applying filtering and inpainting to elevation maps during deployment. Finally, we distill all six skills into a single policy that autonomously selects behaviors and transitions based on local geometry and commands. Experiments on a 29-DoF Unitree G1 humanoid demonstrate zero-shot sim-to-real traversal of 0.8 meter platforms (approximately 114% of leg length), with robust adaptation to platform height and initial pose, as well as smooth and stable multi-skill transitions.",
        "authors_display": "Ding Zhao Team",
        "pdf_url": "http://arxiv.org/abs/2602.11143",
        "code_url": "https://apex-humanoid.github.io/",
        "date": "2026-02-11",
        "primary_category": "cs.RO",
        "chinese_summary": "为解决人形机器人难以安全有效地攀爬超过腿长的高平台的问题，本研究提出了APEX系统。该系统通过组合多种地形条件行为（攀爬、行走、姿态调整），并引入广义棘轮进度奖励来训练接触丰富的目标达成机动，同时利用LiDAR感知和双重策略减小仿真到真实的感知差距。在Unitree G1人形机器人上的实验表明，APEX实现了0.8米高平台的零样本仿真到真实穿越，展现出鲁棒的平台高度和初始姿态适应性及平滑的多技能转换能力。"
      },
      {
        "paper_id": "2602.10943",
        "title": "Towards Learning a Generalizable 3D Scene Representation from 2D Observations",
        "abstract": "We introduce a Generalizable Neural Radiance Field approach for predicting 3D workspace occupancy from egocentric robot observations. Unlike prior methods operating in camera-centric coordinates, our model constructs occupancy representations in a global workspace frame, making it directly applicable to robotic manipulation. The model integrates flexible source views and generalizes to unseen object arrangements without scene-specific finetuning. We demonstrate the approach on a humanoid robot and evaluate predicted geometry against 3D sensor ground truth. Trained on 40 real scenes, our model achieves 26mm reconstruction error, including occluded regions, validating its ability to infer complete 3D occupancy beyond traditional stereo vision methods.",
        "authors_display": "Stefan Wermter Team",
        "pdf_url": "http://arxiv.org/abs/2602.10943",
        "code_url": null,
        "date": "2026-02-11",
        "primary_category": "cs.CV",
        "chinese_summary": "为解决现有神经辐射场方法在机器人操作中对3D工作空间占用预测的局限性，本研究提出了一种可泛化的神经辐射场方法。该模型在全局工作空间框架中构建占用表示，使其能直接应用于机器人操作，并通过整合灵活的源视图实现对未见物体排列的泛化，无需场景特定微调。在人形机器人上的实验结果显示，该模型在包含遮挡区域的情况下实现了26mm的重建误差，验证了其超越传统立体视觉方法的完整3D占用推断能力。"
      },
      {
        "paper_id": "2602.08594",
        "title": "MOSAIC: Bridging the Sim-to-Real Gap in Generalist Humanoid Motion Tracking and Teleoperation with Rapid Residual Adaptation",
        "abstract": "Generalist humanoid motion trackers have recently achieved strong simulation metrics by scaling data and training, yet often remain brittle on hardware during sustained teleoperation due to interface- and dynamics-induced errors. We present MOSAIC, an open-source, full-stack system for humanoid motion tracking and whole-body teleoperation across multiple interfaces. MOSAIC first learns a teleoperation-oriented general motion tracker via RL on a multi-source motion bank with adaptive resampling and rewards that emphasize world-frame motion consistency, which is critical for mobile teleoperation. To bridge the sim-to-real interface gap without sacrificing generality, MOSAIC then performs rapid residual adaptation: an interface-specific policy is trained using minimal interface-specific data, and then distilled into the general tracker through an additive residual module, outperforming naive fine-tuning or continual learning. We validate MOSAIC with systematic ablations, out-of-distribution benchmarking, and real-robot experiments demonstrating robust offline motion replay and online long-horizon teleoperation under realistic latency and noise. Project page: baai-humanoid.github.io/MOSAIC.",
        "authors_display": "Alois Knoll Team",
        "pdf_url": "http://arxiv.org/abs/2602.08594",
        "code_url": null,
        "date": "2026-02-11",
        "primary_category": "cs.RO",
        "chinese_summary": "针对通用人形机器人运动追踪器在硬件上持续遥操作时易受接口和动力学误差影响的问题，本研究提出了MOSAIC，一个开源、全栈的运动追踪与全身遥操作系统。MOSAIC通过强化学习训练通用运动追踪器，并采用自适应重采样和强调世界坐标系运动一致性的奖励。为弥合仿真到真实接口差距，系统采用快速残差适应机制，通过接口特定数据训练残差模块并将其蒸馏到通用追踪器。实验验证了MOSAIC在真实延迟和噪声下的鲁棒离线运动回放和在线长时程遥操作能力。"
      },
      {
        "paper_id": "2602.10106",
        "title": "EgoHumanoid: Unlocking In-the-Wild Loco-Manipulation with Robot-Free Egocentric Demonstration",
        "abstract": "Human demonstrations offer rich environmental diversity and scale naturally, making them an appealing alternative to robot teleoperation. While this paradigm has advanced robot-arm manipulation, its potential for the more challenging, data-hungry problem of humanoid loco-manipulation remains largely unexplored. We present EgoHumanoid, the first framework to co-train a vision-language-action policy using abundant egocentric human demonstrations together with a limited amount of robot data, enabling humanoids to perform loco-manipulation across diverse real-world environments. To bridge the embodiment gap between humans and robots, including discrepancies in physical morphology and viewpoint, we introduce a systematic alignment pipeline spanning from hardware design to data processing. A portable system for scalable human data collection is developed, and we establish practical collection protocols to improve transferability. At the core of our human-to-humanoid alignment pipeline lies two key components. The view alignment reduces visual domain discrepancies caused by camera height and perspective variation. The action alignment maps human motions into a unified, kinematically feasible action space for humanoid control. Extensive real-world experiments demonstrate that incorporating robot-free egocentric data significantly outperforms robot-only baselines by 51\\%, particularly in unseen environments. Our analysis further reveals which behaviors transfer effectively and the potential for scaling human data.",
        "authors_display": "Li Chen Team",
        "pdf_url": "http://arxiv.org/abs/2602.10106",
        "code_url": "https://opendrivelab.com/EgoHumanoid",
        "date": "2026-02-10",
        "primary_category": "cs.RO",
        "chinese_summary": "针对人形机器人运动-操作任务中数据饥渴和人机本体差异问题，本研究提出了EgoHumanoid框架。该框架首次共同训练了一个视觉-语言-动作策略，融合了大量的自我中心人体演示数据和有限的机器人数据，并设计了从硬件到数据处理的系统对齐管线，包括视图对齐和动作对齐。实验结果表明，该方法在多样化真实环境中显著提升了人形机器人的运动-操作性能，尤其是在未见过的新环境中，超越了仅使用机器人数据的基线模型51%。"
      },
      {
        "paper_id": "2602.10069",
        "title": "Humanoid Factors: Design Principles for AI Humanoids in Human Worlds",
        "abstract": "Human factors research has long focused on optimizing environments, tools, and systems to account for human performance. Yet, as humanoid robots begin to share our workplaces, homes, and public spaces, the design challenge expands. We must now consider not only factors for humans but also factors for humanoids, since both will coexist and interact within the same environments. Unlike conventional machines, humanoids introduce expectations of human-like behavior, communication, and social presence, which reshape usability, trust, and safety considerations. In this article, we introduce the concept of humanoid factors as a framework structured around four pillars - physical, cognitive, social, and ethical - that shape the development of humanoids to help them effectively coexist and collaborate with humans. This framework characterizes the overlap and divergence between human capabilities and those of general-purpose humanoids powered by AI foundation models. To demonstrate our framework's practical utility, we then apply the framework to evaluate a real-world humanoid control algorithm, illustrating how conventional task completion metrics in robotics overlook key human cognitive and interaction principles. We thus position humanoid factors as a foundational framework for designing, evaluating, and governing sustained human-humanoid coexistence.",
        "authors_display": "Lixiao Huang Team",
        "pdf_url": "http://arxiv.org/abs/2602.10069",
        "code_url": null,
        "date": "2026-02-10",
        "primary_category": "cs.RO",
        "chinese_summary": "随着人形机器人逐渐与人类共享工作和生活空间，设计挑战从传统“人类因素”扩展至“人形机器人因素”。本研究引入了“人形机器人因素”框架，围绕物理、认知、社交和伦理四大支柱，旨在指导人形机器人开发，促使其与人类有效共存与协作。该框架通过评估真实机器人控制算法，揭示了传统机器人指标对人类认知和交互原则的忽视，为设计、评估和管理持续人机共存提供了基础性指导。"
      },
      {
        "paper_id": "2602.09628",
        "title": "TeleGate: Whole-Body Humanoid Teleoperation via Gated Expert Selection with Motion Prior",
        "abstract": "Real-time whole-body teleoperation is a critical method for humanoid robots to perform complex tasks in unstructured environments. However, developing a unified controller that robustly supports diverse human motions remains a significant challenge. Existing methods typically distill multiple expert policies into a single general policy, which often inevitably leads to performance degradation, particularly on highly dynamic motions. This paper presents TeleGate, a unified whole-body teleoperation framework for humanoid robots that achieves high-precision tracking across various motions while avoiding the performance loss inherent in knowledge distillation. Our key idea is to preserve the full capability of domain-specific expert policies by training a lightweight gating network, which dynamically activates experts in real-time based on proprioceptive states and reference trajectories. Furthermore, to compensate for the absence of future reference trajectories in real-time teleoperation, we introduce a VAE-based motion prior module that extracts implicit future motion intent from historical observations, enabling anticipatory control for motions requiring prediction such as jumping and standing up. We conducted empirical evaluations in simulation and also deployed our technique on the Unitree G1 humanoid robot. Using only 2.5 hours of motion capture data for training, our TeleGate achieves high-precision real-time teleoperation across diverse dynamic motions (e.g., running, fall recovery, and jumping), significantly outperforming the baseline methods in both tracking accuracy and success rate.",
        "authors_display": "Rongyun Cao Team",
        "pdf_url": "http://arxiv.org/abs/2602.09628",
        "code_url": null,
        "date": "2026-02-10",
        "primary_category": "cs.RO",
        "chinese_summary": "为解决实时全身遥操作中统一控制器在多样化人体运动上性能下降的挑战，本研究提出了TeleGate框架。该框架通过训练一个轻量级门控网络，根据本体感知状态和参考轨迹动态激活领域特定专家策略，以避免知识蒸馏带来的性能损失。此外，引入VAE基运动先验模块从历史观测中提取未来运动意图，实现前瞻性控制。实验结果表明，TeleGate在Unitree G1人形机器人上仅用2.5小时动捕数据，便在多种动态运动（如跑步、跌倒恢复、跳跃）中实现了高精度实时遥操作，显著优于基线方法。"
      },
      {
        "paper_id": "2602.08518",
        "title": "Characteristics, Management, and Utilization of Muscles in Musculoskeletal Humanoids: Empirical Study on Kengoro and Musashi",
        "abstract": "Various musculoskeletal humanoids have been developed so far, and numerous studies on control mechanisms have been conducted to leverage the advantages of their biomimetic bodies. However, there has not been sufficient and unified discussion on the diverse properties inherent in these musculoskeletal structures, nor on how to manage and utilize them. Therefore, this study categorizes and analyzes the characteristics of muscles, as well as their management and utilization methods, based on the various research conducted on the musculoskeletal humanoids we have developed, Kengoro and Musashi. We classify the features of the musculoskeletal structure into five properties: Redundancy, Independency, Anisotropy, Variable Moment Arm, and Nonlinear Elasticity. We then organize the diverse advantages and disadvantages of musculoskeletal humanoids that arise from the combination of these properties. In particular, we discuss body schema learning and reflex control, along with muscle grouping and body schema adaptation. Also, we describe the implementation of movements through an integrated system and discuss future challenges and prospects.",
        "authors_display": "Masayuki Inaba Team",
        "pdf_url": "http://arxiv.org/abs/2602.08518",
        "code_url": null,
        "date": "2026-02-09",
        "primary_category": "cs.RO",
        "chinese_summary": "当前肌肉骨骼人形机器人研究在利用其仿生优势方面取得了进展，但对其多样化特性及管理利用方法缺乏统一深入讨论。本研究基于Kengoro和Musashi机器人，将肌肉骨骼结构特征归纳为冗余性、独立性、各向异性、可变力臂和非线性弹性五类。在此基础上，文章分析了这些特性组合带来的优缺点，并讨论了身体图式学习、反射控制、肌肉分组和身体图式适应的实现，为未来研究方向提供了展望。"
      },
      {
        "paper_id": "2602.08370",
        "title": "Learning Human-Like Badminton Skills for Humanoid Robots",
        "abstract": "Realizing versatile and human-like performance in high-demand sports like badminton remains a formidable challenge for humanoid robotics. Unlike standard locomotion or static manipulation, this task demands a seamless integration of explosive whole-body coordination and precise, timing-critical interception. While recent advances have achieved lifelike motion mimicry, bridging the gap between kinematic imitation and functional, physics-aware striking without compromising stylistic naturalness is non-trivial. To address this, we propose Imitation-to-Interaction, a progressive reinforcement learning framework designed to evolve a robot from a \"mimic\" to a capable \"striker.\" Our approach establishes a robust motor prior from human data, distills it into a compact, model-based state representation, and stabilizes dynamics via adversarial priors. Crucially, to overcome the sparsity of expert demonstrations, we introduce a manifold expansion strategy that generalizes discrete strike points into a dense interaction volume. We validate our framework through the mastery of diverse skills, including lifts and drop shots, in simulation. Furthermore, we demonstrate the first zero-shot sim-to-real transfer of anthropomorphic badminton skills to a humanoid robot, successfully replicating the kinetic elegance and functional precision of human athletes in the physical world.",
        "authors_display": "Peng Lu Team",
        "pdf_url": "http://arxiv.org/abs/2602.08370",
        "code_url": null,
        "date": "2026-02-09",
        "primary_category": "cs.RO",
        "chinese_summary": "人形机器人在羽毛球等高要求运动中实现类人表现面临挑战，需要爆发性的全身协调和精确拦截，同时保持运动自然性。本研究提出了“模仿到交互”的渐进式强化学习框架，通过从人类数据建立运动先验、模型化状态表示和对抗性先验来稳定动态，并引入流形扩展策略解决专家演示稀疏性问题。该框架在仿真中掌握了多样技能，并首次成功实现拟人化羽毛球技能从仿真到真实人形机器人的零样本迁移，展现了人类运动员的动能优雅和功能精度。"
      },
      {
        "paper_id": "2602.07506",
        "title": "VividFace: Real-Time and Realistic Facial Expression Shadowing for Humanoid Robots",
        "abstract": "Humanoid facial expression shadowing enables robots to realistically imitate human facial expressions in real time, which is critical for lifelike, facially expressive humanoid robots and affective human-robot interaction. Existing progress in humanoid facial expression imitation remains limited, often failing to achieve either real-time performance or realistic expressiveness due to offline video-based inference designs and insufficient ability to capture and transfer subtle expression details. To address these limitations, we present VividFace, a real-time and realistic facial expression shadowing system for humanoid robots. An optimized imitation framework X2CNet++ enhances expressiveness by fine-tuning the human-to-humanoid facial motion transfer module and introducing a feature-adaptation training strategy for better alignment across different image sources. Real-time shadowing is further enabled by a video-stream-compatible inference pipeline and a streamlined workflow based on asynchronous I/O for efficient communication across devices. VividFace produces vivid humanoid faces by mimicking human facial expressions within 0.05 seconds, while generalizing across diverse facial configurations. Extensive real-world demonstrations validate its practical utility. Videos are available at: https://lipzh5.github.io/VividFace/.",
        "authors_display": "Yang Zhang Team",
        "pdf_url": "http://arxiv.org/abs/2602.07506",
        "code_url": null,
        "date": "2026-02-07",
        "primary_category": "cs.RO",
        "chinese_summary": "现有的人形机器人面部表情模仿系统因离线推理和对细微表情细节捕捉不足，难以同时达到实时性和逼真表现力。本研究提出了VividFace，一个实时、逼真的人形机器人面部表情影子系统。该系统通过优化X2CNet++框架以增强面部运动转移的准确性，并采用特征适应训练策略，同时利用视频流兼容推理管道和异步I/O实现实时性能。VividFace能在0.05秒内模仿人类表情并生成生动的人形面部，已通过广泛的真实世界演示验证了其实用性。"
      },
      {
        "paper_id": "2602.07439",
        "title": "TextOp: Real-time Interactive Text-Driven Humanoid Robot Motion Generation and Control",
        "abstract": "Recent advances in humanoid whole-body motion tracking have enabled the execution of diverse and highly coordinated motions on real hardware. However, existing controllers are commonly driven either by predefined motion trajectories, which offer limited flexibility when user intent changes, or by continuous human teleoperation, which requires constant human involvement and limits autonomy. This work addresses the problem of how to drive a universal humanoid controller in a real-time and interactive manner. We present TextOp, a real-time text-driven humanoid motion generation and control framework that supports streaming language commands and on-the-fly instruction modification during execution. TextOp adopts a two-level architecture in which a high-level autoregressive motion diffusion model continuously generates short-horizon kinematic trajectories conditioned on the current text input, while a low-level motion tracking policy executes these trajectories on a physical humanoid robot. By bridging interactive motion generation with robust whole-body control, TextOp unlocks free-form intent expression and enables smooth transitions across multiple challenging behaviors such as dancing and jumping, within a single continuous motion execution. Extensive real-robot experiments and offline evaluations demonstrate instant responsiveness, smooth whole-body motion, and precise control. The project page and the open-source code are available at https://text-op.github.io/",
        "authors_display": "Xuelong Li Team",
        "pdf_url": "http://arxiv.org/abs/2602.07439",
        "code_url": "https://text-op.github.io/",
        "date": "2026-02-07",
        "primary_category": "cs.RO",
        "chinese_summary": "人形机器人全身运动控制受限于预定义轨迹或持续遥操作，缺乏灵活性和自主性。本研究提出了TextOp，一个实时文本驱动的人形机器人运动生成与控制框架，支持流式语言命令和即时指令修改。该框架采用两级架构，高级自回归运动扩散模型根据文本生成短时域运动学轨迹，低级策略则在物理机器人上执行这些轨迹。真实机器人实验表明TextOp具有即时响应性、流畅全身运动和精确控制，能实现多种复杂行为间的平稳过渡。"
      },
      {
        "paper_id": "2602.07434",
        "title": "Bridging Speech, Emotion, and Motion: a VLM-based Multimodal Edge-deployable Framework for Humanoid Robots",
        "abstract": "Effective human-robot interaction requires emotionally rich multimodal expressions, yet most humanoid robots lack coordinated speech, facial expressions, and gestures. Meanwhile, real-world deployment demands on-device solutions that can operate autonomously without continuous cloud connectivity. To bridging \\underline{\\textit{S}}peech, \\underline{\\textit{E}}motion, and \\underline{\\textit{M}}otion, we present \\textit{SeM$^2$}, a Vision Language Model-based framework that orchestrates emotionally coherent multimodal interactions through three key components: a multimodal perception module capturing user contextual cues, a Chain-of-Thought reasoning for response planning, and a novel Semantic-Sequence Aligning Mechanism (SSAM) that ensures precise temporal coordination between verbal content and physical expressions. We implement both cloud-based and \\underline{\\textit{e}}dge-deployed versions (\\textit{SeM$^2_e$}), with the latter knowledge distilled to operate efficiently on edge hardware while maintaining 95\\% of the relative performance. Comprehensive evaluations demonstrate that our approach significantly outperforms unimodal baselines in naturalness, emotional clarity, and modal coherence, advancing socially expressive humanoid robotics for diverse real-world environments.",
        "authors_display": "Miao Li Team",
        "pdf_url": "http://arxiv.org/abs/2602.07434",
        "code_url": null,
        "date": "2026-02-07",
        "primary_category": "cs.RO",
        "chinese_summary": "有效的人机交互需要情感丰富的多模态表达，但现有机器人缺乏语音、面部表情和姿态的协调，且实际部署要求设备端自主运行。本研究提出了SeM²框架，一个基于视觉语言模型（VLM）的框架，通过多模态感知、思维链推理和新型语义序列对齐机制（SSAM）协调情感一致的多模态交互。框架提供了云端和边缘部署版本，后者通过知识蒸馏高效运行。综合评估证明SeM²在自然性、情感清晰度和模态一致性方面显著优于基线，推动了社交表达型人形机器人发展。"
      },
      {
        "paper_id": "2602.07227",
        "title": "Cerebellar-Inspired Residual Control for Fault Recovery: From Inference-Time Adaptation to Structural Consolidation",
        "abstract": "Robotic policies deployed in real-world environments often encounter post-training faults, where retraining, exploration, or system identification are impractical. We introduce an inference-time, cerebellar-inspired residual control framework that augments a frozen reinforcement learning policy with online corrective actions, enabling fault recovery without modifying base policy parameters. The framework instantiates core cerebellar principles, including high-dimensional pattern separation via fixed feature expansion, parallel microzone-style residual pathways, and local error-driven plasticity with excitatory and inhibitory eligibility traces operating at distinct time scales. These mechanisms enable fast, localized correction under post-training disturbances while avoiding destabilizing global policy updates. A conservative, performance-driven meta-adaptation regulates residual authority and plasticity, preserving nominal behavior and suppressing unnecessary intervention. Experiments on MuJoCo benchmarks under actuator, dynamic, and environmental perturbations show improvements of up to $+66\\%$ on \\texttt{HalfCheetah-v5} and $+53\\%$ on \\texttt{Humanoid-v5} under moderate faults, with graceful degradation under severe shifts and complementary robustness from consolidating persistent residual corrections into policy parameters.",
        "authors_display": "Amit Ranjan Trivedi Team",
        "pdf_url": "http://arxiv.org/abs/2602.07227",
        "code_url": null,
        "date": "2026-02-06",
        "primary_category": "cs.LG",
        "chinese_summary": "机器人策略在真实世界部署后常面临故障，而重新训练或系统识别并不总是可行。本研究提出了一种受小脑启发的推理时残差控制框架，通过在线校正动作增强冻结的强化学习策略，无需修改基础策略参数即可实现故障恢复。该框架利用高维模式分离、并行残差路径和误差驱动可塑性等机制，并通过元适应调节残差权限。在MuJoCo基准测试中，该方法在执行器、动态和环境扰动下，对HalfCheetah-v5和Humanoid-v5分别实现了高达+66%和+53%的性能提升，且在严重故障下仍能优雅降级。"
      },
      {
        "paper_id": "2602.06827",
        "title": "DynaRetarget: Dynamically-Feasible Retargeting using Sampling-Based Trajectory Optimization",
        "abstract": "In this paper, we introduce DynaRetarget, a complete pipeline for retargeting human motions to humanoid control policies. The core component of DynaRetarget is a novel Sampling-Based Trajectory Optimization (SBTO) framework that refines imperfect kinematic trajectories into dynamically feasible motions. SBTO incrementally advances the optimization horizon, enabling optimization over the entire trajectory for long-horizon tasks. We validate DynaRetarget by successfully retargeting hundreds of humanoid-object demonstrations and achieving higher success rates than the state of the art. The framework also generalizes across varying object properties, such as mass, size, and geometry, using the same tracking objective. This ability to robustly retarget diverse demonstrations opens the door to generating large-scale synthetic datasets of humanoid loco-manipulation trajectories, addressing a major bottleneck in real-world data collection.",
        "authors_display": "Majid Khadiv Team",
        "pdf_url": "http://arxiv.org/abs/2602.06827",
        "code_url": null,
        "date": "2026-02-06",
        "primary_category": "cs.RO",
        "chinese_summary": "将人类运动重定向至人形机器人控制策略并生成动态可行运动是一个挑战。本研究引入了DynaRetarget，一个完整的人类运动到人形机器人控制策略的重定向流程。其核心是新型基于采样的轨迹优化（SBTO）框架，能够将不完美的运动学轨迹精炼为动态可行的运动，并通过增量优化实现长时域任务的轨迹优化。实验验证DynaRetarget在重定向数百个人形机器人与物体交互演示时取得了更高的成功率，并能泛化到不同物体属性，为生成大规模合成数据集提供了可能。"
      },
      {
        "paper_id": "2602.06445",
        "title": "ECO: Energy-Constrained Optimization with Reinforcement Learning for Humanoid Walking",
        "abstract": "Achieving stable and energy-efficient locomotion is essential for humanoid robots to operate continuously in real-world applications. Existing MPC and RL approaches often rely on energy-related metrics embedded within a multi-objective optimization framework, which require extensive hyperparameter tuning and often result in suboptimal policies. To address these challenges, we propose ECO (Energy-Constrained Optimization), a constrained RL framework that separates energy-related metrics from rewards, reformulating them as explicit inequality constraints. This method provides a clear and interpretable physical representation of energy costs, enabling more efficient and intuitive hyperparameter tuning for improved energy efficiency. ECO introduces dedicated constraints for energy consumption and reference motion, enforced by the Lagrangian method, to achieve stable, symmetric, and energy-efficient walking for humanoid robots. We evaluated ECO against MPC, standard RL with reward shaping, and four state-of-the-art constrained RL methods. Experiments, including sim-to-sim and sim-to-real transfers on the kid-sized humanoid robot BRUCE, demonstrate that ECO significantly reduces energy consumption compared to baselines while maintaining robust walking performance. These results highlight a substantial advancement in energy-efficient humanoid locomotion. All experimental demonstrations can be found on the project website: https://sites.google.com/view/eco-humanoid.",
        "authors_display": "Yao Su Team",
        "pdf_url": "http://arxiv.org/abs/2602.06445",
        "code_url": null,
        "date": "2026-02-06",
        "primary_category": "cs.RO",
        "chinese_summary": "实现人形机器人稳定、节能的运动对于实际应用至关重要，但现有方法常将能量指标嵌入多目标优化中，导致超参数调整复杂且易得次优策略。本研究提出了ECO（Energy-Constrained Optimization），一个约束强化学习框架，将能量相关指标作为显式不等式约束。该方法通过拉格朗日乘子法对能量消耗和参考运动施加专用约束，以实现人形机器人的稳定、对称和节能行走。在与多种SOTA方法的对比评估和模拟到真实迁移实验中，ECO在保持鲁棒行走性能的同时显著降低了能耗。"
      },
      {
        "paper_id": "2602.06382",
        "title": "Now You See That: Learning End-to-End Humanoid Locomotion from Raw Pixels",
        "abstract": "Achieving robust vision-based humanoid locomotion remains challenging due to two fundamental issues: the sim-to-real gap introduces significant perception noise that degrades performance on fine-grained tasks, and training a unified policy across diverse terrains is hindered by conflicting learning objectives. To address these challenges, we present an end-to-end framework for vision-driven humanoid locomotion. For robust sim-to-real transfer, we develop a high-fidelity depth sensor simulation that captures stereo matching artifacts and calibration uncertainties inherent in real-world sensing. We further propose a vision-aware behavior distillation approach that combines latent space alignment with noise-invariant auxiliary tasks, enabling effective knowledge transfer from privileged height maps to noisy depth observations. For versatile terrain adaptation, we introduce terrain-specific reward shaping integrated with multi-critic and multi-discriminator learning, where dedicated networks capture the distinct dynamics and motion priors of each terrain type. We validate our approach on two humanoid platforms equipped with different stereo depth cameras. The resulting policy demonstrates robust performance across diverse environments, seamlessly handling extreme challenges such as high platforms and wide gaps, as well as fine-grained tasks including bidirectional long-term staircase traversal.",
        "authors_display": "Zongwu Xie Team",
        "pdf_url": "http://arxiv.org/abs/2602.06382",
        "code_url": null,
        "date": "2026-02-06",
        "primary_category": "cs.RO",
        "chinese_summary": "基于视觉的人形机器人稳定运动面临模拟到真实差距造成的感知噪声和不同地形下学习目标冲突的挑战。本研究提出了一个端到端的视觉驱动人形机器人运动框架。为克服模拟到真实差距，开发了高保真深度传感器模拟并提出视觉感知行为蒸馏方法，结合潜在空间对齐和噪声不变辅助任务。为适应多样地形，引入地形特定奖励整形并集成了多评论家和多判别器学习。在配备不同立体深度摄像头的两个人形平台上的验证显示，该策略在处理极端挑战和精细任务时表现出强大的鲁棒性。"
      }
    ]
  }
}