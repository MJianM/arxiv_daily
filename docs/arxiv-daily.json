{
  "meta": {
    "updated": "2026-02-28",
    "max_papers_per_category": 500
  },
  "categories": {
    "Manipulation": [
      {
        "paper_id": "2602.23287",
        "title": "Interface-Aware Trajectory Reconstruction of Limited Demonstrations for Robot Learning",
        "abstract": "Assistive robots offer agency to humans with severe motor impairments. Often, these users control high-DoF robots through low-dimensional interfaces, such as using a 1-D sip-and-puff interface to operate a 6-DoF robotic arm. This mismatch results in having access to only a subset of control dimensions at a given time, imposing unintended and artificial constraints on robot motion. As a result, interface-limited demonstrations embed suboptimal motions that reflect interface restrictions rather than user intent. To address this, we present a trajectory reconstruction algorithm that reasons about task, environment, and interface constraints to lift demonstrations into the robot's full control space. We evaluate our approach using real-world demonstrations of ADL-inspired tasks performed via a 2-D joystick and 1-D sip-and-puff control interface, teleoperating two distinct 7-DoF robotic arms. Analyses of the reconstructed demonstrations and derived control policies show that lifted trajectories are faster and more efficient than their interface-constrained counterparts while respecting user preferences.",
        "authors_display": "Brenna D. Argall Team",
        "pdf_url": "http://arxiv.org/abs/2602.23287",
        "code_url": null,
        "date": "2026-02-26",
        "primary_category": "cs.RO",
        "chinese_summary": "针对严重运动障碍用户通过低维度接口控制高自由度机器人时，因控制维度不匹配导致的次优运动问题，本文提出了一种轨迹重建算法。该算法通过考虑任务、环境和接口约束，将受限的演示轨迹提升至机器人的全控制空间。实验证明，重建后的轨迹比受接口限制的轨迹更快、更高效，同时能有效反映用户意图。"
      },
      {
        "paper_id": "2602.23259",
        "title": "Risk-Aware World Model Predictive Control for Generalizable End-to-End Autonomous Driving",
        "abstract": "With advances in imitation learning (IL) and large-scale driving datasets, end-to-end autonomous driving (E2E-AD) has made great progress recently. Currently, IL-based methods have become a mainstream paradigm: models rely on standard driving behaviors given by experts, and learn to minimize the discrepancy between their actions and expert actions. However, this objective of \"only driving like the expert\" suffers from limited generalization: when encountering rare or unseen long-tail scenarios outside the distribution of expert demonstrations, models tend to produce unsafe decisions in the absence of prior experience. This raises a fundamental question: Can an E2E-AD system make reliable decisions without any expert action supervision? Motivated by this, we propose a unified framework named Risk-aware World Model Predictive Control (RaWMPC) to address this generalization dilemma through robust control, without reliance on expert demonstrations. Practically, RaWMPC leverages a world model to predict the consequences of multiple candidate actions and selects low-risk actions through explicit risk evaluation. To endow the world model with the ability to predict the outcomes of risky driving behaviors, we design a risk-aware interaction strategy that systematically exposes the world model to hazardous behaviors, making catastrophic outcomes predictable and thus avoidable. Furthermore, to generate low-risk candidate actions at test time, we introduce a self-evaluation distillation method to distill riskavoidance capabilities from the well-trained world model into a generative action proposal network without any expert demonstration. Extensive experiments show that RaWMPC outperforms state-of-the-art methods in both in-distribution and out-of-distribution scenarios, while providing superior decision interpretability.",
        "authors_display": "Nicu Sebe Team",
        "pdf_url": "http://arxiv.org/abs/2602.23259",
        "code_url": null,
        "date": "2026-02-26",
        "primary_category": "cs.CV",
        "chinese_summary": "当前模仿学习（IL）驱动的端到端自动驾驶（E2E-AD）方法在面对罕见或分布外场景时泛化能力差，可能导致不安全决策。为解决这一难题，本文提出了一个名为“风险感知世界模型预测控制”（RaWMPC）的统一框架，它不依赖专家演示，通过鲁棒控制实现可靠决策。RaWMPC利用世界模型预测动作后果并进行显式风险评估，同时设计风险感知交互策略训练世界模型识别危险行为，并采用自评估蒸馏方法将避险能力转移到动作提议网络。实验结果表明，RaWMPC在分布内和分布外场景中均表现优异，并提供了出色的决策可解释性。"
      },
      {
        "paper_id": "2602.22952",
        "title": "Automated Robotic Needle Puncture for Percutaneous Dilatational Tracheostomy",
        "abstract": "Percutaneous dilatational tracheostomy (PDT) is frequently performed on patients in intensive care units for prolonged mechanical ventilation. The needle puncture, as the most critical step of PDT, could lead to adverse consequences such as major bleeding and posterior tracheal wall perforation if performed inaccurately. Current practices of PDT puncture are all performed manually with no navigation assistance, which leads to large position and angular errors (5 mm and 30 degree). To improve the accuracy and reduce the difficulty of the PDT procedure, we propose a system that automates the needle insertion using a velocity-controlled robotic manipulator. Guided using pose data from two electromagnetic sensors, one at the needle tip and the other inside the trachea, the robotic system uses an adaptive constrained controller to adapt the uncertain kinematic parameters online and avoid collisions with the patient's body and tissues near the target. Simulations were performed to validate the controller's implementation, and then four hundred PDT punctures were performed on a mannequin to evaluate the position and angular accuracy. The absolute median puncture position error was 1.7 mm (IQR: 1.9 mm) and midline deviation was 4.13 degree (IQR: 4.55 degree), measured by the sensor inside the trachea. The small deviations from the nominal puncture in a simulated experimental setup and formal guarantees of collision-free insertions suggest the feasibility of the robotic PDT puncture.",
        "authors_display": "Andrew Weightman Team",
        "pdf_url": "http://arxiv.org/abs/2602.22952",
        "code_url": null,
        "date": "2026-02-26",
        "primary_category": "cs.RO",
        "chinese_summary": "经皮扩张气管切开术（PDT）中的穿刺环节若不准确可能导致严重并发症，而现有手动操作缺乏导航辅助，误差较大。为此，本文提出了一种利用速度控制机器人机械臂实现自动化穿刺的系统。该系统通过两个电磁传感器提供姿态引导，并采用自适应约束控制器在线调整参数，避免碰撞。在人体模型上的实验显示，中位穿刺位置绝对误差为1.7毫米，中线偏差为4.13度，验证了机器人PDT穿刺的可行性和准确性。"
      },
      {
        "paper_id": "2602.22896",
        "title": "DySL-VLA: Efficient Vision-Language-Action Model Inference via Dynamic-Static Layer-Skipping for Robot Manipulation",
        "abstract": "Vision-Language-Action (VLA) models have shown remarkable success in robotic tasks like manipulation by fusing a language model's reasoning with a vision model's 3D understanding. However, their high computational cost remains a major obstacle for real-world applications that require real-time performance. We observe that the actions within a task have varying levels of importance: critical steps demand high precision, while less important ones can tolerate more variance. Leveraging this insight, we propose DySL-VLA, a novel framework that addresses computational cost by dynamically skipping VLA layers based on each action's importance. DySL-VLA categorizes its layers into two types: informative layers, which are consistently executed, and incremental layers, which can be selectively skipped. To intelligently skip layers without sacrificing accuracy, we invent a prior-post skipping guidance mechanism to determine when to initiate layer-skipping. We also propose a skip-aware two-stage knowledge distillation algorithm to efficiently train a standard VLA into a DySL-VLA. Our experiments indicate that DySL-VLA achieves 2.1% improvement in success length over Deer-VLA on the Calvin dataset, while simultaneously reducing trainable parameters by a factor of 85.7 and providing a 3.75x speedup relative to the RoboFlamingo baseline at iso-accuracy. Our code is available on https://github.com/PKU-SEC-Lab/DYSL_VLA.",
        "authors_display": "Meng Li Team",
        "pdf_url": "http://arxiv.org/abs/2602.22896",
        "code_url": null,
        "date": "2026-02-26",
        "primary_category": "cs.RO",
        "chinese_summary": "视觉-语言-动作（VLA）模型在机器人任务中表现出色，但其高计算成本阻碍了实时应用。本文基于任务中动作重要性差异的洞察，提出了DySL-VLA框架，通过动态跳过VLA层来降低计算成本。DySL-VLA将层分为信息层和可跳过的增量层，并引入前后跳过指导机制和跳过感知两阶段知识蒸馏算法。实验结果表明，DySL-VLA在Calvin数据集上提高了成功率，同时显著减少了可训练参数并提升了推理速度。"
      },
      {
        "paper_id": "2602.22862",
        "title": "GraspLDP: Towards Generalizable Grasping Policy via Latent Diffusion",
        "abstract": "This paper focuses on enhancing the grasping precision and generalization of manipulation policies learned via imitation learning. Diffusion-based policy learning methods have recently become the mainstream approach for robotic manipulation tasks. As grasping is a critical subtask in manipulation, the ability of imitation-learned policies to execute precise and generalizable grasps merits particular attention. Existing imitation learning techniques for grasping often suffer from imprecise grasp executions, limited spatial generalization, and poor object generalization. To address these challenges, we incorporate grasp prior knowledge into the diffusion policy framework. In particular, we employ a latent diffusion policy to guide action chunk decoding with grasp pose prior, ensuring that generated motion trajectories adhere closely to feasible grasp configurations. Furthermore, we introduce a self-supervised reconstruction objective during diffusion to embed the graspness prior: at each reverse diffusion step, we reconstruct wrist-camera images back-projected the graspness from the intermediate representations. Both simulation and real robot experiments demonstrate that our approach significantly outperforms baseline methods and exhibits strong dynamic grasping capabilities.",
        "authors_display": "Di Huang Team",
        "pdf_url": "http://arxiv.org/abs/2602.22862",
        "code_url": null,
        "date": "2026-02-26",
        "primary_category": "cs.RO",
        "chinese_summary": "模仿学习策略在机器人抓取任务中常面临执行不精确、泛化能力有限的问题。为增强抓取精度和泛化性，本文提出将抓取先验知识融入扩散策略框架。该方法采用潜在扩散策略，利用抓取姿态先验引导动作块解码，并引入自监督重建目标在扩散过程中嵌入抓取性先验。仿真和真实机器人实验均表明，该方法显著优于基线，展现出强大的动态抓取能力。"
      },
      {
        "paper_id": "2602.22818",
        "title": "LeRobot: An Open-Source Library for End-to-End Robot Learning",
        "abstract": "Robotics is undergoing a significant transformation powered by advances in high-level control techniques based on machine learning, giving rise to the field of robot learning. Recent progress in robot learning has been accelerated by the increasing availability of affordable teleoperation systems, large-scale openly available datasets, and scalable learning-based methods. However, development in the field of robot learning is often slowed by fragmented, closed-source tools designed to only address specific sub-components within the robotics stack. In this paper, we present \\texttt{lerobot}, an open-source library that integrates across the entire robot learning stack, from low-level middleware communication for motor controls to large-scale dataset collection, storage and streaming. The library is designed with a strong focus on real-world robotics, supporting accessible hardware platforms while remaining extensible to new embodiments. It also supports efficient implementations for various state-of-the-art robot learning algorithms from multiple prominent paradigms, as well as a generalized asynchronous inference stack. Unlike traditional pipelines which heavily rely on hand-crafted techniques, \\texttt{lerobot} emphasizes scalable learning approaches that improve directly with more data and compute. Designed for accessibility, scalability, and openness, \\texttt{lerobot} lowers the barrier to entry for researchers and practitioners to robotics while providing a platform for reproducible, state-of-the-art robot learning.",
        "authors_display": "Thomas Wolf Team",
        "pdf_url": "http://arxiv.org/abs/2602.22818",
        "code_url": "https://github.com/huggingface/lerobot",
        "date": "2026-02-26",
        "primary_category": "cs.RO",
        "chinese_summary": "机器人学习领域的发展常受限于碎片化、闭源且仅解决特定子组件的工具。为解决此问题，本文提出了lerobot，一个开源库，它整合了整个机器人学习堆栈，涵盖从低级电机控制到大规模数据集收集、存储和流式传输。该库专注于实际机器人，支持主流硬件平台并具可扩展性，同时支持高效实现多种最先进机器人学习算法。lerobot旨在通过可扩展的学习方法降低机器人研究门槛，并提供可复现的机器人学习平台。"
      },
      {
        "paper_id": "2602.22810",
        "title": "Multi-agent imitation learning with function approximation: Linear Markov games and beyond",
        "abstract": "In this work, we present the first theoretical analysis of multi-agent imitation learning (MAIL) in linear Markov games where both the transition dynamics and each agent's reward function are linear in some given features. We demonstrate that by leveraging this structure, it is possible to replace the state-action level \"all policy deviation concentrability coefficient\" (Freihaut et al., arXiv:2510.09325) with a concentrability coefficient defined at the feature level which can be much smaller than the state-action analog when the features are informative about states' similarity. Furthermore, to circumvent the need for any concentrability coefficient, we turn to the interactive setting. We provide the first, computationally efficient, interactive MAIL algorithm for linear Markov games and show that its sample complexity depends only on the dimension of the feature map $d$. Building on these theoretical findings, we propose a deep MAIL interactive algorithm which clearly outperforms BC on games such as Tic-Tac-Toe and Connect4.",
        "authors_display": "Giorgia Ramponi Team",
        "pdf_url": "http://arxiv.org/abs/2602.22810",
        "code_url": null,
        "date": "2026-02-26",
        "primary_category": "cs.LG",
        "chinese_summary": "针对线性马尔可夫博弈中多智能体模仿学习（MAIL）理论分析的空白及现有集中系数的局限性，本文首次提出在特征层面而非状态-动作层面定义可集中系数，当特征信息丰富时，该系数显著减小。进一步地，为避免对集中系数的依赖，本文在交互式设置下提出了首个计算高效的交互式MAIL算法，并证明其样本复杂度仅依赖于特征映射维度。基于理论发现，本文提出的深度MAIL交互算法在Tic-Tac-Toe和Connect4等游戏中表现优于行为克隆算法。"
      },
      {
        "paper_id": "2602.22666",
        "title": "ArtPro: Self-Supervised Articulated Object Reconstruction with Adaptive Integration of Mobility Proposals",
        "abstract": "Reconstructing articulated objects into high-fidelity digital twins is crucial for applications such as robotic manipulation and interactive simulation. Recent self-supervised methods using differentiable rendering frameworks like 3D Gaussian Splatting remain highly sensitive to the initial part segmentation. Their reliance on heuristic clustering or pre-trained models often causes optimization to converge to local minima, especially for complex multi-part objects. To address these limitations, we propose ArtPro, a novel self-supervised framework that introduces adaptive integration of mobility proposals. Our approach begins with an over-segmentation initialization guided by geometry features and motion priors, generating part proposals with plausible motion hypotheses. During optimization, we dynamically merge these proposals by analyzing motion consistency among spatial neighbors, while a collision-aware motion pruning mechanism prevents erroneous kinematic estimation. Extensive experiments on both synthetic and real-world objects demonstrate that ArtPro achieves robust reconstruction of complex multi-part objects, significantly outperforming existing methods in accuracy and stability.",
        "authors_display": "Changhe Tu Team",
        "pdf_url": "http://arxiv.org/abs/2602.22666",
        "code_url": null,
        "date": "2026-02-26",
        "primary_category": "cs.CV",
        "chinese_summary": "将铰接物体重建为高保真数字孪生对机器人操作和交互仿真至关重要，但现有自监督方法对初始部件分割敏感，易陷入局部最小值。为克服这些限制，本文提出了ArtPro框架，一种引入移动性提议自适应集成的自监督方法。ArtPro通过几何特征和运动先验进行过分割初始化，生成具有合理运动假设的部件提议，并在优化过程中动态合并提议，同时通过碰撞感知运动剪枝机制防止错误的运动学估计。实验证明，ArtPro能鲁棒地重建复杂多部件物体，并在准确性和稳定性方面显著优于现有方法。"
      },
      {
        "paper_id": "2602.22514",
        "title": "SignVLA: A Gloss-Free Vision-Language-Action Framework for Real-Time Sign Language-Guided Robotic Manipulation",
        "abstract": "We present, to our knowledge, the first sign language-driven Vision-Language-Action (VLA) framework for intuitive and inclusive human-robot interaction. Unlike conventional approaches that rely on gloss annotations as intermediate supervision, the proposed system adopts a gloss-free paradigm and directly maps visual sign gestures to semantic instructions. This design reduces annotation cost and avoids the information loss introduced by gloss representations, enabling more natural and scalable multimodal interaction.   In this work, we focus on a real-time alphabet-level finger-spelling interface that provides a robust and low-latency communication channel for robotic control. Compared with large-scale continuous sign language recognition, alphabet-level interaction offers improved reliability, interpretability, and deployment feasibility in safety-critical embodied environments. The proposed pipeline transforms continuous gesture streams into coherent language commands through geometric normalization, temporal smoothing, and lexical refinement, ensuring stable and consistent interaction.   Furthermore, the framework is designed to support future integration of transformer-based gloss-free sign language models, enabling scalable word-level and sentence-level semantic understanding. Experimental results demonstrate the effectiveness of the proposed system in grounding sign-derived instructions into precise robotic actions under diverse interaction scenarios. These results highlight the potential of the framework to advance accessible, scalable, and multimodal embodied intelligence.",
        "authors_display": "Zezhi Tang Team",
        "pdf_url": "http://arxiv.org/abs/2602.22514",
        "code_url": null,
        "date": "2026-02-26",
        "primary_category": "cs.RO",
        "chinese_summary": "传统的基于手语注音（gloss）的人机交互方法在手语驱动的视觉-语言-动作（VLA）框架中存在标注成本高和信息损失问题。为此，本文首次提出一种无注音手语驱动的VLA框架，旨在实现直观包容的人机交互。该系统直接将视觉手势映射为语义指令，并专注于实时的字母级手指拼写接口，通过几何归一化、时间平滑和词汇细化实现稳定通信。实验结果表明，该系统能有效地将手语指令转化为精确的机器人动作，展示了其在推进可访问、可扩展多模态具身智能方面的潜力。"
      },
      {
        "paper_id": "2602.22056",
        "title": "FlowCorrect: Efficient Interactive Correction of Generative Flow Policies for Robotic Manipulation",
        "abstract": "Generative manipulation policies can fail catastrophically under deployment-time distribution shift, yet many failures are near-misses: the robot reaches almost-correct poses and would succeed with a small corrective motion. We present FlowCorrect, a deployment-time correction framework that converts near-miss failures into successes using sparse human nudges, without full policy retraining. During execution, a human provides brief corrective pose nudges via a lightweight VR interface. FlowCorrect uses these sparse corrections to locally adapt the policy, improving actions without retraining the backbone while preserving the model performance on previously learned scenarios. We evaluate on a real-world robot across three tabletop tasks: pick-and-place, pouring, and cup uprighting. With a low correction budget, FlowCorrect improves success on hard cases by 85\\% while preserving performance on previously solved scenarios. The results demonstrate clearly that FlowCorrect learns only with very few demonstrations and enables fast and sample-efficient incremental, human-in-the-loop corrections of generative visuomotor policies at deployment time in real-world robotics.",
        "authors_display": "Rania Rayyes Team",
        "pdf_url": "http://arxiv.org/abs/2602.22056",
        "code_url": null,
        "date": "2026-02-25",
        "primary_category": "cs.RO",
        "chinese_summary": "针对生成式操作策略在部署时因分布偏移而可能出现的“险些成功”失败，本研究提出了FlowCorrect框架。该框架通过轻量级VR界面接收稀疏的人工姿态纠正，从而在不重新训练策略核心的情况下局部调整策略，将近乎成功的失败转化为实际成功，同时保持对先前学习场景的性能。在真实机器人上进行的三项桌面任务评估显示，FlowCorrect在低纠正预算下将困难案例的成功率提高了85%，证明了其在现实世界机器人部署中，通过极少演示实现快速、样本高效的人机协作增量纠正生成式视觉运动策略的能力。"
      },
      {
        "paper_id": "2602.21811",
        "title": "DexRepNet++: Learning Dexterous Robotic Manipulation with Geometric and Spatial Hand-Object Representations",
        "abstract": "Robotic dexterous manipulation is a challenging problem due to high degrees of freedom (DoFs) and complex contacts of multi-fingered robotic hands. Many existing deep reinforcement learning (DRL) based methods aim at improving sample efficiency in high-dimensional output action spaces. However, existing works often overlook the role of representations in achieving generalization of a manipulation policy in the complex input space during the hand-object interaction. In this paper, we propose DexRep, a novel hand-object interaction representation to capture object surface features and spatial relations between hands and objects for dexterous manipulation skill learning. Based on DexRep, policies are learned for three dexterous manipulation tasks, i.e. grasping, in-hand reorientation, bimanual handover, and extensive experiments are conducted to verify the effectiveness. In simulation, for grasping, the policy learned with 40 objects achieves a success rate of 87.9% on more than 5000 unseen objects of diverse categories, significantly surpassing existing work trained with thousands of objects; for the in-hand reorientation and handover tasks, the policies also boost the success rates and other metrics of existing hand-object representations by 20% to 40%. The grasp policies with DexRep are deployed to the real world under multi-camera and single-camera setups and demonstrate a small sim-to-real gap.",
        "authors_display": "Qi Ye Team",
        "pdf_url": "http://arxiv.org/abs/2602.21811",
        "code_url": null,
        "date": "2026-02-25",
        "primary_category": "cs.RO",
        "chinese_summary": "鉴于机器人灵巧操作中多自由度和复杂接触带来的挑战，且现有深度强化学习方法常忽视表示在策略泛化中的作用，本文提出了DexRep，一种新颖的手-物体交互表示。该表示旨在捕捉物体表面特征及手物体的空间关系，以学习灵巧操作技能。基于DexRep，研究人员在抓取、手内重定向和双手交接任务中学习了策略。仿真结果表明，其抓取策略在未见过物体上的成功率显著优于现有方法，且手内重定向和交接任务的成功率也大幅提升；DexRep的抓取策略在真实世界部署中也展现出小的仿真到现实差距。"
      },
      {
        "paper_id": "2602.21736",
        "title": "Joint-Aligned Latent Action: Towards Scalable VLA Pretraining in the Wild",
        "abstract": "Despite progress, Vision-Language-Action models (VLAs) are limited by a scarcity of large-scale, diverse robot data. While human manipulation videos offer a rich alternative, existing methods are forced to choose between small, precisely-labeled datasets and vast in-the-wild footage with unreliable hand tracking labels. We present JALA, a pretraining framework that learns Jointly-Aligned Latent Actions. JALA bypasses full visual dynamic reconstruction, instead learns a predictive action embedding aligned with both inverse dynamics and real actions. This yields a transition-aware, behavior-centric latent space for learning from heterogeneous human data. We scale this approach with UniHand-Mix, a 7.5M video corpus (>2,000 hours) blending laboratory and in-the-wild footage. Experiments demonstrate that JALA generates more realistic hand motions in both controlled and unconstrained scenarios, significantly improving downstream robot manipulation performance in both simulation and real-world tasks. These results indicate that jointly-aligned latent actions offer a scalable pathway for VLA pretraining from human data.",
        "authors_display": "Zongqing Lu Team",
        "pdf_url": "http://arxiv.org/abs/2602.21736",
        "code_url": null,
        "date": "2026-02-25",
        "primary_category": "cs.RO",
        "chinese_summary": "鉴于视觉-语言-动作（VLA）模型受限于大规模、多样化机器人数据的稀缺性，且现有方法难以有效利用包含不可靠手部追踪标签的野外人类操作视频，本文提出了JALA（Jointly-Aligned Latent Actions）预训练框架。JALA通过学习与逆动力学和真实动作对齐的预测性动作嵌入，来构建一个对过渡敏感、以行为为中心的潜在空间，从而规避了完整的视觉动态重建，并能够从异构人类数据中学习。研究人员利用一个包含750万视频的UniHand-Mix语料库对此方法进行扩展，实验结果表明JALA在受控和非受限场景中均能生成更真实的手部动作，显著提升了下游机器人操作任务在仿真和真实世界中的表现。"
      },
      {
        "paper_id": "2602.21684",
        "title": "Primary-Fine Decoupling for Action Generation in Robotic Imitation",
        "abstract": "Multi-modal distribution in robotic manipulation action sequences poses critical challenges for imitation learning. To this end, existing approaches often model the action space as either a discrete set of tokens or a continuous, latent-variable distribution. However, both approaches present trade-offs: some methods discretize actions into tokens and therefore lose fine-grained action variations, while others generate continuous actions in a single stage tend to produce unstable mode transitions. To address these limitations, we propose Primary-Fine Decoupling for Action Generation (PF-DAG), a two-stage framework that decouples coarse action consistency from fine-grained variations. First, we compress action chunks into a small set of discrete modes, enabling a lightweight policy to select consistent coarse modes and avoid mode bouncing. Second, a mode conditioned MeanFlow policy is learned to generate high-fidelity continuous actions. Theoretically, we prove PF-DAG's two-stage design achieves a strictly lower MSE bound than single-stage generative policies. Empirically, PF-DAG outperforms state-of-the-art baselines across 56 tasks from Adroit, DexArt, and MetaWorld benchmarks. It further generalizes to real-world tactile dexterous manipulation tasks. Our work demonstrates that explicit mode-level decoupling enables both robust multi-modal modeling and reactive closed-loop control for robotic manipulation.",
        "authors_display": "Houqiang Li Team",
        "pdf_url": "http://arxiv.org/abs/2602.21684",
        "code_url": null,
        "date": "2026-02-25",
        "primary_category": "cs.RO",
        "chinese_summary": "针对机器人操作动作序列中多模态分布对模仿学习造成的挑战，特别是现有方法在离散化动作时损失精细变化或在连续生成时易产生不稳定模态转换的问题，本文提出了Primary-Fine Decoupling for Action Generation (PF-DAG) 框架。该框架采用两阶段设计，将粗粒度动作一致性与细粒度变化解耦：首先将动作块压缩为少量离散模态以实现一致的粗粒度选择，然后学习一个模态条件下的MeanFlow策略生成高保真连续动作。理论上，PF-DAG的两阶段设计证明能实现更低的MSE界限。实验结果显示，PF-DAG在多个基准测试中超越了现有先进方法，并能泛化到真实世界的触觉灵巧操作任务，证明了其在多模态建模和闭环控制中的鲁棒性。"
      },
      {
        "paper_id": "2602.21666",
        "title": "Biomechanical Comparisons Reveal Divergence of Human and Humanoid Gaits",
        "abstract": "It remains challenging to achieve human-like locomotion in legged robots due to fundamental discrepancies between biological and mechanical structures. Although imitation learning has emerged as a promising approach for generating natural robotic movements, simply replicating joint angle trajectories fails to capture the underlying principles of human motion. This study proposes a Gait Divergence Analysis Framework (GDAF), a unified biomechanical evaluation framework that systematically quantifies kinematic and kinetic discrepancies between humans and bipedal robots. We apply GDAF to systematically compare human and humanoid locomotion across 28 walking speeds. To enable reproducible analysis, we collect and release a speed-continuous humanoid locomotion dataset from a state-of-the-art humanoid controller. We further provide an open-source implementation of GDAF, including analysis, visualization, and MuJoCo-based tools, enabling quantitative, interpretable, and reproducible biomechanical analysis of humanoid locomotion. Results demonstrate that despite visually human-like motion generated by modern humanoid controllers, significant biomechanical divergence persists across speeds. Robots exhibit systematic deviations in gait symmetry, energy distribution, and joint coordination, indicating that substantial room remains for improving the biomechanical fidelity and energetic efficiency of humanoid locomotion. This work provides a quantitative benchmark for evaluating humanoid locomotion and offers data and versatile tools to support the development of more human-like and energetically efficient locomotion controllers. The data and code will be made publicly available upon acceptance of the paper.",
        "authors_display": "Wei Chen Team",
        "pdf_url": "http://arxiv.org/abs/2602.21666",
        "code_url": null,
        "date": "2026-02-25",
        "primary_category": "cs.RO",
        "chinese_summary": "鉴于生物与机械结构差异导致腿式机器人难以实现类人运动，且模仿学习简单复制关节轨迹无法捕捉人类运动底层原理的问题，本研究提出了步态发散分析框架（GDAF）。该框架旨在系统量化人类与双足机器人在运动学和动力学上的差异，并将其应用于比较人类和人形机器人在28种行走速度下的步态。研究发布了一个速度连续人形机器人运动数据集和GDAF的开源实现。结果表明，尽管现代人形控制器生成的运动视觉上似人，但在生物力学上仍存在显著差异，如步态对称性、能量分布和关节协调性等方面，提示人形机器人运动的生物力学保真度和能量效率仍有大幅提升空间。"
      },
      {
        "paper_id": "2602.21633",
        "title": "Self-Correcting VLA: Online Action Refinement via Sparse World Imagination",
        "abstract": "Standard vision-language-action (VLA) models rely on fitting statistical data priors, limiting their robust understanding of underlying physical dynamics. Reinforcement learning enhances physical grounding through exploration yet typically relies on external reward signals that remain isolated from the agent's internal states. World action models have emerged as a promising paradigm that integrates imagination and control to enable predictive planning. However, they rely on implicit context modeling, lacking explicit mechanisms for self-improvement. To solve these problems, we propose Self-Correcting VLA (SC-VLA), which achieve self-improvement by intrinsically guiding action refinement through sparse imagination. We first design sparse world imagination by integrating auxiliary predictive heads to forecast current task progress and future trajectory trends, thereby constraining the policy to encode short-term physical evolution. Then we introduce the online action refinement module to reshape progress-dependent dense rewards, adjusting trajectory orientation based on the predicted sparse future states. Evaluations on challenging robot manipulation tasks from simulation benchmarks and real-world settings demonstrate that SC-VLA achieve state-of-the-art performance, yielding the highest task throughput with 16% fewer steps and a 9% higher success rate than the best-performing baselines, alongside a 14% gain in real-world experiments. Code is available at https://github.com/Kisaragi0/SC-VLA.",
        "authors_display": "Heng Tao Shen Team",
        "pdf_url": "http://arxiv.org/abs/2602.21633",
        "code_url": null,
        "date": "2026-02-25",
        "primary_category": "cs.RO",
        "chinese_summary": "针对标准视觉-语言-动作（VLA）模型因拟合统计数据先验而缺乏对物理动力学鲁棒理解，以及世界动作模型在集成想象与控制时缺乏显式自我改进机制的问题，本文提出了自纠正VLA（SC-VLA）模型。该模型通过稀疏想象内在地引导动作细化以实现自我改进：首先，通过整合辅助预测头来预测任务进展和未来轨迹趋势，从而设计稀疏世界想象以编码短期物理演化；其次，引入在线动作细化模块，根据预测的稀疏未来状态调整轨迹方向，重塑依赖进度的密集奖励。在仿真和真实世界机器人操作任务中的评估表明，SC-VLA达到了最先进的性能，以更少步骤和更高成功率完成了任务，并在真实世界实验中获得了显著提升。"
      },
      {
        "paper_id": "2602.21625",
        "title": "Tacmap: Bridging the Tactile Sim-to-Real Gap via Geometry-Consistent Penetration Depth Map",
        "abstract": "Vision-Based Tactile Sensors (VBTS) are essential for achieving dexterous robotic manipulation, yet the tactile sim-to-real gap remains a fundamental bottleneck. Current tactile simulations suffer from a persistent dilemma: simplified geometric projections lack physical authenticity, while high-fidelity Finite Element Methods (FEM) are too computationally prohibitive for large-scale reinforcement learning. In this work, we present Tacmap, a high-fidelity, computationally efficient tactile simulation framework anchored in volumetric penetration depth. Our key insight is to bridge the tactile sim-to-real gap by unifying both domains through a shared deform map representation. Specifically, we compute 3D intersection volumes as depth maps in simulation, while in the real world, we employ an automated data-collection rig to learn a robust mapping from raw tactile images to ground-truth depth maps. By aligning simulation and real-world in this unified geometric space, Tacmap minimizes domain shift while maintaining physical consistency. Quantitative evaluations across diverse contact scenarios demonstrate that Tacmap's deform maps closely mirror real-world measurements. Moreover, we validate the utility of Tacmap through an in-hand rotation task, where a policy trained exclusively in simulation achieves zero-shot transfer to a physical robot.",
        "authors_display": "Xuezhou Zhu Team",
        "pdf_url": "http://arxiv.org/abs/2602.21625",
        "code_url": null,
        "date": "2026-02-25",
        "primary_category": "cs.RO",
        "chinese_summary": "鉴于基于视觉的触觉传感器（VBTS）在灵巧机器人操作中的重要性，以及触觉仿真到现实之间存在的瓶颈——简化的几何投影缺乏真实性而高保真有限元方法计算成本过高，本文提出了Tacmap，一个高保真、计算高效的触觉仿真框架。Tacmap通过将仿真中的3D交集体积计算为深度图，并在现实世界中通过自动化数据收集设备学习原始触觉图像到真实深度图的映射，实现以统一的形变图表示来弥合仿真与现实的鸿沟。在多样接触场景下的定量评估表明，Tacmap的形变图与真实世界测量高度吻合。此外，在仿真中训练的策略成功实现了零样本迁移到物理机器人上，验证了Tacmap的实用性。"
      },
      {
        "paper_id": "2602.21622",
        "title": "ADM-DP: Adaptive Dynamic Modality Diffusion Policy through Vision-Tactile-Graph Fusion for Multi-Agent Manipulation",
        "abstract": "Multi-agent robotic manipulation remains challenging due to the combined demands of coordination, grasp stability, and collision avoidance in shared workspaces. To address these challenges, we propose the Adaptive Dynamic Modality Diffusion Policy (ADM-DP), a framework that integrates vision, tactile, and graph-based (multi-agent pose) modalities for coordinated control. ADM-DP introduces four key innovations. First, an enhanced visual encoder merges RGB and point-cloud features via Feature-wise Linear Modulation (FiLM) modulation to enrich perception. Second, a tactile-guided grasping strategy uses Force-Sensitive Resistor (FSR) feedback to detect insufficient contact and trigger corrective grasp refinement, improving grasp stability. Third, a graph-based collision encoder leverages shared tool center point (TCP) positions of multiple agents as structured kinematic context to maintain spatial awareness and reduce inter-agent interference. Fourth, an Adaptive Modality Attention Mechanism (AMAM) dynamically re-weights modalities according to task context, enabling flexible fusion. For scalability and modularity, a decoupled training paradigm is employed in which agents learn independent policies while sharing spatial information. This maintains low interdependence between agents while retaining collective awareness. Across seven multi-agent tasks, ADM-DP achieves 12-25% performance gains over state-of-the-art baselines. Ablation studies show the greatest improvements in tasks requiring multiple sensory modalities, validating our adaptive fusion strategy and demonstrating its robustness for diverse manipulation scenarios.",
        "authors_display": "Dandan Zhang Team",
        "pdf_url": "http://arxiv.org/abs/2602.21622",
        "code_url": null,
        "date": "2026-02-25",
        "primary_category": "cs.RO",
        "chinese_summary": "面对多智能体机器人操作在协调、抓取稳定性和避碰方面的挑战，本文提出了自适应动态模态扩散策略（ADM-DP）框架。该框架整合了视觉、触觉和基于图的多智能体姿态模态以实现协调控制，并引入四项创新：增强型视觉编码器融合RGB和点云特征、触觉引导抓取策略、基于图的碰撞编码器以维持空间感知、以及自适应模态注意力机制动态加权模态。为实现可伸缩性和模块化，采用了分离式训练范式。在七项多智能体任务中的实验结果显示，ADM-DP比最先进的基线性能提升12-25%，且消融研究验证了其自适应融合策略在需要多模态感知的任务中的鲁棒性。"
      },
      {
        "paper_id": "2602.21450",
        "title": "Constructive Vector Fields for Path Following in Fully-Actuated Systems on Matrix Lie Groups",
        "abstract": "This paper presents a novel vector field strategy for controlling fully-actuated systems on connected matrix Lie groups, ensuring convergence to and traversal along a curve defined on the group. Our approach generalizes our previous work (Rezende et al., 2022) and reduces to it when considering the Lie group of translations in Euclidean space. Since the proofs in Rezende et al. (2022) rely on key properties such as the orthogonality between the convergent and traversal components, we extend these results by leveraging Lie group properties. These properties also allow the control input to be non-redundant, meaning it matches the dimension of the Lie group, rather than the potentially larger dimension of the space in which the group is embedded. This can lead to more practical control inputs in certain scenarios. A particularly notable application of our strategy is in controlling systems on SE(3) -- in this case, the non-redundant input corresponds to the object's mechanical twist -- making it well-suited for controlling objects that can move and rotate freely, such as omnidirectional drones. In this case, we provide an efficient algorithm to compute the vector field. We experimentally validate the proposed method using a robotic manipulator to demonstrate its effectiveness.",
        "authors_display": "Luciano C. A. Pimenta Team",
        "pdf_url": "http://arxiv.org/abs/2602.21450",
        "code_url": null,
        "date": "2026-02-25",
        "primary_category": "cs.RO",
        "chinese_summary": "针对在连通矩阵李群上控制完全驱动系统，确保收敛并沿群上定义的曲线运动的挑战，本文提出了一种新颖的矢量场策略。该方法通过利用李群性质，在泛化现有工作的基础上，实现了非冗余的控制输入，即其维度与李群维度匹配，从而在特定场景下提供更实用的控制输入。值得注意的是，该策略在SE(3)上的应用可使非冗余输入对应于物体的机械螺旋，特别适合控制全向无人机等自由移动和旋转的物体，并提供了一种高效的矢量场计算算法。实验验证了所提方法使用机器人操纵器控制的有效性。"
      },
      {
        "paper_id": "2602.22344",
        "title": "Spatiotemporal modulation of surface texture for information encoding and object manipulation",
        "abstract": "Dynamically tunable surface textures offer a powerful route to spatiotemporally regulate surface and interfacial properties, enabling emerging applications ranging from adaptive optics to soft robotic manipulation. However, achieving programmable, reversible, and spatiotemporal modulation of surface texture remains a fundamental challenge. Here, we present a photothermal-actuated liquid crystal elastomer bilayer that enables reversible, on-demand spatiotemporal modulation of surface textures through dynamically emerging and propagating wrinkles. Using direct laser writing or projected light fields, programmable and self-erasable wrinkle patterns are generated for dynamic information encoding. This spatiotemporal wrinkling enables object manipulation across diverse geometries, including uphill transport and navigation along predesigned paths. By coupling wrinkle-driven motion with thermally reversible dynamic bonding, the bilayer further enables assembly and disassembly of dynamic polymers, as well as cargo transportation. This work demonstrates spatiotemporally programmable wrinkling as a powerful mechanism for dynamic modulation of surface textures, establishing a versatile platform for multifunctional and reconfigurable smart surfaces.",
        "authors_display": "Ruike Renee Zhao Team",
        "pdf_url": "http://arxiv.org/abs/2602.22344",
        "code_url": null,
        "date": "2026-02-25",
        "primary_category": "physics.app-ph",
        "chinese_summary": "动态可调谐表面纹理在自适应光学和软机器人操纵中具有广泛应用前景，但实现可编程、可逆且时空调制的表面纹理仍是挑战。本文提出一种光热驱动的液晶弹性体双层结构，通过动态产生和传播的褶皱，实现可逆、按需的时空表面纹理调制。利用激光写入或投影光场，可生成可编程、自擦除的褶皱图案，实现动态信息编码。这种时空褶皱能驱动物体在不同几何形状上移动，并结合热可逆动态键合实现聚合物的组装与拆卸及货物运输。这项工作展示了时空可编程褶皱作为动态调制表面纹理的强大机制，为多功能和可重构智能表面提供了通用平台。"
      },
      {
        "paper_id": "2602.21161",
        "title": "ActionReasoning: Robot Action Reasoning in 3D Space with LLM for Robotic Brick Stacking",
        "abstract": "Classical robotic systems typically rely on custom planners designed for constrained environments. While effective in restricted settings, these systems lack generalization capabilities, limiting the scalability of embodied AI and general-purpose robots. Recent data-driven Vision-Language-Action (VLA) approaches aim to learn policies from large-scale simulation and real-world data. However, the continuous action space of the physical world significantly exceeds the representational capacity of linguistic tokens, making it unclear if scaling data alone can yield general robotic intelligence. To address this gap, we propose ActionReasoning, an LLM-driven framework that performs explicit action reasoning to produce physics-consistent, prior-guided decisions for robotic manipulation. ActionReasoning leverages the physical priors and real-world knowledge already encoded in Large Language Models (LLMs) and structures them within a multi-agent architecture. We instantiate this framework on a tractable case study of brick stacking, where the environment states are assumed to be already accurately measured. The environmental states are then serialized and passed to a multi-agent LLM framework that generates physics-aware action plans. The experiments demonstrate that the proposed multi-agent LLM framework enables stable brick placement while shifting effort from low-level domain-specific coding to high-level tool invocation and prompting, highlighting its potential for broader generalization. This work introduces a promising approach to bridging perception and execution in robotic manipulation by integrating physical reasoning with LLMs.",
        "authors_display": "Brian Sheil Team",
        "pdf_url": "http://arxiv.org/abs/2602.21161",
        "code_url": null,
        "date": "2026-02-24",
        "primary_category": "cs.RO",
        "chinese_summary": "传统机器人系统在泛化能力上受限，而大规模VLA（视觉-语言-动作）方法因物理世界连续动作空间的复杂性，难以仅通过数据扩展实现通用机器人智能。为弥补这一差距，本文提出了ActionReasoning框架，它利用大型语言模型（LLM）中编码的物理先验和真实世界知识，通过多智能体架构进行显式动作推理，生成符合物理原理的机器人操作决策。在砖块堆叠任务上的案例研究表明，该框架能实现稳定放置，并将开发重心从低级编码转向高级工具调用和提示，展现了其在泛化方面的潜力。"
      },
      {
        "paper_id": "2602.21157",
        "title": "HALO: A Unified Vision-Language-Action Model for Embodied Multimodal Chain-of-Thought Reasoning",
        "abstract": "Vision-Language-Action (VLA) models have shown strong performance in robotic manipulation, but often struggle in long-horizon or out-of-distribution scenarios due to the lack of explicit mechanisms for multimodal reasoning and anticipating how the world will evolve under action. Recent works introduce textual chain-of-thought or visual subgoal prediction within VLA models to reason, but still fail to offer a unified human-like reasoning framework for joint textual reasoning, visual foresight, and action prediction. To this end, we propose HALO, a unified VLA model that enables embodied multimodal chain-of-thought (EM-CoT) reasoning through a sequential process of textual task reasoning, visual subgoal prediction for fine-grained guidance, and EM-CoT-augmented action prediction. We instantiate HALO with a Mixture-of-Transformers (MoT) architecture that decouples semantic reasoning, visual foresight, and action prediction into specialized experts while allowing seamless cross-expert collaboration. To enable HALO learning at scale, we introduce an automated pipeline to synthesize EM-CoT training data along with a carefully crafted training recipe. Extensive experiments demonstrate that: (1) HALO achieves superior performance in both simulated and real-world environments, surpassing baseline policy pi_0 by 34.1% on RoboTwin benchmark; (2) all proposed components of the training recipe and EM-CoT design help improve task success rate; and (3) HALO exhibits strong generalization capabilities under aggressive unseen environmental randomization with our proposed EM-CoT reasoning.",
        "authors_display": "Song Guo Team",
        "pdf_url": "http://arxiv.org/abs/2602.21157",
        "code_url": null,
        "date": "2026-02-24",
        "primary_category": "cs.RO",
        "chinese_summary": "VLA模型在机器人操作中表现良好，但在长周期或分布外场景中，由于缺乏多模态推理和对未来状态的预测能力而面临挑战。本文提出了HALO模型，一个统一的VLA框架，通过具身多模态链式思维（EM-CoT）实现人类般的推理过程，包括文本任务推理、视觉子目标预测和EM-CoT增强的动作预测。HALO采用混合Transformer架构将不同推理模块解耦并协同工作，并引入自动化流程合成训练数据。实验结果表明，HALO在模拟和真实环境中均表现优异，显著提升了任务成功率，并展现了在未知环境随机化下的强大泛化能力。"
      },
      {
        "paper_id": "2602.21020",
        "title": "Matching Multiple Experts: On the Exploitability of Multi-Agent Imitation Learning",
        "abstract": "Multi-agent imitation learning (MA-IL) aims to learn optimal policies from expert demonstrations of interactions in multi-agent interactive domains. Despite existing guarantees on the performance of the resulting learned policies, characterizations of how far the learned polices are from a Nash equilibrium are missing for offline MA-IL. In this paper, we demonstrate impossibility and hardness results of learning low-exploitable policies in general $n$-player Markov Games. We do so by providing examples where even exact measure matching fails, and demonstrating a new hardness result on characterizing the Nash gap given a fixed measure matching error. We then show how these challenges can be overcome using strategic dominance assumptions on the expert equilibrium. Specifically, for the case of dominant strategy expert equilibria, assuming Behavioral Cloning error $ε_{\\text{BC}}$, this provides a Nash imitation gap of $\\mathcal{O}\\left(nε_{\\text{BC}}/(1-γ)^2\\right)$ for a discount factor $γ$. We generalize this result with a new notion of best-response continuity, and argue that this is implicitly encouraged by standard regularization techniques.",
        "authors_display": "Negar Mehr Team",
        "pdf_url": "http://arxiv.org/abs/2602.21020",
        "code_url": null,
        "date": "2026-02-24",
        "primary_category": "cs.LG",
        "chinese_summary": "多智能体模仿学习（MA-IL）旨在从专家演示中学习最优策略，但目前缺乏对离线MA-IL中学习策略与纳什均衡之间距离的量化分析。本文首先证明了在通用n玩家马尔可夫博弈中学习低可利用策略的困难性与不可能性。随后，通过对专家均衡施加战略主导性假设，克服了这些挑战。研究指出，在主导策略专家均衡下，若行为克隆误差为$ε_{\\text{BC}}$，则纳什模仿间隙为$\\\\mathcal{O}\\\\left(nε_{\\\\text{BC}}/(1-γ)^2\\\\right)$，并通过引入最佳响应连续性概念进一步推广了这一结果，这为理解模仿学习的理论界限提供了重要见解。"
      },
      {
        "paper_id": "2602.20715",
        "title": "IG-RFT: An Interaction-Guided RL Framework for VLA Models in Long-Horizon Robotic Manipulation",
        "abstract": "Vision-Language-Action (VLA) models have demonstrated significant potential for generalist robotic policies; however, they struggle to generalize to long-horizon complex tasks in novel real-world domains due to distribution shifts and the scarcity of high-quality demonstrations. Although reinforcement learning (RL) offers a promising avenue for policy improvement, applying it to real-world VLA fine-tuning faces challenges regarding exploration efficiency, training stability, and sample cost. To address these issues, we propose IG-RFT, a novel Interaction-Guided Reinforced Fine-Tuning system designed for flow-based VLA models. Firstly, to facilitate effective policy optimization, we introduce Interaction-Guided Advantage Weighted Regression (IG-AWR), an RL algorithm that dynamically modulates exploration intensity based on the robot's interaction status. Furthermore, to address the limitations of sparse or task-specific rewards, we design a novel hybrid dense reward function that integrates the trajectory-level reward and the subtask-level reward. Finally, we construct a three-stage RL system comprising SFT, Offline RL, and Human-in-the-Loop RL for fine-tuning VLA models. Extensive real-world experiments on four challenging long-horizon tasks demonstrate that IG-RFT achieves an average success rate of 85.0%, significantly outperforming SFT (18.8%) and standard Offline RL baselines (40.0%). Ablation studies confirm the critical contributions of IG-AWR and hybrid reward shaping. In summary, our work establishes and validates a novel reinforced fine-tuning system for VLA models in real-world robotic manipulation.",
        "authors_display": "Huixu Dong Team",
        "pdf_url": "http://arxiv.org/abs/2602.20715",
        "code_url": null,
        "date": "2026-02-24",
        "primary_category": "cs.RO",
        "chinese_summary": "VLA模型虽在通用机器人策略上潜力巨大，但在真实世界长周期复杂任务中因分布偏移和高质量演示稀缺而泛化能力不足。强化学习（RL）虽能改进策略，但在VLA模型微调中面临探索效率、训练稳定性和样本成本挑战。本文提出了IG-RFT系统，针对基于流的VLA模型进行强化微调。该系统引入了交互引导优势加权回归（IG-AWR）算法来动态调节探索强度，设计了结合轨迹级和子任务级奖励的混合密集奖励函数，并构建了包括SFT、离线RL和人机交互RL的三阶段微调系统。真实世界实验表明，IG-RFT显著提高了长周期任务的成功率，并验证了其核心组件的有效性。"
      },
      {
        "paper_id": "2602.20566",
        "title": "BFA++: Hierarchical Best-Feature-Aware Token Prune for Multi-View Vision Language Action Model",
        "abstract": "Vision-Language-Action (VLA) models have achieved significant breakthroughs by leveraging Large Vision Language Models (VLMs) to jointly interpret instructions and visual inputs. However, the substantial increase in visual tokens, particularly from multi-view inputs, poses serious challenges to real-time robotic manipulation. Existing acceleration techniques for VLMs, such as token pruning, often result in degraded performance when directly applied to VLA models, as they overlook the relationships between different views and fail to account for the dynamic and task-specific characteristics of robotic operation. To address this, we propose BFA++, a dynamic token pruning framework designed specifically for VLA models. BFA++ introduces a hierarchical pruning strategy guided by two-level importance predictors: an intra-view predictor highlights task-relevant regions within each image to suppress spatial noise, while an inter-view predictor identifies critical camera views throughout different manipulation phases to reduce cross-view redundancy. This design enables efficient token selection while preserving essential visual cues, resulting in improved computational efficiency and higher manipulation success rates. Evaluations on the RoboTwin benchmark and real-world robotic tasks demonstrate that BFA++ consistently outperforms existing methods. BFA++ improves the success rate by about 10% on both the π0 and RDT models, achieving speedup of 1.8X and 1.5X, respectively. Our results highlight that context-sensitive and task-aware token pruning serves as a more effective strategy than full visual processing, enabling faster inference and improved manipulation accuracy in real-world robotic systems.",
        "authors_display": "Hua Chen Team",
        "pdf_url": "http://arxiv.org/abs/2602.20566",
        "code_url": null,
        "date": "2026-02-24",
        "primary_category": "cs.RO",
        "chinese_summary": "VLA模型通过利用大型视觉语言模型实现了显著突破，但多视图输入带来的海量视觉令牌对实时机器人操作构成严峻挑战。现有令牌剪枝技术直接应用于VLA模型时常导致性能下降，因其忽视了不同视图关系及机器人操作的动态性。为解决此问题，本文提出了BFA++动态令牌剪枝框架，它采用两级重要性预测器引导的层次化剪枝策略：视图内预测器聚焦图像内任务相关区域以抑制空间噪声，视图间预测器识别不同操作阶段的关键相机视图以减少跨视图冗余。实验结果表明，BFA++在提高计算效率的同时，显著提升了操作成功率，实现了约1.8倍和1.5倍的速度提升，并验证了上下文敏感和任务感知剪枝策略的有效性。"
      },
      {
        "paper_id": "2602.20517",
        "title": "Inner Speech as Behavior Guides: Steerable Imitation of Diverse Behaviors for Human-AI coordination",
        "abstract": "Effective human-AI coordination requires artificial agents capable of exhibiting and responding to human-like behaviors while adapting to changing contexts. Imitation learning has emerged as one of the prominent approaches to build such agents by training them to mimic human-demonstrated behaviors. However, current methods struggle to capture the inherent diversity and non-Markovian nature of human behavior and lack the ability to steer behavior at inference time. Drawing inspiration from the theory of human cognitive processes, where inner speech guides action selection before execution, we propose MIMIC (Modeling Inner Motivations for Imitation and Control), a framework that uses language as an internal representation of behavioral intent. MIMIC employs the novel use of vision-language models as linguistic scaffolding to train a conditional variational autoencoder capable of generating inner speech from observations. A diffusion-based behavior cloning policy then selects actions conditioned on current observations and the generated inner speech. MIMIC enables fine-grained steering of behavior at inference time by conditioning the agent on behavior-specific speech. Experiments across robotic manipulation tasks and human-AI collaboration games demonstrate that MIMIC significantly enhances both behavior diversity and fidelity to human demonstrations while enabling nuanced behavioral steering without training on additional demonstrations. We open source our code and provide pre-trained MIMIC agents and qualitative demos at: https://mimic-research.github.io.",
        "authors_display": "David C Parkes Team",
        "pdf_url": "http://arxiv.org/abs/2602.20517",
        "code_url": null,
        "date": "2026-02-24",
        "primary_category": "cs.AI",
        "chinese_summary": "有效的人机协作要求AI代理能展现并响应类人行为，同时适应多变上下文。模仿学习虽旨在模仿人类行为，但现有方法难以捕捉人类行为多样性、非马尔可夫性质，且缺乏推理时的行为引导能力。受人类内心独白指导行动的启发，本文提出了MIMIC框架，利用语言作为行为意图的内部表示。MIMIC创新性地使用视觉语言模型作为语言支架，训练生成“内心独白”的条件变分自编码器，并结合扩散行为克隆策略，根据观察和生成的独白选择行动。实验证明，MIMIC显著增强了行为多样性和对人类演示的忠实度，且无需额外训练即可实现精细的行为引导。"
      },
      {
        "paper_id": "2602.21445",
        "title": "VLA Knows Its Limits",
        "abstract": "Action chunking has recently emerged as a standard practice in flow-based Vision-Language-Action (VLA) models. However, the effect and choice of the execution horizon - the number of actions to be executed from each predicted chunk - remains underexplored. In this work, we first show that varying the execution horizon leads to substantial performance deviations, with performance initially improving and then declining as the horizon increases. To uncover the reasons, we analyze the cross- and self-attention weights in flow-based VLAs and reveal two key phenomena: (i) intra-chunk actions attend invariantly to vision-language tokens, limiting adaptability to environmental changes; and (ii) the initial and terminal action tokens serve as stable anchors, forming latent centers around which intermediate actions are organized. Motivated by these insights, we interpret action self-attention weights as a proxy for the model's predictive limit and propose AutoHorizon, the first test-time method that dynamically estimates the execution horizon for each predicted action chunk to adapt to changing perceptual conditions. Across simulated and real-world robotic manipulation tasks, AutoHorizon is performant, incurs negligible computational overhead, and generalizes across diverse tasks and flow-based models.",
        "authors_display": "Gaowen Liu Team",
        "pdf_url": "http://arxiv.org/abs/2602.21445",
        "code_url": "https://hatchetproject.github.io/autohorizon/",
        "date": "2026-02-24",
        "primary_category": "cs.RO",
        "chinese_summary": "鉴于动作分块在基于流的视觉-语言-动作（VLA）模型中已成常规做法，但执行范围（从预测块中执行的动作数量）的影响和选择仍未被充分探索，本研究首先揭示了执行范围变化会导致显著的性能波动。通过分析流式VLA模型中的注意力权重，发现块内动作对视觉-语言token的注意力不变性限制了环境适应性，且初始和终止动作token是稳定的锚点。受此启发，本文将动作自注意权重作为模型预测极限的代理，提出了AutoHorizon，这是一种在测试时动态估计每个预测动作块执行范围以适应感知条件的方法。实验结果表明，AutoHorizon在仿真和真实机器人操作任务中表现出色，计算开销可忽略，并能泛化到多样任务和流式模型。"
      },
      {
        "paper_id": "2602.21429",
        "title": "Provably Safe Generative Sampling with Constricting Barrier Functions",
        "abstract": "Flow-based generative models, such as diffusion models and flow matching models, have achieved remarkable success in learning complex data distributions. However, a critical gap remains for their deployment in safety-critical domains: the lack of formal guarantees that generated samples will satisfy hard constraints. We address this by proposing a safety filtering framework that acts as an online shield for any pre-trained generative model. Our key insight is to cooperate with the generative process rather than override it. We define a constricting safety tube that is relaxed at the initial noise distribution and progressively tightens to the target safe set at the final data distribution, mirroring the coarse-to-fine structure of the generative process itself. By characterizing this tube via Control Barrier Functions (CBFs), we synthesize a feedback control input through a convex Quadratic Program (QP) at each sampling step. As the tube is loosest when noise is high and intervention is cheapest in terms of control energy, most constraint enforcement occurs when it least disrupts the model's learned structure. We prove that this mechanism guarantees safe sampling while minimizing the distributional shift from the original model at each sampling step, as quantified by the KL divergence. Our framework applies to any pre-trained flow-based generative scheme requiring no retraining or architectural modifications. We validate the approach across constrained image generation, physically-consistent trajectory sampling, and safe robotic manipulation policies, achieving 100% constraint satisfaction while preserving semantic fidelity.",
        "authors_display": "Fabio Pasqualetti Team",
        "pdf_url": "http://arxiv.org/abs/2602.21429",
        "code_url": null,
        "date": "2026-02-24",
        "primary_category": "cs.LG",
        "chinese_summary": "流式生成模型在处理复杂数据分布时表现出色，但在安全关键领域缺乏对生成样本满足硬约束的正式保证。针对此问题，本文提出了一个安全过滤框架，其核心思想是构建一个在初始噪声分布时宽松、在最终数据分布时收紧的“收缩安全管”，并通过控制屏障函数（CBFs）和凸二次规划（QP）在采样过程中动态施加控制。实验证明，该框架在不重新训练模型的情况下，可保证100%满足约束条件，同时最小化对原始模型分布的扰动，并在约束图像生成、物理轨迹采样和机器人操作中取得了成功。"
      },
      {
        "paper_id": "2602.20057",
        "title": "AdaWorldPolicy: World-Model-Driven Diffusion Policy with Online Adaptive Learning for Robotic Manipulation",
        "abstract": "Effective robotic manipulation requires policies that can anticipate physical outcomes and adapt to real-world environments. Effective robotic manipulation requires policies that can anticipate physical outcomes and adapt to real-world environments. In this work, we introduce a unified framework, World-Model-Driven Diffusion Policy with Online Adaptive Learning (AdaWorldPolicy) to enhance robotic manipulation under dynamic conditions with minimal human involvement. Our core insight is that world models provide strong supervision signals, enabling online adaptive learning in dynamic environments, which can be complemented by force-torque feedback to mitigate dynamic force shifts. Our AdaWorldPolicy integrates a world model, an action expert, and a force predictor-all implemented as interconnected Flow Matching Diffusion Transformers (DiT). They are interconnected via the multi-modal self-attention layers, enabling deep feature exchange for joint learning while preserving their distinct modularity characteristics. We further propose a novel Online Adaptive Learning (AdaOL) strategy that dynamically switches between an Action Generation mode and a Future Imagination mode to drive reactive updates across all three modules. This creates a powerful closed-loop mechanism that adapts to both visual and physical domain shifts with minimal overhead. Across a suite of simulated and real-robot benchmarks, our AdaWorldPolicy achieves state-of-the-art performance, with dynamical adaptive capacity to out-of-distribution scenarios.",
        "authors_display": "Dong Xu Team",
        "pdf_url": "http://arxiv.org/abs/2602.20057",
        "code_url": "https://AdaWorldPolicy.github.io",
        "date": "2026-02-23",
        "primary_category": "cs.RO",
        "chinese_summary": "有效的机器人操作需要策略能够预测物理结果并适应真实环境，尤其是在动态条件下如何减少人工干预是一个挑战。本文提出了AdaWorldPolicy，一个统一的、由世界模型驱动的在线自适应学习扩散策略框架，以增强动态条件下的机器人操作。该框架将世界模型、动作专家和力预测器集成为相互连接的流匹配扩散Transformer，通过多模态自注意力层实现深度特征交换。此外，引入了在线自适应学习（AdaOL）策略，动态切换“动作生成”与“未来想象”模式，驱动模块实时更新，以适应视觉和物理领域的变化。在多项模拟和真实机器人基准测试中，AdaWorldPolicy取得了领先性能，展现了强大的动态自适应能力。"
      },
      {
        "paper_id": "2602.19930",
        "title": "Beyond Mimicry: Toward Lifelong Adaptability in Imitation Learning",
        "abstract": "Imitation learning stands at a crossroads: despite decades of progress, current imitation learning agents remain sophisticated memorisation machines, excelling at replay but failing when contexts shift or goals evolve. This paper argues that this failure is not technical but foundational: imitation learning has been optimised for the wrong objective. We propose a research agenda that redefines success from perfect replay to compositional adaptability. Such adaptability hinges on learning behavioural primitives once and recombining them through novel contexts without retraining. We establish metrics for compositional generalisation, propose hybrid architectures, and outline interdisciplinary research directions drawing on cognitive science and cultural evolution. Agents that embed adaptability at the core of imitation learning thus have an essential capability for operating in an open-ended world.",
        "authors_display": "Odinaldo Rodrigues Team",
        "pdf_url": "http://arxiv.org/abs/2602.19930",
        "code_url": null,
        "date": "2026-02-23",
        "primary_category": "cs.AI",
        "chinese_summary": "模仿学习（IL）虽进展显著，但当前代理仍停留在记忆重放层面，在上下文变化或目标演变时表现不佳。本文指出这种失败是基础性的，并提出了一个研究议程，将模仿学习的成功目标重新定义为“组合适应性”而非完美重放。这种适应性要求代理能够一次性学习行为原语，并在新颖的上下文中无需重新训练即可进行重组。为此，文章提出了组合泛化指标、混合架构以及借鉴认知科学和文化演进的跨学科研究方向，强调将适应性嵌入模仿学习核心是未来在开放世界中操作的关键能力。"
      },
      {
        "paper_id": "2602.19400",
        "title": "Hilbert-Augmented Reinforcement Learning for Scalable Multi-Robot Coverage and Exploration",
        "abstract": "We present a coverage framework that integrates Hilbert space-filling priors into decentralized multi-robot learning and execution. We augment DQN and PPO with Hilbert-based spatial indices to structure exploration and reduce redundancy in sparse-reward environments, and we evaluate scalability in multi-robot grid coverage. We further describe a waypoint interface that converts Hilbert orderings into curvature-bounded, time-parameterized SE(2) trajectories (planar (x, y, θ)), enabling onboard feasibility on resource-constrained robots. Experiments show improvements in coverage efficiency, redundancy, and convergence speed over DQN/PPO baselines. In addition, we validate the approach on a Boston Dynamics Spot legged robot, executing the generated trajectories in indoor environments and observing reliable coverage with low redundancy. These results indicate that geometric priors improve autonomy and scalability for swarm and legged robotics.",
        "authors_display": "Aryya Gangopadhyay Team",
        "pdf_url": "http://arxiv.org/abs/2602.19400",
        "code_url": null,
        "date": "2026-02-23",
        "primary_category": "cs.RO",
        "chinese_summary": "多机器人系统在稀疏奖励环境中进行探索和覆盖时常面临效率和冗余问题。为解决此问题，本文提出了一个覆盖框架，将希尔伯特空间填充先验整合到去中心化多机器人学习和执行中。该框架通过希尔伯特空间索引增强DQN和PPO，以结构化探索并减少冗余，并设计了路径点接口将希尔伯特排序转换为可执行的SE(2)轨迹。实验结果表明，该方法显著提高了覆盖效率、降低了冗余并加速了收敛。在Boston Dynamics Spot腿足机器人上的验证进一步证实了其在室内环境中实现可靠且低冗余覆盖的有效性。"
      },
      {
        "paper_id": "2602.20225",
        "title": "FACTO: Function-space Adaptive Constrained Trajectory Optimization for Robotic Manipulators",
        "abstract": "This paper introduces Function-space Adaptive Constrained Trajectory Optimization (FACTO), a new trajectory optimization algorithm for both single- and multi-arm manipulators. Trajectory representations are parameterized as linear combinations of orthogonal basis functions, and optimization is performed directly in the coefficient space. The constrained problem formulation consists of both an objective functional and a finite-dimensional objective defined over truncated coefficients. To address nonlinearity, FACTO uses a Gauss-Newton approximation with exponential moving averaging, yielding a smoothed quadratic subproblem. Trajectory-wide constraints are addressed using coefficient-space mappings, and an adaptive constrained update using the Levenberg-Marquardt algorithm is performed in the null space of active constraints. Comparisons with optimization-based planners (CHOMP, TrajOpt, GPMP2) and sampling-based planners (RRT-Connect, RRT*, PRM) show the improved solution quality and feasibility, especially in constrained single- and multi-arm scenarios. The experimental evaluation of FACTO on Franka robots verifies the feasibility of deployment.",
        "authors_display": "Minghui Zheng Team",
        "pdf_url": "http://arxiv.org/abs/2602.20225",
        "code_url": null,
        "date": "2026-02-23",
        "primary_category": "cs.RO",
        "chinese_summary": "本文介绍了一种新的轨迹优化算法FACTO（Function-space Adaptive Constrained Trajectory Optimization），旨在解决单臂和多臂机械手的轨迹规划问题。该方法将轨迹参数化为正交基函数的线性组合，并在系数空间中直接进行优化。为处理非线性约束，FACTO采用了高斯-牛顿近似和指数移动平均，生成平滑的二次子问题，并通过系数空间映射和Levenberg-Marquardt算法在活跃约束的零空间执行自适应更新。实验结果表明，FACTO在有约束的单臂和多臂场景中，相较于其他优化和采样规划器，显著提高了解决方案的质量和可行性，并在Franka机器人上验证了其部署能力。"
      },
      {
        "paper_id": "2602.19372",
        "title": "Seeing Farther and Smarter: Value-Guided Multi-Path Reflection for VLM Policy Optimization",
        "abstract": "Solving complex, long-horizon robotic manipulation tasks requires a deep understanding of physical interactions, reasoning about their long-term consequences, and precise high-level planning. Vision-Language Models (VLMs) offer a general perceive-reason-act framework for this goal. However, previous approaches using reflective planning to guide VLMs in correcting actions encounter significant limitations. These methods rely on inefficient and often inaccurate implicit learning of state-values from noisy foresight predictions, evaluate only a single greedy future, and suffer from substantial inference latency. To address these limitations, we propose a novel test-time computation framework that decouples state evaluation from action generation. This provides a more direct and fine-grained supervisory signal for robust decision-making. Our method explicitly models the advantage of an action plan, quantified by its reduction in distance to the goal, and uses a scalable critic to estimate. To address the stochastic nature of single-trajectory evaluation, we employ beam search to explore multiple future paths and aggregate them during decoding to model their expected long-term returns, leading to more robust action generation. Additionally, we introduce a lightweight, confidence-based trigger that allows for early exit when direct predictions are reliable, invoking reflection only when necessary. Extensive experiments on diverse, unseen multi-stage robotic manipulation tasks demonstrate a 24.6% improvement in success rate over state-of-the-art baselines, while significantly reducing inference time by 56.5%.",
        "authors_display": "Dimitris N. Metaxas Team",
        "pdf_url": "http://arxiv.org/abs/2602.19372",
        "code_url": null,
        "date": "2026-02-22",
        "primary_category": "cs.RO",
        "chinese_summary": "解决复杂、长期的机器人操作任务需要精确规划和对物理交互的深刻理解。现有VLM反射式规划方法存在效率低下、依赖隐式学习和推理延迟等问题。为此，本研究提出一种新的测试时计算框架，将状态评估与行动生成解耦，通过显式建模行动计划的优势并使用束搜索聚合多条未来路径来生成更鲁棒的行动。该方法在多阶段机器人操作任务上将成功率提高了24.6%，并显著减少了56.5%的推理时间。"
      },
      {
        "paper_id": "2602.19260",
        "title": "The Price Is Not Right: Neuro-Symbolic Methods Outperform VLAs on Structured Long-Horizon Manipulation Tasks with Significantly Lower Energy Consumption",
        "abstract": "Vision-Language-Action (VLA) models have recently been proposed as a pathway toward generalist robotic policies capable of interpreting natural language and visual inputs to generate manipulation actions. However, their effectiveness and efficiency on structured, long-horizon manipulation tasks remain unclear. In this work, we present a head-to-head empirical comparison between a fine-tuned open-weight VLA model π0 and a neuro-symbolic architecture that combines PDDL-based symbolic planning with learned low-level control. We evaluate both approaches on structured variants of the Towers of Hanoi manipulation task in simulation while measuring both task performance and energy consumption during training and execution. On the 3-block task, the neuro-symbolic model achieves 95% success compared to 34% for the best-performing VLA. The neuro-symbolic model also generalizes to an unseen 4-block variant (78% success), whereas both VLAs fail to complete the task. During training, VLA fine-tuning consumes nearly two orders of magnitude more energy than the neuro-symbolic approach. These results highlight important trade-offs between end-to-end foundation-model approaches and structured reasoning architectures for long-horizon robotic manipulation, emphasizing the role of explicit symbolic structure in improving reliability, data efficiency, and energy efficiency. Code and models are available at https://price-is-not-right.github.io",
        "authors_display": "Matthias Scheutz Team",
        "pdf_url": "http://arxiv.org/abs/2602.19260",
        "code_url": null,
        "date": "2026-02-22",
        "primary_category": "cs.RO",
        "chinese_summary": "视觉-语言-行动（VLA）模型在通用机器人策略方面展现潜力，但其在结构化、长程操作任务上的有效性和效率尚不明确。本研究对比了微调的VLA模型π0与结合PDDL符号规划和低层控制的神经-符号架构在汉诺塔任务中的表现。结果显示，神经-符号模型成功率（95%）远高于VLA模型（34%），且能更好地泛化到未见任务，同时训练能耗显著低于VLA模型。这强调了显式符号结构在提升机器人操作的可靠性、数据效率和能效方面的重要性。"
      },
      {
        "paper_id": "2602.19184",
        "title": "Human-to-Robot Interaction: Learning from Video Demonstration for Robot Imitation",
        "abstract": "Learning from Demonstration (LfD) offers a promising paradigm for robot skill acquisition. Recent approaches attempt to extract manipulation commands directly from video demonstrations, yet face two critical challenges: (1) general video captioning models prioritize global scene features over task-relevant objects, producing descriptions unsuitable for precise robotic execution, and (2) end-to-end architectures coupling visual understanding with policy learning require extensive paired datasets and struggle to generalize across objects and scenarios. To address these limitations, we propose a novel ``Human-to-Robot'' imitation learning pipeline that enables robots to acquire manipulation skills directly from unstructured video demonstrations, inspired by the human ability to learn by watching and imitating. Our key innovation is a modular framework that decouples the learning process into two distinct stages: (1) Video Understanding, which combines Temporal Shift Modules (TSM) with Vision-Language Models (VLMs) to extract actions and identify interacted objects, and (2) Robot Imitation, which employs TD3-based deep reinforcement learning to execute the demonstrated manipulations. We validated our approach in PyBullet simulation environments with a UR5e manipulator and in a real-world experiment with a UF850 manipulator across four fundamental actions: reach, pick, move, and put. For video understanding, our method achieves 89.97% action classification accuracy and BLEU-4 scores of 0.351 on standard objects and 0.265 on novel objects, representing improvements of 76.4% and 128.4% over the best baseline, respectively. For robot manipulation, our framework achieves an average success rate of 87.5% across all actions, with 100% success on reaching tasks and up to 90% on complex pick-and-place operations. The project website is available at https://thanhnguyencanh.github.io/LfD4hri.",
        "authors_display": "Xiem HoangVan Team",
        "pdf_url": "http://arxiv.org/abs/2602.19184",
        "code_url": null,
        "date": "2026-02-22",
        "primary_category": "cs.RO",
        "chinese_summary": "现有从视频中学习机器人操作的方法面临视频描述不精确和难以泛化的挑战。本研究提出一种“人到机器人”模仿学习新管道，解耦为视频理解和机器人模仿两个阶段。视频理解阶段结合时移模块和视觉-语言模型提取动作和识别对象；机器人模仿阶段利用TD3深度强化学习执行操作。实验表明，该方法在动作分类和对象识别方面显著优于基线，并在模拟和真实机器人操作中取得了87.5%的平均成功率，在复杂任务中表现良好。"
      },
      {
        "paper_id": "2602.20200",
        "title": "Global Prior Meets Local Consistency: Dual-Memory Augmented Vision-Language-Action Model for Efficient Robotic Manipulation",
        "abstract": "Hierarchical Vision-Language-Action (VLA) models have rapidly become a dominant paradigm for robotic manipulation. It typically comprising a Vision-Language backbone for perception and understanding, together with a generative policy for action generation. However, its performance is increasingly bottlenecked by the action generation proceess. (i) Low inference efficiency. A pronounced distributional gap between isotropic noise priors and target action distributions, which increases denoising steps and the incidence of infeasible samples. (ii) Poor robustness. Existing policies condition solely on the current observation, neglecting the constraint of history sequence and thus lacking awareness of task progress and temporal consistency. To address these issues, we introduce OptimusVLA, a dual-memory VLA framework with Global Prior Memory (GPM) and Local Consistency Memory (LCM). GPM replaces Gaussian noise with task-level priors retrieved from semantically similar trajectories, thereby shortening the generative path and reducing the umber of function evaluations (NFE). LCM dynamically models executed action sequence to infer task progress and injects a learned consistency constraint that enforces temporal coherence and smoothness of trajectory. Across three simulation benchmarks, OptimusVLA consistently outperforms strong baselines: it achieves 98.6% average success rate on LIBERO, improves over pi_0 by 13.5% on CALVIN, and attains 38% average success rate on RoboTwin 2.0 Hard. In Real-World evaluation, OptimusVLA ranks best on Generalization and Long-horizon suites, surpassing pi_0 by 42.9% and 52.4%, respectively, while delivering 2.9x inference speedup.",
        "authors_display": "Liqiang Nie Team",
        "pdf_url": "http://arxiv.org/abs/2602.20200",
        "code_url": null,
        "date": "2026-02-22",
        "primary_category": "cs.RO",
        "chinese_summary": "分层视觉-语言-行动（VLA）模型在机器人操作中面临推理效率低和鲁棒性差的瓶颈。为解决这些问题，本研究引入OptimusVLA，一个采用全局先验记忆（GPM）和局部一致性记忆（LCM）的双记忆VLA框架。GPM通过任务级先验取代高斯噪声，缩短生成路径；LCM动态建模行动序列并注入一致性约束以增强时间连贯性。OptimusVLA在三个模拟基准测试中表现优异，并在真实世界任务中实现了更高的成功率和2.9倍的推理加速。"
      },
      {
        "paper_id": "2602.18967",
        "title": "TactEx: An Explainable Multimodal Robotic Interaction Framework for Human-Like Touch and Hardness Estimation",
        "abstract": "Accurate perception of object hardness is essential for safe and dexterous contact-rich robotic manipulation. Here, we present TactEx, an explainable multimodal robotic interaction framework that unifies vision, touch, and language for human-like hardness estimation and interactive guidance. We evaluate TactEx on fruit-ripeness assessment, a representative task that requires both tactile sensing and contextual understanding. The system fuses GelSight-Mini tactile streams with RGB observations and language prompts. A ResNet50+LSTM model estimates hardness from sequential tactile data, while a cross-modal alignment module combines visual cues with guidance from a large language model (LLM). This explainable multimodal interface allows users to distinguish ripeness levels with statistically significant class separation (p < 0.01 for all fruit pairs). For touch placement, we compare YOLO with Grounded-SAM (GSAM) and find GSAM to be more robust for fine-grained segmentation and contact-site selection. A lightweight LLM parses user instructions and produces grounded natural-language explanations linked to the tactile outputs. In end-to-end evaluations, TactEx attains 90% task success on simple user queries and generalises to novel tasks without large-scale tuning. These results highlight the promise of combining pretrained visual and tactile models with language grounding to advance explainable, human-like touch perception and decision-making in robotics.",
        "authors_display": "Dandan Zhang Team",
        "pdf_url": "http://arxiv.org/abs/2602.18967",
        "code_url": null,
        "date": "2026-02-21",
        "primary_category": "cs.RO",
        "chinese_summary": "准确感知物体硬度对于机器人灵巧操作至关重要。本研究提出TactEx，一个可解释的多模态机器人交互框架，它融合视觉、触觉和语言以实现类人硬度估计和交互式指导。该系统通过ResNet50+LSTM模型从触觉数据估算硬度，并通过跨模态对齐模块结合视觉线索和大型语言模型指导。TactEx在水果成熟度评估任务中成功率达90%，能有效区分成熟度等级并泛化到新任务，突显了预训练模型与语言接地相结合在机器人触觉感知和决策中的潜力。"
      },
      {
        "paper_id": "2602.18884",
        "title": "TPRU: Advancing Temporal and Procedural Understanding in Large Multimodal Models",
        "abstract": "Multimodal Large Language Models (MLLMs), particularly smaller, deployable variants, exhibit a critical deficiency in understanding temporal and procedural visual data, a bottleneck hindering their application in real-world embodied AI. This gap is largely caused by a systemic failure in training paradigms, which lack large-scale, procedurally coherent data. To address this problem, we introduce TPRU, a large-scale dataset sourced from diverse embodied scenarios such as robotic manipulation and GUI navigation. TPRU is systematically designed to cultivate temporal reasoning through three complementary tasks: Temporal Reordering, Next-Frame Prediction, and Previous-Frame Review. A key feature is the inclusion of challenging negative samples, compelling models to transition from passive observation to active, cross-modal validation. We leverage TPRU with a reinforcement learning (RL) fine-tuning methodology, specifically targeting the enhancement of resource-efficient models. Experiments show our approach yields dramatic gains: on our manually curated TPRU-Test, the accuracy of TPRU-7B soars from 50.33\\% to 75.70\\%, a state-of-the-art result that significantly outperforms vastly larger baselines, including GPT-4o. Crucially, these capabilities generalize effectively, demonstrating substantial improvements on established benchmarks. The codebase is available at https://github.com/Stephen-gzk/TPRU/ .",
        "authors_display": "Yuan Xie Team",
        "pdf_url": "http://arxiv.org/abs/2602.18884",
        "code_url": "https://github.com/Stephen-gzk/TPRU",
        "date": "2026-02-21",
        "primary_category": "cs.AI",
        "chinese_summary": "小型多模态大型语言模型（MLLMs）在理解时序和程序性视觉数据方面存在缺陷，阻碍了其在具身AI中的应用。为解决训练数据中缺乏大规模、程序连贯数据的问题，本研究引入了TPRU数据集，其包含来自机器人操作和GUI导航等场景的数据，并通过时间重排序、下一帧预测和前一帧回顾任务培养时间推理能力。利用强化学习微调后，TPRU-7B模型在TPRU-Test数据集上准确率从50.33%提升至75.70%，显著优于GPT-4o等大型基线模型，并展现了出色的泛化能力。"
      },
      {
        "paper_id": "2602.18856",
        "title": "Issues with Measuring Task Complexity via Random Policies in Robotic Tasks",
        "abstract": "Reinforcement learning (RL) has enabled major advances in fields such as robotics and natural language processing. A key challenge in RL is measuring task complexity, which is essential for creating meaningful benchmarks and designing effective curricula. While there are numerous well-established metrics for assessing task complexity in tabular settings, relatively few exist in non-tabular domains. These include (i) Statistical analysis of the performance of random policies via Random Weight Guessing (RWG), and (ii) information-theoretic metrics Policy Information Capacity (PIC) and Policy-Optimal Information Capacity (POIC), which are reliant on RWG. In this paper, we evaluate these methods using progressively difficult robotic manipulation setups, with known relative complexity, with both dense and sparse reward formulations. Our empirical results reveal that measuring complexity is still nuanced. Specifically, under the same reward formulation, PIC suggests that a two-link robotic arm setup is easier than a single-link setup - which contradicts the robotic control and empirical RL perspective whereby the two-link setup is inherently more complex. Likewise, for the same setup, POIC estimates that tasks with sparse rewards are easier than those with dense rewards. Thus, we show that both PIC and POIC contradict typical understanding and empirical results from RL. These findings highlight the need to move beyond RWG-based metrics towards better metrics that can more reliably capture task complexity in non-tabular RL with our task framework as a starting point.",
        "authors_display": "Aditya Gilra Team",
        "pdf_url": "http://arxiv.org/abs/2602.18856",
        "code_url": null,
        "date": "2026-02-21",
        "primary_category": "cs.LG",
        "chinese_summary": "衡量强化学习（RL）任务复杂性对于基准测试和课程设计至关重要，但在非表格领域仍缺乏可靠指标。本研究评估了基于随机权重猜测（RWG）的策略信息容量（PIC）和策略最优信息容量（POIC）等现有复杂性指标。通过在不同难度机器人操作设置下的实证分析，发现PIC和POIC在某些情况下与RL的典型理解和经验结果相矛盾，例如认为双连杆机械臂比单连杆更简单，或稀疏奖励任务比密集奖励任务更容易。这强调了需要开发超越RWG、更可靠捕捉非表格RL任务复杂性的新指标。"
      },
      {
        "paper_id": "2602.18817",
        "title": "HeRO: Hierarchical 3D Semantic Representation for Pose-aware Object Manipulation",
        "abstract": "Imitation learning for robotic manipulation has progressed from 2D image policies to 3D representations that explicitly encode geometry. Yet purely geometric policies often lack explicit part-level semantics, which are critical for pose-aware manipulation (e.g., distinguishing a shoe's toe from heel). In this paper, we present HeRO, a diffusion-based policy that couples geometry and semantics via hierarchical semantic fields. HeRO employs dense semantics lifting to fuse discriminative, geometry-sensitive features from DINOv2 with the smooth, globally coherent correspondences from Stable Diffusion, yielding dense features that are both fine-grained and spatially consistent. These features are processed and partitioned to construct a global field and a set of local fields. A hierarchical conditioning module conditions the generative denoiser on global and local fields using permutation-invariant network architecture, thereby avoiding order-sensitive bias and producing a coherent control policy for pose-aware manipulation. In various tests, HeRO establishes a new state-of-the-art, improving success on Place Dual Shoes by 12.3% and averaging 6.5% gains across six challenging pose-aware tasks. Code is available at https://github.com/Chongyang-99/HeRO.",
        "authors_display": "Shuaicheng Liu Team",
        "pdf_url": "http://arxiv.org/abs/2602.18817",
        "code_url": null,
        "date": "2026-02-21",
        "primary_category": "cs.CV",
        "chinese_summary": "纯几何策略的机器人模仿学习缺乏部件级语义，这限制了其在姿态感知操作中的能力。本研究提出HeRO，一个基于扩散的策略，通过分层语义场结合几何和语义。HeRO通过密集语义提升，融合DINOv2的几何敏感特征与Stable Diffusion的全局一致对应，生成细粒度且空间一致的特征。这些特征用于构建全局和局部场，并由分层条件模块以置换不变网络架构调节生成去噪器，从而实现连贯的姿态感知控制策略。HeRO在多项任务中达到SOTA，成功率平均提升6.5%。"
      },
      {
        "paper_id": "2602.18742",
        "title": "RoboCurate: Harnessing Diversity with Action-Verified Neural Trajectory for Robot Learning",
        "abstract": "Synthetic data generated by video generative models has shown promise for robot learning as a scalable pipeline, but it often suffers from inconsistent action quality due to imperfectly generated videos. Recently, vision-language models (VLMs) have been leveraged to validate video quality, but they have limitations in distinguishing physically accurate videos and, even then, cannot directly evaluate the generated actions themselves. To tackle this issue, we introduce RoboCurate, a novel synthetic robot data generation framework that evaluates and filters the quality of annotated actions by comparing them with simulation replay. Specifically, RoboCurate replays the predicted actions in a simulator and assesses action quality by measuring the consistency of motion between the simulator rollout and the generated video. In addition, we unlock observation diversity beyond the available dataset via image-to-image editing and apply action-preserving video-to-video transfer to further augment appearance. We observe RoboCurate's generated data yield substantial relative improvements in success rates compared to using real data only, achieving +70.1% on GR-1 Tabletop (300 demos), +16.1% on DexMimicGen in the pre-training setup, and +179.9% in the challenging real-world ALLEX humanoid dexterous manipulation setting.",
        "authors_display": "Jinwoo Shin Team",
        "pdf_url": "http://arxiv.org/abs/2602.18742",
        "code_url": "https://seungkukim.github.io/robocurate/",
        "date": "2026-02-21",
        "primary_category": "cs.RO",
        "chinese_summary": "视频生成模型产生的合成数据在机器人学习中存在行动质量不一致的问题，而VLM在区分物理精确视频和评估行动方面有局限。本研究引入RoboCurate框架，通过在模拟器中重放预测行动并比较其与生成视频的运动一致性来评估和过滤带注释行动的质量。此外，通过图像编辑和视频到视频传输增强观察多样性。结果显示，RoboCurate生成的数据显著提升了机器人任务的成功率，例如在GR-1 Tabletop任务中提高了70.1%，并在ALLEX人形机器人操作中提高了179.9%。"
      },
      {
        "paper_id": "2602.16898",
        "title": "MALLVI: A Multi-Agent Framework for Integrated Generalized Robotics Manipulation",
        "abstract": "Task planning for robotic manipulation with large language models (LLMs) is an emerging area. Prior approaches rely on specialized models, fine tuning, or prompt tuning, and often operate in an open loop manner without robust environmental feedback, making them fragile in dynamic settings.MALLVi present a Multi Agent Large Language and Vision framework that enables closed loop feedback driven robotic manipulation. Given a natural language instruction and an image of the environment, MALLVi generates executable atomic actions for a robot manipulator. After action execution, a Vision Language Model (VLM) evaluates environmental feedback and decides whether to repeat the process or proceed to the next step Rather than using a single model, MALLVi coordinates specialized agents, Decomposer, Localizer, Thinker, and Reflector, to manage perception, localization, reasoning, and high level planning. An optional Descriptor agent provides visual memory of the initial state. The Reflector supports targeted error detection and recovery by reactivating only relevant agents, avoiding full replanning.Experiments in simulation and real world settings show that iterative closed loop multi agent coordination improves generalization and increases success rates in zero shot manipulation tasks.Code available at https://github.com/iman1234ahmadi/MALLVI.",
        "authors_display": "Babak Khalaj Team",
        "pdf_url": "http://arxiv.org/abs/2602.16898",
        "code_url": null,
        "date": "2026-02-20",
        "primary_category": "cs.RO",
        "chinese_summary": "针对大型语言模型（LLMs）在机器人操作任务规划中开环、缺乏环境反馈的脆弱性，本文提出了MALLVi框架。该框架是一个多智能体大语言和视觉系统，通过协调分解器、定位器、思考者和反射器等专用智能体，实现了闭环反馈驱动的机器人操作，其中反射器支持有针对性的错误检测和恢复。实验结果表明，这种迭代闭环多智能体协调显著提高了零样本操作任务的泛化性和成功率。"
      },
      {
        "paper_id": "2602.18224",
        "title": "SimVLA: A Simple VLA Baseline for Robotic Manipulation",
        "abstract": "Vision-Language-Action (VLA) models have emerged as a promising paradigm for general-purpose robotic manipulation, leveraging large-scale pre-training to achieve strong performance. The field has rapidly evolved with additional spatial priors and diverse architectural innovations. However, these advancements are often accompanied by varying training recipes and implementation details, which can make it challenging to disentangle the precise source of empirical gains. In this work, we introduce SimVLA, a streamlined baseline designed to establish a transparent reference point for VLA research. By strictly decoupling perception from control, using a standard vision-language backbone and a lightweight action head, and standardizing critical training dynamics, we demonstrate that a minimal design can achieve state-of-the-art performance. Despite having only 0.5B parameters, SimVLA outperforms multi-billion-parameter models on standard simulation benchmarks without robot pretraining. SimVLA also reaches on-par real-robot performance compared to pi0.5. Our results establish SimVLA as a robust, reproducible baseline that enables clear attribution of empirical gains to future architectural innovations. Website: https://frontierrobo.github.io/SimVLA",
        "authors_display": "Zhenguo Li Team",
        "pdf_url": "http://arxiv.org/abs/2602.18224",
        "code_url": null,
        "date": "2026-02-20",
        "primary_category": "cs.RO",
        "chinese_summary": "鉴于视觉-语言-动作（VLA）模型研究中因训练方法和实现细节多样而难以识别性能提升来源的问题，本研究引入了SimVLA这一精简基线。SimVLA严格解耦感知与控制，采用标准视觉-语言骨干和轻量级动作头，并标准化了关键训练动态。实验证明，尽管SimVLA参数量仅为0.5B，但在未进行机器人预训练的标准仿真基准上超越了数十亿参数的模型，并在真实机器人性能上与pi0.5相当，为VLA研究提供了透明且可复现的参考基准。"
      },
      {
        "paper_id": "2602.18020",
        "title": "UAOR: Uncertainty-aware Observation Reinjection for Vision-Language-Action Models",
        "abstract": "Vision-Language-Action (VLA) models leverage pretrained Vision-Language Models (VLMs) as backbones to map images and instructions to actions, demonstrating remarkable potential for generalizable robotic manipulation. To enhance performance, existing methods often incorporate extra observation cues (e.g., depth maps, point clouds) or auxiliary modules (e.g., object detectors, encoders) to enable more precise and reliable task execution, yet these typically require costly data collection and additional training. Inspired by the finding that Feed-Forward Network (FFN) in language models can act as \"key-value memory\", we propose Uncertainty-aware Observation Reinjection (UAOR), an effective, training-free and plug-and-play module for VLA models. Specifically, when the current language model layer exhibits high uncertainty, measured by Action Entropy, it reinjects key observation information into the next layer's Feed-Forward Network (FFN) through attention retrieval. This mechanism helps VLAs better attend to observations during inference, enabling more confident and faithful action generation. Comprehensive experiments show that our method consistently improves diverse VLA models across simulation and real-world tasks with minimal overhead. Notably, UAOR eliminates the need for additional observation cues or modules, making it a versatile and practical plug-in for existing VLA pipelines. The project page is at https://uaor.jiabingyang.cn.",
        "authors_display": "Liang Wang Team",
        "pdf_url": "http://arxiv.org/abs/2602.18020",
        "code_url": null,
        "date": "2026-02-20",
        "primary_category": "cs.CV",
        "chinese_summary": "针对视觉-语言-动作（VLA）模型在增强性能时常需额外观察线索或辅助模块，导致数据收集和训练成本高昂的问题，本研究受语言模型FFN启发，提出了一种无需训练、即插即用的“不确定性感知观测重注”（UAOR）模块。该模块在语言模型层表现出高不确定性时，通过注意力检索将关键观测信息重新注入下一层的前馈网络。广泛实验表明，UAOR以最小开销持续提升了多种VLA模型在模拟和真实世界任务中的表现，且无需额外观察线索或模块，具有良好的通用性和实用性。"
      },
      {
        "paper_id": "2602.18014",
        "title": "Quasi-Periodic Gaussian Process Predictive Iterative Learning Control",
        "abstract": "Repetitive motion tasks are common in robotics, but performance can degrade over time due to environmental changes and robot wear and tear. Iterative learning control (ILC) improves performance by using information from previous iterations to compensate for expected errors in future iterations. This work incorporates the use of Quasi-Periodic Gaussian Processes (QPGPs) into a predictive ILC framework to model and forecast disturbances and drift across iterations. Using a recent structural equation formulation of QPGPs, the proposed approach enables efficient inference with complexity $\\mathcal{O}(p^3)$ instead of $\\mathcal{O}(i^2p^3)$, where $p$ denotes the number of points within an iteration and $i$ represents the total number of iterations, specially for larger $i$. This formulation also enables parameter estimation without loss of information, making continual GP learning computationally feasible within the control loop. By predicting next-iteration error profiles rather than relying only on past errors, the controller achieves faster convergence and maintains this under time-varying disturbances. We benchmark the method against both standard ILC and conventional Gaussian Process (GP)-based predictive ILC on three tasks, autonomous vehicle trajectory tracking, a three-link robotic manipulator, and a real-world Stretch robot experiment. Across all cases, the proposed approach converges faster and remains robust under injected and natural disturbances while reducing computational cost. This highlights its practicality across a range of repetitive dynamical systems.",
        "authors_display": "Michael Burke Team",
        "pdf_url": "http://arxiv.org/abs/2602.18014",
        "code_url": null,
        "date": "2026-02-20",
        "primary_category": "cs.RO",
        "chinese_summary": "鉴于重复运动任务中机器人性能随时间退化的问题，本研究将准周期高斯过程（QPGPs）引入预测性迭代学习控制（ILC）框架，以高效建模和预测跨迭代的扰动与漂移。通过QPGPs的结构方程表述，该方法实现了O(p^3)的计算复杂度，远低于传统方法。实验结果表明，与标准ILC及传统GP-ILC相比，所提方法在自动驾驶、机械臂和真实世界机器人等任务中能更快收敛，在扰动下保持鲁棒性，并显著降低了计算成本。"
      },
      {
        "paper_id": "2602.17978",
        "title": "Learning Optimal and Sample-Efficient Decision Policies with Guarantees",
        "abstract": "The paradigm of decision-making has been revolutionised by reinforcement learning and deep learning. Although this has led to significant progress in domains such as robotics, healthcare, and finance, the use of RL in practice is challenging, particularly when learning decision policies in high-stakes applications that may require guarantees. Traditional RL algorithms rely on a large number of online interactions with the environment, which is problematic in scenarios where online interactions are costly, dangerous, or infeasible. However, learning from offline datasets is hindered by the presence of hidden confounders. Such confounders can cause spurious correlations in the dataset and can mislead the agent into taking suboptimal or adversarial actions. Firstly, we address the problem of learning from offline datasets in the presence of hidden confounders. We work with instrumental variables (IVs) to identify the causal effect, which is an instance of a conditional moment restrictions (CMR) problem. Inspired by double/debiased machine learning, we derive a sample-efficient algorithm for solving CMR problems with convergence and optimality guarantees, which outperforms state-of-the-art algorithms. Secondly, we relax the conditions on the hidden confounders in the setting of (offline) imitation learning, and adapt our CMR estimator to derive an algorithm that can learn effective imitator policies with convergence rate guarantees. Finally, we consider the problem of learning high-level objectives expressed in linear temporal logic (LTL) and develop a provably optimal learning algorithm that improves sample efficiency over existing methods. Through evaluation on reinforcement learning benchmarks and synthetic and semi-synthetic datasets, we demonstrate the usefulness of the methods developed in this thesis in real-world decision making.",
        "authors_display": "Daqian Shao Team",
        "pdf_url": "http://arxiv.org/abs/2602.17978",
        "code_url": null,
        "date": "2026-02-20",
        "primary_category": "cs.LG",
        "chinese_summary": "面对高风险应用中强化学习在线交互成本高昂且离线学习受隐蔽混淆变量阻碍的挑战，本研究首先利用工具变量解决了存在混淆变量的离线学习问题，并提出了一种样本高效的条件矩约束算法，具有收敛性和最优性保证。其次，放宽模仿学习中对混淆变量的条件，并调整估计器以学习有效的模仿策略。最后，针对学习线性时序逻辑表达的高级目标，开发了一种可证明最优且样本效率更高的学习算法。实验证明了这些方法在实际决策中的有效性。"
      },
      {
        "paper_id": "2602.17954",
        "title": "Graph-Neural Multi-Agent Coordination for Distributed Access-Point Selection in Cell-Free Massive MIMO",
        "abstract": "Cell-free massive MIMO (CFmMIMO) systems require scalable and reliable distributed coordination mechanisms to operate under stringent communication and latency constraints. A central challenge is the Access Point Selection (APS) problem, which seeks to determine the subset of serving Access Points (APs) for each User Equipment (UE) that can satisfy UEs' Spectral Efficiency (SE) requirements while minimizing network power consumption. We introduce APS-GNN, a scalable distributed multi-agent learning framework that decomposes APS into agents operating at the granularity of individual AP-UE connections. Agents coordinate via local observation exchange over a novel Graph Neural Network (GNN) architecture and share parameters to reuse their knowledge and experience. APS-GNN adopts a constrained reinforcement learning approach to provide agents with explicit observability of APS' conflicting objectives, treating SE satisfaction as a cost and power reduction as a reward. Both signals are defined locally, facilitating effective credit assignment and scalable coordination in large networks. To further improve training stability and exploration efficiency, the policy is initialized via supervised imitation learning from a heuristic APS baseline. We develop a realistic CFmMIMO simulator and demonstrate that APS-GNN delivers the target SE while activating 50-70% fewer APs than heuristic and centralized Multi-agent Reinforcement Learning (MARL) baselines in different evaluation scenarios. Moreover, APS-GNN achieves one to two orders of magnitude lower inference latency than centralized MARL approaches due to its fully parallel and distributed execution. These results establish APS-GNN as a practical and scalable solution for APS in large-scale CFmMIMO networks.",
        "authors_display": "Raouf Boutaba Team",
        "pdf_url": "http://arxiv.org/abs/2602.17954",
        "code_url": null,
        "date": "2026-02-20",
        "primary_category": "cs.NI",
        "chinese_summary": "为解决免蜂窝大规模MIMO (CFmMIMO) 系统中接入点选择（APS）的挑战，本研究提出了APS-GNN，一个可扩展的分布式多智能体学习框架。该框架将APS分解为基于图神经网络的智能体，通过局部观测交换和参数共享进行协调，并采用受约束强化学习方法处理冲突目标。为提高训练效率，策略通过监督模仿学习初始化。实验结果表明，APS-GNN在实现目标频谱效率的同时，激活的接入点数量比启发式和集中式多智能体强化学习基线减少50-70%，且推理延迟低一到两个数量级，为大型CFmMIMO网络提供了实用且可扩展的APS解决方案。"
      },
      {
        "paper_id": "2602.17951",
        "title": "ROCKET: Residual-Oriented Multi-Layer Alignment for Spatially-Aware Vision-Language-Action Models",
        "abstract": "Vision-Language-Action (VLA) models enable instruction-following robotic manipulation, but they are typically pretrained on 2D data and lack 3D spatial understanding. An effective approach is representation alignment, where a strong vision foundation model is used to guide a 2D VLA model. However, existing methods usually apply supervision at only a single layer, failing to fully exploit the rich information distributed across depth; meanwhile, naïve multi-layer alignment can cause gradient interference. We introduce ROCKET, a residual-oriented multi-layer representation alignment framework that formulates multi-layer alignment as aligning one residual stream to another. Concretely, ROCKET employs a shared projector to align multiple layers of the VLA backbone with multiple layers of a powerful 3D vision foundation model via a layer-invariant mapping, which reduces gradient conflicts. We provide both theoretical justification and empirical analyses showing that a shared projector is sufficient and outperforms prior designs, and further propose a Matryoshka-style sparse activation scheme for the shared projector to balance multiple alignment losses. Our experiments show that, combined with a training-free layer selection strategy, ROCKET requires only about 4% of the compute budget while achieving 98.5% state-of-the-art success rate on LIBERO. We further demonstrate the superior performance of ROCKET across LIBERO-Plus and RoboTwin, as well as multiple VLA models. The code and model weights can be found at https://github.com/CASE-Lab-UMD/ROCKET-VLA.",
        "authors_display": "Ang Li Team",
        "pdf_url": "http://arxiv.org/abs/2602.17951",
        "code_url": null,
        "date": "2026-02-20",
        "primary_category": "cs.CV",
        "chinese_summary": "针对视觉-语言-动作（VLA）模型在3D空间理解上的不足以及现有表示对齐方法未能充分利用多层信息或可能导致梯度冲突的问题，本研究提出了ROCKET，一个残差导向的多层表示对齐框架。ROCKET通过共享投影器和层不变映射，将VLA骨干的多个层与强大的3D视觉基础模型的多个层进行对齐，并引入Matryoshka风格的稀疏激活方案以平衡多个对齐损失。实验证明，ROCKET在计算开销极小的情况下，在LIBERO、LIBERO-Plus和RoboTwin等任务上取得了98.5%的SOTA成功率，且优于现有设计。"
      },
      {
        "paper_id": "2602.18603",
        "title": "Enhancing Goal Inference via Correction Timing",
        "abstract": "Corrections offer a natural modality for people to provide feedback to a robot, by (i) intervening in the robot's behavior when they believe the robot is failing (or will fail) the task objectives and (ii) modifying the robot's behavior to successfully fulfill the task. Each correction offers information on what the robot should and should not do, where the corrected behavior is more aligned with task objectives than the original behavior. Most prior work on learning from corrections involves interpreting a correction as a new demonstration (consisting of the modified robot behavior), or a preference (for the modified trajectory compared to the robot's original behavior). However, this overlooks one essential element of the correction feedback, which is the human's decision to intervene in the robot's behavior in the first place. This decision can be influenced by multiple factors including the robot's task progress, alignment with human expectations, dynamics, motion legibility, and optimality. In this work, we investigate whether the timing of this decision can offer a useful signal for inferring these task-relevant influences. In particular, we investigate three potential applications for this learning signal: (1) identifying features of a robot's motion that may prompt people to correct it, (2) quickly inferring the final goal of a human's correction based on the timing and initial direction of their correction motion, and (3) learning more precise constraints for task objectives. Our results indicate that correction timing results in improved learning for the first two of these applications. Overall, our work provides new insights on the value of correction timing as a signal for robot learning.",
        "authors_display": "Tesca Fitzgerald Team",
        "pdf_url": "http://arxiv.org/abs/2602.18603",
        "code_url": null,
        "date": "2026-02-20",
        "primary_category": "cs.RO",
        "chinese_summary": "人类对机器人的纠正反馈通常被视为新的示范或偏好，但忽略了人类决定干预的时机信息。本研究探讨了纠正时机作为一种有用信号，用于推断机器人任务相关的因素，如进度、期望一致性、动力学等。研究集中于三个应用：识别促使人类纠正的运动特征、快速推断纠正目标以及学习更精确的任务目标约束。结果表明，纠正时机能有效改善前两个应用的学习效果，为机器人学习提供了新的见解。"
      },
      {
        "paper_id": "2602.16444",
        "title": "RoboGene: Boosting VLA Pre-training via Diversity-Driven Agentic Framework for Real-World Task Generation",
        "abstract": "The pursuit of general-purpose robotic manipulation is hindered by the scarcity of diverse, real-world interaction data. Unlike data collection from web in vision or language, robotic data collection is an active process incurring prohibitive physical costs. Consequently, automated task curation to maximize data value remains a critical yet under-explored challenge. Existing manual methods are unscalable and biased toward common tasks, while off-the-shelf foundation models often hallucinate physically infeasible instructions. To address this, we introduce RoboGene, an agentic framework designed to automate the generation of diverse, physically plausible manipulation tasks across single-arm, dual-arm, and mobile robots. RoboGene integrates three core components: diversity-driven sampling for broad task coverage, self-reflection mechanisms to enforce physical constraints, and human-in-the-loop refinement for continuous improvement. We conduct extensive quantitative analysis and large-scale real-world experiments, collecting datasets of 18k trajectories and introducing novel metrics to assess task quality, feasibility, and diversity. Results demonstrate that RoboGene significantly outperforms state-of-the-art foundation models (e.g., GPT-4o, Gemini 2.5 Pro). Furthermore, real-world experiments show that VLA models pre-trained with RoboGene achieve higher success rates and superior generalization, underscoring the importance of high-quality task generation. Our project is available at https://robogene-boost-vla.github.io.",
        "authors_display": "Jian Tang Team",
        "pdf_url": "http://arxiv.org/abs/2602.16444",
        "code_url": null,
        "date": "2026-02-19",
        "primary_category": "cs.RO",
        "chinese_summary": "针对通用机器人操作面临的真实世界交互数据稀缺和自动化任务生成挑战，本文提出了RoboGene框架。该代理框架通过多样性驱动采样、自我反思机制和人机循环改进，自动生成单臂、双臂和移动机器人的多样化、物理合理的操纵任务。实验证明，RoboGene显著优于现有基础模型，且其生成的数据能提高预训练VLA模型的成功率和泛化能力。"
      },
      {
        "paper_id": "2602.17537",
        "title": "IRIS: Learning-Driven Task-Specific Cinema Robot Arm for Visuomotor Motion Control",
        "abstract": "Robotic camera systems enable dynamic, repeatable motion beyond human capabilities, yet their adoption remains limited by the high cost and operational complexity of industrial-grade platforms. We present the Intelligent Robotic Imaging System (IRIS), a task-specific 6-DOF manipulator designed for autonomous, learning-driven cinematic motion control. IRIS integrates a lightweight, fully 3D-printed hardware design with a goal-conditioned visuomotor imitation learning framework based on Action Chunking with Transformers (ACT). The system learns object-aware and perceptually smooth camera trajectories directly from human demonstrations, eliminating the need for explicit geometric programming. The complete platform costs under $1,000 USD, supports a 1.5 kg payload, and achieves approximately 1 mm repeatability. Real-world experiments demonstrate accurate trajectory tracking, reliable autonomous execution, and generalization across diverse cinematic motions.",
        "authors_display": "Ali Bereyhi Team",
        "pdf_url": "http://arxiv.org/abs/2602.17537",
        "code_url": null,
        "date": "2026-02-19",
        "primary_category": "cs.RO",
        "chinese_summary": "鉴于工业级机器人摄像系统成本高昂且操作复杂，限制了其普及，本研究提出了智能机器人成像系统（IRIS）。IRIS是一款专为自主、学习驱动的电影级运动控制设计的6自由度机械手，结合了轻量级3D打印硬件与基于Action Chunking with Transformers（ACT）的目标条件视觉运动模仿学习框架，直接从人类演示中学习摄像机轨迹，无需显式编程。该系统成本低于1000美元，支持1.5公斤载荷，并实现了约1毫米的重复精度。真实世界实验验证了其准确的轨迹跟踪、可靠的自主执行以及在多样化电影级运动中的泛化能力。"
      },
      {
        "paper_id": "2602.17101",
        "title": "Benchmarking the Effects of Object Pose Estimation and Reconstruction on Robotic Grasping Success",
        "abstract": "3D reconstruction serves as the foundational layer for numerous robotic perception tasks, including 6D object pose estimation and grasp pose generation. Modern 3D reconstruction methods for objects can produce visually and geometrically impressive meshes from multi-view images, yet standard geometric evaluations do not reflect how reconstruction quality influences downstream tasks such as robotic manipulation performance. This paper addresses this gap by introducing a large-scale, physics-based benchmark that evaluates 6D pose estimators and 3D mesh models based on their functional efficacy in grasping. We analyze the impact of model fidelity by generating grasps on various reconstructed 3D meshes and executing them on the ground-truth model, simulating how grasp poses generated with an imperfect model affect interaction with the real object. This assesses the combined impact of pose error, grasp robustness, and geometric inaccuracies from 3D reconstruction. Our results show that reconstruction artifacts significantly decrease the number of grasp pose candidates but have a negligible effect on grasping performance given an accurately estimated pose. Our results also reveal that the relationship between grasp success and pose error is dominated by spatial error, and even a simple translation error provides insight into the success of the grasping pose of symmetric objects. This work provides insight into how perception systems relate to object manipulation using robots.",
        "authors_display": "Torsten Sattler Team",
        "pdf_url": "http://arxiv.org/abs/2602.17101",
        "code_url": null,
        "date": "2026-02-19",
        "primary_category": "cs.RO",
        "chinese_summary": "针对3D重建在机器人感知中作为基础，但传统几何评估未能反映其对下游任务（如机器人抓取）性能影响的空白，本研究引入了一个大规模、基于物理的基准。该基准通过在不同重建的3D网格上生成抓取，并在真实模型上执行，来评估6D姿态估计器和3D网格模型在抓取方面的功能有效性。实验结果表明，重建缺陷显著减少了抓取姿态候选数量，但给定准确姿态估计后对抓取性能影响可忽略；抓取成功率与姿态误差的关系主要由空间误差主导，即使简单的平移误差也能为对称物体抓取姿态的成功提供洞察，为理解感知系统与机器人物体操作之间的关系提供了见解。"
      },
      {
        "paper_id": "2602.17832",
        "title": "MePoly: Max Entropy Polynomial Policy Optimization",
        "abstract": "Stochastic Optimal Control provides a unified mathematical framework for solving complex decision-making problems, encompassing paradigms such as maximum entropy reinforcement learning(RL) and imitation learning(IL). However, conventional parametric policies often struggle to represent the multi-modality of the solutions. Though diffusion-based policies are aimed at recovering the multi-modality, they lack an explicit probability density, which complicates policy-gradient optimization. To bridge this gap, we propose MePoly, a novel policy parameterization based on polynomial energy-based models. MePoly provides an explicit, tractable probability density, enabling exact entropy maximization. Theoretically, we ground our method in the classical moment problem, leveraging the universal approximation capabilities for arbitrary distributions. Empirically, we demonstrate that MePoly effectively captures complex non-convex manifolds and outperforms baselines in performance across diverse benchmarks.",
        "authors_display": "Maani Ghaffari Team",
        "pdf_url": "http://arxiv.org/abs/2602.17832",
        "code_url": null,
        "date": "2026-02-19",
        "primary_category": "cs.LG",
        "chinese_summary": "针对随机最优控制中传统参数化策略难以表示多模态解决方案，以及基于扩散的策略缺乏明确概率密度的问题，本研究提出了MePoly，一种基于多项式能量模型的策略参数化方法。MePoly提供了明确且可处理的概率密度，从而实现了精确的熵最大化，并基于经典矩问题，利用其对任意分布的通用逼近能力。实验证明MePoly能有效捕获复杂的非凸流形，并在各种基准测试中超越基线模型，展现了其在处理多模态决策问题中的优越性能。"
      },
      {
        "paper_id": "2602.15010",
        "title": "BPP: Long-Context Robot Imitation Learning by Focusing on Key History Frames",
        "abstract": "Many robot tasks require attending to the history of past observations. For example, finding an item in a room requires remembering which places have already been searched. However, the best-performing robot policies typically condition only on the current observation, limiting their applicability to such tasks. Naively conditioning on past observations often fails due to spurious correlations: policies latch onto incidental features of training histories that do not generalize to out-of-distribution trajectories upon deployment. We analyze why policies latch onto these spurious correlations and find that this problem stems from limited coverage over the space of possible histories during training, which grows exponentially with horizon. Existing regularization techniques provide inconsistent benefits across tasks, as they do not fundamentally address this coverage problem. Motivated by these findings, we propose Big Picture Policies (BPP), an approach that conditions on a minimal set of meaningful keyframes detected by a vision-language model. By projecting diverse rollouts onto a compact set of task-relevant events, BPP substantially reduces distribution shift between training and deployment, without sacrificing expressivity. We evaluate BPP on four challenging real-world manipulation tasks and three simulation tasks, all requiring history conditioning. BPP achieves 70% higher success rates than the best comparison on real-world evaluations. Videos are available at https://bigpicturepolicies.github.io/",
        "authors_display": "Aviral Kumar Team",
        "pdf_url": "http://arxiv.org/abs/2602.15010",
        "code_url": null,
        "date": "2026-02-18",
        "primary_category": "cs.RO",
        "chinese_summary": "针对机器人任务中历史观测的重要性及现有策略因虚假关联导致泛化性差的问题，本文提出了大局策略（BPP）。该方法通过视觉-语言模型检测有意义的关键帧，并以此最小关键帧集为条件进行策略学习，从而将多样化的轨迹投射到紧凑的任务相关事件集上，显著减少了训练和部署之间的分布漂移。在真实世界和模拟操作任务中的实验结果表明，BPP的成功率比最佳对比方法高70%。"
      },
      {
        "paper_id": "2602.16705",
        "title": "Learning Humanoid End-Effector Control for Open-Vocabulary Visual Loco-Manipulation",
        "abstract": "Visual loco-manipulation of arbitrary objects in the wild with humanoid robots requires accurate end-effector (EE) control and a generalizable understanding of the scene via visual inputs (e.g., RGB-D images). Existing approaches are based on real-world imitation learning and exhibit limited generalization due to the difficulty in collecting large-scale training datasets. This paper presents a new paradigm, HERO, for object loco-manipulation with humanoid robots that combines the strong generalization and open-vocabulary understanding of large vision models with strong control performance from simulated training. We achieve this by designing an accurate residual-aware EE tracking policy. This EE tracking policy combines classical robotics with machine learning. It uses a) inverse kinematics to convert residual end-effector targets into reference trajectories, b) a learned neural forward model for accurate forward kinematics, c) goal adjustment, and d) replanning. Together, these innovations help us cut down the end-effector tracking error by 3.2x. We use this accurate end-effector tracker to build a modular system for loco-manipulation, where we use open-vocabulary large vision models for strong visual generalization. Our system is able to operate in diverse real-world environments, from offices to coffee shops, where the robot is able to reliably manipulate various everyday objects (e.g., mugs, apples, toys) on surfaces ranging from 43cm to 92cm in height. Systematic modular and end-to-end tests in simulation and the real world demonstrate the effectiveness of our proposed design. We believe the advances in this paper can open up new ways of training humanoid robots to interact with daily objects.",
        "authors_display": "Saurabh Gupta Team",
        "pdf_url": "http://arxiv.org/abs/2602.16705",
        "code_url": "https://hero-humanoid.github.io/",
        "date": "2026-02-18",
        "primary_category": "cs.RO",
        "chinese_summary": "为解决人形机器人在野外视觉运动操作中末端执行器（EE）控制和场景理解的泛化性限制，本文提出了HERO范式。该方法结合了大型视觉模型的强泛化能力和开放词汇理解与模拟训练的控制性能，并通过设计精确的残差感知EE跟踪策略，结合经典机器人学和机器学习技术。实验表明，HERO将末端执行器跟踪误差降低了3.2倍，并在多样化的真实世界环境中实现了对多种日常物品的可靠操作。"
      },
      {
        "paper_id": "2602.16511",
        "title": "VIGOR: Visual Goal-In-Context Inference for Unified Humanoid Fall Safety",
        "abstract": "Reliable fall recovery is critical for humanoids operating in cluttered environments. Unlike quadrupeds or wheeled robots, humanoids experience high-energy impacts, complex whole-body contact, and large viewpoint changes during a fall, making recovery essential for continued operation. Existing methods fragment fall safety into separate problems such as fall avoidance, impact mitigation, and stand-up recovery, or rely on end-to-end policies trained without vision through reinforcement learning or imitation learning, often on flat terrain. At a deeper level, fall safety is treated as monolithic data complexity, coupling pose, dynamics, and terrain and requiring exhaustive coverage, limiting scalability and generalization. We present a unified fall safety approach that spans all phases of fall recovery. It builds on two insights: 1) Natural human fall and recovery poses are highly constrained and transferable from flat to complex terrain through alignment, and 2) Fast whole-body reactions require integrated perceptual-motor representations. We train a privileged teacher using sparse human demonstrations on flat terrain and simulated complex terrains, and distill it into a deployable student that relies only on egocentric depth and proprioception. The student learns how to react by matching the teacher's goal-in-context latent representation, which combines the next target pose with the local terrain, rather than separately encoding what it must perceive and how it must act. Results in simulation and on a real Unitree G1 humanoid demonstrate robust, zero-shot fall safety across diverse non-flat environments without real-world fine-tuning. The project page is available at https://vigor2026.github.io/",
        "authors_display": "Stella X. Yu Team",
        "pdf_url": "http://arxiv.org/abs/2602.16511",
        "code_url": null,
        "date": "2026-02-18",
        "primary_category": "cs.RO",
        "chinese_summary": "鉴于人形机器人在杂乱环境中跌倒恢复的复杂性和现有方法零碎或泛化性差的问题，本文提出了一种统一的跌倒安全方法。该方法基于人类跌倒姿态的可迁移性以及整合感知-运动表征的需求，通过训练一个特权教师模型并蒸馏到仅依赖自我中心深度和本体感觉的学生模型。仿真和真实Unitree G1人形机器人实验证明，该方法在多样化的非平面环境中实现了鲁棒的零样本跌倒安全，无需真实世界微调。"
      },
      {
        "paper_id": "2602.16911",
        "title": "SparTa: Sparse Graphical Task Models from a Handful of Demonstrations",
        "abstract": "Learning long-horizon manipulation tasks efficiently is a central challenge in robot learning from demonstration. Unlike recent endeavors that focus on directly learning the task in the action domain, we focus on inferring what the robot should achieve in the task, rather than how to do so. To this end, we represent evolving scene states using a series of graphical object relationships. We propose a demonstration segmentation and pooling approach that extracts a series of manipulation graphs and estimates distributions over object states across task phases. In contrast to prior graph-based methods that capture only partial interactions or short temporal windows, our approach captures complete object interactions spanning from the onset of control to the end of the manipulation. To improve robustness when learning from multiple demonstrations, we additionally perform object matching using pre-trained visual features. In extensive experiments, we evaluate our method's demonstration segmentation accuracy and the utility of learning from multiple demonstrations for finding a desired minimal task model. Finally, we deploy the fitted models both in simulation and on a real robot, demonstrating that the resulting task representations support reliable execution across environments.",
        "authors_display": "Abhinav Valada Team",
        "pdf_url": "http://arxiv.org/abs/2602.16911",
        "code_url": null,
        "date": "2026-02-18",
        "primary_category": "cs.RO",
        "chinese_summary": "针对机器人从演示中高效学习长距离操作任务的挑战，特别是现有方法多关注动作执行而非任务目标的问题，本研究侧重于推断机器人应实现的目标，而非如何实现。该方法通过图形化对象关系表示场景状态，并提出一种演示分割和池化方法，以提取操作图序列并估计任务阶段中对象状态的分布。实验证明，该方法能准确分割演示并从多个演示中有效学习，所构建的任务表示在仿真和真实机器人环境中均支持可靠的执行。"
      },
      {
        "paper_id": "2602.15724",
        "title": "Learning to Retrieve Navigable Candidates for Efficient Vision-and-Language Navigation",
        "abstract": "Vision-and-Language Navigation (VLN) requires an agent to follow natural-language instructions and navigate through previously unseen environments. Recent approaches increasingly employ large language models (LLMs) as high-level navigators due to their flexibility and reasoning capability. However, prompt-based LLM navigation often suffers from inefficient decision-making, as the model must repeatedly interpret instructions from scratch and reason over noisy and verbose navigable candidates at each step. In this paper, we propose a retrieval-augmented framework to improve the efficiency and stability of LLM-based VLN without modifying or fine-tuning the underlying language model. Our approach introduces retrieval at two complementary levels. At the episode level, an instruction-level embedding retriever selects semantically similar successful navigation trajectories as in-context exemplars, providing task-specific priors for instruction grounding. At the step level, an imitation-learned candidate retriever prunes irrelevant navigable directions before LLM inference, reducing action ambiguity and prompt complexity. Both retrieval modules are lightweight, modular, and trained independently of the LLM. We evaluate our method on the Room-to-Room (R2R) benchmark. Experimental results demonstrate consistent improvements in Success Rate, Oracle Success Rate, and SPL on both seen and unseen environments. Ablation studies further show that instruction-level exemplar retrieval and candidate pruning contribute complementary benefits to global guidance and step-wise decision efficiency. These results indicate that retrieval-augmented decision support is an effective and scalable strategy for enhancing LLM-based vision-and-language navigation.",
        "authors_display": "Lina Yao Team",
        "pdf_url": "http://arxiv.org/abs/2602.15724",
        "code_url": null,
        "date": "2026-02-17",
        "primary_category": "cs.CV",
        "chinese_summary": "为解决基于大语言模型（LLM）的视觉-语言导航（VLN）在决策效率和稳定性方面的挑战，本文提出了一个检索增强框架。该框架在剧集和步骤两个层面引入检索机制：指令级嵌入检索器提供上下文示例进行全局指导，模仿学习的候选检索器修剪不相关的导航方向以提高步进决策效率。在R2R基准测试上的实验结果表明，该方法在成功率和SPL方面均取得显著提升，验证了检索增强决策支持的有效性。"
      },
      {
        "paper_id": "2602.15543",
        "title": "Selective Perception for Robot: Task-Aware Attention in Multimodal VLA",
        "abstract": "In robotics, Vision-Language-Action (VLA) models that integrate diverse multimodal signals from multi-view inputs have emerged as an effective approach. However, most prior work adopts static fusion that processes all visual inputs uniformly, which incurs unnecessary computational overhead and allows task-irrelevant background information to act as noise. Inspired by the principles of human active perception, we propose a dynamic information fusion framework designed to maximize the efficiency and robustness of VLA models. Our approach introduces a lightweight adaptive routing architecture that analyzes the current text prompt and observations from a wrist-mounted camera in real-time to predict the task-relevance of multiple camera views. By conditionally attenuating computations for views with low informational utility and selectively providing only essential visual features to the policy network, Our framework achieves computation efficiency proportional to task relevance. Furthermore, to efficiently secure large-scale annotation data for router training, we established an automated labeling pipeline utilizing Vision-Language Models (VLMs) to minimize data collection and annotation costs. Experimental results in real-world robotic manipulation scenarios demonstrate that the proposed approach achieves significant improvements in both inference efficiency and control performance compared to existing VLA models, validating the effectiveness and practicality of dynamic information fusion in resource-constrained, real-time robot control environments.",
        "authors_display": "Soo-Chul Lim Team",
        "pdf_url": "http://arxiv.org/abs/2602.15543",
        "code_url": null,
        "date": "2026-02-17",
        "primary_category": "cs.RO",
        "chinese_summary": "针对现有机器人视觉-语言-动作（VLA）模型静态融合多视图输入导致的计算开销和背景噪声问题，本文提出了一种动态信息融合框架。该方法引入轻量级自适应路由架构，实时评估摄像机视图的任务相关性，有条件地衰减低信息效用视图的计算，从而选择性地提供关键视觉特征。真实世界机器人操作实验结果表明，该方法在推理效率和控制性能上均优于现有VLA模型，验证了其在资源受限实时控制环境中的实用性。"
      },
      {
        "paper_id": "2602.15351",
        "title": "Feasibility-aware Imitation Learning from Observation with Multimodal Feedback",
        "abstract": "Imitation learning frameworks that learn robot control policies from demonstrators' motions via hand-mounted demonstration interfaces have attracted increasing attention. However, due to differences in physical characteristics between demonstrators and robots, this approach faces two limitations: i) the demonstration data do not include robot actions, and ii) the demonstrated motions may be infeasible for robots. These limitations make policy learning difficult. To address them, we propose Feasibility-Aware Behavior Cloning from Observation (FABCO). FABCO integrates behavior cloning from observation, which complements robot actions using robot dynamics models, with feasibility estimation. In feasibility estimation, the demonstrated motions are evaluated using a robot-dynamics model, learned from the robot's execution data, to assess reproducibility under the robot's dynamics. The estimated feasibility is used for multimodal feedback and feasibility-aware policy learning to improve the demonstrator's motions and learn robust policies. Multimodal feedback provides feasibility through the demonstrator's visual and haptic senses to promote feasible demonstrated motions. Feasibility-aware policy learning reduces the influence of demonstrated motions that are infeasible for robots, enabling the learning of policies that robots can execute stably. We conducted experiments with 15 participants on two tasks and confirmed that FABCO improves imitation learning performance by more than 3.2 times compared to the case without feasibility feedback.",
        "authors_display": "Takamitsu Matsubara Team",
        "pdf_url": "http://arxiv.org/abs/2602.15351",
        "code_url": null,
        "date": "2026-02-17",
        "primary_category": "cs.RO",
        "chinese_summary": "针对模仿学习中示教者动作与机器人物理特性不符导致学习困难的问题，本文提出了“可行性感知行为克隆自观察”（FABCO）框架。FABCO通过结合行为克隆自观察与可行性估计，利用机器人动力学模型评估示教动作的可再现性，并将估计的可行性用于多模态反馈和可行性感知策略学习。实验结果表明，相比无可行性反馈的情况，FABCO将模仿学习性能提高了3.2倍以上。"
      },
      {
        "paper_id": "2602.14968",
        "title": "PhyScensis: Physics-Augmented LLM Agents for Complex Physical Scene Arrangement",
        "abstract": "Automatically generating interactive 3D environments is crucial for scaling up robotic data collection in simulation. While prior work has primarily focused on 3D asset placement, it often overlooks the physical relationships between objects (e.g., contact, support, balance, and containment), which are essential for creating complex and realistic manipulation scenarios such as tabletop arrangements, shelf organization, or box packing. Compared to classical 3D layout generation, producing complex physical scenes introduces additional challenges: (a) higher object density and complexity (e.g., a small shelf may hold dozens of books), (b) richer supporting relationships and compact spatial layouts, and (c) the need to accurately model both spatial placement and physical properties. To address these challenges, we propose PhyScensis, an LLM agent-based framework powered by a physics engine, to produce physically plausible scene configurations with high complexity. Specifically, our framework consists of three main components: an LLM agent iteratively proposes assets with spatial and physical predicates; a solver, equipped with a physics engine, realizes these predicates into a 3D scene; and feedback from the solver informs the agent to refine and enrich the configuration. Moreover, our framework preserves strong controllability over fine-grained textual descriptions and numerical parameters (e.g., relative positions, scene stability), enabled through probabilistic programming for stability and a complementary heuristic that jointly regulates stability and spatial relations. Experimental results show that our method outperforms prior approaches in scene complexity, visual quality, and physical accuracy, offering a unified pipeline for generating complex physical scene layouts for robotic manipulation.",
        "authors_display": "Chuang Gan Team",
        "pdf_url": "http://arxiv.org/abs/2602.14968",
        "code_url": null,
        "date": "2026-02-16",
        "primary_category": "cs.RO",
        "chinese_summary": "为解决自动化3D环境生成中忽略物体间物理关系导致场景不真实和复杂性不足的问题，本文提出了PhyScensis框架。该框架是一个基于LLM代理并由物理引擎驱动的系统，通过LLM代理迭代提出带有空间和物理谓词的资产，并由求解器利用物理引擎将其实现为3D场景。实验结果表明，PhyScensis在场景复杂度、视觉质量和物理精度方面均优于现有方法，为机器人操作提供了生成复杂物理场景布局的统一管道。"
      },
      {
        "paper_id": "2602.14874",
        "title": "Affordance Transfer Across Object Instances via Semantically Anchored Functional Map",
        "abstract": "Traditional learning from demonstration (LfD) generally demands a cumbersome collection of physical demonstrations, which can be time-consuming and challenging to scale. Recent advances show that robots can instead learn from human videos by extracting interaction cues without direct robot involvement. However, a fundamental challenge remains: how to generalize demonstrated interactions across different object instances that share similar functionality but vary significantly in geometry. In this work, we propose \\emph{Semantic Anchored Functional Maps} (SemFM), a framework for transferring affordances across objects from a single visual demonstration. Starting from a coarse mesh reconstructed from an image, our method identifies semantically corresponding functional regions between objects, selects mutually exclusive semantic anchors, and propagates these constraints over the surface using a functional map to obtain a dense, semantically consistent correspondence. This enables demonstrated interaction regions to be transferred across geometrically diverse objects in a lightweight and interpretable manner. Experiments on synthetic object categories and real-world robotic manipulation tasks show that our approach enables accurate affordance transfer with modest computational cost, making it well-suited for practical robotic perception-to-action pipelines.",
        "authors_display": "Weiming Zhi Team",
        "pdf_url": "http://arxiv.org/abs/2602.14874",
        "code_url": null,
        "date": "2026-02-16",
        "primary_category": "cs.RO",
        "chinese_summary": "针对传统从示范学习（LfD）数据收集成本高昂，以及如何将示范交互泛化到几何差异大但功能相似物体上的挑战，本文提出了语义锚定功能图（SemFM）框架。该方法从单一视觉示范中，通过识别语义对应功能区域、选择语义锚点并利用功能图传播约束，实现了跨对象的可用性（affordance）转移。实验证明，SemFM以适度的计算成本实现了准确的可用性转移，适用于实际机器人感知-动作流程。"
      },
      {
        "paper_id": "2602.14577",
        "title": "DriveFine: Refining-Augmented Masked Diffusion VLA for Precise and Robust Driving",
        "abstract": "Vision-Language-Action (VLA) models for autonomous driving increasingly adopt generative planners trained with imitation learning followed by reinforcement learning. Diffusion-based planners suffer from modality alignment difficulties, low training efficiency, and limited generalization. Token-based planners are plagued by cumulative causal errors and irreversible decoding. In summary, the two dominant paradigms exhibit complementary strengths and weaknesses. In this paper, we propose DriveFine, a masked diffusion VLA model that combines flexible decoding with self-correction capabilities. In particular, we design a novel plug-and-play block-MoE, which seamlessly injects a refinement expert on top of the generation expert. By enabling explicit expert selection during inference and gradient blocking during training, the two experts are fully decoupled, preserving the foundational capabilities and generic patterns of the pretrained weights, which highlights the flexibility and extensibility of the block-MoE design. Furthermore, we design a hybrid reinforcement learning strategy that encourages effective exploration of refinement expert while maintaining training stability. Extensive experiments on NAVSIM v1, v2, and Navhard benchmarks demonstrate that DriveFine exhibits strong efficacy and robustness. The code will be released at https://github.com/MSunDYY/DriveFine.",
        "authors_display": "Yan Wang Team",
        "pdf_url": "http://arxiv.org/abs/2602.14577",
        "code_url": null,
        "date": "2026-02-16",
        "primary_category": "cs.CV",
        "chinese_summary": "鉴于自动驾驶领域视觉-语言-行动（VLA）模型中的生成式规划器（如扩散模型和基于Token的模型）存在的缺点，如模态对齐困难、训练效率低、泛化能力有限及累积因果误差等问题，本研究提出了DriveFine，一个结合灵活解码和自校正能力的掩码扩散VLA模型。该模型引入了即插即用的block-MoE（专家混合）模块，将精炼专家与生成专家无缝结合，并通过推理时的显式专家选择和训练时的梯度阻断实现专家解耦。此外，设计了一种混合强化学习策略来促进精炼专家的有效探索并保持训练稳定性。在NAVSIM v1、v2和Navhard基准上的大量实验证明，DriveFine表现出强大的有效性和鲁棒性。"
      },
      {
        "paper_id": "2602.14438",
        "title": "RoboSolver: A Multi-Agent Large Language Model Framework for Solving Robotic Arm Problems",
        "abstract": "This study proposes an intelligent multi-agent framework built on LLMs and VLMs and specifically tailored to robotics. The goal is to integrate the strengths of LLMs and VLMs with computational tools to automatically analyze and solve problems related to robotic manipulators. Our developed framework accepts both textual and visual inputs and can automatically perform forward and inverse kinematics, compute velocities and accelerations of key points, generate 3D simulations of the robot, and ultimately execute motion control within the simulated environment, all according to the user's query. To evaluate the framework, three benchmark tests were designed, each consisting of ten questions. In the first benchmark test, the framework was evaluated while connected to GPT-4o, DeepSeek-V3.2, and Claude-Sonnet-4.5, as well as their corresponding raw models. The objective was to extract the forward kinematics of robots directly from textual descriptions. The results showed that the framework integrated with GPT-4o achieved the highest accuracy, reaching 0.97 in computing the final solution, whereas the raw model alone attained an accuracy of only 0.30 for the same task. Similarly, for the other two models, the framework consistently outperformed the corresponding raw models in terms of accuracy. The second benchmark test was identical to the first, except that the input was provided in visual form. In this test, the GPT-4o LLM was used alongside the Gemini 2.5 Pro VLM. The results showed that the framework achieved an accuracy of 0.93 in obtaining the final answer, which is approximately 20% higher than that of the corresponding raw model. The third benchmark test encompassed a range of robotic tasks, including simulation, control, velocity and acceleration computation, as well as inverse kinematics and Jacobian calculation, for which the framework achieved an accuracy of 0.97.",
        "authors_display": "Alireza Taheri Team",
        "pdf_url": "http://arxiv.org/abs/2602.14438",
        "code_url": null,
        "date": "2026-02-16",
        "primary_category": "cs.RO",
        "chinese_summary": "为整合大型语言模型（LLMs）和视觉-语言模型（VLMs）与计算工具的能力以自动化解决机器人操作臂问题，本研究提出了一种智能多智能体框架。该框架能够接受文本和视觉输入，并根据用户查询自动执行正逆运动学计算、关键点速度和加速度计算、机器人3D仿真生成，最终在仿真环境中执行运动控制。通过与GPT-4o、DeepSeek-V3.2和Claude-Sonnet-4.5等模型及Gemini 2.5 Pro VLM结合的基准测试，结果显示该框架显著提高了各种任务的准确性，例如与GPT-4o结合在文本描述下的正运动学计算准确率达到0.97，在视觉输入下达到0.93，且在多类机器人任务中也表现出0.97的准确率，远超单一模型的表现。"
      },
      {
        "paper_id": "2602.14434",
        "title": "A Soft Wrist with Anisotropic and Selectable Stiffness for Robust Robot Learning in Contact-rich Manipulation",
        "abstract": "Contact-rich manipulation tasks in unstructured environments pose significant robustness challenges for robot learning, where unexpected collisions can cause damage and hinder policy acquisition. Existing soft end-effectors face fundamental limitations: they either provide a limited deformation range, lack directional stiffness control, or require complex actuation systems that compromise practicality. This study introduces CLAW (Compliant Leaf-spring Anisotropic soft Wrist), a novel soft wrist mechanism that addresses these limitations through a simple yet effective design using two orthogonal leaf springs and rotary joints with a locking mechanism. CLAW provides large 6-degree-of-freedom deformation (40mm lateral, 20mm vertical), anisotropic stiffness that is tunable across three distinct modes, while maintaining lightweight construction (330g) at low cost ($550). Experimental evaluations using imitation learning demonstrate that CLAW achieves 76% success rate in benchmark peg-insertion tasks, outperforming both the Fin Ray gripper (43%) and rigid gripper alternatives (36%). CLAW successfully handles diverse contact-rich scenarios, including precision assembly with tight tolerances and delicate object manipulation, demonstrating its potential to enable robust robot learning in contact-rich domains. Project page: https://project-page-manager.github.io/CLAW/",
        "authors_display": "Masashi Hamaya Team",
        "pdf_url": "http://arxiv.org/abs/2602.14434",
        "code_url": null,
        "date": "2026-02-16",
        "primary_category": "cs.RO",
        "chinese_summary": "针对非结构化环境中接触密集型操作任务中现有软末端执行器变形范围有限、缺乏定向刚度控制或驱动系统复杂等问题，本研究引入了一种新型柔性腕部机构CLAW（Compliant Leaf-spring Anisotropic soft Wrist）。该机制通过两个正交的板簧和带锁定机构的旋转关节，实现了大范围的六自由度变形、可调的三种模式各向异性刚度，同时保持了轻量化和低成本。在模仿学习的实验评估中，CLAW在插栓任务中的成功率达到76%，优于Fin Ray夹持器（43%）和刚性夹持器（36%），展示了其在处理精密装配和精细物体操作等接触密集型场景中的潜力，有望提高机器人学习的鲁棒性。"
      },
      {
        "paper_id": "2602.14363",
        "title": "AdaptManip: Learning Adaptive Whole-Body Object Lifting and Delivery with Online Recurrent State Estimation",
        "abstract": "This paper presents Adaptive Whole-body Loco-Manipulation, AdaptManip, a fully autonomous framework for humanoid robots to perform integrated navigation, object lifting, and delivery. Unlike prior imitation learning-based approaches that rely on human demonstrations and are often brittle to disturbances, AdaptManip aims to train a robust loco-manipulation policy via reinforcement learning without human demonstrations or teleoperation data. The proposed framework consists of three coupled components: (1) a recurrent object state estimator that tracks the manipulated object in real time under limited field-of-view and occlusions; (2) a whole-body base policy for robust locomotion with residual manipulation control for stable object lifting and delivery; and (3) a LiDAR-based robot global position estimator that provides drift-robust localization. All components are trained in simulation using reinforcement learning and deployed on real hardware in a zero-shot manner. Experimental results show that AdaptManip significantly outperforms baseline methods, including imitation learning-based approaches, in adaptability and overall success rate, while accurate object state estimation improves manipulation performance even under occlusion. We further demonstrate fully autonomous real-world navigation, object lifting, and delivery on a humanoid robot.",
        "authors_display": "Sehoon Ha Team",
        "pdf_url": "http://arxiv.org/abs/2602.14363",
        "code_url": "https://morganbyrd03.github.io/adaptmanip/",
        "date": "2026-02-16",
        "primary_category": "cs.RO",
        "chinese_summary": "针对人形机器人集成导航、物体抓取和递送（运动操作）任务中，现有模仿学习方法依赖人类示教且对干扰脆弱的问题，本研究提出了AdaptManip框架。该框架通过强化学习训练鲁棒的运动操作策略，无需人类示教数据，并由三个耦合组件构成：实时跟踪被操纵物体的循环对象状态估计器、用于稳定运动和操作控制的全身基础策略以及基于激光雷达的机器人全局位姿估计器。所有组件均在仿真中训练并零样本部署到真实硬件。实验结果表明，AdaptManip在适应性和整体成功率上显著优于包括模仿学习在内的基线方法，且精确的对象状态估计即使在遮挡下也能提升操作性能，并在真实世界中成功展示了自主导航、物体抓取和递送能力。"
      },
      {
        "paper_id": "2602.14252",
        "title": "GRAIL: Goal Recognition Alignment through Imitation Learning",
        "abstract": "Understanding an agent's goals from its behavior is fundamental to aligning AI systems with human intentions. Existing goal recognition methods typically rely on an optimal goal-oriented policy representation, which may differ from the actor's true behavior and hinder the accurate recognition of their goal. To address this gap, this paper introduces Goal Recognition Alignment through Imitation Learning (GRAIL), which leverages imitation learning and inverse reinforcement learning to learn one goal-directed policy for each candidate goal directly from (potentially suboptimal) demonstration trajectories. By scoring an observed partial trajectory with each learned goal-directed policy in a single forward pass, GRAIL retains the one-shot inference capability of classical goal recognition while leveraging learned policies that can capture suboptimal and systematically biased behavior. Across the evaluated domains, GRAIL increases the F1-score by more than 0.5 under systematically biased optimal behavior, achieves gains of approximately 0.1-0.3 under suboptimal behavior, and yields improvements of up to 0.4 under noisy optimal trajectories, while remaining competitive in fully optimal settings. This work contributes toward scalable and robust models for interpreting agent goals in uncertain environments.",
        "authors_display": "Reuth Mirsky Team",
        "pdf_url": "http://arxiv.org/abs/2602.14252",
        "code_url": null,
        "date": "2026-02-15",
        "primary_category": "cs.AI",
        "chinese_summary": "鉴于从智能体行为中理解其目标对于AI系统与人类意图对齐至关重要，但现有目标识别方法常依赖最优策略表示而与实际（可能次优）行为不符，本研究提出了通过模仿学习实现目标识别对齐（GRAIL）的方法。GRAIL利用模仿学习和逆强化学习，直接从（可能次优的）示教轨迹中为每个候选目标学习一个目标导向策略。通过对观察到的部分轨迹进行单次前向传播，GRAIL在保留经典目标识别的单次推理能力的同时，能够捕获次优和系统性偏差行为。实验结果显示，GRAIL在系统性偏差最优行为下将F1分数提高超过0.5，在次优行为下提高约0.1-0.3，在带噪声的最优轨迹下提高高达0.4，同时在完全最优设置下保持竞争力。"
      },
      {
        "paper_id": "2602.14193",
        "title": "Learning Part-Aware Dense 3D Feature Field for Generalizable Articulated Object Manipulation",
        "abstract": "Articulated object manipulation is essential for various real-world robotic tasks, yet generalizing across diverse objects remains a major challenge. A key to generalization lies in understanding functional parts (e.g., door handles and knobs), which indicate where and how to manipulate across diverse object categories and shapes. Previous works attempted to achieve generalization by introducing foundation features, while these features are mostly 2D-based and do not specifically consider functional parts. When lifting these 2D features to geometry-profound 3D space, challenges arise, such as long runtimes, multi-view inconsistencies, and low spatial resolution with insufficient geometric information. To address these issues, we propose Part-Aware 3D Feature Field (PA3FF), a novel dense 3D feature with part awareness for generalizable articulated object manipulation. PA3FF is trained by 3D part proposals from a large-scale labeled dataset, via a contrastive learning formulation. Given point clouds as input, PA3FF predicts a continuous 3D feature field in a feedforward manner, where the distance between point features reflects the proximity of functional parts: points with similar features are more likely to belong to the same part. Building on this feature, we introduce the Part-Aware Diffusion Policy (PADP), an imitation learning framework aimed at enhancing sample efficiency and generalization for robotic manipulation. We evaluate PADP on several simulated and real-world tasks, demonstrating that PA3FF consistently outperforms a range of 2D and 3D representations in manipulation scenarios, including CLIP, DINOv2, and Grounded-SAM. Beyond imitation learning, PA3FF enables diverse downstream methods, including correspondence learning and segmentation tasks, making it a versatile foundation for robotic manipulation. Project page: https://pa3ff.github.io",
        "authors_display": "Hao Dong Team",
        "pdf_url": "http://arxiv.org/abs/2602.14193",
        "code_url": "https://pa3ff.github.io",
        "date": "2026-02-15",
        "primary_category": "cs.RO",
        "chinese_summary": "鉴于关节物体操作在多样化物体间的泛化挑战，以及现有2D基础特征在提升至3D空间时面临运行时间长、多视角不一致及空间分辨率低等问题，本研究提出了Part-Aware 3D Feature Field (PA3FF)。PA3FF是一种新型的密集3D特征，通过对比学习利用大规模标注数据集中的3D部件提议进行训练，能从点云输入中预测连续的3D特征场，其中点特征的距离反映功能部件的接近程度。在此基础上，研究引入了Part-Aware Diffusion Policy (PADP) 模仿学习框架，以提升样本效率和泛化能力。实验证明，PA3FF在模拟和真实世界的操作任务中始终优于多种2D和3D表征，并能支持对应学习和分割等多种下游任务。"
      },
      {
        "paper_id": "2602.14032",
        "title": "RoboAug: One Annotation to Hundreds of Scenes via Region-Contrastive Data Augmentation for Robotic Manipulation",
        "abstract": "Enhancing the generalization capability of robotic learning to enable robots to operate effectively in diverse, unseen scenes is a fundamental and challenging problem. Existing approaches often depend on pretraining with large-scale data collection, which is labor-intensive and time-consuming, or on semantic data augmentation techniques that necessitate an impractical assumption of flawless upstream object detection in real-world scenarios. In this work, we propose RoboAug, a novel generative data augmentation framework that significantly minimizes the reliance on large-scale pretraining and the perfect visual recognition assumption by requiring only the bounding box annotation of a single image during training. Leveraging this minimal information, RoboAug employs pre-trained generative models for precise semantic data augmentation and integrates a plug-and-play region-contrastive loss to help models focus on task-relevant regions, thereby improving generalization and boosting task success rates. We conduct extensive real-world experiments on three robots, namely UR-5e, AgileX, and Tien Kung 2.0, spanning over 35k rollouts. Empirical results demonstrate that RoboAug significantly outperforms state-of-the-art data augmentation baselines. Specifically, when evaluating generalization capabilities in unseen scenes featuring diverse combinations of backgrounds, distractors, and lighting conditions, our method achieves substantial gains over the baseline without augmentation. The success rates increase from 0.09 to 0.47 on UR-5e, from 0.16 to 0.60 on AgileX, and from 0.19 to 0.67 on Tien Kung 2.0. These results highlight the superior generalization and effectiveness of RoboAug in real-world manipulation tasks. Our project is available at https://x-roboaug.github.io/.",
        "authors_display": "Jian Tang Team",
        "pdf_url": "http://arxiv.org/abs/2602.14032",
        "code_url": null,
        "date": "2026-02-15",
        "primary_category": "cs.RO",
        "chinese_summary": "为解决机器人学习在多样化、未知场景中泛化能力差的问题，同时避免对大规模预训练和完美上游物体检测的依赖，本研究提出了RoboAug，一个生成式数据增强框架。该方法仅需单张图像的边界框标注，即可利用预训练的生成模型进行精确的语义数据增强，并整合即插即用的区域对比损失，使模型聚焦于任务相关区域。在UR-5e、AgileX和Tien Kung 2.0三款机器人上进行的超过3.5万次真实世界实验表明，RoboAug显著优于现有数据增强基线方法。在包含多样化背景、干扰物和光照条件的未知场景中，该方法将UR-5e的成功率从0.09提升至0.47，AgileX从0.16提升至0.60，Tien Kung 2.0从0.19提升至0.67，凸显了其在真实世界操作任务中的卓越泛化能力和有效性。"
      },
      {
        "paper_id": "2602.13977",
        "title": "WoVR: World Models as Reliable Simulators for Post-Training VLA Policies with RL",
        "abstract": "Reinforcement learning (RL) promises to unlock capabilities beyond imitation learning for Vision-Language-Action (VLA) models, but its requirement for massive real-world interaction prevents direct deployment on physical robots. Recent work attempts to use learned world models as simulators for policy optimization, yet closed-loop imagined rollouts inevitably suffer from hallucination and long-horizon error accumulation. Such errors do not merely degrade visual fidelity; they corrupt the optimization signal, encouraging policies to exploit model inaccuracies rather than genuine task progress. We propose WoVR, a reliable world-model-based reinforcement learning framework for post-training VLA policies. Instead of assuming a faithful world model, WoVR explicitly regulates how RL interacts with imperfect imagined dynamics. It improves rollout stability through a controllable action-conditioned video world model, reshapes imagined interaction to reduce effective error depth via Keyframe-Initialized Rollouts, and maintains policy-simulator alignment through World Model-Policy co-evolution. Extensive experiments on LIBERO benchmarks and real-world robotic manipulation demonstrate that WoVR enables stable long-horizon imagined rollouts and effective policy optimization, improving average LIBERO success from 39.95% to 69.2% (+29.3 points) and real-robot success from 61.7% to 91.7% (+30.0 points). These results show that learned world models can serve as practical simulators for reinforcement learning when hallucination is explicitly controlled.",
        "authors_display": "Dongbin Zhao Team",
        "pdf_url": "http://arxiv.org/abs/2602.13977",
        "code_url": null,
        "date": "2026-02-15",
        "primary_category": "cs.RO",
        "chinese_summary": "为克服强化学习在机器人VLA模型中因大量真实世界交互需求而难以直接部署，以及世界模型模拟器在长周期想象轨迹中易产生幻觉和误差累积的问题，本研究提出了WoVR框架。WoVR通过可控的动作条件视频世界模型提高轨迹稳定性，利用关键帧初始化轨迹减少有效误差深度，并通过世界模型-策略协同演化保持对齐。实验结果显示，WoVR在LIBERO基准和真实机器人操作任务中实现了稳定的长周期想象轨迹和有效的策略优化，平均成功率在LIBERO上提升了29.3个百分点，在真实机器人上提升了30.0个百分点，证明了在有效控制幻觉的情况下，学习到的世界模型可作为实用的强化学习模拟器。"
      },
      {
        "paper_id": "2602.13865",
        "title": "Enabling Option Learning in Sparse Rewards with Hindsight Experience Replay",
        "abstract": "Hierarchical Reinforcement Learning (HRL) frameworks like Option-Critic (OC) and Multi-updates Option Critic (MOC) have introduced significant advancements in learning reusable options. However, these methods underperform in multi-goal environments with sparse rewards, where actions must be linked to temporally distant outcomes. To address this limitation, we first propose MOC-HER, which integrates the Hindsight Experience Replay (HER) mechanism into the MOC framework. By relabeling goals from achieved outcomes, MOC-HER can solve sparse reward environments that are intractable for the original MOC. However, this approach is insufficient for object manipulation tasks, where the reward depends on the object reaching the goal rather than on the agent's direct interaction. This makes it extremely difficult for HRL agents to discover how to interact with these objects. To overcome this issue, we introduce Dual Objectives Hindsight Experience Replay (2HER), a novel extension that creates two sets of virtual goals. In addition to relabeling goals based on the object's final state (standard HER), 2HER also generates goals from the agent's effector positions, rewarding the agent for both interacting with the object and completing the task. Experimental results in robotic manipulation environments show that MOC-2HER achieves success rates of up to 90%, compared to less than 11% for both MOC and MOC-HER. These results highlight the effectiveness of our dual objective relabeling strategy in sparse reward, multi-goal tasks.",
        "authors_display": "Gabriel de Oliveira Ramos Team",
        "pdf_url": "http://arxiv.org/abs/2602.13865",
        "code_url": null,
        "date": "2026-02-14",
        "primary_category": "cs.AI",
        "chinese_summary": "针对分层强化学习框架（如MOC）在稀疏奖励多目标环境中表现不佳，特别是在对象操作任务中难以发现与对象的交互策略问题，本研究首先提出了MOC-HER，将回溯经验回放（HER）集成到MOC中。在此基础上，为更有效处理对象操作任务，进一步引入了Dual Objectives Hindsight Experience Replay (2HER)，通过同时生成对象最终状态目标和智能体效应器位置目标，奖励智能体与对象的交互和任务完成。实验结果表明，MOC-2HER在机器人操作环境中的成功率高达90%，远高于MOC和MOC-HER的不足11%，验证了双目标重标记策略的有效性。"
      },
      {
        "paper_id": "2602.13810",
        "title": "Mean Flow Policy with Instantaneous Velocity Constraint for One-step Action Generation",
        "abstract": "Learning expressive and efficient policy functions is a promising direction in reinforcement learning (RL). While flow-based policies have recently proven effective in modeling complex action distributions with a fast deterministic sampling process, they still face a trade-off between expressiveness and computational burden, which is typically controlled by the number of flow steps. In this work, we propose mean velocity policy (MVP), a new generative policy function that models the mean velocity field to achieve the fastest one-step action generation. To ensure its high expressiveness, an instantaneous velocity constraint (IVC) is introduced on the mean velocity field during training. We theoretically prove that this design explicitly serves as a crucial boundary condition, thereby improving learning accuracy and enhancing policy expressiveness. Empirically, our MVP achieves state-of-the-art success rates across several challenging robotic manipulation tasks from Robomimic and OGBench. It also delivers substantial improvements in training and inference speed over existing flow-based policy baselines.",
        "authors_display": "Shengbo Eben Li Team",
        "pdf_url": "http://arxiv.org/abs/2602.13810",
        "code_url": null,
        "date": "2026-02-14",
        "primary_category": "cs.LG",
        "chinese_summary": "针对流基策略在强化学习中表达能力与计算负担的权衡问题，本研究提出了一种新的生成式策略函数——平均速度策略（MVP）。MVP通过建模平均速度场实现最快的一步动作生成，并引入瞬时速度约束（IVC）以确保高表达能力。理论上证明IVC作为关键边界条件可提高学习精度和策略表达力。实验结果表明，MVP在Robomimic和OGBench的多个机器人操作任务中取得了最先进的成功率，并在训练和推理速度上显著优于现有流基策略基线。"
      },
      {
        "paper_id": "2602.13806",
        "title": "Gaussian Sequences with Multi-Scale Dynamics for 4D Reconstruction from Monocular Casual Videos",
        "abstract": "Understanding dynamic scenes from casual videos is critical for scalable robot learning, yet four-dimensional (4D) reconstruction under strictly monocular settings remains highly ill-posed. To address this challenge, our key insight is that real-world dynamics exhibits a multi-scale regularity from object to particle level. To this end, we design the multi-scale dynamics mechanism that factorizes complex motion fields. Within this formulation, we propose Gaussian sequences with multi-scale dynamics, a novel representation for dynamic 3D Gaussians derived through compositions of multi-level motion. This layered structure substantially alleviates ambiguity of reconstruction and promotes physically plausible dynamics. We further incorporate multi-modal priors from vision foundation models to establish complementary supervision, constraining the solution space and improving the reconstruction fidelity. Our approach enables accurate and globally consistent 4D reconstruction from monocular casual videos. Experiments of dynamic novel-view synthesis (NVS) on benchmark and real-world manipulation datasets demonstrate considerable improvements over existing methods.",
        "authors_display": "Lei Sun Team",
        "pdf_url": "http://arxiv.org/abs/2602.13806",
        "code_url": null,
        "date": "2026-02-14",
        "primary_category": "cs.CV",
        "chinese_summary": "为解决从单目日常视频中进行四维（4D）动态场景重建的ill-posed问题，本研究基于真实世界动态的多尺度规律性，设计了多尺度动态机制以分解复杂运动场。在此基础上，提出了具有多尺度动态的高斯序列，通过多级运动组合构建动态3D高斯表示，显著减轻了重建歧义并促进物理合理性。同时，结合视觉基础模型的多模态先验提供补充监督，进一步约束解空间并提高重建保真度。实验证明，该方法在动态新视角合成任务中，在基准和真实世界操作数据集上均显著优于现有方法，实现了从单目视频中准确且全局一致的4D重建。"
      },
      {
        "paper_id": "2602.13764",
        "title": "MOTIF: Learning Action Motifs for Few-shot Cross-Embodiment Transfer",
        "abstract": "While vision-language-action (VLA) models have advanced generalist robotic learning, cross-embodiment transfer remains challenging due to kinematic heterogeneity and the high cost of collecting sufficient real-world demonstrations to support fine-tuning. Existing cross-embodiment policies typically rely on shared-private architectures, which suffer from limited capacity of private parameters and lack explicit adaptation mechanisms. To address these limitations, we introduce MOTIF for efficient few-shot cross-embodiment transfer that decouples embodiment-agnostic spatiotemporal patterns, termed action motifs, from heterogeneous action data. Specifically, MOTIF first learns unified motifs via vector quantization with progress-aware alignment and embodiment adversarial constraints to ensure temporal and cross-embodiment consistency. We then design a lightweight predictor that predicts these motifs from real-time inputs to guide a flow-matching policy, fusing them with robot-specific states to enable action generation on new embodiments. Evaluations across both simulation and real-world environments validate the superiority of MOTIF, which significantly outperforms strong baselines in few-shot transfer scenarios by 6.5% in simulation and 43.7% in real-world settings. Code is available at https://github.com/buduz/MOTIF.",
        "authors_display": "Heng Tao Shen Team",
        "pdf_url": "http://arxiv.org/abs/2602.13764",
        "code_url": null,
        "date": "2026-02-14",
        "primary_category": "cs.RO",
        "chinese_summary": "尽管视觉-语言-动作（VLA）模型推动了通用机器人学习，但由于运动学异构性以及收集足够真实世界示范数据进行微调的高成本，跨具身（cross-embodiment）迁移仍然充满挑战。现有跨具身策略通常依赖共享-私有架构，其私有参数容量有限且缺乏明确的适应机制。为解决这些局限性，本文提出了MOTIF框架，旨在实现高效的小样本跨具身迁移，它将具身无关的时空模式（称为动作基序）与异构动作数据解耦。具体而言，MOTIF首先通过带有进度感知对齐和具身对抗约束的矢量量化学习统一的基序，以确保时间和跨具身一致性。然后，设计一个轻量级预测器从实时输入预测这些基序，并将其与机器人特定状态融合，以指导流匹配策略在新的具身上生成动作。模拟和真实世界环境的评估均验证了MOTIF的优越性，在小样本迁移场景中显著优于强基线，模拟中提升6.5%，真实世界中提升43.7%。"
      },
      {
        "paper_id": "2602.13718",
        "title": "HybridFlow: A Two-Step Generative Policy for Robotic Manipulation",
        "abstract": "Limited by inference latency, existing robot manipulation policies lack sufficient real-time interaction capability with the environment. Although faster generation methods such as flow matching are gradually replacing diffusion methods, researchers are pursuing even faster generation suitable for interactive robot control. MeanFlow, as a one-step variant of flow matching, has shown strong potential in image generation, but its precision in action generation does not meet the stringent requirements of robotic manipulation. We therefore propose \\textbf{HybridFlow}, a \\textbf{3-stage method} with \\textbf{2-NFE}: Global Jump in MeanFlow mode, ReNoise for distribution alignment, and Local Refine in ReFlow mode. This method balances inference speed and generation quality by leveraging the rapid advantage of MeanFlow one-step generation while ensuring action precision with minimal generation steps. Through real-world experiments, HybridFlow outperforms the 16-step Diffusion Policy by \\textbf{15--25\\%} in success rate while reducing inference time from 152ms to 19ms (\\textbf{8$\\times$ speedup}, \\textbf{$\\sim$52Hz}); it also achieves 70.0\\% success on unseen-color OOD grasping and 66.3\\% on deformable object folding. We envision HybridFlow as a practical low-latency method to enhance real-world interaction capabilities of robotic manipulation policies.",
        "authors_display": "Yide Liu Team",
        "pdf_url": "http://arxiv.org/abs/2602.13718",
        "code_url": null,
        "date": "2026-02-14",
        "primary_category": "cs.RO",
        "chinese_summary": "现有的机器人操作策略受推理延迟限制，缺乏足够的实时环境交互能力。尽管流匹配等更快的生成方法正逐步取代扩散方法，但其精度仍难以满足机器人操作的严格要求。本文关注MeanFlow作为流匹配的单步变体虽然快速但在动作生成精度上的不足。为平衡推理速度和生成质量，本文提出了HybridFlow，这是一种具有2-NFE（函数评估次数）的三阶段方法，包括MeanFlow模式下的全局跳转、用于分布对齐的ReNoise以及ReFlow模式下的局部细化。该方法利用MeanFlow单步生成的快速优势，同时以最少的生成步骤确保动作精度。真实世界实验表明，HybridFlow在成功率上比16步扩散策略高出15-25%，并将推理时间从152毫秒缩短到19毫秒（8倍加速，约52赫兹）；在未见颜色OOD抓取和可变形物体折叠任务上分别达到了70.0%和66.3%的成功率。这些结果表明HybridFlow是一种实用的低延迟方法，能增强机器人操作策略的真实世界交互能力。"
      },
      {
        "paper_id": "2602.13689",
        "title": "Symmetry-Aware Fusion of Vision and Tactile Sensing via Bilateral Force Priors for Robotic Manipulation",
        "abstract": "Insertion tasks in robotic manipulation demand precise, contact-rich interactions that vision alone cannot resolve. While tactile feedback is intuitively valuable, existing studies have shown that naïve visuo-tactile fusion often fails to deliver consistent improvements. In this work, we propose a Cross-Modal Transformer (CMT) for visuo-tactile fusion that integrates wrist-camera observations with tactile signals through structured self- and cross-attention. To stabilize tactile embeddings, we further introduce a physics-informed regularization that encourages bilateral force balance, reflecting principles of human motor control. Experiments on the TacSL benchmark show that CMT with symmetry regularization achieves a 96.59% insertion success rate, surpassing naïve and gated fusion baselines and closely matching the privileged \"wrist + contact force\" configuration (96.09%). These results highlight two central insights: (i) tactile sensing is indispensable for precise alignment, and (ii) principled multimodal fusion, further strengthened by physics-informed regularization, unlocks complementary strengths of vision and touch, approaching privileged performance under realistic sensing.",
        "authors_display": "Tao Yu Team",
        "pdf_url": "http://arxiv.org/abs/2602.13689",
        "code_url": null,
        "date": "2026-02-14",
        "primary_category": "cs.RO",
        "chinese_summary": "机器人插入任务需要精密的、富接触的交互，仅凭视觉难以解决。尽管触觉反馈具有直观价值，但现有研究表明，朴素的视觉-触觉融合往往未能持续提供改进。为解决此问题，本文提出了一种用于视觉-触觉融合的跨模态Transformer（CMT），它通过结构化的自注意力与交叉注意力机制整合腕部摄像头观测和触觉信号。为稳定触觉嵌入，本文进一步引入了物理信息正则化，鼓励双边力平衡，反映了人类运动控制的原理。在TacSL基准上的实验表明，带有对称正则化的CMT实现了96.59%的插入成功率，超越了朴素和门控融合基线，并与“腕部+接触力”的优越配置（96.09%）非常接近。这些结果突出表明：触觉感知对于精确对齐不可或缺，以及经过物理信息正则化强化的原则性多模态融合，能够充分发挥视觉和触觉的互补优势，在现实感知条件下接近最优性能。"
      },
      {
        "paper_id": "2602.13640",
        "title": "Hierarchical Audio-Visual-Proprioceptive Fusion for Precise Robotic Manipulation",
        "abstract": "Existing robotic manipulation methods primarily rely on visual and proprioceptive observations, which may struggle to infer contact-related interaction states in partially observable real-world environments. Acoustic cues, by contrast, naturally encode rich interaction dynamics during contact, yet remain underexploited in current multimodal fusion literature. Most multimodal fusion approaches implicitly assume homogeneous roles across modalities, and thus design flat and symmetric fusion structures. However, this assumption is ill-suited for acoustic signals, which are inherently sparse and contact-driven. To achieve precise robotic manipulation through acoustic-informed perception, we propose a hierarchical representation fusion framework that progressively integrates audio, vision, and proprioception. Our approach first conditions visual and proprioceptive representations on acoustic cues, and then explicitly models higher-order cross-modal interactions to capture complementary dependencies among modalities. The fused representation is leveraged by a diffusion-based policy to directly generate continuous robot actions from multimodal observations. The combination of end-to-end learning and hierarchical fusion structure enables the policy to exploit task-relevant acoustic information while mitigating interference from less informative modalities. The proposed method has been evaluated on real-world robotic manipulation tasks, including liquid pouring and cabinet opening. Extensive experiment results demonstrate that our approach consistently outperforms state-of-the-art multimodal fusion frameworks, particularly in scenarios where acoustic cues provide task-relevant information not readily available from visual observations alone. Furthermore, a mutual information analysis is conducted to interpret the effect of audio cues in robotic manipulation via multimodal fusion.",
        "authors_display": "Peng Liu Team",
        "pdf_url": "http://arxiv.org/abs/2602.13640",
        "code_url": null,
        "date": "2026-02-14",
        "primary_category": "cs.RO",
        "chinese_summary": "现有机器人操作方法主要依赖视觉和本体感受，在部分可观测的真实世界环境中难以推断接触相关的交互状态。而声学线索能自然编码丰富的接触动态，但在多模态融合中却未被充分利用，且大多数融合方法错误地假设模态作用均一。为实现基于声学信息的精确机器人操作，本文提出一种分层表示融合框架，逐步整合音频、视觉和本体感受。该方法首先将视觉和本体感受表示条件化于声学线索，然后明确建模高阶跨模态交互以捕捉模态间的互补依赖。融合后的表示被扩散策略用于直接从多模态观测生成连续机器人动作。在真实世界机器人操作任务（如倒液体和开柜门）上的广泛实验表明，该方法持续优于现有最先进的多模态融合框架，尤其是在声学线索提供视觉无法轻易获得的任务相关信息时。此外，通过互信息分析解释了音频线索在机器人操作中的作用。"
      },
      {
        "paper_id": "2602.13197",
        "title": "Imitating What Works: Simulation-Filtered Modular Policy Learning from Human Videos",
        "abstract": "The ability to learn manipulation skills by watching videos of humans has the potential to unlock a new source of highly scalable data for robot learning. Here, we tackle prehensile manipulation, in which tasks involve grasping an object before performing various post-grasp motions. Human videos offer strong signals for learning the post-grasp motions, but they are less useful for learning the prerequisite grasping behaviors, especially for robots without human-like hands. A promising way forward is to use a modular policy design, leveraging a dedicated grasp generator to produce stable grasps. However, arbitrary stable grasps are often not task-compatible, hindering the robot's ability to perform the desired downstream motion. To address this challenge, we present Perceive-Simulate-Imitate (PSI), a framework for training a modular manipulation policy using human video motion data processed by paired grasp-trajectory filtering in simulation. This simulation step extends the trajectory data with grasp suitability labels, which allows for supervised learning of task-oriented grasping capabilities. We show through real-world experiments that our framework can be used to learn precise manipulation skills efficiently without any robot data, resulting in significantly more robust performance than using a grasp generator naively.",
        "authors_display": "Wei-Chiu Ma Team",
        "pdf_url": "http://arxiv.org/abs/2602.13197",
        "code_url": null,
        "date": "2026-02-13",
        "primary_category": "cs.RO",
        "chinese_summary": "通过观看人类视频学习操作技能，有望为机器人学习提供大规模数据的新来源。然而，人类视频在学习抓取后动作方面提供了强信号，但在学习抓取行为方面用处较小，特别是对于没有类似人手的机器人而言，任意稳定的抓取通常不兼容任务。为解决这一挑战，本文提出了Perceive-Simulate-Imitate (PSI) 框架，用于使用经过模拟中成对抓取-轨迹过滤处理的人类视频运动数据训练模块化操作策略。这一模拟步骤通过抓取适用性标签扩展了轨迹数据，从而能够监督学习面向任务的抓取能力。真实世界实验表明，该框架可以无需任何机器人数据高效学习精确操作技能，与简单使用抓取生成器相比，性能显著更鲁棒。"
      },
      {
        "paper_id": "2602.13086",
        "title": "UniManip: General-Purpose Zero-Shot Robotic Manipulation with Agentic Operational Graph",
        "abstract": "Achieving general-purpose robotic manipulation requires robots to seamlessly bridge high-level semantic intent with low-level physical interaction in unstructured environments. However, existing approaches falter in zero-shot generalization: end-to-end Vision-Language-Action (VLA) models often lack the precision required for long-horizon tasks, while traditional hierarchical planners suffer from semantic rigidity when facing open-world variations. To address this, we present UniManip, a framework grounded in a Bi-level Agentic Operational Graph (AOG) that unifies semantic reasoning and physical grounding. By coupling a high-level Agentic Layer for task orchestration with a low-level Scene Layer for dynamic state representation, the system continuously aligns abstract planning with geometric constraints, enabling robust zero-shot execution. Unlike static pipelines, UniManip operates as a dynamic agentic loop: it actively instantiates object-centric scene graphs from unstructured perception, parameterizes these representations into collision-free trajectories via a safety-aware local planner, and exploits structured memory to autonomously diagnose and recover from execution failures. Extensive experiments validate the system's robust zero-shot capability on unseen objects and tasks, demonstrating a 22.5% and 25.0% higher success rate compared to state-of-the-art VLA and hierarchical baselines, respectively. Notably, the system enables direct zero-shot transfer from fixed-base setups to mobile manipulation without fine-tuning or reconfiguration. Our open-source project page can be found at https://henryhcliu.github.io/unimanip.",
        "authors_display": "Ziwei Wang Team",
        "pdf_url": "http://arxiv.org/abs/2602.13086",
        "code_url": "https://henryhcliu.github.io/unimanip",
        "date": "2026-02-13",
        "primary_category": "cs.RO",
        "chinese_summary": "针对现有机器人操作方法在零样本泛化方面的不足，即端到端VLA模型缺乏精度而传统规划器语义刚性问题，本文提出了UniManip框架。该框架基于双层Agentic Operational Graph (AOG)，通过高层Agentic层进行任务编排和低层Scene层表示动态状态，实现语义推理与物理接地的统一，并以动态智能体循环方式主动实例化场景图、规划无碰撞轨迹并自主恢复失败。实验结果表明，UniManip在未见对象和任务上展现出鲁棒的零样本能力，成功率显著高于现有VLA和分层基线，且支持从固定基座到移动操作的零样本迁移。"
      },
      {
        "paper_id": "2602.13016",
        "title": "How Swarms Differ: Challenges in Collective Behaviour Comparison",
        "abstract": "Collective behaviours often need to be expressed through numerical features, e.g., for classification or imitation learning. This problem is often addressed by proposing an ad-hoc feature set for a particular swarm behaviour context, usually without further consideration of the solution's resilience outside of the conceived context. Yet, the development of automatic methods to design swarm behaviours is dependent on the ability to measure quantitatively the similarity of swarm behaviours. Hence, we investigate the impact of feature sets for collective behaviours. We select swarm feature sets and similarity measures from prior swarm robotics works, which mainly considered a narrow behavioural context and assess their robustness. We demonstrate that the interplay of feature set and similarity measure makes some combinations more suitable to distinguish groups of similar behaviours. We also propose a self-organised map-based approach to identify regions of the feature space where behaviours cannot be easily distinguished.",
        "authors_display": "Jonas Kuckling Team",
        "pdf_url": "http://arxiv.org/abs/2602.13016",
        "code_url": null,
        "date": "2026-02-13",
        "primary_category": "cs.RO",
        "chinese_summary": "针对群体行为分析中数值特征集通常缺乏通用性且难以定量衡量行为相似性的问题，本研究深入探讨了特征集对集体行为的影响。我们从现有群体机器人学工作中筛选出特征集和相似性度量，并评估了它们在特定行为背景外的鲁棒性。研究发现，特征集和相似性度量的相互作用决定了区分相似行为群体的有效性，并提出了一种基于自组织图的方法来识别特征空间中行为难以区分的区域。"
      },
      {
        "paper_id": "2602.12794",
        "title": "SafeFlowMPC: Predictive and Safe Trajectory Planning for Robot Manipulators with Learning-based Policies",
        "abstract": "The emerging integration of robots into everyday life brings several major challenges. Compared to classical industrial applications, more flexibility is needed in combination with real-time reactivity. Learning-based methods can train powerful policies based on demonstrated trajectories, such that the robot generalizes a task to similar situations. However, these black-box models lack interpretability and rigorous safety guarantees. Optimization-based methods provide these guarantees but lack the required flexibility and generalization capabilities. This work proposes SafeFlowMPC, a combination of flow matching and online optimization to combine the strengths of learning and optimization. This method guarantees safety at all times and is designed to meet the demands of real-time execution by using a suboptimal model-predictive control formulation. SafeFlowMPC achieves strong performance in three real-world experiments on a KUKA 7-DoF manipulator, namely two grasping experiment and a dynamic human-robot object handover experiment. A video of the experiments is available at http://www.acin.tuwien.ac.at/42d6. The code is available at https://github.com/TU-Wien-ACIN-CDS/SafeFlowMPC.",
        "authors_display": "Andreas Kugi Team",
        "pdf_url": "http://arxiv.org/abs/2602.12794",
        "code_url": null,
        "date": "2026-02-13",
        "primary_category": "cs.RO",
        "chinese_summary": "面对机器人日益融入日常生活对灵活性和实时反应能力的需求，以及学习方法缺乏安全保证和优化方法泛化能力不足的挑战，本文提出了SafeFlowMPC框架。该框架结合了流匹配与在线优化，旨在融合学习和优化方法的优势，并通过次优模型预测控制公式，实时确保操作安全性。在KUKA 7自由度机械臂上的真实世界实验（包括抓取和人机交接任务）中，SafeFlowMPC展现了强大的性能。"
      },
      {
        "paper_id": "2602.12734",
        "title": "Scaling Single Human Demonstrations for Imitation Learning using Generative Foundational Models",
        "abstract": "Imitation learning is a popular paradigm to teach robots new tasks, but collecting robot demonstrations through teleoperation or kinesthetic teaching is tedious and time-consuming. In contrast, directly demonstrating a task using our human embodiment is much easier and data is available in abundance, yet transfer to the robot can be non-trivial. In this work, we propose Real2Gen to train a manipulation policy from a single human demonstration. Real2Gen extracts required information from the demonstration and transfers it to a simulation environment, where a programmable expert agent can demonstrate the task arbitrarily many times, generating an unlimited amount of data to train a flow matching policy. We evaluate Real2Gen on human demonstrations from three different real-world tasks and compare it to a recent baseline. Real2Gen shows an average increase in the success rate of 26.6% and better generalization of the trained policy due to the abundance and diversity of training data. We further deploy our purely simulation-trained policy zero-shot in the real world. We make the data, code, and trained models publicly available at real2gen.cs.uni-freiburg.de.",
        "authors_display": "Abhinav Valada Team",
        "pdf_url": "http://arxiv.org/abs/2602.12734",
        "code_url": null,
        "date": "2026-02-13",
        "primary_category": "cs.RO",
        "chinese_summary": "鉴于模仿学习中收集机器人演示数据的困难以及人类演示到机器人转移的挑战，本文提出了Real2Gen框架，仅通过单个人类演示来训练操作策略。Real2Gen从人类演示中提取关键信息并传输到模拟环境，利用可编程专家智能体生成无限量的训练数据来学习流匹配策略。实验结果表明，Real2Gen平均成功率提高了26.6%，并且由于训练数据的丰富性，训练出的策略具有更强的泛化能力，实现了纯模拟训练策略的零样本真实世界部署。"
      },
      {
        "paper_id": "2602.12674",
        "title": "$\\mathcal{X}$-KD: General Experiential Knowledge Distillation for Large Language Models",
        "abstract": "Knowledge Distillation (KD) for Large Language Models (LLMs) has become increasingly important as models grow in size and complexity. While existing distillation approaches focus on imitating teacher behavior, they often overlook the original learning environment that shaped the teacher's knowledge. Inspired by the experiential learning theory and inverse reinforcement learning, we propose Experiential Knowledge Distillation ($\\mathcal{X}$-KD), a novel and general framework that enables student models to learn in the teacher's original learning environment. $\\mathcal{X}$-KD adopts the Approximated Variational Reward Imitation Learning (AVRIL) framework to jointly model the teacher's original reward function and perform policy distillation, encouraging consistency between the student policy and the original reward function. Our derivation demonstrates that $\\mathcal{X}$-KD follows the supervised learning framework and applies to both sequence-level and divergence-based distillation methods, underlining the simplicity and flexibility of our approach. Empirical results show that $\\mathcal{X}$-KD outperforms the generalized KD and MiniLLM baselines on abstractive summarization, machine translation, and arithmetic reasoning tasks. Additionally, $\\mathcal{X}$-KD achieves better performance-diversity trade-off and data efficiency than baseline KD approaches.",
        "authors_display": "Yuyu Yuan Team",
        "pdf_url": "http://arxiv.org/abs/2602.12674",
        "code_url": null,
        "date": "2026-02-13",
        "primary_category": "cs.CL",
        "chinese_summary": "针对大语言模型知识蒸馏（KD）中，现有方法常忽视教师模型原始学习环境的问题，本文提出了Experiential Knowledge Distillation ($\\mathcal{X}$-KD) 框架。受经验学习理论和逆强化学习启发，$\\mathcal{X}$-KD采用Approximated Variational Reward Imitation Learning (AVRIL) 框架，联合建模教师的原始奖励函数并执行策略蒸馏，使学生模型能在教师的原始学习环境中学习。实验证明，$\\mathcal{X}$-KD在抽象摘要、机器翻译和算术推理任务上均优于基线方法，并实现了更好的性能-多样性权衡和数据效率。"
      },
      {
        "paper_id": "2602.12656",
        "title": "PMG: Parameterized Motion Generator for Human-like Locomotion Control",
        "abstract": "Recent advances in data-driven reinforcement learning and motion tracking have substantially improved humanoid locomotion, yet critical practical challenges remain. In particular, while low-level motion tracking and trajectory-following controllers are mature, whole-body reference-guided methods are difficult to adapt to higher-level command interfaces and diverse task contexts: they require large, high-quality datasets, are brittle across speed and pose regimes, and are sensitive to robot-specific calibration. To address these limitations, we propose the Parameterized Motion Generator (PMG), a real-time motion generator grounded in an analysis of human motion structure that synthesizes reference trajectories using only a compact set of parameterized motion data together with High-dimensional control commands. Combined with an imitation-learning pipeline and an optimization-based sim-to-real motor parameter identification module, we validate the complete approach on our humanoid prototype ZERITH Z1 and show that, within a single integrated system, PMG produces natural, human-like locomotion, responds precisely to high-dimensional control inputs-including VR-based teleoperation-and enables efficient, verifiable sim-to-real transfer. Together, these results establish a practical, experimentally validated pathway toward natural and deployable humanoid control.",
        "authors_display": "Houde Liu Team",
        "pdf_url": "http://arxiv.org/abs/2602.12656",
        "code_url": null,
        "date": "2026-02-13",
        "primary_category": "cs.RO",
        "chinese_summary": "为解决人形机器人运动中，现有全身体参考引导方法对高级命令接口适应性差、对数据和校准敏感等实际挑战，本文提出了Parameterized Motion Generator (PMG)。PMG是一种基于人类运动结构分析的实时运动生成器，通过紧凑的参数化运动数据和高维控制命令合成参考轨迹，并结合模仿学习流水线和仿真到现实电机参数识别模块。实验证明，该集成系统能生成自然、类人运动，精确响应高维控制输入（如VR远程操作），并实现高效、可验证的仿真到现实迁移。"
      },
      {
        "paper_id": "2602.12633",
        "title": "Real-to-Sim for Highly Cluttered Environments via Physics-Consistent Inter-Object Reasoning",
        "abstract": "Reconstructing physically valid 3D scenes from single-view observations is a prerequisite for bridging the gap between visual perception and robotic control. However, in scenarios requiring precise contact reasoning, such as robotic manipulation in highly cluttered environments, geometric fidelity alone is insufficient. Standard perception pipelines often neglect physical constraints, resulting in invalid states, e.g., floating objects or severe inter-penetration, rendering downstream simulation unreliable. To address these limitations, we propose a novel physics-constrained Real-to-Sim pipeline that reconstructs physically consistent 3D scenes from single-view RGB-D data. Central to our approach is a differentiable optimization pipeline that explicitly models spatial dependencies via a contact graph, jointly refining object poses and physical properties through differentiable rigid-body simulation. Extensive evaluations in both simulation and real-world settings demonstrate that our reconstructed scenes achieve high physical fidelity and faithfully replicate real-world contact dynamics, enabling stable and reliable contact-rich manipulation.",
        "authors_display": "Jun Ma Team",
        "pdf_url": "http://arxiv.org/abs/2602.12633",
        "code_url": "https://physics-constrained-real2sim.github.io",
        "date": "2026-02-13",
        "primary_category": "cs.RO",
        "chinese_summary": "为解决从单视图观测重建物理有效3D场景时，现有方法常忽略物理约束导致无效状态，进而影响下游模拟可靠性的问题，本文提出了一种新颖的物理约束Real-to-Sim管道。该管道能够从单视图RGB-D数据重建物理一致的3D场景，其核心是一个可微分优化管道，通过接触图建模空间依赖，并利用可微分刚体模拟联合优化物体姿态和物理属性。实验结果表明，重建场景具有高物理保真度，能忠实复现真实世界接触动力学，从而实现稳定可靠的接触密集型操作。"
      },
      {
        "paper_id": "2602.13444",
        "title": "FlowHOI: Flow-based Semantics-Grounded Generation of Hand-Object Interactions for Dexterous Robot Manipulation",
        "abstract": "Recent vision-language-action (VLA) models can generate plausible end-effector motions, yet they often fail in long-horizon, contact-rich tasks because the underlying hand-object interaction (HOI) structure is not explicitly represented. An embodiment-agnostic interaction representation that captures this structure would make manipulation behaviors easier to validate and transfer across robots. We propose FlowHOI, a two-stage flow-matching framework that generates semantically grounded, temporally coherent HOI sequences, comprising hand poses, object poses, and hand-object contact states, conditioned on an egocentric observation, a language instruction, and a 3D Gaussian splatting (3DGS) scene reconstruction. We decouple geometry-centric grasping from semantics-centric manipulation, conditioning the latter on compact 3D scene tokens and employing a motion-text alignment loss to semantically ground the generated interactions in both the physical scene layout and the language instruction. To address the scarcity of high-fidelity HOI supervision, we introduce a reconstruction pipeline that recovers aligned hand-object trajectories and meshes from large-scale egocentric videos, yielding an HOI prior for robust generation. Across the GRAB and HOT3D benchmarks, FlowHOI achieves the highest action recognition accuracy and a 1.7$\\times$ higher physics simulation success rate than the strongest diffusion-based baseline, while delivering a 40$\\times$ inference speedup. We further demonstrate real-robot execution on four dexterous manipulation tasks, illustrating the feasibility of retargeting generated HOI representations to real-robot execution pipelines.",
        "authors_display": "Xingxing Zuo Team",
        "pdf_url": "http://arxiv.org/abs/2602.13444",
        "code_url": "https://huajian-zeng.github.io/projects/flowhoi/",
        "date": "2026-02-13",
        "primary_category": "cs.RO",
        "chinese_summary": "近期视觉-语言-动作（VLA）模型能够生成看似合理的末端执行器运动，但在长程、富接触的任务中常常失败，因为缺乏对手-物体交互（HOI）结构的显式表示。为解决此问题，本文提出了FlowHOI，一个两阶段流匹配框架，它能根据第一人称观测、语言指令和3D高斯飞溅（3DGS）场景重建，生成语义接地、时间连贯的HOI序列，包括手部姿态、物体姿态和手-物体接触状态。该框架将以几何为中心的抓取与以语义为中心的操作解耦，后者通过紧凑的3D场景令牌进行条件化，并采用运动-文本对齐损失来语义化生成的交互。为解决高保真HOI监督数据稀缺的问题，本文引入了一个重建流水线，从大规模第一人称视频中恢复对齐的手-物体轨迹和网格，为鲁棒生成提供了HOI先验。FlowHOI在GRAB和HOT3D基准上实现了最高的动作识别精度和比最强扩散基线高1.7倍的物理模拟成功率，同时推理速度提升了40倍。此外，通过将生成的HOI表示重定向到真实机器人执行流程，本文在四个灵巧操作任务上验证了真实机器人执行的可行性。"
      },
      {
        "paper_id": "2602.12155",
        "title": "FAIL: Flow Matching Adversarial Imitation Learning for Image Generation",
        "abstract": "Post-training of flow matching models-aligning the output distribution with a high-quality target-is mathematically equivalent to imitation learning. While Supervised Fine-Tuning mimics expert demonstrations effectively, it cannot correct policy drift in unseen states. Preference optimization methods address this but require costly preference pairs or reward modeling. We propose Flow Matching Adversarial Imitation Learning (FAIL), which minimizes policy-expert divergence through adversarial training without explicit rewards or pairwise comparisons. We derive two algorithms: FAIL-PD exploits differentiable ODE solvers for low-variance pathwise gradients, while FAIL-PG provides a black-box alternative for discrete or computationally constrained settings. Fine-tuning FLUX with only 13,000 demonstrations from Nano Banana pro, FAIL achieves competitive performance on prompt following and aesthetic benchmarks. Furthermore, the framework generalizes effectively to discrete image and video generation, and functions as a robust regularizer to mitigate reward hacking in reward-based optimization. Code and data are available at https://github.com/HansPolo113/FAIL.",
        "authors_display": "Weidi Xie Team",
        "pdf_url": "http://arxiv.org/abs/2602.12155",
        "code_url": null,
        "date": "2026-02-12",
        "primary_category": "cs.CV",
        "chinese_summary": "鉴于流匹配模型后训练与模仿学习的数学等同性，以及监督微调无法纠正策略漂移而偏好优化成本高昂的问题，本文提出了Flow Matching Adversarial Imitation Learning (FAIL) 框架。该框架通过对抗训练最小化策略与专家之间的散度，无需明确奖励或成对比较，并推导出了FAIL-PD和FAIL-PG两种算法。实验证明，FAIL在仅使用少量演示数据的情况下，能在提示遵循和美学基准上取得竞争性性能，并能有效泛化至离散图像和视频生成，同时作为鲁棒正则化器减轻奖励欺骗。"
      },
      {
        "paper_id": "2602.12099",
        "title": "GigaBrain-0.5M*: a VLA That Learns From World Model-Based Reinforcement Learning",
        "abstract": "Vision-language-action (VLA) models that directly predict multi-step action chunks from current observations face inherent limitations due to constrained scene understanding and weak future anticipation capabilities. In contrast, video world models pre-trained on web-scale video corpora exhibit robust spatiotemporal reasoning and accurate future prediction, making them a natural foundation for enhancing VLA learning. Therefore, we propose \\textit{GigaBrain-0.5M*}, a VLA model trained via world model-based reinforcement learning. Built upon \\textit{GigaBrain-0.5}, which is pre-trained on over 10,000 hours of robotic manipulation data, whose intermediate version currently ranks first on the international RoboChallenge benchmark. \\textit{GigaBrain-0.5M*} further integrates world model-based reinforcement learning via \\textit{RAMP} (Reinforcement leArning via world Model-conditioned Policy) to enable robust cross-task adaptation. Empirical results demonstrate that \\textit{RAMP} achieves substantial performance gains over the RECAP baseline, yielding improvements of approximately 30\\% on challenging tasks including \\texttt{Laundry Folding}, \\texttt{Box Packing}, and \\texttt{Espresso Preparation}. Critically, \\textit{GigaBrain-0.5M$^*$} exhibits reliable long-horizon execution, consistently accomplishing complex manipulation tasks without failure as validated by real-world deployment videos on our \\href{https://gigabrain05m.github.io}{project page}.",
        "authors_display": "Zheng Zhu Team",
        "pdf_url": "http://arxiv.org/abs/2602.12099",
        "code_url": "https://gigabrain05m.github.io/",
        "date": "2026-02-12",
        "primary_category": "cs.CV",
        "chinese_summary": "针对传统VLA模型在场景理解和未来预测上的局限性，本研究提出了GigaBrain-0.5M*，一个基于世界模型强化学习的VLA模型。该模型在预训练的GigaBrain-0.5基础上，通过RAMP（Reinforcement leArning via world Model-conditioned Policy）进一步整合了世界模型强化学习，以实现鲁棒的跨任务适应性。实验结果表明，RAMP在洗衣折叠、箱子包装和意式浓缩咖啡制作等复杂任务中，相较于RECAP基线性能提升了约30%，并且在实际部署中展示了可靠的长期执行能力，能够无故障完成复杂操作任务。"
      },
      {
        "paper_id": "2602.12065",
        "title": "Affordance-Graphed Task Worlds: Self-Evolving Task Generation for Scalable Embodied Learning",
        "abstract": "Training robotic policies directly in the real world is expensive and unscalable. Although generative simulation enables large-scale data synthesis, current approaches often fail to generate logically coherent long-horizon tasks and struggle with dynamic physical uncertainties due to open-loop execution. To address these challenges, we propose Affordance-Graphed Task Worlds (AGT-World), a unified framework that autonomously constructs interactive simulated environments and corresponding robot task policies based on real-world observations. Unlike methods relying on random proposals or static replication, AGT-World formalizes the task space as a structured graph, enabling the precise, hierarchical decomposition of complex goals into theoretically grounded atomic primitives. Furthermore, we introduce a Self-Evolution mechanism with hybrid feedback to autonomously refine policies, combining Vision-Language Model reasoning and geometric verification. Extensive experiments demonstrate that our method significantly outperforms in success rates and generalization, achieving a self-improving cycle of proposal, execution, and correction for scalable robot learning.",
        "authors_display": "Changshui Zhang Team",
        "pdf_url": "http://arxiv.org/abs/2602.12065",
        "code_url": null,
        "date": "2026-02-12",
        "primary_category": "cs.RO",
        "chinese_summary": "鉴于在真实世界中训练机器人策略成本高昂且难以扩展，而现有生成模拟方法难以生成逻辑连贯的长时任务并处理动态物理不确定性，本研究提出了Affordance-Graphed Task Worlds (AGT-World) 框架。该框架能够根据真实世界观测自主构建交互式模拟环境和相应的机器人任务策略，通过将任务空间形式化为结构化图实现复杂目标的精确分层分解，并引入结合视觉-语言模型推理和几何验证的混合反馈自进化机制来完善策略。广泛实验证明，AGT-World在成功率和泛化能力上显著优于现有方法，实现了提议、执行和修正的自改进循环，以支持可扩展的机器人学习。"
      },
      {
        "paper_id": "2602.12062",
        "title": "HoloBrain-0 Technical Report",
        "abstract": "In this work, we introduce HoloBrain-0, a comprehensive Vision-Language-Action (VLA) framework that bridges the gap between foundation model research and reliable real-world robot deployment. The core of our system is a novel VLA architecture that explicitly incorporates robot embodiment priors, including multi-view camera parameters and kinematic descriptions (URDF), to enhance 3D spatial reasoning and support diverse embodiments. We validate this design through a scalable ``pre-train then post-train\" paradigm, achieving state-of-the-art results on simulation benchmarks such as RoboTwin 2.0, LIBERO, and GenieSim, as well as strong results on challenging long-horizon real-world manipulation tasks. Notably, our efficient 0.2B-parameter variant rivals significantly larger baselines, enabling low-latency on-device deployment. To further accelerate research and practical adoption, we fully open-source the entire HoloBrain ecosystem, which includes: (1) powerful pre-trained VLA foundations; (2) post-trained checkpoints for multiple simulation suites and real-world tasks; and (3) RoboOrchard, a full-stack VLA infrastructure for data curation, model training and deployment. Together with standardized data collection protocols, this release provides the community with a complete, reproducible path toward high-performance robotic manipulation.",
        "authors_display": "Zhizhong Su Team",
        "pdf_url": "http://arxiv.org/abs/2602.12062",
        "code_url": null,
        "date": "2026-02-12",
        "primary_category": "cs.RO",
        "chinese_summary": "为了弥合基础模型研究与可靠的真实世界机器人部署之间的差距，本研究引入了HoloBrain-0，一个全面的视觉-语言-动作 (VLA) 框架。该框架的核心是一个新颖的VLA架构，明确融入了机器人本体先验信息（如多视图相机参数和运动学描述），以增强3D空间推理并支持多样化的本体。通过“预训练-后训练”范式进行验证，该系统在RoboTwin 2.0、LIBERO和GenieSim等模拟基准测试中取得了最先进的成果，并在长时程真实世界操作任务中表现出色。值得注意的是，其高效的0.2B参数变体能与大得多的基线媲美，并支持低延迟的设备部署。为加速研究和实际应用，HoloBrain生态系统已完全开源。"
      },
      {
        "paper_id": "2602.12032",
        "title": "When would Vision-Proprioception Policies Fail in Robotic Manipulation?",
        "abstract": "Proprioceptive information is critical for precise servo control by providing real-time robotic states. Its collaboration with vision is highly expected to enhance performances of the manipulation policy in complex tasks. However, recent studies have reported inconsistent observations on the generalization of vision-proprioception policies. In this work, we investigate this by conducting temporally controlled experiments. We found that during task sub-phases that robot's motion transitions, which require target localization, the vision modality of the vision-proprioception policy plays a limited role. Further analysis reveals that the policy naturally gravitates toward concise proprioceptive signals that offer faster loss reduction when training, thereby dominating the optimization and suppressing the learning of the visual modality during motion-transition phases. To alleviate this, we propose the Gradient Adjustment with Phase-guidance (GAP) algorithm that adaptively modulates the optimization of proprioception, enabling dynamic collaboration within the vision-proprioception policy. Specifically, we leverage proprioception to capture robotic states and estimate the probability of each timestep in the trajectory belonging to motion-transition phases. During policy learning, we apply fine-grained adjustment that reduces the magnitude of proprioception's gradient based on estimated probabilities, leading to robust and generalizable vision-proprioception policies. The comprehensive experiments demonstrate GAP is applicable in both simulated and real-world environments, across one-arm and dual-arm setups, and compatible with both conventional and Vision-Language-Action models. We believe this work can offer valuable insights into the development of vision-proprioception policies in robotic manipulation.",
        "authors_display": "Di Hu Team",
        "pdf_url": "http://arxiv.org/abs/2602.12032",
        "code_url": null,
        "date": "2026-02-12",
        "primary_category": "cs.RO",
        "chinese_summary": "针对视觉-本体感受策略在复杂任务中泛化能力不一致的问题，本研究发现，在机器人运动转换的子阶段，视觉模态的作用有限，策略倾向于更简洁的本体感受信号，抑制了视觉学习。为此，我们提出了梯度调整与阶段引导 (GAP) 算法，通过利用本体感受估计运动转换阶段的概率，并据此自适应地调节本体感受梯度的幅度，从而实现视觉与本体感受的动态协作。综合实验表明，GAP算法在模拟和真实世界环境、单臂和双臂设置以及不同模型类型中均适用，并能形成鲁棒且可泛化的视觉-本体感受策略。"
      },
      {
        "paper_id": "2602.11978",
        "title": "Accelerating Robotic Reinforcement Learning with Agent Guidance",
        "abstract": "Reinforcement Learning (RL) offers a powerful paradigm for autonomous robots to master generalist manipulation skills through trial-and-error. However, its real-world application is stifled by severe sample inefficiency. Recent Human-in-the-Loop (HIL) methods accelerate training by using human corrections, yet this approach faces a scalability barrier. Reliance on human supervisors imposes a 1:1 supervision ratio that limits fleet expansion, suffers from operator fatigue over extended sessions, and introduces high variance due to inconsistent human proficiency. We present Agent-guided Policy Search (AGPS), a framework that automates the training pipeline by replacing human supervisors with a multimodal agent. Our key insight is that the agent can be viewed as a semantic world model, injecting intrinsic value priors to structure physical exploration. By using executable tools, the agent provides precise guidance via corrective waypoints and spatial constraints for exploration pruning. We validate our approach on two tasks, ranging from precision insertion to deformable object manipulation. Results demonstrate that AGPS outperforms HIL methods in sample efficiency. This automates the supervision pipeline, unlocking the path to labor-free and scalable robot learning. Project website: https://agps-rl.github.io/agps.",
        "authors_display": "Yaodong Yang Team",
        "pdf_url": "http://arxiv.org/abs/2602.11978",
        "code_url": null,
        "date": "2026-02-12",
        "primary_category": "cs.RO",
        "chinese_summary": "强化学习在机器人操作中面临严重的样本效率问题，而现有人机协作 (HIL) 方法虽能加速训练，但受限于可扩展性、操作员疲劳和不一致的人类专业知识。为解决此问题，本研究提出了Agent-guided Policy Search (AGPS) 框架，通过多模态智能体取代人工监督者，实现训练流程自动化。其核心思想是将智能体视为语义世界模型，注入内在价值先验来结构化物理探索，并利用可执行工具通过纠正性路点和空间约束提供精确指导。实验证明，AGPS在样本效率方面优于HIL方法，从而实现了无劳动力的可扩展机器人学习路径。"
      },
      {
        "paper_id": "2602.11934",
        "title": "Robot-DIFT: Distilling Diffusion Features for Geometrically Consistent Visuomotor Control",
        "abstract": "We hypothesize that a key bottleneck in generalizable robot manipulation is not solely data scale or policy capacity, but a structural mismatch between current visual backbones and the physical requirements of closed-loop control. While state-of-the-art vision encoders (including those used in VLAs) optimize for semantic invariance to stabilize classification, manipulation typically demands geometric sensitivity the ability to map millimeter-level pose shifts to predictable feature changes. Their discriminative objective creates a \"blind spot\" for fine-grained control, whereas generative diffusion models inherently encode geometric dependencies within their latent manifolds, encouraging the preservation of dense multi-scale spatial structure. However, directly deploying stochastic diffusion features for control is hindered by stochastic instability, inference latency, and representation drift during fine-tuning. To bridge this gap, we propose Robot-DIFT, a framework that decouples the source of geometric information from the process of inference via Manifold Distillation. By distilling a frozen diffusion teacher into a deterministic Spatial-Semantic Feature Pyramid Network (S2-FPN), we retain the rich geometric priors of the generative model while ensuring temporal stability, real-time execution, and robustness against drift. Pretrained on the large-scale DROID dataset, Robot-DIFT demonstrates superior geometric consistency and control performance compared to leading discriminative baselines, supporting the view that how a model learns to see dictates how well it can learn to act.",
        "authors_display": "Georgia Chalvatzaki Team",
        "pdf_url": "http://arxiv.org/abs/2602.11934",
        "code_url": null,
        "date": "2026-02-12",
        "primary_category": "cs.RO",
        "chinese_summary": "本研究认为机器人操作泛化性的瓶颈在于当前视觉骨干网络与闭环控制物理需求之间的结构性不匹配，尤其在于现有模型缺乏精细的几何敏感性。鉴于生成扩散模型内在地编码了几何依赖性，但其随机性和延迟阻碍了直接应用，我们提出了Robot-DIFT框架。该框架通过流形蒸馏 (Manifold Distillation) 将冻结的扩散教师模型蒸馏到确定性空间-语义特征金字塔网络 (S2-FPN) 中，从而在保持生成模型丰富几何先验的同时，确保了时间稳定性、实时执行和抗漂移鲁棒性。在DROID数据集上的预训练结果表明，Robot-DIFT在几何一致性和控制性能上均优于领先的判别式基线，验证了视觉学习方式对机器人行为能力的关键影响。"
      },
      {
        "paper_id": "2602.11832",
        "title": "JEPA-VLA: Video Predictive Embedding is Needed for VLA Models",
        "abstract": "Recent vision-language-action (VLA) models built upon pretrained vision-language models (VLMs) have achieved significant improvements in robotic manipulation. However, current VLAs still suffer from low sample efficiency and limited generalization. This paper argues that these limitations are closely tied to an overlooked component, pretrained visual representation, which offers insufficient knowledge on both aspects of environment understanding and policy prior. Through an in-depth analysis, we find that commonly used visual representations in VLAs, whether pretrained via language-image contrastive learning or image-based self-supervised learning, remain inadequate at capturing crucial, task-relevant environment information and at inducing effective policy priors, i.e., anticipatory knowledge of how the environment evolves under successful task execution. In contrast, we discover that predictive embeddings pretrained on videos, in particular V-JEPA 2, are adept at flexibly discarding unpredictable environment factors and encoding task-relevant temporal dynamics, thereby effectively compensating for key shortcomings of existing visual representations in VLAs. Building on these observations, we introduce JEPA-VLA, a simple yet effective approach that adaptively integrates predictive embeddings into existing VLAs. Our experiments demonstrate that JEPA-VLA yields substantial performance gains across a range of benchmarks, including LIBERO, LIBERO-plus, RoboTwin2.0, and real-robot tasks.",
        "authors_display": "Mingsheng Long Team",
        "pdf_url": "http://arxiv.org/abs/2602.11832",
        "code_url": null,
        "date": "2026-02-12",
        "primary_category": "cs.CV",
        "chinese_summary": "现有VLA模型在机器人操作中仍面临样本效率低和泛化能力有限的问题，本研究认为这与预训练视觉表示在环境理解和策略先验方面知识不足有关。通过深入分析，我们发现现有VLA中常用的视觉表示未能有效捕获关键任务相关信息及诱导有效策略先验，而通过视频预训练的预测嵌入，特别是V-JEPA 2，能够灵活地处理不可预测因素并编码任务相关的时间动态。基于此，我们提出了JEPA-VLA，一种将预测嵌入自适应集成到现有VLA的简单有效方法。实验证明，JEPA-VLA在LIBERO、LIBERO-plus、RoboTwin2.0和真实机器人任务等基准测试中均取得了显著的性能提升。"
      },
      {
        "paper_id": "2602.11660",
        "title": "Clutt3R-Seg: Sparse-view 3D Instance Segmentation for Language-grounded Grasping in Cluttered Scenes",
        "abstract": "Reliable 3D instance segmentation is fundamental to language-grounded robotic manipulation. Its critical application lies in cluttered environments, where occlusions, limited viewpoints, and noisy masks degrade perception. To address these challenges, we present Clutt3R-Seg, a zero-shot pipeline for robust 3D instance segmentation for language-grounded grasping in cluttered scenes. Our key idea is to introduce a hierarchical instance tree of semantic cues. Unlike prior approaches that attempt to refine noisy masks, our method leverages them as informative cues: through cross-view grouping and conditional substitution, the tree suppresses over- and under-segmentation, yielding view-consistent masks and robust 3D instances. Each instance is enriched with open-vocabulary semantic embeddings, enabling accurate target selection from natural language instructions. To handle scene changes during multi-stage tasks, we further introduce a consistency-aware update that preserves instance correspondences from only a single post-interaction image, allowing efficient adaptation without rescanning. Clutt3R-Seg is evaluated on both synthetic and real-world datasets, and validated on a real robot. Across all settings, it consistently outperforms state-of-the-art baselines in cluttered and sparse-view scenarios. Even on the most challenging heavy-clutter sequences, Clutt3R-Seg achieves an AP@25 of 61.66, over 2.2x higher than baselines, and with only four input views it surpasses MaskClustering with eight views by more than 2x. The code is available at: https://github.com/jeonghonoh/clutt3r-seg.",
        "authors_display": "Ayoung Kim Team",
        "pdf_url": "http://arxiv.org/abs/2602.11660",
        "code_url": null,
        "date": "2026-02-12",
        "primary_category": "cs.CV",
        "chinese_summary": "针对杂乱环境中3D实例分割在遮挡、有限视角和噪声掩码下的挑战，本研究提出了Clutt3R-Seg，一种用于语言引导抓取的零样本鲁棒3D实例分割流水线。该方法的核心思想是引入语义线索的分层实例树，通过跨视图分组和条件替换，将噪声掩码作为信息线索，从而抑制过分割和欠分割，产生视图一致的掩码和鲁棒的3D实例。为应对多阶段任务中的场景变化，该方法还引入了一致性感知更新机制。在合成和真实世界数据集及真实机器人上的验证表明，Clutt3R-Seg在杂乱和稀疏视图场景中持续优于现有最先进基线，尤其在重度杂乱序列中表现出超过2.2倍的性能提升。"
      },
      {
        "paper_id": "2602.11643",
        "title": "ViTaS: Visual Tactile Soft Fusion Contrastive Learning for Visuomotor Learning",
        "abstract": "Tactile information plays a crucial role in human manipulation tasks and has recently garnered increasing attention in robotic manipulation. However, existing approaches mostly focus on the alignment of visual and tactile features and the integration mechanism tends to be direct concatenation. Consequently, they struggle to effectively cope with occluded scenarios due to neglecting the inherent complementary nature of both modalities and the alignment may not be exploited enough, limiting the potential of their real-world deployment. In this paper, we present ViTaS, a simple yet effective framework that incorporates both visual and tactile information to guide the behavior of an agent. We introduce Soft Fusion Contrastive Learning, an advanced version of conventional contrastive learning method and a CVAE module to utilize the alignment and complementarity within visuo-tactile representations. We demonstrate the effectiveness of our method in 12 simulated and 3 real-world environments, and our experiments show that ViTaS significantly outperforms existing baselines. Project page: https://skyrainwind.github.io/ViTaS/index.html.",
        "authors_display": "Huazhe Xu Team",
        "pdf_url": "http://arxiv.org/abs/2602.11643",
        "code_url": null,
        "date": "2026-02-12",
        "primary_category": "cs.RO",
        "chinese_summary": "针对现有机器人操作中视觉与触觉信息融合方法在遮挡场景下效果不佳、未能充分利用两种模态互补性且集成机制多为直接拼接的问题，本研究提出了ViTaS框架。该框架旨在结合视觉和触觉信息指导智能体行为，并引入了软融合对比学习（Soft Fusion Contrastive Learning）以及一个CVAE模块，以更好地利用视觉-触觉表示中的对齐和互补性。在12个模拟环境和3个真实世界环境中的实验验证表明，ViTaS显著优于现有基线，证明了其在利用多模态信息方面的有效性。"
      },
      {
        "paper_id": "2602.11464",
        "title": "EasyMimic: A Low-Cost Framework for Robot Imitation Learning from Human Videos",
        "abstract": "Robot imitation learning is often hindered by the high cost of collecting large-scale, real-world data. This challenge is especially significant for low-cost robots designed for home use, as they must be both user-friendly and affordable. To address this, we propose the EasyMimic framework, a low-cost and replicable solution that enables robots to quickly learn manipulation policies from human video demonstrations captured with standard RGB cameras. Our method first extracts 3D hand trajectories from the videos. An action alignment module then maps these trajectories to the gripper control space of a low-cost robot. To bridge the human-to-robot domain gap, we introduce a simple and user-friendly hand visual augmentation strategy. We then use a co-training method, fine-tuning a model on both the processed human data and a small amount of robot data, enabling rapid adaptation to new tasks. Experiments on the low-cost LeRobot platform demonstrate that EasyMimic achieves high performance across various manipulation tasks. It significantly reduces the reliance on expensive robot data collection, offering a practical path for bringing intelligent robots into homes. Project website: https://zt375356.github.io/EasyMimic-Project/.",
        "authors_display": "Qin Jin Team",
        "pdf_url": "http://arxiv.org/abs/2602.11464",
        "code_url": null,
        "date": "2026-02-12",
        "primary_category": "cs.RO",
        "chinese_summary": "鉴于大规模真实世界数据采集成本高昂阻碍了机器人模仿学习，尤其对低成本家用机器人而言，本研究提出了EasyMimic框架。该框架是一个低成本、可复制的解决方案，使机器人能通过标准RGB相机捕获的人类视频演示快速学习操作策略。其方法首先从视频中提取3D手部轨迹，并利用动作对齐模块将其映射到低成本机器人的夹持器控制空间；为弥合人机领域差距，引入了简单的手部视觉增强策略，并通过协同训练方法在处理过的人类数据和少量机器人数据上微调模型。实验证明，EasyMimic在LeRobot平台上在多种操作任务中取得了高性能，显著减少了对昂贵机器人数据采集的依赖，为智能机器人进入家庭提供了实用途径。"
      },
      {
        "paper_id": "2602.10983",
        "title": "Scaling World Model for Hierarchical Manipulation Policies",
        "abstract": "Vision-Language-Action (VLA) models are promising for generalist robot manipulation but remain brittle in out-of-distribution (OOD) settings, especially with limited real-robot data. To resolve the generalization bottleneck, we introduce a hierarchical Vision-Language-Action framework \\our{} that leverages the generalization of large-scale pre-trained world model for robust and generalizable VIsual Subgoal TAsk decomposition VISTA. Our hierarchical framework \\our{} consists of a world model as the high-level planner and a VLA as the low-level executor. The high-level world model first divides manipulation tasks into subtask sequences with goal images, and the low-level policy follows the textual and visual guidance to generate action sequences. Compared to raw textual goal specification, these synthesized goal images provide visually and physically grounded details for low-level policies, making it feasible to generalize across unseen objects and novel scenarios. We validate both visual goal synthesis and our hierarchical VLA policies in massive out-of-distribution scenarios, and the performance of the same-structured VLA in novel scenarios could boost from 14% to 69% with the guidance generated by the world model. Results demonstrate that our method outperforms previous baselines with a clear margin, particularly in out-of-distribution scenarios. Project page: \\href{https://vista-wm.github.io/}{https://vista-wm.github.io}",
        "authors_display": "Xinghang Li Team",
        "pdf_url": "http://arxiv.org/abs/2602.10983",
        "code_url": null,
        "date": "2026-02-12",
        "primary_category": "cs.RO",
        "chinese_summary": "视觉-语言-动作（VLA）模型在通用机器人操作中虽有前景，但在有限真实机器人数据下，其在分布外（OOD）场景中的泛化能力仍脆弱。本研究提出VISTA，一个分层视觉-语言-动作框架，利用大规模预训练世界模型的泛化能力实现鲁棒且可泛化的视觉子目标任务分解。该框架将世界模型作为高层规划器来生成带有目标图像的子任务序列，VLA作为低层执行器遵循指导生成动作。实验结果表明，与原始文本目标相比，合成的目标图像提供了更具视觉和物理细节的指导，使低层策略能泛化到新物体和场景，将VLA在OOD场景中的性能从14%提升至69%。"
      },
      {
        "paper_id": "2602.12405",
        "title": "Self-Refining Vision Language Model for Robotic Failure Detection and Reasoning",
        "abstract": "Reasoning about failures is crucial for building reliable and trustworthy robotic systems. Prior approaches either treat failure reasoning as a closed-set classification problem or assume access to ample human annotations. Failures in the real world are typically subtle, combinatorial, and difficult to enumerate, whereas rich reasoning labels are expensive to acquire. We address this problem by introducing ARMOR: Adaptive Round-based Multi-task mOdel for Robotic failure detection and reasoning. We formulate detection and reasoning as a multi-task self-refinement process, where the model iteratively predicts detection outcomes and natural language reasoning conditioned on past outputs. During training, ARMOR learns from heterogeneous supervision - large-scale sparse binary labels and small-scale rich reasoning annotations - optimized via a combination of offline and online imitation learning. At inference time, ARMOR generates multiple refinement trajectories and selects the most confident prediction via a self-certainty metric. Experiments across diverse environments show that ARMOR achieves state-of-the-art performance by improving over the previous approaches by up to 30% on failure detection rate and up to 100% in reasoning measured through LLM fuzzy match score, demonstrating robustness to heterogeneous supervision and open-ended reasoning beyond predefined failure modes. We provide dditional visualizations on our website: https://sites.google.com/utexas.edu/armor",
        "authors_display": "Yesh Dattatreya Team",
        "pdf_url": "http://arxiv.org/abs/2602.12405",
        "code_url": null,
        "date": "2026-02-12",
        "primary_category": "cs.RO",
        "chinese_summary": "针对机器人故障推理中，真实世界故障的复杂性及丰富推理标签获取成本高昂的问题，本文提出了ARMOR框架。该框架将故障检测和推理建模为一个多任务自细化过程，模型通过迭代预测检测结果和自然语言推理，并从大规模稀疏二元标签和少量丰富推理标注的异构监督中学习。实验结果表明，ARMOR在故障检测率上比现有方法提升高达30%，在LLM模糊匹配分数测量的推理能力上提升高达100%，展现了对异构监督和开放式推理的鲁棒性。"
      },
      {
        "paper_id": "2602.11393",
        "title": "Human Preference Modeling Using Visual Motion Prediction Improves Robot Skill Learning from Egocentric Human Video",
        "abstract": "We present an approach to robot learning from egocentric human videos by modeling human preferences in a reward function and optimizing robot behavior to maximize this reward. Prior work on reward learning from human videos attempts to measure the long-term value of a visual state as the temporal distance between it and the terminal state in a demonstration video. These approaches make assumptions that limit performance when learning from video. They must also transfer the learned value function across the embodiment and environment gap. Our method models human preferences by learning to predict the motion of tracked points between subsequent images and defines a reward function as the agreement between predicted and observed object motion in a robot's behavior at each step. We then use a modified Soft Actor Critic (SAC) algorithm initialized with 10 on-robot demonstrations to estimate a value function from this reward and optimize a policy that maximizes this value function, all on the robot. Our approach is capable of learning on a real robot, and we show that policies learned with our reward model match or outperform prior work across multiple tasks in both simulation and on the real robot.",
        "authors_display": "Christopher G. Atkeson Team",
        "pdf_url": "http://arxiv.org/abs/2602.11393",
        "code_url": null,
        "date": "2026-02-11",
        "primary_category": "cs.RO",
        "chinese_summary": "当前从第一视角人类视频中学习机器人的方法，在度量视觉状态长期价值时存在假设限制，且在跨实体和环境时需要迁移学习到的价值函数。本研究提出一种新方法，通过预测后续图像中跟踪点的运动来建模人类偏好，并将奖励函数定义为机器人行为中预测与观察到的物体运动的一致性。随后，利用改进的Soft Actor Critic (SAC) 算法（通过10次机器人演示初始化）来估计此奖励的价值函数，并在机器人上优化策略。结果显示，该方法在真实机器人上有效，并且在模拟和真实机器人上的多项任务中，其学习到的策略与现有工作相当或超越。"
      },
      {
        "paper_id": "2602.11337",
        "title": "MolmoSpaces: A Large-Scale Open Ecosystem for Robot Navigation and Manipulation",
        "abstract": "Deploying robots at scale demands robustness to the long tail of everyday situations. The countless variations in scene layout, object geometry, and task specifications that characterize real environments are vast and underrepresented in existing robot benchmarks. Measuring this level of generalization requires infrastructure at a scale and diversity that physical evaluation alone cannot provide. We introduce MolmoSpaces, a fully open ecosystem to support large-scale benchmarking of robot policies. MolmoSpaces consists of over 230k diverse indoor environments, ranging from handcrafted household scenes to procedurally generated multiroom houses, populated with 130k richly annotated object assets, including 48k manipulable objects with 42M stable grasps. Crucially, these environments are simulator-agnostic, supporting popular options such as MuJoCo, Isaac, and ManiSkill. The ecosystem supports the full spectrum of embodied tasks: static and mobile manipulation, navigation, and multiroom long-horizon tasks requiring coordinated perception, planning, and interaction across entire indoor environments. We also design MolmoSpaces-Bench, a benchmark suite of 8 tasks in which robots interact with our diverse scenes and richly annotated objects. Our experiments show MolmoSpaces-Bench exhibits strong sim-to-real correlation (R = 0.96, \\r{ho} = 0.98), confirm newer and stronger zero-shot policies outperform earlier versions in our benchmarks, and identify key sensitivities to prompt phrasing, initial joint positions, and camera occlusion. Through MolmoSpaces and its open-source assets and tooling, we provide a foundation for scalable data generation, policy training, and benchmark creation for robot learning research.",
        "authors_display": "Ranjay Krishna Team",
        "pdf_url": "http://arxiv.org/abs/2602.11337",
        "code_url": null,
        "date": "2026-02-11",
        "primary_category": "cs.RO",
        "chinese_summary": "机器人大规模部署要求对日常情境具备鲁棒性，但现有基准测试缺乏足够多样化的场景。本研究引入了MolmoSpaces，一个支持机器人策略大规模基准测试的开放生态系统，包含超过23万个多样化室内环境和13万个丰富注释的物体资产，且与模拟器无关。同时设计了MolmoSpaces-Bench基准套件。实验证明，MolmoSpaces-Bench具有强大的模拟到真实关联性，验证了新型零样本策略的优越性，并揭示了对提示措辞、初始关节位置和相机遮挡的关键敏感性，为机器人学习研究提供了可扩展的基础。"
      },
      {
        "paper_id": "2602.11150",
        "title": "YOR: Your Own Mobile Manipulator for Generalizable Robotics",
        "abstract": "Recent advances in robot learning have generated significant interest in capable platforms that may eventually approach human-level competence. This interest, combined with the commoditization of actuators, has propelled growth in low-cost robotic platforms. However, the optimal form factor for mobile manipulation, especially on a budget, remains an open question. We introduce YOR, an open-source, low-cost mobile manipulator that integrates an omnidirectional base, a telescopic vertical lift, and two arms with grippers to achieve whole-body mobility and manipulation. Our design emphasizes modularity, ease of assembly using off-the-shelf components, and affordability, with a bill-of-materials cost under 10,000 USD. We demonstrate YOR's capability by completing tasks that require coordinated whole-body control, bimanual manipulation, and autonomous navigation. Overall, YOR offers competitive functionality for mobile manipulation research at a fraction of the cost of existing platforms. Project website: https://www.yourownrobot.ai/",
        "authors_display": "Zichen Jeff Cui Team",
        "pdf_url": "http://arxiv.org/abs/2602.11150",
        "code_url": null,
        "date": "2026-02-11",
        "primary_category": "cs.RO",
        "chinese_summary": "随着机器人学习的发展，低成本机器人平台日益增多，但移动操作的最佳外形仍然是待解问题。本研究推出YOR，一个开源、低成本的移动操纵器，集成了全向基座、伸缩式垂直升降器和带抓手的双臂，以实现全身移动和操作。其设计强调模块化、易于组装和经济性（物料清单成本低于1万美元）。YOR在需要协调全身控制、双臂操作和自主导航的任务中展现出其能力，以远低于现有平台的成本提供了具有竞争力的移动操作研究功能。"
      },
      {
        "paper_id": "2602.11236",
        "title": "ABot-M0: VLA Foundation Model for Robotic Manipulation with Action Manifold Learning",
        "abstract": "Building general-purpose embodied agents across diverse hardware remains a central challenge in robotics, often framed as the ''one-brain, many-forms'' paradigm. Progress is hindered by fragmented data, inconsistent representations, and misaligned training objectives. We present ABot-M0, a framework that builds a systematic data curation pipeline while jointly optimizing model architecture and training strategies, enabling end-to-end transformation of heterogeneous raw data into unified, efficient representations. From six public datasets, we clean, standardize, and balance samples to construct UniACT-dataset, a large-scale dataset with over 6 million trajectories and 9,500 hours of data, covering diverse robot morphologies and task scenarios. Unified pre-training improves knowledge transfer and generalization across platforms and tasks, supporting general-purpose embodied intelligence. To improve action prediction efficiency and stability, we propose the Action Manifold Hypothesis: effective robot actions lie not in the full high-dimensional space but on a low-dimensional, smooth manifold governed by physical laws and task constraints. Based on this, we introduce Action Manifold Learning (AML), which uses a DiT backbone to predict clean, continuous action sequences directly. This shifts learning from denoising to projection onto feasible manifolds, improving decoding speed and policy stability. ABot-M0 supports modular perception via a dual-stream mechanism that integrates VLM semantics with geometric priors and multi-view inputs from plug-and-play 3D modules such as VGGT and Qwen-Image-Edit, enhancing spatial understanding without modifying the backbone and mitigating standard VLM limitations in 3D reasoning. Experiments show components operate independently with additive benefits. We will release all code and pipelines for reproducibility and future research.",
        "authors_display": "Mu Xu Team",
        "pdf_url": "http://arxiv.org/abs/2602.11236",
        "code_url": "https://amap-cvlab.github.io/ABot-Manipulation/",
        "date": "2026-02-11",
        "primary_category": "cs.CV",
        "chinese_summary": "在异构硬件上构建通用具身智能体是机器人学的一大挑战，面临数据碎片化、表示不一致和训练目标不匹配等问题。本研究提出了ABot-M0框架，它通过建立系统的数据整理管道，同时优化模型架构和训练策略，将异构原始数据转换为统一、高效的表示。通过构建大规模UniACT数据集，统一预训练提高了跨平台和任务的知识迁移和泛化能力。为提升动作预测效率和稳定性，引入了动作流形学习（AML）。此外，框架通过双流机制实现模块化感知，整合VLM语义与几何先验和多视角输入。实验结果表明各组件独立且具有累加效益。"
      },
      {
        "paper_id": "2602.11018",
        "title": "OSIL: Learning Offline Safe Imitation Policies with Safety Inferred from Non-preferred Trajectories",
        "abstract": "This work addresses the problem of offline safe imitation learning (IL), where the goal is to learn safe and reward-maximizing policies from demonstrations that do not have per-timestep safety cost or reward information. In many real-world domains, online learning in the environment can be risky, and specifying accurate safety costs can be difficult. However, it is often feasible to collect trajectories that reflect undesirable or unsafe behavior, implicitly conveying what the agent should avoid. We refer to these as non-preferred trajectories. We propose a novel offline safe IL algorithm, OSIL, that infers safety from non-preferred demonstrations. We formulate safe policy learning as a Constrained Markov Decision Process (CMDP). Instead of relying on explicit safety cost and reward annotations, OSIL reformulates the CMDP problem by deriving a lower bound on reward maximizing objective and learning a cost model that estimates the likelihood of non-preferred behavior. Our approach allows agents to learn safe and reward-maximizing behavior entirely from offline demonstrations. We empirically demonstrate that our approach can learn safer policies that satisfy cost constraints without degrading the reward performance, thus outperforming several baselines.",
        "authors_display": "Balaraman Ravindran Team",
        "pdf_url": "http://arxiv.org/abs/2602.11018",
        "code_url": null,
        "date": "2026-02-11",
        "primary_category": "cs.LG",
        "chinese_summary": "离线安全模仿学习面临在缺乏每时间步安全成本或奖励信息的演示数据中学习安全和奖励最大化策略的难题。本研究针对此问题提出了一种新颖的离线安全模仿学习算法OSIL，它通过非首选轨迹推断安全性。OSIL将安全策略学习表述为约束马尔可夫决策过程（CMDP），并通过推导奖励最大化目标的下界并学习一个估计非首选行为可能性的成本模型来重新构建CMDP问题，从而无需明确的安全成本和奖励标注。实验证明，OSIL能学习到更安全的策略，满足成本约束而不降低奖励性能，优于现有基线方法。"
      },
      {
        "paper_id": "2602.10943",
        "title": "Towards Learning a Generalizable 3D Scene Representation from 2D Observations",
        "abstract": "We introduce a Generalizable Neural Radiance Field approach for predicting 3D workspace occupancy from egocentric robot observations. Unlike prior methods operating in camera-centric coordinates, our model constructs occupancy representations in a global workspace frame, making it directly applicable to robotic manipulation. The model integrates flexible source views and generalizes to unseen object arrangements without scene-specific finetuning. We demonstrate the approach on a humanoid robot and evaluate predicted geometry against 3D sensor ground truth. Trained on 40 real scenes, our model achieves 26mm reconstruction error, including occluded regions, validating its ability to infer complete 3D occupancy beyond traditional stereo vision methods.",
        "authors_display": "Stefan Wermter Team",
        "pdf_url": "http://arxiv.org/abs/2602.10943",
        "code_url": null,
        "date": "2026-02-11",
        "primary_category": "cs.CV",
        "chinese_summary": "现有方法在相机坐标系中构建表示，限制了其在机器人操作中的直接应用。本研究引入了一种可泛化的神经辐射场方法，用于根据以机器人自我为中心的观察来预测3D工作空间占用。该模型在全局工作空间框架中构建占用表示，使其直接适用于机器人操作，并能集成灵活的源视图，无需场景特定微调即可泛化到未见物体布局。在人形机器人上的实验证明，该模型在40个真实场景上训练后，实现了26毫米的重建误差（包括遮挡区域），验证了其推断完整3D占用（超越传统立体视觉方法）的能力。"
      },
      {
        "paper_id": "2602.10793",
        "title": "Semi-Supervised Cross-Domain Imitation Learning",
        "abstract": "Cross-domain imitation learning (CDIL) accelerates policy learning by transferring expert knowledge across domains, which is valuable in applications where the collection of expert data is costly. Existing methods are either supervised, relying on proxy tasks and explicit alignment, or unsupervised, aligning distributions without paired data, but often unstable. We introduce the Semi-Supervised CDIL (SS-CDIL) setting and propose the first algorithm for SS-CDIL with theoretical justification. Our method uses only offline data, including a small number of target expert demonstrations and some unlabeled imperfect trajectories. To handle domain discrepancy, we propose a novel cross-domain loss function for learning inter-domain state-action mappings and design an adaptive weight function to balance the source and target knowledge. Experiments on MuJoCo and Robosuite show consistent gains over the baselines, demonstrating that our approach achieves stable and data-efficient policy learning with minimal supervision. Our code is available at~ https://github.com/NYCU-RL-Bandits-Lab/CDIL.",
        "authors_display": "Ping-Chun Hsieh Team",
        "pdf_url": "http://arxiv.org/abs/2602.10793",
        "code_url": null,
        "date": "2026-02-11",
        "primary_category": "cs.LG",
        "chinese_summary": "跨域模仿学习（CDIL）通过领域知识迁移加速策略学习，但在专家数据收集成本高昂时尤为重要。现有CDIL方法或依赖监督（不稳定），或无监督（不稳健）。本研究提出了半监督CDIL（SS-CDIL）设置及其首个具有理论依据的算法。该方法仅使用少量目标专家演示和未标记的不完善轨迹等离线数据。为解决域差异，提出一种新颖的跨域损失函数来学习域间状态-动作映射，并设计自适应权重函数以平衡源域和目标域知识。实验表明，该方法在MuJoCo和Robosuite上始终优于基线，实现了最小监督下稳定且数据高效的策略学习。"
      },
      {
        "paper_id": "2602.10717",
        "title": "Say, Dream, and Act: Learning Video World Models for Instruction-Driven Robot Manipulation",
        "abstract": "Robotic manipulation requires anticipating how the environment evolves in response to actions, yet most existing systems lack this predictive capability, often resulting in errors and inefficiency. While Vision-Language Models (VLMs) provide high-level guidance, they cannot explicitly forecast future states, and existing world models either predict only short horizons or produce spatially inconsistent frames. To address these challenges, we propose a framework for fast and predictive video-conditioned action. Our approach first selects and adapts a robust video generation model to ensure reliable future predictions, then applies adversarial distillation for fast, few-step video generation, and finally trains an action model that leverages both generated videos and real observations to correct spatial errors. Extensive experiments show that our method produces temporally coherent, spatially accurate video predictions that directly support precise manipulation, achieving significant improvements in embodiment consistency, spatial referring ability, and task completion over existing baselines. Codes & Models will be released.",
        "authors_display": "Yanwei Fu Team",
        "pdf_url": "http://arxiv.org/abs/2602.10717",
        "code_url": null,
        "date": "2026-02-11",
        "primary_category": "cs.RO",
        "chinese_summary": "机器人操作通常缺乏预测环境响应动作的能力，导致错误和低效，且现有视觉-语言模型（VLM）和世界模型在预测未来状态或生成空间一致帧方面存在局限。本研究提出一种用于快速且可预测的视频条件动作框架。该方法首先选择并适应一个鲁棒的视频生成模型以确保可靠的未来预测，然后应用对抗蒸馏进行快速、少步骤的视频生成，最后训练一个动作模型，利用生成的视频和真实观察来纠正空间误差。大量实验表明，该方法生成的时间连贯、空间准确的视频预测直接支持精确操作，在具身一致性、空间指代能力和任务完成度方面显著优于现有基线。"
      }
    ],
    "World Model": [
      {
        "paper_id": "2602.22208",
        "title": "Solaris: Building a Multiplayer Video World Model in Minecraft",
        "abstract": "Existing action-conditioned video generation models (video world models) are limited to single-agent perspectives, failing to capture the multi-agent interactions of real-world environments. We introduce Solaris, a multiplayer video world model that simulates consistent multi-view observations. To enable this, we develop a multiplayer data system designed for robust, continuous, and automated data collection on video games such as Minecraft. Unlike prior platforms built for single-player settings, our system supports coordinated multi-agent interaction and synchronized videos + actions capture. Using this system, we collect 12.64 million multiplayer frames and propose an evaluation framework for multiplayer movement, memory, grounding, building, and view consistency. We train Solaris using a staged pipeline that progressively transitions from single-player to multiplayer modeling, combining bidirectional, causal, and Self Forcing training. In the final stage, we introduce Checkpointed Self Forcing, a memory-efficient Self Forcing variant that enables a longer-horizon teacher. Results show our architecture and training design outperform existing baselines. Through open-sourcing our system and models, we hope to lay the groundwork for a new generation of multi-agent world models.",
        "authors_display": "Saining Xie Team",
        "pdf_url": "http://arxiv.org/abs/2602.22208",
        "code_url": "https://solaris-wm.github.io/",
        "date": "2026-02-26",
        "primary_category": "cs.CV",
        "chinese_summary": "现有动作条件视频生成模型多限于单智能体视角，无法捕捉真实世界中的多智能体交互。本文介绍了Solaris，一个多玩家视频世界模型，旨在模拟一致的多视角观测。为此，研究团队开发了一个支持多智能体交互和同步视频与动作捕捉的数据系统，并在Minecraft等游戏中收集了1264万帧多玩家数据，同时提出了一套多玩家评估框架。Solaris采用分阶段训练管线，从单玩家逐步过渡到多玩家建模，并引入了内存高效的检查点自强制训练。实验结果表明，该架构和训练设计优于现有基线，为新一代多智能体世界模型的开发奠定了基础。"
      },
      {
        "paper_id": "2602.23287",
        "title": "Interface-Aware Trajectory Reconstruction of Limited Demonstrations for Robot Learning",
        "abstract": "Assistive robots offer agency to humans with severe motor impairments. Often, these users control high-DoF robots through low-dimensional interfaces, such as using a 1-D sip-and-puff interface to operate a 6-DoF robotic arm. This mismatch results in having access to only a subset of control dimensions at a given time, imposing unintended and artificial constraints on robot motion. As a result, interface-limited demonstrations embed suboptimal motions that reflect interface restrictions rather than user intent. To address this, we present a trajectory reconstruction algorithm that reasons about task, environment, and interface constraints to lift demonstrations into the robot's full control space. We evaluate our approach using real-world demonstrations of ADL-inspired tasks performed via a 2-D joystick and 1-D sip-and-puff control interface, teleoperating two distinct 7-DoF robotic arms. Analyses of the reconstructed demonstrations and derived control policies show that lifted trajectories are faster and more efficient than their interface-constrained counterparts while respecting user preferences.",
        "authors_display": "Brenna D. Argall Team",
        "pdf_url": "http://arxiv.org/abs/2602.23287",
        "code_url": null,
        "date": "2026-02-26",
        "primary_category": "cs.RO",
        "chinese_summary": "在机器人辅助领域，严重运动障碍用户通过低维接口控制高自由度机器人时，由于接口限制导致机器人运动次优，无法真实反映用户意图。为解决此问题，本研究提出一种轨迹重建算法，该算法综合考虑任务、环境和接口约束，将低维演示轨迹提升到机器人的全控制空间。通过对使用2D操纵杆和1D吸吹接口操作7自由度机械臂的实际任务进行评估，结果表明重建后的轨迹比受限轨迹更快速、高效，且能更好地满足用户偏好。"
      },
      {
        "paper_id": "2602.23259",
        "title": "Risk-Aware World Model Predictive Control for Generalizable End-to-End Autonomous Driving",
        "abstract": "With advances in imitation learning (IL) and large-scale driving datasets, end-to-end autonomous driving (E2E-AD) has made great progress recently. Currently, IL-based methods have become a mainstream paradigm: models rely on standard driving behaviors given by experts, and learn to minimize the discrepancy between their actions and expert actions. However, this objective of \"only driving like the expert\" suffers from limited generalization: when encountering rare or unseen long-tail scenarios outside the distribution of expert demonstrations, models tend to produce unsafe decisions in the absence of prior experience. This raises a fundamental question: Can an E2E-AD system make reliable decisions without any expert action supervision? Motivated by this, we propose a unified framework named Risk-aware World Model Predictive Control (RaWMPC) to address this generalization dilemma through robust control, without reliance on expert demonstrations. Practically, RaWMPC leverages a world model to predict the consequences of multiple candidate actions and selects low-risk actions through explicit risk evaluation. To endow the world model with the ability to predict the outcomes of risky driving behaviors, we design a risk-aware interaction strategy that systematically exposes the world model to hazardous behaviors, making catastrophic outcomes predictable and thus avoidable. Furthermore, to generate low-risk candidate actions at test time, we introduce a self-evaluation distillation method to distill riskavoidance capabilities from the well-trained world model into a generative action proposal network without any expert demonstration. Extensive experiments show that RaWMPC outperforms state-of-the-art methods in both in-distribution and out-of-distribution scenarios, while providing superior decision interpretability.",
        "authors_display": "Nicu Sebe Team",
        "pdf_url": "http://arxiv.org/abs/2602.23259",
        "code_url": null,
        "date": "2026-02-26",
        "primary_category": "cs.CV",
        "chinese_summary": "当前端到端自动驾驶（E2E-AD）中的模仿学习方法过度依赖专家演示，导致在罕见或未见的长尾场景中泛化能力有限，可能产生不安全决策。为实现无需专家行动监督的可靠决策，本研究提出统一的风险感知世界模型预测控制（RaWMPC）框架。该框架利用世界模型预测候选行动后果并进行风险评估以选择低风险行动，并通过风险感知交互策略训练世界模型预测危险行为。实验证明，RaWMPC在分布内和分布外场景中均优于现有方法，并提供卓越的决策可解释性。"
      },
      {
        "paper_id": "2602.23199",
        "title": "SC-Arena: A Natural Language Benchmark for Single-Cell Reasoning with Knowledge-Augmented Evaluation",
        "abstract": "Large language models (LLMs) are increasingly applied in scientific research, offering new capabilities for knowledge discovery and reasoning. In single-cell biology, however, evaluation practices for both general and specialized LLMs remain inadequate: existing benchmarks are fragmented across tasks, adopt formats such as multiple-choice classification that diverge from real-world usage, and rely on metrics lacking interpretability and biological grounding. We present SC-ARENA, a natural language evaluation framework tailored to single-cell foundation models. SC-ARENA formalizes a virtual cell abstraction that unifies evaluation targets by representing both intrinsic attributes and gene-level interactions. Within this paradigm, we define five natural language tasks (cell type annotation, captioning, generation, perturbation prediction, and scientific QA) that probe core reasoning capabilities in cellular biology. To overcome the limitations of brittle string-matching metrics, we introduce knowledge-augmented evaluation, which incorporates external ontologies, marker databases, and scientific literature to support biologically faithful and interpretable judgments. Experiments and analysis across both general-purpose and domain-specialized LLMs demonstrate that (i) under the Virtual Cell unified evaluation paradigm, current models achieve uneven performance on biologically complex tasks, particularly those demanding mechanistic or causal understanding; and (ii) our knowledge-augmented evaluation framework ensures biological correctness, provides interpretable, evidence-grounded rationales, and achieves high discriminative capacity, overcoming the brittleness and opacity of conventional metrics. SC-Arena thus provides a unified and interpretable framework for assessing LLMs in single-cell biology, pointing toward the development of biology-aligned, generalizable foundation models.",
        "authors_display": "Min Yang Team",
        "pdf_url": "http://arxiv.org/abs/2602.23199",
        "code_url": null,
        "date": "2026-02-26",
        "primary_category": "cs.AI",
        "chinese_summary": "大语言模型（LLMs）在单细胞生物学研究中应用日益广泛，但现有评估基准分散、格式不符实际，且指标缺乏可解释性和生物学基础。为此，本研究提出SC-ARENA框架，通过“虚拟细胞”抽象统一评估目标，并定义了五种自然语言任务。为克服传统指标局限性，引入知识增强评估，整合外部本体、标记数据库和科学文献。实验表明，当前模型在生物学复杂任务上表现不均衡，而所提出的知识增强评估框架能确保生物学正确性，提供可解释的证据支持理由，并具有高判别能力。"
      },
      {
        "paper_id": "2602.23164",
        "title": "MetaOthello: A Controlled Study of Multiple World Models in Transformers",
        "abstract": "Foundation models must handle multiple generative processes, yet mechanistic interpretability largely studies capabilities in isolation; it remains unclear how a single transformer organizes multiple, potentially conflicting \"world models\". Previous experiments on Othello playing neural-networks test world-model learning but focus on a single game with a single set of rules. We introduce MetaOthello, a controlled suite of Othello variants with shared syntax but different rules or tokenizations, and train small GPTs on mixed-variant data to study how multiple world models are organized in a shared representation space. We find that transformers trained on mixed-game data do not partition their capacity into isolated sub-models; instead, they converge on a mostly shared board-state representation that transfers causally across variants. Linear probes trained on one variant can intervene on another's internal state with effectiveness approaching that of matched probes. For isomorphic games with token remapping, representations are equivalent up to a single orthogonal rotation that generalizes across layers. When rules partially overlap, early layers maintain game-agnostic representations while a middle layer identifies game identity, and later layers specialize. MetaOthello offers a path toward understanding not just whether transformers learn world models, but how they organize many at once.",
        "authors_display": "Juniper Lovato Team",
        "pdf_url": "http://arxiv.org/abs/2602.23164",
        "code_url": null,
        "date": "2026-02-26",
        "primary_category": "cs.LG",
        "chinese_summary": "基础模型需处理多种生成过程，但现有对机制可解释性的研究多是孤立进行，不清楚Transformer如何组织多个世界模型。本研究引入MetaOthello，一个受控的奥赛罗变体套件，用于在共享表示空间中研究多个世界模型的组织方式。通过在混合数据上训练小型GPT模型，结果发现Transformer并未将能力划分为孤立子模型，而是收敛于一个在变体间具有因果转移性的共享棋盘状态表示。线性探测器可有效干预其他变体状态，且同构游戏的表示在单一正交旋转下等效，规则部分重叠时层级功能逐渐专业化。"
      },
      {
        "paper_id": "2602.23152",
        "title": "The Trinity of Consistency as a Defining Principle for General World Models",
        "abstract": "The construction of World Models capable of learning, simulating, and reasoning about objective physical laws constitutes a foundational challenge in the pursuit of Artificial General Intelligence. Recent advancements represented by video generation models like Sora have demonstrated the potential of data-driven scaling laws to approximate physical dynamics, while the emerging Unified Multimodal Model (UMM) offers a promising architectural paradigm for integrating perception, language, and reasoning. Despite these advances, the field still lacks a principled theoretical framework that defines the essential properties requisite for a General World Model. In this paper, we propose that a World Model must be grounded in the Trinity of Consistency: Modal Consistency as the semantic interface, Spatial Consistency as the geometric basis, and Temporal Consistency as the causal engine. Through this tripartite lens, we systematically review the evolution of multimodal learning, revealing a trajectory from loosely coupled specialized modules toward unified architectures that enable the synergistic emergence of internal world simulators. To complement this conceptual framework, we introduce CoW-Bench, a benchmark centered on multi-frame reasoning and generation scenarios. CoW-Bench evaluates both video generation models and UMMs under a unified evaluation protocol. Our work establishes a principled pathway toward general world models, clarifying both the limitations of current systems and the architectural requirements for future progress.",
        "authors_display": "Cheng Tan Team",
        "pdf_url": "http://arxiv.org/abs/2602.23152",
        "code_url": null,
        "date": "2026-02-26",
        "primary_category": "cs.AI",
        "chinese_summary": "构建能学习、模拟和推理物理定律的世界模型是通用人工智能的核心挑战。尽管视频生成模型和统一多模态模型取得进展，但仍缺乏定义通用世界模型基本属性的理论框架。本研究提出世界模型必须基于“一致性三位一体”：模态、空间和时间一致性，并系统回顾了多模态学习的演变。为验证此框架，引入CoW-Bench基准，用于在统一协议下评估视频生成模型和统一多模态模型。该工作为通用世界模型建立了原则性路径，并阐明了当前系统的局限性及未来发展方向。"
      },
      {
        "paper_id": "2602.23148",
        "title": "On Sample-Efficient Generalized Planning via Learned Transition Models",
        "abstract": "Generalized planning studies the construction of solution strategies that generalize across families of planning problems sharing a common domain model, formally defined by a transition function $γ: S \\times A \\rightarrow S$. Classical approaches achieve such generalization through symbolic abstractions and explicit reasoning over $γ$. In contrast, recent Transformer-based planners, such as PlanGPT and Plansformer, largely cast generalized planning as direct action-sequence prediction, bypassing explicit transition modeling. While effective on in-distribution instances, these approaches typically require large datasets and model sizes, and often suffer from state drift in long-horizon settings due to the absence of explicit world-state evolution. In this work, we formulate generalized planning as a transition-model learning problem, in which a neural model explicitly approximates the successor-state function $\\hatγ \\approx γ$ and generates plans by rolling out symbolic state trajectories. Instead of predicting actions directly, the model autoregressively predicts intermediate world states, thereby learning the domain dynamics as an implicit world model. To study size-invariant generalization and sample efficiency, we systematically evaluate multiple state representations and neural architectures, including relational graph encodings. Our results show that learning explicit transition models yields higher out-of-distribution satisficing-plan success than direct action-sequence prediction in multiple domains, while achieving these gains with significantly fewer training instances and smaller models. This is an extended version of a short paper accepted at ICAPS 2026 under the same title.",
        "authors_display": "Biplav Srivastava Team",
        "pdf_url": "http://arxiv.org/abs/2602.23148",
        "code_url": null,
        "date": "2026-02-26",
        "primary_category": "cs.AI",
        "chinese_summary": "广义规划传统方法依赖符号抽象，而基于Transformer的规划器多通过直接动作序列预测，导致长程规划中状态漂移且需大量数据。为解决此问题，本研究将广义规划公式化为转移模型学习，使神经网络模型显式近似后继状态函数，通过自回归预测中间世界状态来学习域动力学作为隐式世界模型。系统评估了不同状态表示和神经网络架构（包括关系图编码），结果表明学习显式转移模型在多个域中实现了更高的域外满意计划成功率，且显著减少了所需的训练实例和模型尺寸。"
      },
      {
        "paper_id": "2602.23088",
        "title": "Cytoarchitecture in Words: Weakly Supervised Vision-Language Modeling for Human Brain Microscopy",
        "abstract": "Foundation models increasingly offer potential to support interactive, agentic workflows that assist researchers during analysis and interpretation of image data. Such workflows often require coupling vision to language to provide a natural-language interface. However, paired image-text data needed to learn this coupling are scarce and difficult to obtain in many research and clinical settings. One such setting is microscopic analysis of cell-body-stained histological human brain sections, which enables the study of cytoarchitecture: cell density and morphology and their laminar and areal organization. Here, we propose a label-mediated method that generates meaningful captions from images by linking images and text only through a label, without requiring curated paired image-text data. Given the label, we automatically mine area descriptions from related literature and use them as synthetic captions reflecting canonical cytoarchitectonic attributes. An existing cytoarchitectonic vision foundation model (CytoNet) is then coupled to a large language model via an image-to-text training objective, enabling microscopy regions to be described in natural language. Across 57 brain areas, the resulting method produces plausible area-level descriptions and supports open-set use through explicit rejection of unseen areas. It matches the cytoarchitectonic reference label for in-scope patches with 90.6% accuracy and, with the area label masked, its descriptions remain discriminative enough to recover the area in an 8-way test with 68.6% accuracy. These results suggest that weak, label-mediated pairing can suffice to connect existing biomedical vision foundation models to language, providing a practical recipe for integrating natural-language in domains where fine-grained paired annotations are scarce.",
        "authors_display": "Christian Schiffer Team",
        "pdf_url": "http://arxiv.org/abs/2602.23088",
        "code_url": null,
        "date": "2026-02-26",
        "primary_category": "cs.CV",
        "chinese_summary": "基础模型在图像数据分析中潜力巨大，但将视觉与语言耦合时常面临缺乏配对图像-文本数据的挑战，尤其是在显微图像分析领域。本研究提出一种标签介导方法，通过标签关联图像和文本，无需人工策展配对数据即可生成有意义的标题。该方法利用现有标签从文献中自动挖掘区域描述作为合成标题，并将细胞结构视觉基础模型与大型语言模型耦合。实验表明，该方法能生成合理的区域描述，支持开放集使用，并在准确性方面表现优异，证明弱标签介导配对足以连接生物医学视觉基础模型与语言。"
      },
      {
        "paper_id": "2602.23058",
        "title": "GeoWorld: Geometric World Models",
        "abstract": "Energy-based predictive world models provide a powerful approach for multi-step visual planning by reasoning over latent energy landscapes rather than generating pixels. However, existing approaches face two major challenges: (i) their latent representations are typically learned in Euclidean space, neglecting the underlying geometric and hierarchical structure among states, and (ii) they struggle with long-horizon prediction, which leads to rapid degradation across extended rollouts. To address these challenges, we introduce GeoWorld, a geometric world model that preserves geometric structure and hierarchical relations through a Hyperbolic JEPA, which maps latent representations from Euclidean space onto hyperbolic manifolds. We further introduce Geometric Reinforcement Learning for energy-based optimization, enabling stable multi-step planning in hyperbolic latent space. Extensive experiments on CrossTask and COIN demonstrate around 3% SR improvement in 3-step planning and 2% SR improvement in 4-step planning compared to the state-of-the-art V-JEPA 2. Project website: https://steve-zeyu-zhang.github.io/GeoWorld.",
        "authors_display": "Richard Hartley Team",
        "pdf_url": "http://arxiv.org/abs/2602.23058",
        "code_url": null,
        "date": "2026-02-26",
        "primary_category": "cs.CV",
        "chinese_summary": "基于能量的预测世界模型在多步视觉规划中具有优势，但现有方法在欧几里得空间学习潜在表示时忽视了几何和层次结构，且长期预测能力不足。为克服这些挑战，本研究引入GeoWorld几何世界模型，通过双曲JEPA将潜在表示映射到双曲流形以保留几何和层次结构。同时，引入几何强化学习进行能量优化，实现双曲潜在空间中的稳定多步规划。实验结果显示，GeoWorld在CrossTask和COIN数据集上的3步和4步规划中，成功率比V-JEPA 2分别提升约3%和2%。"
      },
      {
        "paper_id": "2602.23013",
        "title": "SubspaceAD: Training-Free Few-Shot Anomaly Detection via Subspace Modeling",
        "abstract": "Detecting visual anomalies in industrial inspection often requires training with only a few normal images per category. Recent few-shot methods achieve strong results employing foundation-model features, but typically rely on memory banks, auxiliary datasets, or multi-modal tuning of vision-language models. We therefore question whether such complexity is necessary given the feature representations of vision foundation models. To answer this question, we introduce SubspaceAD, a training-free method, that operates in two simple stages. First, patch-level features are extracted from a small set of normal images by a frozen DINOv2 backbone. Second, a Principal Component Analysis (PCA) model is fit to these features to estimate the low-dimensional subspace of normal variations. At inference, anomalies are detected via the reconstruction residual with respect to this subspace, producing interpretable and statistically grounded anomaly scores. Despite its simplicity, SubspaceAD achieves state-of-the-art performance across one-shot and few-shot settings without training, prompt tuning, or memory banks. In the one-shot anomaly detection setting, SubspaceAD achieves image-level and pixel-level AUROC of 98.0% and 97.6% on the MVTec-AD dataset, and 93.3% and 98.3% on the VisA dataset, respectively, surpassing prior state-of-the-art results. Code and demo are available at https://github.com/CLendering/SubspaceAD.",
        "authors_display": "Egor Bondarev Team",
        "pdf_url": "http://arxiv.org/abs/2602.23013",
        "code_url": null,
        "date": "2026-02-26",
        "primary_category": "cs.CV",
        "chinese_summary": "工业检测中视觉异常检测常需少量正常图像进行训练，现有少样本方法虽效果好但依赖复杂技术（如内存库或多模态调优）。本研究提出SubspaceAD，一个无需训练的简单方法，以探究视觉基础模型特征的有效性。该方法分两阶段：首先，使用冻结的DINOv2骨干网络从少量正常图像提取补丁特征；其次，通过PCA模型拟合这些特征，估计正常变化的低维子空间。推理时，利用重建残差检测异常，生成可解释的统计异常分数。尽管简单，SubspaceAD在一次性（one-shot）和少样本异常检测中均取得了最先进的性能，超越了现有方法。"
      },
      {
        "paper_id": "2602.22960",
        "title": "UCM: Unifying Camera Control and Memory with Time-aware Positional Encoding Warping for World Models",
        "abstract": "World models based on video generation demonstrate remarkable potential for simulating interactive environments but face persistent difficulties in two key areas: maintaining long-term content consistency when scenes are revisited and enabling precise camera control from user-provided inputs. Existing methods based on explicit 3D reconstruction often compromise flexibility in unbounded scenarios and fine-grained structures. Alternative methods rely directly on previously generated frames without establishing explicit spatial correspondence, thereby constraining controllability and consistency. To address these limitations, we present UCM, a novel framework that unifies long-term memory and precise camera control via a time-aware positional encoding warping mechanism. To reduce computational overhead, we design an efficient dual-stream diffusion transformer for high-fidelity generation. Moreover, we introduce a scalable data curation strategy utilizing point-cloud-based rendering to simulate scene revisiting, facilitating training on over 500K monocular videos. Extensive experiments on real-world and synthetic benchmarks demonstrate that UCM significantly outperforms state-of-the-art methods in long-term scene consistency, while also achieving precise camera controllability in high-fidelity video generation.",
        "authors_display": "Song-Hai Zhang Team",
        "pdf_url": "http://arxiv.org/abs/2602.22960",
        "code_url": "https://humanaigc.github.io/ucm-webpage/",
        "date": "2026-02-26",
        "primary_category": "cs.CV",
        "chinese_summary": "基于视频生成的世界模型在模拟交互环境方面潜力巨大，但在场景重访时保持长期内容一致性及精确相机控制方面存在困难。现有方法要么牺牲灵活性，要么缺乏显式空间对应。为解决这些局限，本研究提出UCM框架，通过时间感知的姿态编码扭曲机制，统一长期记忆与精确相机控制。为降低计算开销，设计了高效双流扩散Transformer，并引入可扩展的点云渲染数据整理策略，在大量单目视频上进行训练。实验表明，UCM在长期场景一致性和高保真视频生成中的相机可控性方面显著优于现有先进方法。"
      },
      {
        "paper_id": "2602.22843",
        "title": "A data- and compute-efficient chest X-ray foundation model beyond aggressive scaling",
        "abstract": "Foundation models for medical imaging are typically pretrained on increasingly large datasets, following a \"scale-at-all-costs\" paradigm. However, this strategy faces two critical challenges: large-scale medical datasets often contain substantial redundancy and severe class imbalance that bias representation learning toward over-represented patterns, and indiscriminate training regardless of heterogeneity in data quality incurs considerable computational inefficiency. Here we demonstrate that active, principled data curation during pretraining can serve as a viable, cost-effective alternative to brute-force dataset enlargement. We introduce CheXficient, a chest X-ray (CXR) foundation model that selectively prioritizes informative training samples. CheXficient is pretrained on only 22.7% of 1,235,004 paired CXR images and reports while consuming under 27.3% of the total compute budget, yet achieving comparable or superior performance to its full-data counterpart and other large-scale pretrained models. We assess CheXficient across 20 individual benchmarks spanning 5 task types, including non-adapted off-the-shelf evaluations (zero-shot findings classification and crossmodal retrieval) and adapted downstream tasks (disease prediction, semantic segmentation, and radiology report generation). Further analyses show that CheXficient systematically prioritizes under-represented training samples, improving generalizability on long-tailed or rare conditions. Overall, our work offers practical insights into the data and computation demands for efficient pretraining and downstream adaptation of medical vision-language foundation models.",
        "authors_display": "Curtis P. Langlotz Team",
        "pdf_url": "http://arxiv.org/abs/2602.22843",
        "code_url": null,
        "date": "2026-02-26",
        "primary_category": "cs.CV",
        "chinese_summary": "医用影像基础模型在预训练时面临数据冗余、类别不平衡和计算效率低等挑战。为解决这些问题，本研究提出了CheXficient模型，该模型通过主动、有原则的数据筛选机制，优先选择信息量大的训练样本，而非依赖蛮力扩充数据集。实验结果表明，CheXficient在仅使用22.7%的数据集和27.3%的计算资源下，在20个多样化的基准测试中，性能与使用全部数据的模型及其他大规模预训练模型相当或更优，并能有效提升对长尾或罕见病症的泛化能力。"
      },
      {
        "paper_id": "2602.22818",
        "title": "LeRobot: An Open-Source Library for End-to-End Robot Learning",
        "abstract": "Robotics is undergoing a significant transformation powered by advances in high-level control techniques based on machine learning, giving rise to the field of robot learning. Recent progress in robot learning has been accelerated by the increasing availability of affordable teleoperation systems, large-scale openly available datasets, and scalable learning-based methods. However, development in the field of robot learning is often slowed by fragmented, closed-source tools designed to only address specific sub-components within the robotics stack. In this paper, we present \\texttt{lerobot}, an open-source library that integrates across the entire robot learning stack, from low-level middleware communication for motor controls to large-scale dataset collection, storage and streaming. The library is designed with a strong focus on real-world robotics, supporting accessible hardware platforms while remaining extensible to new embodiments. It also supports efficient implementations for various state-of-the-art robot learning algorithms from multiple prominent paradigms, as well as a generalized asynchronous inference stack. Unlike traditional pipelines which heavily rely on hand-crafted techniques, \\texttt{lerobot} emphasizes scalable learning approaches that improve directly with more data and compute. Designed for accessibility, scalability, and openness, \\texttt{lerobot} lowers the barrier to entry for researchers and practitioners to robotics while providing a platform for reproducible, state-of-the-art robot learning.",
        "authors_display": "Thomas Wolf Team",
        "pdf_url": "http://arxiv.org/abs/2602.22818",
        "code_url": "https://github.com/huggingface/lerobot",
        "date": "2026-02-26",
        "primary_category": "cs.RO",
        "chinese_summary": "机器人学习领域虽快速发展，但现有工具的碎片化和闭源特性阻碍了其效率提升。针对此问题，本文介绍了一个名为lerobot的开源库，旨在整合整个机器人学习堆栈，从低级电机控制到大规模数据收集、存储与流式传输。该库专注于真实世界机器人应用，支持易于访问的硬件平台，并可扩展，同时支持多种先进机器人学习算法的有效实现。lerobot通过强调可扩展学习方法，显著降低了研究人员和从业者进入机器人学习领域的门槛，并为可复现的最先进机器人学习提供了平台。"
      },
      {
        "paper_id": "2602.22613",
        "title": "Spectrally Distilled Representations Aligned with Instruction-Augmented LLMs for Satellite Imagery",
        "abstract": "Vision-language foundation models (VLFMs) promise zero-shot and retrieval understanding for Earth observation. While operational satellite systems often lack full multi-spectral coverage, making RGB-only inference highly desirable for scalable deployment, the adoption of VLFMs for satellite imagery remains hindered by two factors: (1) multi-spectral inputs are informative but difficult to exploit consistently due to band redundancy and misalignment; and (2) CLIP-style text encoders limit semantic expressiveness and weaken fine-grained alignment. We present SATtxt, a spectrum-aware VLFM that operates with RGB inputs only at inference while retaining spectral cues learned during training. Our framework comprises two stages. First, Spectral Representation Distillation transfers spectral priors from a frozen multi-spectral teacher to an RGB student via a lightweight projector. Second, Spectrally Grounded Alignment with Instruction-Augmented LLMs bridges the distilled visual space and an expressive LLM embedding space. Across EuroSAT, BigEarthNet, and ForestNet, SATtxt improves zero-shot classification on average by 4.2%, retrieval by 5.9%, and linear probing by 2.7% over baselines, showing an efficient path toward spectrum-aware vision-language learning for Earth observation. Project page: https://ikhado.github.io/sattxt/",
        "authors_display": "Ramana Rao Kompella Team",
        "pdf_url": "http://arxiv.org/abs/2602.22613",
        "code_url": null,
        "date": "2026-02-26",
        "primary_category": "cs.CV",
        "chinese_summary": "地球观测领域的视觉-语言基础模型（VLFMs）在零样本和检索理解方面潜力巨大，但由于缺乏全多光谱覆盖及现有文本编码器的限制，其在卫星图像应用中面临挑战。为此，本文提出了SATtxt模型，该模型在推理时仅依赖RGB输入，但通过两阶段框架保留了训练中学到的光谱信息：首先，通过光谱表示蒸馏将多光谱教师模型的先验知识转移给RGB学生模型；其次，通过指令增强型大型语言模型实现光谱接地对齐。实验结果显示，SATtxt在多个数据集上显著提升了零样本分类、检索和线性探测的性能，为地球观测领域的高效光谱感知视觉-语言学习提供了一条新途径。"
      },
      {
        "paper_id": "2602.22596",
        "title": "BetterScene: 3D Scene Synthesis with Representation-Aligned Generative Model",
        "abstract": "We present BetterScene, an approach to enhance novel view synthesis (NVS) quality for diverse real-world scenes using extremely sparse, unconstrained photos. BetterScene leverages the production-ready Stable Video Diffusion (SVD) model pretrained on billions of frames as a strong backbone, aiming to mitigate artifacts and recover view-consistent details at inference time. Conventional methods have developed similar diffusion-based solutions to address these challenges of novel view synthesis. Despite significant improvements, these methods typically rely on off-the-shelf pretrained diffusion priors and fine-tune only the UNet module while keeping other components frozen, which still leads to inconsistent details and artifacts even when incorporating geometry-aware regularizations like depth or semantic conditions. To address this, we investigate the latent space of the diffusion model and introduce two components: (1) temporal equivariance regularization and (2) vision foundation model-aligned representation, both applied to the variational autoencoder (VAE) module within the SVD pipeline. BetterScene integrates a feed-forward 3D Gaussian Splatting (3DGS) model to render features as inputs for the SVD enhancer and generate continuous, artifact-free, consistent novel views. We evaluate on the challenging DL3DV-10K dataset and demonstrate superior performance compared to state-of-the-art methods.",
        "authors_display": "Alper Yilmaz Team",
        "pdf_url": "http://arxiv.org/abs/2602.22596",
        "code_url": null,
        "date": "2026-02-26",
        "primary_category": "cs.CV",
        "chinese_summary": "新颖视图合成（NVS）在稀疏、无约束的真实场景图像中生成高质量视图仍面临细节不一致和伪影等问题。本文提出了BetterScene方法，利用预训练在数十亿帧上的Stable Video Diffusion (SVD) 模型作为强大骨干，通过引入时序等变正则化和视觉基础模型对齐表示，作用于SVD管线中的变分自编码器（VAE）模块，以探索扩散模型的潜在空间。此外，该方法还集成了前馈3D高斯飞溅模型来渲染特征。实验结果表明，BetterScene在具有挑战性的DL3DV-10K数据集上，性能显著优于现有最先进方法，有效缓解了NVS中的伪影和不一致性问题。"
      },
      {
        "paper_id": "2602.22586",
        "title": "TabDLM: Free-Form Tabular Data Generation via Joint Numerical-Language Diffusion",
        "abstract": "Synthetic tabular data generation has attracted growing attention due to its importance for data augmentation, foundation models, and privacy. However, real-world tabular datasets increasingly contain free-form text fields (e.g., reviews or clinical notes) alongside structured numerical and categorical attributes. Generating such heterogeneous tables with joint modeling of different modalities remains challenging. Existing approaches broadly fall into two categories: diffusion-based methods and LLM-based methods. Diffusion models can capture complex dependencies over numerical and categorical features in continuous or discrete spaces, but extending them to open-ended text is nontrivial and often leads to degraded text quality. In contrast, LLM-based generators naturally produce fluent text, yet their discrete tokenization can distort precise or wide-range numerical values, hindering accurate modeling of both numbers and language. In this work, we propose TabDLM, a unified framework for free-form tabular data generation via a joint numerical--language diffusion model built on masked diffusion language models (MDLMs). TabDLM models textual and categorical features through masked diffusion, while modeling numerical features with a continuous diffusion process through learned specialized numeric tokens embedding; bidirectional attention then captures cross-modality interactions within a single model. Extensive experiments on diverse benchmarks demonstrate the effectiveness of TabDLM compared to strong diffusion- and LLM-based baselines.",
        "authors_display": "Muhan Zhang Team",
        "pdf_url": "http://arxiv.org/abs/2602.22586",
        "code_url": null,
        "date": "2026-02-26",
        "primary_category": "cs.LG",
        "chinese_summary": "生成含有自由文本字段的异构表格数据是一项复杂挑战，现有扩散模型难以有效处理开放文本，而大型语言模型则可能扭曲精确数值。为解决这一问题，本文提出了TabDLM框架，这是一个基于掩码扩散语言模型（MDLM）的统一数值-语言联合扩散模型。TabDLM通过掩码扩散建模文本和分类特征，同时利用连续扩散过程和学习到的专用数值令牌嵌入来建模数值特征，并通过双向注意力捕捉跨模态交互。广泛的实验证明，TabDLM在多种基准测试中表现出优于现有扩散和LLM基线模型的有效性。"
      },
      {
        "paper_id": "2602.22176",
        "title": "Mixed Magnification Aggregation for Generalizable Region-Level Representations in Computational Pathology",
        "abstract": "In recent years, a standard computational pathology workflow has emerged where whole slide images are cropped into tiles, these tiles are processed using a foundation model, and task-specific models are built using the resulting representations. At least 15 different foundation models have been proposed, and the vast majority are trained exclusively with tiles using the 20$\\times$ magnification. However, it is well known that certain histologic features can only be discerned with larger context windows and requires a pathologist to zoom in and out when analyzing a whole slide image. Furthermore, creating 224$\\times$224 pixel crops at 20$\\times$ leads to a large number of tiles per slide, which can be gigapixel in size. To more accurately capture multi-resolution features and investigate the possibility of reducing the number of representations per slide, we propose a region-level mixing encoder. Our approach jointly fuses image tile representations of a mixed magnification foundation model using a masked embedding modeling pretraining step. We explore a design space for pretraining the proposed mixed-magnification region aggregators and evaluate our models on transfer to biomarker prediction tasks representing various cancer types. Results demonstrate cancer dependent improvements in predictive performance, highlighting the importance of spatial context and understanding.",
        "authors_display": "Kristen A Severson Team",
        "pdf_url": "http://arxiv.org/abs/2602.22176",
        "code_url": null,
        "date": "2026-02-25",
        "primary_category": "cs.CV",
        "chinese_summary": "计算病理学中，现有基础模型多采用20倍放大率的切片进行训练，忽略了多分辨率特征的重要性，且导致每张切片产生大量图块，增加了计算负担。为更准确地捕获多分辨率特征并减少每张切片的表示数量，本文提出了一种区域级混合编码器。该方法通过掩码嵌入建模预训练步骤，联合融合来自混合放大基础模型的图像切片表示。研究团队探索了预训练混合放大区域聚合器的设计空间，并在多种癌症类型的生物标志物预测任务上评估了模型。结果表明，该方法在预测性能上取得了依赖于癌症的改进，强调了空间背景和理解的重要性。"
      },
      {
        "paper_id": "2602.22091",
        "title": "Learning to Drive is a Free Gift: Large-Scale Label-Free Autonomy Pretraining from Unposed In-The-Wild Videos",
        "abstract": "Ego-centric driving videos available online provide an abundant source of visual data for autonomous driving, yet their lack of annotations makes it difficult to learn representations that capture both semantic structure and 3D geometry. Recent advances in large feedforward spatial models demonstrate that point maps and ego-motion can be inferred in a single forward pass, suggesting a promising direction for scalable driving perception. We therefore propose a label-free, teacher-guided framework for learning autonomous driving representations directly from unposed videos. Unlike prior self-supervised approaches that focus primarily on frame-to-frame consistency, we posit that safe and reactive driving depends critically on temporal context. To this end, we leverage a feedforward architecture equipped with a lightweight autoregressive module, trained using multi-modal supervisory signals that guide the model to jointly predict current and future point maps, camera poses, semantic segmentation, and motion masks. Multi-modal teachers provide sequence-level pseudo-supervision, enabling LFG to learn a unified pseudo-4D representation from raw YouTube videos without poses, labels, or LiDAR. The resulting encoder not only transfers effectively to downstream autonomous driving planning on the NAVSIM benchmark, surpassing multi-camera and LiDAR baselines with only a single monocular camera, but also yields strong performance when evaluated on a range of semantic, geometric, and qualitative motion prediction tasks. These geometry and motion-aware features position LFG as a compelling video-centric foundation model for autonomous driving.",
        "authors_display": "Wei Zhan Team",
        "pdf_url": "http://arxiv.org/abs/2602.22091",
        "code_url": null,
        "date": "2026-02-25",
        "primary_category": "cs.CV",
        "chinese_summary": "在线第一视角自动驾驶视频数据丰富但缺乏标注，难以学习结合语义和3D几何的表示，且现有自监督方法忽视了时间上下文的重要性。为此，本文提出了LFG，一个无标签、教师引导的框架，直接从无姿态视频中学习自动驾驶表示。LFG利用带有自回归模块的前馈架构，通过多模态教师提供序列级伪监督，共同预测点图、姿态、语义分割和运动掩码。实验证明，该编码器在NAVSIM基准上超越了多摄像头和激光雷达基线，并在语义、几何和运动预测任务上表现出色。"
      },
      {
        "paper_id": "2602.22066",
        "title": "DualWeaver: Synergistic Feature Weaving Surrogates for Multivariate Forecasting with Univariate Time Series Foundation Models",
        "abstract": "Time-series foundation models (TSFMs) have achieved strong univariate forecasting through large-scale pre-training, yet effectively extending this success to multivariate forecasting remains challenging. To address this, we propose DualWeaver, a novel framework that adapts univariate TSFMs (Uni-TSFMs) for multivariate forecasting by using a pair of learnable, structurally symmetric surrogate series. Generated by a shared auxiliary feature-fusion module that captures cross-variable dependencies, these surrogates are mapped to TSFM-compatible series via the forecasting objective. The symmetric structure enables parameter-free reconstruction of final predictions directly from the surrogates, without additional parametric decoding. A theoretically grounded regularization term is further introduced to enhance robustness against adaptation collapse. Extensive experiments on diverse real-world datasets show that DualWeaver outperforms state-of-the-art multivariate forecasters in both accuracy and stability. We release the code at https://github.com/li-jinpeng/DualWeaver.",
        "authors_display": "Jianmin Wang Team",
        "pdf_url": "http://arxiv.org/abs/2602.22066",
        "code_url": null,
        "date": "2026-02-25",
        "primary_category": "cs.LG",
        "chinese_summary": "时间序列基础模型在单变量预测中表现卓越，但其成功难以直接扩展到多变量预测。为解决此挑战，本文提出了DualWeaver框架，该框架通过一对可学习、结构对称的代理序列来适应单变量TSFMs进行多变量预测。这些代理序列由一个辅助特征融合模块生成，并引入了理论基础的正则化项。在多个真实世界数据集上的广泛实验表明，DualWeaver在准确性和稳定性方面均优于现有最先进的多变量预测器。"
      },
      {
        "paper_id": "2602.22026",
        "title": "RGB-Event HyperGraph Prompt for Kilometer Marker Recognition based on Pre-trained Foundation Models",
        "abstract": "Metro trains often operate in highly complex environments, characterized by illumination variations, high-speed motion, and adverse weather conditions. These factors pose significant challenges for visual perception systems, especially those relying solely on conventional RGB cameras. To tackle these difficulties, we explore the integration of event cameras into the perception system, leveraging their advantages in low-light conditions, high-speed scenarios, and low power consumption. Specifically, we focus on Kilometer Marker Recognition (KMR), a critical task for autonomous metro localization under GNSS-denied conditions. In this context, we propose a robust baseline method based on a pre-trained RGB OCR foundation model, enhanced through multi-modal adaptation. Furthermore, we construct the first large-scale RGB-Event dataset, EvMetro5K, containing 5,599 pairs of synchronized RGB-Event samples, split into 4,479 training and 1,120 testing samples. Extensive experiments on EvMetro5K and other widely used benchmarks demonstrate the effectiveness of our approach for KMR. Both the dataset and source code will be released on https://github.com/Event-AHU/EvMetro5K_benchmark",
        "authors_display": "Yan Tian Team",
        "pdf_url": "http://arxiv.org/abs/2602.22026",
        "code_url": null,
        "date": "2026-02-25",
        "primary_category": "cs.CV",
        "chinese_summary": "地铁列车在复杂运行环境中面临光照、高速和恶劣天气带来的视觉感知挑战，传统RGB相机性能受限。为解决此问题，本文探索将事件相机集成到感知系统，并针对GNSS拒止条件下的关键任务——公里标识别（KMR）提出了一种基于预训练RGB OCR基础模型的鲁棒多模态适应方法。同时，构建了首个大规模RGB-事件数据集EvMetro5K。实验结果表明，该方法在KMR任务上表现出显著有效性。"
      },
      {
        "paper_id": "2602.22010",
        "title": "World Guidance: World Modeling in Condition Space for Action Generation",
        "abstract": "Leveraging future observation modeling to facilitate action generation presents a promising avenue for enhancing the capabilities of Vision-Language-Action (VLA) models. However, existing approaches struggle to strike a balance between maintaining efficient, predictable future representations and preserving sufficient fine-grained information to guide precise action generation. To address this limitation, we propose WoG (World Guidance), a framework that maps future observations into compact conditions by injecting them into the action inference pipeline. The VLA is then trained to simultaneously predict these compressed conditions alongside future actions, thereby achieving effective world modeling within the condition space for action inference. We demonstrate that modeling and predicting this condition space not only facilitates fine-grained action generation but also exhibits superior generalization capabilities. Moreover, it learns effectively from substantial human manipulation videos. Extensive experiments across both simulation and real-world environments validate that our method significantly outperforms existing methods based on future prediction. Project page is available at: https://selen-suyue.github.io/WoGNet/",
        "authors_display": "Xihui Liu Team",
        "pdf_url": "http://arxiv.org/abs/2602.22010",
        "code_url": "https://selen-suyue.github.io/WoGNet/",
        "date": "2026-02-25",
        "primary_category": "cs.RO",
        "chinese_summary": "利用未来观测建模来指导动作生成是增强VLA模型能力的有效途径，但现有方法难以平衡高效未来表示与细粒度动作指导。针对此问题，本文提出了WoG（World Guidance）框架，通过将未来观测映射为紧凑条件并注入动作推理管道。VLA模型被训练同时预测这些压缩条件和未来动作。实验结果表明，WoG不仅促进了细粒度动作生成，还展现了卓越的泛化能力，并在仿真和真实环境中显著优于现有基于未来预测的方法。"
      },
      {
        "paper_id": "2602.22001",
        "title": "Are Foundation Models the Route to Full-Stack Transfer in Robotics?",
        "abstract": "In humans and robots alike, transfer learning occurs at different levels of abstraction, from high-level linguistic transfer to low-level transfer of motor skills. In this article, we provide an overview of the impact that foundation models and transformer networks have had on these different levels, bringing robots closer than ever to \"full-stack transfer\". Considering LLMs, VLMs and VLAs from a robotic transfer learning perspective allows us to highlight recurring concepts for transfer, beyond specific implementations. We also consider the challenges of data collection and transfer benchmarks for robotics in the age of foundation models. Are foundation models the route to full-stack transfer in robotics? Our expectation is that they will certainly stay on this route as a key technology.",
        "authors_display": "Shuran Song Team",
        "pdf_url": "http://arxiv.org/abs/2602.22001",
        "code_url": null,
        "date": "2026-02-25",
        "primary_category": "cs.RO",
        "chinese_summary": "迁移学习在人类和机器人中都存在于不同抽象层次。本文探讨了基础模型和Transformer网络如何影响这些层次，使机器人更接近“全栈迁移”。文章从机器人迁移学习角度审视LLMs、VLMs和VLAs，并分析了基础模型时代机器人数据收集和迁移基准的挑战。结论认为，基础模型作为关键技术，将持续推动机器人实现全栈迁移。"
      },
      {
        "paper_id": "2602.21835",
        "title": "UniVBench: Towards Unified Evaluation for Video Foundation Models",
        "abstract": "Video foundation models aim to integrate video understanding, generation, editing, and instruction following within a single framework, making them a central direction for next-generation multimodal systems. However, existing evaluation benchmarks remain fragmented and limited in scope, as they each target a single task, rely on task-specific metrics, and typically use short or simple video clips. As a result, they do not capture the unified capabilities that these models are designed to deliver. To address this gap, we introduce UniVBench, a benchmark purpose-built for evaluating video foundation models across four core abilities: video understanding, video generation, video editing, and a newly proposed task, video reconstruction, which assesses how faithfully a model can reproduce video content it has encountered. Our benchmark substantially expands the complexity of evaluation by incorporating 200 high-quality, diverse and multi-shot videos, each paired with detailed captions, multi-format editing instructions, and reference images. All videos are human-created and carefully validated, offering richer cinematic information than prior benchmarks. In addition, we develop a unified agentic evaluation system (UniV-Eval) that standardizes prompting, instruction parsing, and scoring across all tasks, enabling fair, scalable, and reproducible comparisons of unified video models. By grounding evaluation in instruction-based multi-shot video tasks, UniVBench provides the first framework for measuring the integrated capabilities that video foundation models aim to achieve. Extensive human annotations ensure our evaluation aligns with human judgment, enabling rigorous assessment and accelerating progress toward robust video intelligence.",
        "authors_display": "Zuozhu Liu Team",
        "pdf_url": "http://arxiv.org/abs/2602.21835",
        "code_url": null,
        "date": "2026-02-25",
        "primary_category": "cs.CV",
        "chinese_summary": "视频基础模型旨在整合视频理解、生成、编辑和指令遵循，但现有评估基准碎片化且范围有限，无法全面评估其统一能力。针对此问题，本文推出了UniVBench，一个专门用于评估视频基础模型四项核心能力（理解、生成、编辑和新提出的重建任务）的基准。该基准包含200个高质量、多样化和多镜头视频，并开发了统一的智能体评估系统UniV-Eval。大量人工标注确保评估与人类判断一致，为衡量视频基础模型的集成能力提供了框架。"
      },
      {
        "paper_id": "2602.21818",
        "title": "SkyReels-V4: Multi-modal Video-Audio Generation, Inpainting and Editing model",
        "abstract": "SkyReels V4 is a unified multi modal video foundation model for joint video audio generation, inpainting, and editing. The model adopts a dual stream Multimodal Diffusion Transformer (MMDiT) architecture, where one branch synthesizes video and the other generates temporally aligned audio, while sharing a powerful text encoder based on the Multimodal Large Language Models (MMLM). SkyReels V4 accepts rich multi modal instructions, including text, images, video clips, masks, and audio references. By combining the MMLMs multi modal instruction following capability with in context learning in the video branch MMDiT, the model can inject fine grained visual guidance under complex conditioning, while the audio branch MMDiT simultaneously leverages audio references to guide sound generation. On the video side, we adopt a channel concatenation formulation that unifies a wide range of inpainting style tasks, such as image to video, video extension, and video editing under a single interface, and naturally extends to vision referenced inpainting and editing via multi modal prompts. SkyReels V4 supports up to 1080p resolution, 32 FPS, and 15 second duration, enabling high fidelity, multi shot, cinema level video generation with synchronized audio. To make such high resolution, long-duration generation computationally feasible, we introduce an efficiency strategy: Joint generation of low resolution full sequences and high-resolution keyframes, followed by dedicated super-resolution and frame interpolation models. To our knowledge, SkyReels V4 is the first video foundation model that simultaneously supports multi-modal input, joint video audio generation, and a unified treatment of generation, inpainting, and editing, while maintaining strong efficiency and quality at cinematic resolutions and durations.",
        "authors_display": "Yahui Zhou Team",
        "pdf_url": "http://arxiv.org/abs/2602.21818",
        "code_url": null,
        "date": "2026-02-25",
        "primary_category": "cs.CV",
        "chinese_summary": "现有视频生成模型在多模态输入、联合音视频生成及生成、修复、编辑统一处理方面存在局限，且难以在高分辨率和长时长下保持效率。本文提出SkyReels V4，一个统一的多模态视频基础模型，采用双流MMDiT架构，通过MMLM支持丰富的多模态指令。模型通过通道拼接统一多种修复任务，并引入效率策略以实现高分辨率长时长生成。据作者所知，SkyReels V4是首个同时支持多模态输入、联合音视频生成以及生成、修复、编辑统一处理的视频基础模型，并在影院级分辨率和时长下保持了高效率和质量。"
      },
      {
        "paper_id": "2602.21757",
        "title": "Learning from Yesterday's Error: An Efficient Online Learning Method for Traffic Demand Prediction",
        "abstract": "Accurately predicting short-term traffic demand is critical for intelligent transportation systems. While deep learning models achieve strong performance under stationary conditions, their accuracy often degrades significantly when faced with distribution shifts caused by external events or evolving urban dynamics. Frequent model retraining to adapt to such changes incurs prohibitive computational costs, especially for large-scale or foundation models. To address this challenge, we propose FORESEE (Forecasting Online with Residual Smoothing and Ensemble Experts), a lightweight online adaptation framework that is accurate, robust, and computationally efficient. FORESEE operates without any parameter updates to the base model. Instead, it corrects today's forecast in each region using yesterday's prediction error, stabilized through exponential smoothing guided by a mixture-of-experts mechanism that adapts to recent error dynamics. Moreover, an adaptive spatiotemporal smoothing component propagates error signals across neighboring regions and time slots, capturing coherent shifts in demand patterns. Extensive experiments on seven real-world datasets with three backbone models demonstrate that FORESEE consistently improves prediction accuracy, maintains robustness even when distribution shifts are minimal (avoiding performance degradation), and achieves the lowest computational overhead among existing online methods. By enabling real-time adaptation of traffic forecasting models with negligible computational cost, FORESEE paves the way for deploying reliable, up-to-date prediction systems in dynamic urban environments. Code and data are available at https://github.com/xiannanhuang/FORESEE",
        "authors_display": "Chao Yang Team",
        "pdf_url": "http://arxiv.org/abs/2602.21757",
        "code_url": null,
        "date": "2026-02-25",
        "primary_category": "cs.LG",
        "chinese_summary": "短期交通需求预测对智能交通系统至关重要，但深度学习模型在分布偏移时性能显著下降，且频繁再训练成本高昂。本文提出了FORESEE（Forecasting Online with Residual Smoothing and Ensemble Experts），一个轻量级在线适应框架。FORESEE无需更新基础模型参数，而是通过指数平滑、专家混合机制和自适应时空平滑组件来纠正预测误差。在七个真实世界数据集上的实验表明，FORESEE显著提高了预测准确性，保持了鲁棒性，并实现了现有在线方法中的最低计算开销。"
      },
      {
        "paper_id": "2602.21637",
        "title": "CARE: A Molecular-Guided Foundation Model with Adaptive Region Modeling for Whole Slide Image Analysis",
        "abstract": "Foundation models have recently achieved impressive success in computational pathology, demonstrating strong generalization across diverse histopathology tasks. However, existing models overlook the heterogeneous and non-uniform organization of pathological regions of interest (ROIs) because they rely on natural image backbones not tailored for tissue morphology. Consequently, they often fail to capture the coherent tissue architecture beyond isolated patches, limiting interpretability and clinical relevance. To address these challenges, we present Cross-modal Adaptive Region Encoder (CARE), a foundation model for pathology that automatically partitions WSIs into several morphologically relevant regions. Specifically, CARE employs a two-stage pretraining strategy: (1) a self-supervised unimodal pretraining stage that learns morphological representations from 34,277 whole-slide images (WSIs) without segmentation annotations, and (2) a cross-modal alignment stage that leverages RNA and protein profiles to refine the construction and representation of adaptive regions. This molecular guidance enables CARE to identify biologically relevant patterns and generate irregular yet coherent tissue regions, selecting the most representative area as ROI. CARE supports a broad range of pathology-related tasks, using either the ROI feature or the slide-level feature obtained by aggregating adaptive regions. Based on only one-tenth of the pretraining data typically used by mainstream foundation models, CARE achieves superior average performance across 33 downstream benchmarks, including morphological classification, molecular prediction, and survival analysis, and outperforms other foundation model baselines overall.",
        "authors_display": "Zeyu Gao Team",
        "pdf_url": "http://arxiv.org/abs/2602.21637",
        "code_url": null,
        "date": "2026-02-25",
        "primary_category": "cs.CV",
        "chinese_summary": "现有计算病理学基础模型在处理病理感兴趣区域时，未能有效捕获异质性和连贯组织结构。为此，研究提出了一种名为跨模态自适应区域编码器（CARE）的基础模型。CARE采用两阶段预训练策略：首先进行自监督单模态预训练以学习形态学表示，然后通过RNA和蛋白质图谱的跨模态对齐来精炼自适应区域的构建和表示。实验结果表明，CARE仅使用主流基础模型十分之一的预训练数据，便在33个下游基准测试中取得了卓越的平均性能，并全面超越了其他基础模型基线。"
      },
      {
        "paper_id": "2602.21636",
        "title": "Axial-Centric Cross-Plane Attention for 3D Medical Image Classification",
        "abstract": "Clinicians commonly interpret three-dimensional (3D) medical images, such as computed tomography (CT) scans, using multiple anatomical planes rather than as a single volumetric representation. In this multi-planar approach, the axial plane typically serves as the primary acquisition and diagnostic reference, while the coronal and sagittal planes provide complementary spatial information to increase diagnostic confidence. However, many existing 3D deep learning methods either process volumetric data holistically or assign equal importance to all planes, failing to reflect the axial-centric clinical interpretation workflow. To address this gap, we propose an axial-centric cross-plane attention architecture for 3D medical image classification that captures the inherent asymmetric dependencies between different anatomical planes. Our architecture incorporates MedDINOv3, a medical vision foundation model pretrained via self-supervised learning on large-scale axial CT images, as a frozen feature extractor for the axial, coronal, and sagittal planes. RICA blocks and intra-plane transformer encoders capture plane-specific positional and contextual information within each anatomical plane, while axial-centric cross-plane transformer encoders condition axial features on complementary information from auxiliary planes. Experimental results on six datasets from the MedMNIST3D benchmark demonstrate that the proposed architecture consistently outperforms existing 3D and multi-plane models in terms of accuracy and AUC. Ablation studies further confirm the importance of axial-centric query-key-value allocation and directional cross-plane fusion. These results highlight the importance of aligning architectural design with clinical interpretation workflows for robust and data-efficient 3D medical image analysis.",
        "authors_display": "Lohendran Baskaran Team",
        "pdf_url": "http://arxiv.org/abs/2602.21636",
        "code_url": null,
        "date": "2026-02-25",
        "primary_category": "cs.CV",
        "chinese_summary": "临床医生在解释3D医学图像时，常以轴位平面为中心，辅以冠状和矢状平面，但现有3D深度学习方法未能模拟这种轴位为中心的解释流程。为此，研究提出了一种以轴位为中心的跨平面注意力架构，用于3D医学图像分类。该架构将预训练的医学视觉基础模型MedDINOv3作为特征提取器，并设计了RICA块和跨平面Transformer编码器以捕获并融合各平面的互补信息。在MedMNIST3D基准测试的六个数据集上，该架构在准确性和AUC方面均优于现有模型，消融实验也证实了轴位中心查询分配和定向跨平面融合的重要性。"
      },
      {
        "paper_id": "2602.21552",
        "title": "Generalizing Visual Geometry Priors to Sparse Gaussian Occupancy Prediction",
        "abstract": "Accurate 3D scene understanding is essential for embodied intelligence, with occupancy prediction emerging as a key task for reasoning about both objects and free space. Existing approaches largely rely on depth priors (e.g., DepthAnything) but make only limited use of 3D cues, restricting performance and generalization. Recently, visual geometry models such as VGGT have shown strong capability in providing rich 3D priors, but similar to monocular depth foundation models, they still operate at the level of visible surfaces rather than volumetric interiors, motivating us to explore how to more effectively leverage these increasingly powerful geometry priors for 3D occupancy prediction. We present GPOcc, a framework that leverages generalizable visual geometry priors (GPs) for monocular occupancy prediction. Our method extends surface points inward along camera rays to generate volumetric samples, which are represented as Gaussian primitives for probabilistic occupancy inference. To handle streaming input, we further design a training-free incremental update strategy that fuses per-frame Gaussians into a unified global representation. Experiments on Occ-ScanNet and EmbodiedOcc-ScanNet demonstrate significant gains: GPOcc improves mIoU by +9.99 in the monocular setting and +11.79 in the streaming setting over prior state of the art. Under the same depth prior, it achieves +6.73 mIoU while running 2.65$\\times$ faster. These results highlight that GPOcc leverages geometry priors more effectively and efficiently. Code will be released at https://github.com/JuIvyy/GPOcc.",
        "authors_display": "Changhao Chen Team",
        "pdf_url": "http://arxiv.org/abs/2602.21552",
        "code_url": null,
        "date": "2026-02-25",
        "primary_category": "cs.CV",
        "chinese_summary": "3D场景理解对具身智能至关重要，但现有占据预测方法主要依赖深度先验且对3D线索利用有限，限制了性能。研究提出GPOcc框架，旨在更有效地利用可泛化的视觉几何先验（GPs）进行单目占据预测。该方法将表面点沿相机光线向内延伸生成体积样本，并以高斯基元表示进行概率占据推断；同时，为处理流式输入，设计了免训练的增量更新策略。实验结果表明，GPOcc在Occ-ScanNet和EmbodiedOcc-ScanNet上显著提升了mIoU，并在相同深度先验下，实现了更高的mIoU和更快的运行速度。"
      },
      {
        "paper_id": "2602.21467",
        "title": "Geometric Priors for Generalizable World Models via Vector Symbolic Architecture",
        "abstract": "A key challenge in artificial intelligence and neuroscience is understanding how neural systems learn representations that capture the underlying dynamics of the world. Most world models represent the transition function with unstructured neural networks, limiting interpretability, sample efficiency, and generalization to unseen states or action compositions. We address these issues with a generalizable world model grounded in Vector Symbolic Architecture (VSA) principles as geometric priors. Our approach utilizes learnable Fourier Holographic Reduced Representation (FHRR) encoders to map states and actions into a high dimensional complex vector space with learned group structure and models transitions with element-wise complex multiplication. We formalize the framework's group theoretic foundation and show how training such structured representations to be approximately invariant enables strong multi-step composition directly in latent space and generalization performances over various experiments. On a discrete grid world environment, our model achieves 87.5% zero shot accuracy to unseen state-action pairs, obtains 53.6% higher accuracy on 20-timestep horizon rollouts, and demonstrates 4x higher robustness to noise relative to an MLP baseline. These results highlight how training to have latent group structure yields generalizable, data-efficient, and interpretable world models, providing a principled pathway toward structured models for real-world planning and reasoning.",
        "authors_display": "Mohsen Imani Team",
        "pdf_url": "http://arxiv.org/abs/2602.21467",
        "code_url": null,
        "date": "2026-02-25",
        "primary_category": "cs.LG",
        "chinese_summary": "理解神经系统如何学习捕捉世界动态的表示是人工智能和神经科学的关键挑战，现有世界模型常采用非结构化神经网络，限制了可解释性和泛化能力。为此，研究提出了一种基于向量符号架构（VSA）作为几何先验的可泛化世界模型。该方法利用可学习的傅里叶全息降维表示（FHRR）编码器将状态和动作映射到高维复向量空间，并利用逐元素复数乘法建模转换，其群论基础能实现强大的多步组合和泛化。实验结果显示，该模型在离散网格世界中对未见状态-动作对实现了高零样本准确率，显著提高了多步预测的准确性，并对噪声具有更强的鲁棒性。"
      },
      {
        "paper_id": "2602.22452",
        "title": "CWM: Contrastive World Models for Action Feasibility Learning in Embodied Agent Pipelines",
        "abstract": "A reliable action feasibility scorer is a critical bottleneck in embodied agent pipelines: before any planning or reasoning occurs, the agent must identify which candidate actions are physically executable in the current state. Existing approaches use supervised fine-tuning (SFT) to train action scorers, but SFT treats each candidate independently and does not explicitly teach the model to discriminate between actions that are physically correct and those that are subtly wrong. We propose the Contrastive World Model (CWM), which fine-tunes a large language model (LLM) as an action scorer using an InfoNCE contrastive objective with hard-mined negative examples. The key idea is to push valid actions away from invalid ones in scoring space, with special emphasis on hard negatives: semantically similar but physically incompatible candidates. We evaluate CWM on the ScienceWorld benchmark through two studies. First, an intrinsic affordance evaluation on 605 hard-negative test pairs shows that CWM outperforms SFT by +6.76 percentage points on Precision@1 for minimal-edit negatives -- cases where a single word changes the physical outcome -- and achieves a higher AUC-ROC (0.929 vs. 0.906). Second, a live filter characterisation study measures how well CWM ranks gold-path actions against all valid environment actions during task execution. Under out-of-distribution stress conditions, CWM maintains a significantly better safety margin (-2.39) than SFT (-3.96), indicating that the gold action is ranked closer to the top. These results support the hypothesis that contrastive training induces representations that capture physical feasibility more faithfully than SFT alone.",
        "authors_display": "Chayan Banerjee Team",
        "pdf_url": "http://arxiv.org/abs/2602.22452",
        "code_url": null,
        "date": "2026-02-25",
        "primary_category": "cs.AI",
        "chinese_summary": "具身智能体管线中，动作可行性评估器的可靠性是一个关键瓶颈，现有监督微调方法无法有效区分物理上正确和细微错误的动作。本文提出了对比世界模型（CWM），该模型利用InfoNCE对比目标和难挖掘负例，对大型语言模型（LLM）进行微调作为动作评估器，旨在评分空间中有效区分有效与无效动作，尤其关注语义相似但物理不兼容的难负例。在ScienceWorld基准测试中，CWM在难负例的Precision@1指标上比SFT提升了6.76个百分点，AUC-ROC更高，并在分布外压力下保持了更好的安全裕度，证明对比训练能更忠实地捕捉物理可行性。"
      },
      {
        "paper_id": "2602.22430",
        "title": "TopoEdit: Fast Post-Optimization Editing of Topology Optimized Structures",
        "abstract": "Despite topology optimization producing high-performance structures, late-stage localized revisions remain brittle: direct density-space edits (e.g., warping pixels, inserting holes, swapping infill) can sever load paths and sharply degrade compliance, while re-running optimization is slow and may drift toward a qualitatively different design. We present TopoEdit, a fast post-optimization editor that demonstrates how structured latent embeddings from a pre-trained topology foundation model (OAT) can be repurposed as an interface for physics-aware engineering edits. Given an optimized topology, TopoEdit encodes it into OAT's spatial latent, applies partial noising to preserve instance identity while increasing editability, and injects user intent through an edit-then-denoise diffusion pipeline. We instantiate three edit operators: drag-based topology warping with boundary-condition-consistent conditioning updates, shell-infill lattice replacement using a lattice-anchored reference latent with updated volume-fraction conditioning, and late-stage no-design region enforcement via masked latent overwrite followed by diffusion-based recovery. A consistency-preserving guided DDIM procedure localizes changes while allowing global structural adaptation; multiple candidates can be sampled and selected using a compliance-aware criterion, with optional short SIMP refinement for warps. Across diverse case studies and large edit sweeps, TopoEdit produces intention-aligned modifications that better preserve mechanical performance and avoid catastrophic failure modes compared to direct density-space edits, while generating edited candidates in sub-second diffusion time per sample.",
        "authors_display": "Faez Ahmed Team",
        "pdf_url": "http://arxiv.org/abs/2602.22430",
        "code_url": null,
        "date": "2026-02-25",
        "primary_category": "cs.GR",
        "chinese_summary": "拓扑优化结构后期局部修改极易损害其力学性能。针对此问题，本文提出了TopoEdit，一种快速的后优化编辑器，它利用预训练拓扑基础模型（OAT）的结构化潜在嵌入作为物理感知工程编辑的接口。该方法通过编码、局部噪声和“编辑-去噪”扩散管道注入用户意图，并实例化了拖拽式变形、晶格替换和无设计区域强制执行等操作。实验结果显示，TopoEdit在保持结构机械性能和避免灾难性故障方面优于直接密度空间编辑，且能在亚秒级时间内生成与用户意图一致的修改。"
      },
      {
        "paper_id": "2602.22347",
        "title": "Enabling clinical use of foundation models in histopathology",
        "abstract": "Foundation models in histopathology are expected to facilitate the development of high-performing and generalisable deep learning systems. However, current models capture not only biologically relevant features, but also pre-analytic and scanner-specific variation that bias the predictions of task-specific models trained from the foundation model features. Here we show that introducing novel robustness losses during training of downstream task-specific models reduces sensitivity to technical variability. A purpose-designed comprehensive experimentation setup with 27,042 WSIs from 6155 patients is used to train thousands of models from the features of eight popular foundation models for computational pathology. In addition to a substantial improvement in robustness, we observe that prediction accuracy improves by focusing on biologically relevant features. Our approach successfully mitigates robustness issues of foundation models for computational pathology without retraining the foundation models themselves, enabling development of robust computational pathology models applicable to real-world data in routine clinical practice.",
        "authors_display": "Andreas Kleppe Team",
        "pdf_url": "http://arxiv.org/abs/2602.22347",
        "code_url": null,
        "date": "2026-02-25",
        "primary_category": "cs.CV",
        "chinese_summary": "组织病理学基础模型在实际应用中，常因捕获到预分析和扫描仪特异性变异而导致任务特定模型预测偏差。为解决这一鲁棒性问题，本研究提出在下游任务特定模型训练过程中引入新的鲁棒性损失。通过一个涵盖27,042张全切片图像的综合实验设置，从八个流行基础模型的特征中训练了数千个模型。结果表明，这种方法显著提高了模型的鲁棒性和预测准确性，因为它更专注于生物学相关特征。该方法在无需重新训练基础模型的情况下，成功缓解了计算病理学基础模型的鲁棒性问题，使其能更好地应用于临床实践中的真实数据。"
      },
      {
        "paper_id": "2602.19863",
        "title": "Brewing Stronger Features: Dual-Teacher Distillation for Multispectral Earth Observation",
        "abstract": "Foundation models are transforming Earth Observation (EO), yet the diversity of EO sensors and modalities makes a single universal model unrealistic. Multiple specialized EO foundation models (EOFMs) will likely coexist, making efficient knowledge transfer across modalities essential. Most existing EO pretraining relies on masked image modeling, which emphasizes local reconstruction but provides limited control over global semantic structure. To address this, we propose a dual-teacher contrastive distillation framework for multispectral imagery that aligns the student's pretraining objective with the contrastive self-distillation paradigm of modern optical vision foundation models (VFMs). Our approach combines a multispectral teacher with an optical VFM teacher, enabling coherent cross-modal representation learning. Experiments across diverse optical and multispectral benchmarks show that our model adapts to multispectral data without compromising performance on optical-only inputs, achieving state-of-the-art results in both settings, with an average improvement of 3.64 percentage points in semantic segmentation, 1.2 in change detection, and 1.31 in classification tasks. This demonstrates that contrastive distillation provides a principled and efficient approach to scalable representation learning across heterogeneous EO data sources. Project page: \\textcolor{magenta}{https://wolfilip.github.io/DEO/}.",
        "authors_display": "Luka Čehovin Zajc Team",
        "pdf_url": "http://arxiv.org/abs/2602.19863",
        "code_url": null,
        "date": "2026-02-24",
        "primary_category": "cs.CV",
        "chinese_summary": "地球观测（EO）领域的基础模型面临传感器多样性挑战，高效的跨模态知识迁移至关重要。现有EO预训练主要依赖掩码图像建模，但对全局语义结构控制有限。本文提出一种用于多光谱图像的双教师对比蒸馏框架，将学生模型的预训练目标与现代光学视觉基础模型（VFMs）的对比自蒸馏范式对齐，通过结合多光谱教师和光学VFM教师实现一致的跨模态表示学习。实验结果表明，该模型在不影响光学数据性能的情况下适应多光谱数据，在语义分割、变化检测和分类任务中均取得显著提升，平均F1分数分别提高3.64、1.2和1.31个百分点。"
      },
      {
        "paper_id": "2602.21100",
        "title": "Skullptor: High Fidelity 3D Head Reconstruction in Seconds with Multi-View Normal Prediction",
        "abstract": "Reconstructing high-fidelity 3D head geometry from images is critical for a wide range of applications, yet existing methods face fundamental limitations. Traditional photogrammetry achieves exceptional detail but requires extensive camera arrays (25-200+ views), substantial computation, and manual cleanup in challenging areas like facial hair. Recent alternatives present a fundamental trade-off: foundation models enable efficient single-image reconstruction but lack fine geometric detail, while optimization-based methods achieve higher fidelity but require dense views and expensive computation. We bridge this gap with a hybrid approach that combines the strengths of both paradigms. Our method introduces a multi-view surface normal prediction model that extends monocular foundation models with cross-view attention to produce geometrically consistent normals in a feed-forward pass. We then leverage these predictions as strong geometric priors within an inverse rendering optimization framework to recover high-frequency surface details. Our approach outperforms state-of-the-art single-image and multi-view methods, achieving high-fidelity reconstruction on par with dense-view photogrammetry while reducing camera requirements and computational cost. The code and model will be released.",
        "authors_display": "Abdallah Dib Team",
        "pdf_url": "http://arxiv.org/abs/2602.21100",
        "code_url": null,
        "date": "2026-02-24",
        "primary_category": "cs.CV",
        "chinese_summary": "从图像重建高保真3D头部几何图形存在挑战，传统方法成本高昂，单视图基础模型缺乏细节，基于优化的方法需要密集视图。为弥补这一鸿沟，研究提出了一种混合方法，结合了基础模型和优化方法的优势。该方法引入了一个多视图表面法线预测模型，通过跨视图注意力扩展单目基础模型以生成几何一致的法线，并将其作为强大的几何先验整合到逆渲染优化框架中，以恢复高频表面细节。该方法在实现与密集视图摄影测量法相当的高保真重建同时，显著减少了相机需求和计算成本，超越了现有最先进的单图像和多视图方法。"
      },
      {
        "paper_id": "2602.21042",
        "title": "OmniOCR: Generalist OCR for Ethnic Minority Languages",
        "abstract": "Optical character recognition (OCR) has advanced rapidly with deep learning and multimodal models, yet most methods focus on well-resourced scripts such as Latin and Chinese. Ethnic minority languages remain underexplored due to complex writing systems, scarce annotations, and diverse historical and modern forms, making generalization in low-resource or zero-shot settings challenging. To address these challenges, we present OmniOCR, a universal framework for ethnic minority scripts. OmniOCR introduces Dynamic Low-Rank Adaptation (Dynamic LoRA) to allocate model capacity across layers and scripts, enabling effective adaptation while preserving knowledge.A sparsity regularization prunes redundant updates, ensuring compact and efficient adaptation without extra inference cost. Evaluations on TibetanMNIST, Shui, ancient Yi, and Dongba show that OmniOCR outperforms zero-shot foundation models and standard post training, achieving state-of-the-art accuracy with superior parameter efficiency, and compared with the state-of-the-art baseline models, it improves accuracy by 39%-66% on these four datasets. Code: https://github.com/AIGeeksGroup/OmniOCR.",
        "authors_display": "Ying Cai Team",
        "pdf_url": "http://arxiv.org/abs/2602.21042",
        "code_url": null,
        "date": "2026-02-24",
        "primary_category": "cs.CV",
        "chinese_summary": "尽管OCR技术在主流语言上发展迅速，但少数民族语言由于书写系统复杂、标注稀缺，在低资源或零样本设置下面临泛化挑战。为此，研究提出了OmniOCR，一个面向少数民族文字的通用OCR框架。OmniOCR引入了动态低秩适应（Dynamic LoRA）以有效分配模型容量并保持知识，并通过稀疏正则化剪枝冗余更新，确保高效适应而无需额外推理成本。在藏文MNIST、水族文、古彝文和东巴文数据集上的评估显示，OmniOCR在参数效率和准确性上均优于零样本基础模型和标准训练后方法，准确率提升了39%-66%。"
      },
      {
        "paper_id": "2602.20989",
        "title": "Cycle-Consistent Tuning for Layered Image Decomposition",
        "abstract": "Disentangling visual layers in real-world images is a persistent challenge in vision and graphics, as such layers often involve non-linear and globally coupled interactions, including shading, reflection, and perspective distortion. In this work, we present an in-context image decomposition framework that leverages large diffusion foundation models for layered separation. We focus on the challenging case of logo-object decomposition, where the goal is to disentangle a logo from the surface on which it appears while faithfully preserving both layers. Our method fine-tunes a pretrained diffusion model via lightweight LoRA adaptation and introduces a cycle-consistent tuning strategy that jointly trains decomposition and composition models, enforcing reconstruction consistency between decomposed and recomposed images. This bidirectional supervision substantially enhances robustness in cases where the layers exhibit complex interactions. Furthermore, we introduce a progressive self-improving process, which iteratively augments the training set with high-quality model-generated examples to refine performance. Extensive experiments demonstrate that our approach achieves accurate and coherent decompositions and also generalizes effectively across other decomposition types, suggesting its potential as a unified framework for layered image decomposition.",
        "authors_display": "Hui Huang Team",
        "pdf_url": "http://arxiv.org/abs/2602.20989",
        "code_url": "https://vcc.tech/research/2026/ImgDecom",
        "date": "2026-02-24",
        "primary_category": "cs.CV",
        "chinese_summary": "在真实世界图像中解耦视觉层是一个长期存在的挑战，因为层间存在非线性、全局耦合的相互作用。本研究提出了一种上下文图像分解框架，利用大型扩散基础模型进行分层分离，尤其关注具有挑战性的徽标-对象分解任务。该方法通过轻量级LoRA自适应微调预训练扩散模型，并引入循环一致性微调策略，共同训练分解和组合模型，确保分解与重组之间的一致性。实验证明，该方法实现了准确、连贯的分解，并能有效泛化到其他分解类型，展现了其作为统一图像分层分解框架的潜力。"
      },
      {
        "paper_id": "2602.20947",
        "title": "Estimation of Confidence Bounds in Binary Classification using Wilson Score Kernel Density Estimation",
        "abstract": "The performance and ease of use of deep learning-based binary classifiers have improved significantly in recent years. This has opened up the potential for automating critical inspection tasks, which have traditionally only been trusted to be done manually. However, the application of binary classifiers in critical operations depends on the estimation of reliable confidence bounds such that system performance can be ensured up to a given statistical significance. We present Wilson Score Kernel Density Classification, which is a novel kernel-based method for estimating confidence bounds in binary classification. The core of our method is the Wilson Score Kernel Density Estimator, which is a function estimator for estimating confidence bounds in Binomial experiments with conditionally varying success probabilities. Our method is evaluated in the context of selective classification on four different datasets, illustrating its use as a classification head of any feature extractor, including vision foundation models. Our proposed method shows similar performance to Gaussian Process Classification, but at a lower computational complexity.",
        "authors_display": "Frederik Hagelskjær Team",
        "pdf_url": "http://arxiv.org/abs/2602.20947",
        "code_url": null,
        "date": "2026-02-24",
        "primary_category": "cs.LG",
        "chinese_summary": "深度学习二元分类器性能的提升为关键检测任务的自动化提供了潜力，但其在关键操作中的应用需要可靠的置信区间估计。为此，研究提出了一种新颖的基于核的方法——Wilson Score核密度分类，用于估计二元分类中的置信区间。该方法的核心是Wilson Score核密度估计器，专为条件成功概率变化的二项式实验设计。在四个数据集上的选择性分类实验表明，该方法作为包括视觉基础模型在内的任何特征提取器的分类头，表现出与高斯过程分类相似的性能，但计算复杂度更低。"
      },
      {
        "paper_id": "2602.20901",
        "title": "SpatiaLQA: A Benchmark for Evaluating Spatial Logical Reasoning in Vision-Language Models",
        "abstract": "Vision-Language Models (VLMs) have been increasingly applied in real-world scenarios due to their outstanding understanding and reasoning capabilities. Although VLMs have already demonstrated impressive capabilities in common visual question answering and logical reasoning, they still lack the ability to make reasonable decisions in complex real-world environments. We define this ability as spatial logical reasoning, which not only requires understanding the spatial relationships among objects in complex scenes, but also the logical dependencies between steps in multi-step tasks. To bridge this gap, we introduce Spatial Logical Question Answering (SpatiaLQA), a benchmark designed to evaluate the spatial logical reasoning capabilities of VLMs. SpatiaLQA consists of 9,605 question answer pairs derived from 241 real-world indoor scenes. We conduct extensive experiments on 41 mainstream VLMs, and the results show that even the most advanced models still struggle with spatial logical reasoning. To address this issue, we propose a method called recursive scene graph assisted reasoning, which leverages visual foundation models to progressively decompose complex scenes into task-relevant scene graphs, thereby enhancing the spatial logical reasoning ability of VLMs, outperforming all previous methods. Code and dataset are available at https://github.com/xieyc99/SpatiaLQA.",
        "authors_display": "Jie Song Team",
        "pdf_url": "http://arxiv.org/abs/2602.20901",
        "code_url": null,
        "date": "2026-02-24",
        "primary_category": "cs.CV",
        "chinese_summary": "视觉-语言模型（VLMs）在理解和推理方面表现出色，但在复杂真实世界环境中进行空间逻辑推理的能力仍有欠缺。为解决这一问题，研究引入了Spatial Logical Question Answering (SpatiaLQA) 基准，以评估VLMs的空间逻辑推理能力，并提出了递归场景图辅助推理方法。该方法利用视觉基础模型将复杂场景逐步分解为与任务相关的场景图，从而增强VLMs的空间逻辑推理能力。实验结果表明，即使是最先进的VLMs在SpatiaLQA上仍面临挑战，而所提出的方法则优于所有现有方法。"
      },
      {
        "paper_id": "2602.20794",
        "title": "VGGDrive: Empowering Vision-Language Models with Cross-View Geometric Grounding for Autonomous Driving",
        "abstract": "The significance of cross-view 3D geometric modeling capabilities for autonomous driving is self-evident, yet existing Vision-Language Models (VLMs) inherently lack this capability, resulting in their mediocre performance. While some promising approaches attempt to mitigate this by constructing Q&A data for auxiliary training, they still fail to fundamentally equip VLMs with the ability to comprehensively handle diverse evaluation protocols. We thus chart a new course, advocating for the infusion of VLMs with the cross-view geometric grounding of mature 3D foundation models, closing this critical capability gap in autonomous driving. In this spirit, we propose a novel architecture, VGGDrive, which empowers Vision-language models with cross-view Geometric Grounding for autonomous Driving. Concretely, to bridge the cross-view 3D geometric features from the frozen visual 3D model with the VLM's 2D visual features, we introduce a plug-and-play Cross-View 3D Geometric Enabler (CVGE). The CVGE decouples the base VLM architecture and effectively empowers the VLM with 3D features through a hierarchical adaptive injection mechanism. Extensive experiments show that VGGDrive enhances base VLM performance across five autonomous driving benchmarks, including tasks like cross-view risk perception, motion prediction, and trajectory planning. It's our belief that mature 3D foundation models can empower autonomous driving tasks through effective integration, and we hope our initial exploration demonstrates the potential of this paradigm to the autonomous driving community.",
        "authors_display": "Long Chen Team",
        "pdf_url": "http://arxiv.org/abs/2602.20794",
        "code_url": null,
        "date": "2026-02-24",
        "primary_category": "cs.CV",
        "chinese_summary": "跨视图3D几何建模能力对自动驾驶至关重要，但现有视觉-语言模型（VLMs）普遍缺乏此能力。为弥补这一关键差距，研究提出了一种名为VGGDrive的架构，旨在将成熟3D基础模型的跨视图几何基础注入VLMs。该架构引入了一个可插拔的跨视图3D几何使能器（CVGE），通过分层自适应注入机制，有效地将冻结的视觉3D模型中的3D几何特征与VLM的2D视觉特征桥接。广泛实验表明，VGGDrive在五个自动驾驶基准（包括风险感知、运动预测和轨迹规划）上显著提升了基础VLM的性能。"
      },
      {
        "paper_id": "2602.20752",
        "title": "OrthoDiffusion: A Generalizable Multi-Task Diffusion Foundation Model for Musculoskeletal MRI Interpretation",
        "abstract": "Musculoskeletal disorders represent a significant global health burden and are a leading cause of disability worldwide. While MRI is essential for accurate diagnosis, its interpretation remains exceptionally challenging. Radiologists must identify multiple potential abnormalities within complex anatomical structures across different imaging planes, a process that requires significant expertise and is prone to variability. We developed OrthoDiffusion, a unified diffusion-based foundation model designed for multi-task musculoskeletal MRI interpretation. The framework utilizes three orientation-specific 3D diffusion models, pre-trained in a self-supervised manner on 15,948 unlabeled knee MRI scans, to learn robust anatomical features from sagittal, coronal, and axial views. These view-specific representations are integrated to support diverse clinical tasks, including anatomical segmentation and multi-label diagnosis. Our evaluation demonstrates that OrthoDiffusion achieves excellent performance in the segmentation of 11 knee structures and the detection of 8 knee abnormalities. The model exhibited remarkable robustness across different clinical centers and MRI field strengths, consistently outperforming traditional supervised models. Notably, in settings where labeled data was scarce, OrthoDiffusion maintained high diagnostic precision using only 10\\% of training labels. Furthermore, the anatomical representations learned from knee imaging proved highly transferable to other joints, achieving strong diagnostic performance across 11 diseases of the ankle and shoulder. These findings suggest that diffusion-based foundation models can serve as a unified platform for multi-disease diagnosis and anatomical segmentation, potentially improving the efficiency and accuracy of musculoskeletal MRI interpretation in real-world clinical workflows.",
        "authors_display": "Dingyu Wang Team",
        "pdf_url": "http://arxiv.org/abs/2602.20752",
        "code_url": null,
        "date": "2026-02-24",
        "primary_category": "cs.CV",
        "chinese_summary": "肌肉骨骼疾病的MRI诊断因其复杂性和对专业知识的高要求而充满挑战。本研究开发了OrthoDiffusion，一个统一的基于扩散的基础模型，用于多任务肌肉骨骼MRI解读。该框架使用三个针对特定方向的3D扩散模型，通过自监督方式在15,948张未标记膝关节MRI扫描上预训练，学习鲁棒的解剖特征，并整合这些视图特定表示以支持解剖分割和多标签诊断。评估结果显示，OrthoDiffusion在膝关节结构分割和异常检测方面表现出色，且在不同临床中心和MRI场强下展现了卓越的鲁棒性，仅用少量标记数据即可保持高诊断精度，并且所学特征可迁移至其他关节疾病诊断。"
      },
      {
        "paper_id": "2602.20685",
        "title": "RAYNOVA: 3D-Geometry-Free Auto-Regressive Driving World Modeling with Unified Spatio-Temporal Representation",
        "abstract": "World foundation models aim to simulate the evolution of the real world with physically plausible behavior. Unlike prior methods that handle spatial and temporal correlations separately, we propose RAYNOVA, a geometry-free world model that employs a dual-causal autoregressive framework. It follows both scale-wise and temporal topological orders in the autoregressive process, and leverages global attention for unified 4D spatio-temporal reasoning. Different from existing works that impose strong 3D geometric priors, RAYNOVA constructs an isotropic spatio-temporal representation across views, frames, and scales based on relative Plücker-ray positional encoding, enabling robust generalization to diverse camera setups and ego motions. We further introduce a recurrent training paradigm to alleviate distribution drift in long-horizon video generation. RAYNOVA achieves state-of-the-art multi-view video generation results on nuScenes, while offering higher throughput and strong controllability under diverse input conditions, generalizing to novel views and camera configurations without explicit 3D scene representation. Our code will be released at http://yichen928.github.io/raynova.",
        "authors_display": "Wei Zhan Team",
        "pdf_url": "http://arxiv.org/abs/2602.20685",
        "code_url": "http://yichen928.github.io/raynova",
        "date": "2026-02-24",
        "primary_category": "cs.CV",
        "chinese_summary": "当前世界基础模型在模拟真实世界演化时，通常独立处理时空相关性并强制强3D几何先验，限制了泛化能力。本研究提出RAYNOVA，一个无几何世界模型，采用双因果自回归框架，遵循尺度感知和时间拓扑顺序，并利用全局注意力进行统一的4D时空推理。RAYNOVA通过相对Plücker射线位置编码构建各向同性时空表示，实现跨视图、帧和尺度的鲁棒泛化，并引入循环训练范式以缓解长时视频生成中的分布漂移。RAYNOVA在nuScenes上实现了最先进的多视图视频生成结果，具有高吞吐量和强大可控性，无需显式3D场景表示即可泛化到新颖视图和相机配置。"
      },
      {
        "paper_id": "2602.20677",
        "title": "UrbanFM: Scaling Urban Spatio-Temporal Foundation Models",
        "abstract": "Urban systems, as dynamic complex systems, continuously generate spatio-temporal data streams that encode the fundamental laws of human mobility and city evolution. While AI for Science has witnessed the transformative power of foundation models in disciplines like genomics and meteorology, urban computing remains fragmented due to \"scenario-specific\" models, which are overfitted to specific regions or tasks, hindering their generalizability. To bridge this gap and advance spatio-temporal foundation models for urban systems, we adopt scaling as the central perspective and systematically investigate two key questions: what to scale and how to scale. Grounded in first-principles analysis, we identify three critical dimensions: heterogeneity, correlation, and dynamics, aligning these principles with the fundamental scientific properties of urban spatio-temporal data. Specifically, to address heterogeneity through data scaling, we construct WorldST. This billion-scale corpus standardizes diverse physical signals, such as traffic flow and speed, from over 100 global cities into a unified data format. To enable computation scaling for modeling correlations, we introduce the MiniST unit, a novel split mechanism that discretizes continuous spatio-temporal fields into learnable computational units to unify representations of grid-based and sensor-based observations. Finally, addressing dynamics via architecture scaling, we propose UrbanFM, a minimalist self-attention architecture designed with limited inductive biases to autonomously learn dynamic spatio-temporal dependencies from massive data. Furthermore, we establish EvalST, the largest-scale urban spatio-temporal benchmark to date. Extensive experiments demonstrate that UrbanFM achieves remarkable zero-shot generalization across unseen cities and tasks, marking a pivotal first step toward large-scale urban spatio-temporal foundation models.",
        "authors_display": "Yuxuan Liang Team",
        "pdf_url": "http://arxiv.org/abs/2602.20677",
        "code_url": null,
        "date": "2026-02-24",
        "primary_category": "cs.LG",
        "chinese_summary": "城市系统产生的时空数据流编码了城市演化的规律，但现有城市计算模型因“场景特定”而泛化能力受限。本研究以“规模化”为核心，系统探讨了城市时空基础模型的构建。通过数据规模化，构建了十亿级语料库WorldST，统一标准化了100多个全球城市的多种物理信号。通过计算规模化，引入MiniST单元以统一网格和传感器观测表示。通过架构规模化，提出了极简主义自注意力架构UrbanFM，旨在从海量数据中自主学习动态时空依赖。同时，建立了最大规模的城市时空基准EvalST。实验证明，UrbanFM在未见城市和任务上实现了卓越的零样本泛化能力，为大规模城市时空基础模型奠定了基础。"
      },
      {
        "paper_id": "2602.20659",
        "title": "Recursive Belief Vision Language Model",
        "abstract": "Current vision-language-action (VLA) models struggle with long-horizon manipulation under partial observability. Most existing approaches remain observation-driven, relying on short context windows or repeated queries to vision-language models (VLMs). This leads to loss of task progress, action repetition under perceptual aliasing, and high inference latency. Semantic reasoning alone is not the primary bottleneck in long-horizon manipulation. Instead, VLAs lack persistent, action-conditioned state representations and exhibit limited temporal and physical reasoning, making them ill-suited for multi-stage control. This paper introduces RB-VLA, a belief-centric architecture trained with self-supervised world-model objectives that maintains a compact latent state encoding task-relevant history, dynamics, and object interactions. Queried once for high-level intent, the VLM provides task specification, while the belief tracks task progress and enables phase-aware, causally grounded control under partial observability without storing raw observations or scaling memory with time. The belief and intent jointly condition a diffusion policy for robust closed-loop execution. RB-VLA outperforms prior VLAs on long-horizon benchmarks, achieving 52.5% and 37.5% higher success on multi-stage pick-and-place and stacking tasks, respectively, compared to π0. It also reduces inference latency by up to 5x relative to baselines and eliminates memory growth across timesteps observed in existing VLAs. Ablations show that the belief module is the primary driver of performance, increasing success rates from 32.5% to 77.5%. These results demonstrate the effectiveness of belief-based state representations for long-horizon VLA policies.",
        "authors_display": "Nirav Patel Team",
        "pdf_url": "http://arxiv.org/abs/2602.20659",
        "code_url": null,
        "date": "2026-02-24",
        "primary_category": "cs.AI",
        "chinese_summary": "当前的视觉-语言-动作（VLA）模型在部分可观测下的长周期操作中表现不佳，主要因其观察驱动特性导致任务进度丢失、动作重复及高延迟。本研究提出了RB-VLA，一种以信念为中心的架构，通过自监督世界模型目标进行训练，维护一个紧凑的潜在状态，编码与任务相关的历史、动态和物体交互。该模型只需一次VLM查询即可获取高级意图，信念模块负责跟踪任务进度并在部分可观测下实现阶段感知、因果导向的控制。RB-VLA在长周期基准测试中表现优异，相比现有VLA模型显著提高了多阶段抓取放置和堆叠任务的成功率，并大幅降低了推理延迟和内存消耗，证明了基于信念的状态表示对长周期VLA策略的有效性。"
      },
      {
        "paper_id": "2602.20550",
        "title": "The Finite Primitive Basis Theorem for Computational Imaging: Formal Foundations of the OperatorGraph Representation",
        "abstract": "Computational imaging forward models, from coded aperture spectral cameras to MRI scanners, are traditionally implemented as monolithic, modality-specific codes. We prove that every forward model in a broad, precisely defined operator class Cimg (encompassing clinical, scientific, and industrial imaging modalities, both linear and nonlinear) admits an epsilon-approximate representation as a typed directed acyclic graph (DAG) whose nodes are drawn from a library of exactly 11 canonical primitives: Propagate, Modulate, Project, Encode, Convolve, Accumulate, Detect, Sample, Disperse, Scatter, and Transform. We call this the Finite Primitive Basis Theorem. The proof is constructive: we provide an algorithm that, given any H in Cimg, produces a DAG G with relative operator error at most epsilon and graph complexity within prescribed bounds. We further prove that the library is minimal: removing any single primitive causes at least one modality to lose its epsilon-approximate representation. A systematic analysis of nonlinearities in imaging physics shows they fall into two structural categories: pointwise scalar functions (handled by Transform) and self-consistent iterations (unrolled into existing linear primitives). Empirical validation on 31 linear modalities confirms eimg below 0.01 with at most 5 nodes and depth 5, and we provide constructive DAG decompositions for 9 additional nonlinear modalities. These results establish mathematical foundations for the Physics World Model (PWM) framework.",
        "authors_display": "Chengshuai Yang Team",
        "pdf_url": "http://arxiv.org/abs/2602.20550",
        "code_url": null,
        "date": "2026-02-24",
        "primary_category": "cs.CV",
        "chinese_summary": "计算成像的前向模型传统上是模态特异的单一代码，本文证明了包括线性和非线性模态在内的广泛操作符类别Cimg中的所有前向模型，都可以通过11个基本原语（如Propagate, Modulate等）组成的有向无环图进行近似表示。研究还分析了成像物理学中的非线性特性，并提供了将任意Cimg模型转换为DAG的算法。在31种线性模态和9种非线性模态上的经验验证显示，该方法实现了低于0.01的相对操作符误差，为物理世界模型（PWM）框架奠定了数学基础。"
      },
      {
        "paper_id": "2602.20539",
        "title": "Progressive Per-Branch Depth Optimization for DEFOM-Stereo and SAM3 Joint Analysis in UAV Forestry Applications",
        "abstract": "Accurate per-branch 3D reconstruction is a prerequisite for autonomous UAV-based tree pruning; however, dense disparity maps from modern stereo matchers often remain too noisy for individual branch analysis in complex forest canopies. This paper introduces a progressive pipeline integrating DEFOM-Stereo foundation-model disparity estimation, SAM3 instance segmentation, and multi-stage depth optimization to deliver robust per-branch point clouds. Starting from a naive baseline, we systematically identify and resolve three error families through successive refinements. Mask boundary contamination is first addressed through morphological erosion and subsequently refined via a skeleton-preserving variant to safeguard thin-branch topology. Segmentation inaccuracy is then mitigated using LAB-space Mahalanobis color validation coupled with cross-branch overlap arbitration. Finally, depth noise - the most persistent error source - is initially reduced by outlier removal and median filtering, before being superseded by a robust five-stage scheme comprising MAD global detection, spatial density consensus, local MAD filtering, RGB-guided filtering, and adaptive bilateral filtering. Evaluated on 1920x1080 stereo imagery of Radiata pine (Pinus radiata) acquired with a ZED Mini camera (63 mm baseline) from a UAV in Canterbury, New Zealand, the proposed pipeline reduces the average per-branch depth standard deviation by 82% while retaining edge fidelity. The result is geometrically coherent 3D point clouds suitable for autonomous pruning tool positioning. All code and processed data are publicly released to facilitate further UAV forestry research.",
        "authors_display": "Richard Green Team",
        "pdf_url": "http://arxiv.org/abs/2602.20539",
        "code_url": null,
        "date": "2026-02-24",
        "primary_category": "eess.IV",
        "chinese_summary": "自主无人机修剪树木需要精确的单分支3D重建，但复杂森林冠层中的立体视差图噪声过大。本文提出了一个渐进式流程，整合了DEFOM-Stereo基础模型视差估计、SAM3实例分割和多阶段深度优化，以生成鲁棒的单分支点云。该流程通过形态学操作、颜色验证及一个五阶段的深度噪声过滤方案，系统性地解决了掩模边界污染、分割不准确性和深度噪声等问题。在辐射松立体图像上的评估显示，该流程将平均单分支深度标准差降低了82%，同时保持了边缘保真度，生成了适用于自主修剪工具定位的几何一致3D点云。"
      },
      {
        "paper_id": "2602.20532",
        "title": "Actor-Curator: Co-adaptive Curriculum Learning via Policy-Improvement Bandits for RL Post-Training",
        "abstract": "Post-training large foundation models with reinforcement learning typically relies on massive and heterogeneous datasets, making effective curriculum learning both critical and challenging. In this work, we propose ACTOR-CURATOR, a scalable and fully automated curriculum learning framework for reinforcement learning post-training of large language models (LLMs). ACTOR-CURATOR learns a neural curator that dynamically selects training problems from large problem banks by directly optimizing for expected policy performance improvement. We formulate problem selection as a non-stationary stochastic bandit problem, derive a principled loss function based on online stochastic mirror descent, and establish regret guarantees under partial feedback. Empirically, ACTOR-CURATOR consistently outperforms uniform sampling and strong curriculum baselines across a wide range of challenging reasoning benchmarks, demonstrating improved training stability and efficiency. Notably, it achieves relative gains of 28.6% on AIME2024 and 30.5% on ARC-1D over the strongest baseline and up to 80% speedup. These results suggest that ACTOR-CURATOR is a powerful and practical approach for scalable LLM post-training.",
        "authors_display": "Yisong Yue Team",
        "pdf_url": "http://arxiv.org/abs/2602.20532",
        "code_url": null,
        "date": "2026-02-24",
        "primary_category": "cs.LG",
        "chinese_summary": "大型基础模型在强化学习后训练中依赖大规模异构数据集，使有效的课程学习成为挑战。本文提出了ACTOR-CURATOR，一个可扩展且全自动的强化学习课程学习框架，用于大型语言模型（LLMs）的后训练。该框架学习一个神经策展器，通过优化预期策略性能改进，将问题选择公式化为非平稳随机多臂老虎机问题，从而动态选择训练问题。经验结果表明，ACTOR-CURATOR在多个推理基准上显著优于现有基线，在AIME2024和ARC-1D上分别取得了28.6%和30.5%的相对增益，并实现了高达80%的速度提升，证明了其在LLM可扩展后训练中的有效性。"
      },
      {
        "paper_id": "2602.20501",
        "title": "Probing and Bridging Geometry-Interaction Cues for Affordance Reasoning in Vision Foundation Models",
        "abstract": "What does it mean for a visual system to truly understand affordance? We argue that this understanding hinges on two complementary capacities: geometric perception, which identifies the structural parts of objects that enable interaction, and interaction perception, which models how an agent's actions engage with those parts. To test this hypothesis, we conduct a systematic probing of Visual Foundation Models (VFMs). We find that models like DINO inherently encode part-level geometric structures, while generative models like Flux contain rich, verb-conditioned spatial attention maps that serve as implicit interaction priors. Crucially, we demonstrate that these two dimensions are not merely correlated but are composable elements of affordance. By simply fusing DINO's geometric prototypes with Flux's interaction maps in a training-free and zero-shot manner, we achieve affordance estimation competitive with weakly-supervised methods. This final fusion experiment confirms that geometric and interaction perception are the fundamental building blocks of affordance understanding in VFMs, providing a mechanistic account of how perception grounds action.",
        "authors_display": "Jing Zhang Team",
        "pdf_url": "http://arxiv.org/abs/2602.20501",
        "code_url": null,
        "date": "2026-02-24",
        "primary_category": "cs.CV",
        "chinese_summary": "视觉系统对“可供性”的真正理解依赖于几何感知和交互感知两种互补能力。本文通过系统探测视觉基础模型（VFMs），发现DINO模型本身编码了部分级别的几何结构，而Flux等生成模型包含丰富的、动词条件的（verb-conditioned）空间注意力图作为隐式交互先验。研究证明，通过免训练、零样本地融合DINO的几何原型和Flux的交互图，即可实现与弱监督方法相当的可供性估计。这些结果证实了几何感知和交互感知是VFM中理解可供性的基本组成部分，为感知如何基础行动提供了机制解释。"
      },
      {
        "paper_id": "2602.20471",
        "title": "SegSEM: Enabling and Enhancing SAM2 for SEM Contour Extraction",
        "abstract": "Extracting high-fidelity 2D contours from Scanning Electron Microscope (SEM) images is critical for calibrating Optical Proximity Correction (OPC) models. While foundation models like Segment Anything 2 (SAM2) are promising, adapting them to specialized domains with scarce annotated data is a major challenge. This paper presents a case study on adapting SAM2 for SEM contour extraction in a few-shot setting. We propose SegSEM, a framework built on two principles: a data-efficient fine-tuning strategy that adapts by selectively training only the model's encoders, and a robust hybrid architecture integrating a traditional algorithm as a confidence-aware fallback. Using a small dataset of 60 production images, our experiments demonstrate this methodology's viability. The primary contribution is a methodology for leveraging foundation models in data-constrained industrial applications.",
        "authors_display": "Mingxuan Yuan Team",
        "pdf_url": "http://arxiv.org/abs/2602.20471",
        "code_url": null,
        "date": "2026-02-24",
        "primary_category": "cs.AR",
        "chinese_summary": "从扫描电子显微镜（SEM）图像中高保真提取2D轮廓对校准光学邻近效应修正（OPC）模型至关重要，但在数据稀缺的专业领域适应基础模型如SAM2面临挑战。本文提出了SegSEM框架，旨在少样本设置下将SAM2适应于SEM轮廓提取。该框架采用数据高效的微调策略，仅训练模型编码器，并整合传统算法作为置信度感知的备用方案，构成一个鲁棒的混合架构。在包含60张生产图像的小型数据集上的实验验证了该方法的可行性，为数据受限的工业应用中利用基础模型提供了一种有效方法。"
      },
      {
        "paper_id": "2602.20057",
        "title": "AdaWorldPolicy: World-Model-Driven Diffusion Policy with Online Adaptive Learning for Robotic Manipulation",
        "abstract": "Effective robotic manipulation requires policies that can anticipate physical outcomes and adapt to real-world environments. Effective robotic manipulation requires policies that can anticipate physical outcomes and adapt to real-world environments. In this work, we introduce a unified framework, World-Model-Driven Diffusion Policy with Online Adaptive Learning (AdaWorldPolicy) to enhance robotic manipulation under dynamic conditions with minimal human involvement. Our core insight is that world models provide strong supervision signals, enabling online adaptive learning in dynamic environments, which can be complemented by force-torque feedback to mitigate dynamic force shifts. Our AdaWorldPolicy integrates a world model, an action expert, and a force predictor-all implemented as interconnected Flow Matching Diffusion Transformers (DiT). They are interconnected via the multi-modal self-attention layers, enabling deep feature exchange for joint learning while preserving their distinct modularity characteristics. We further propose a novel Online Adaptive Learning (AdaOL) strategy that dynamically switches between an Action Generation mode and a Future Imagination mode to drive reactive updates across all three modules. This creates a powerful closed-loop mechanism that adapts to both visual and physical domain shifts with minimal overhead. Across a suite of simulated and real-robot benchmarks, our AdaWorldPolicy achieves state-of-the-art performance, with dynamical adaptive capacity to out-of-distribution scenarios.",
        "authors_display": "Dong Xu Team",
        "pdf_url": "http://arxiv.org/abs/2602.20057",
        "code_url": "https://AdaWorldPolicy.github.io",
        "date": "2026-02-23",
        "primary_category": "cs.RO",
        "chinese_summary": "有效的机器人操作需要策略能够预测物理结果并适应动态环境。本文提出了AdaWorldPolicy框架，一个由世界模型驱动的扩散策略，结合在线自适应学习，以最少的人工干预增强动态条件下的机器人操作。其核心思想是世界模型提供强监督信号，支持在线自适应学习，并通过力矩反馈缓解动态力偏移。AdaWorldPolicy将世界模型、动作专家和力预测器整合为相互连接的流匹配扩散Transformer，并提出在线自适应学习策略，动态切换“动作生成”和“未来想象”模式以驱动模块更新。在模拟和真实机器人基准测试中，AdaWorldPolicy实现了最先进的性能，展现了对分布外场景的动态自适应能力。"
      },
      {
        "paper_id": "2602.19881",
        "title": "Make Some Noise: Unsupervised Remote Sensing Change Detection Using Latent Space Perturbations",
        "abstract": "Unsupervised change detection (UCD) in remote sensing aims to localise semantic changes between two images of the same region without relying on labelled data during training. Most recent approaches rely either on frozen foundation models in a training-free manner or on training with synthetic changes generated in pixel space. Both strategies inherently rely on predefined assumptions about change types, typically introduced through handcrafted rules, external datasets, or auxiliary generative models. Due to these assumptions, such methods fail to generalise beyond a few change types, limiting their real-world usage, especially in rare or complex scenarios. To address this, we propose MaSoN (Make Some Noise), an end-to-end UCD framework that synthesises diverse changes directly in the latent feature space during training. It generates changes that are dynamically estimated using feature statistics of target data, enabling diverse yet data-driven variation aligned with the target domain. It also easily extends to new modalities, such as SAR. MaSoN generalises strongly across diverse change types and achieves state-of-the-art performance on five benchmarks, improving the average F1 score by 14.1 percentage points. Project page: https://blaz-r.github.io/mason_ucd",
        "authors_display": "Luka Čehovin Zajc Team",
        "pdf_url": "http://arxiv.org/abs/2602.19881",
        "code_url": null,
        "date": "2026-02-23",
        "primary_category": "cs.CV",
        "chinese_summary": "遥感中的无监督变化检测（UCD）在不依赖标注数据的情况下定位语义变化，但现有方法因依赖预设变化类型假设而泛化能力受限。本文提出了MaSoN（Make Some Noise），一个端到端的UCD框架，在训练期间直接在潜在特征空间中合成多样化变化。MaSoN根据目标数据的特征统计动态估计并生成变化，确保了多样性并与目标域对齐，且易于扩展到新模态如SAR。在五个基准测试中，MaSoN取得了最先进的性能，平均F1分数提高了14.1个百分点，展现了强大的泛化能力。"
      },
      {
        "paper_id": "2602.19843",
        "title": "MAS-FIRE: Fault Injection and Reliability Evaluation for LLM-Based Multi-Agent Systems",
        "abstract": "As LLM-based Multi-Agent Systems (MAS) are increasingly deployed for complex tasks, ensuring their reliability has become a pressing challenge. Since MAS coordinate through unstructured natural language rather than rigid protocols, they are prone to semantic failures (e.g., hallucinations, misinterpreted instructions, and reasoning drift) that propagate silently without raising runtime exceptions. Prevailing evaluation approaches, which measure only end-to-end task success, offer limited insight into how these failures arise or how effectively agents recover from them. To bridge this gap, we propose MAS-FIRE, a systematic framework for fault injection and reliability evaluation of MAS. We define a taxonomy of 15 fault types covering intra-agent cognitive errors and inter-agent coordination failures, and inject them via three non-invasive mechanisms: prompt modification, response rewriting, and message routing manipulation. Applying MAS-FIRE to three representative MAS architectures, we uncover a rich set of fault-tolerant behaviors that we organize into four tiers: mechanism, rule, prompt, and reasoning. This tiered view enables fine-grained diagnosis of where and why systems succeed or fail. Our findings reveal that stronger foundation models do not uniformly improve robustness. We further show that architectural topology plays an equally decisive role, with iterative, closed-loop designs neutralizing over 40% of faults that cause catastrophic collapse in linear workflows. MAS-FIRE provides the process-level observability and actionable guidance needed to systematically improve multi-agent systems.",
        "authors_display": "Zibin Zheng Team",
        "pdf_url": "http://arxiv.org/abs/2602.19843",
        "code_url": null,
        "date": "2026-02-23",
        "primary_category": "cs.SE",
        "chinese_summary": "基于LLM的多智能体系统（MAS）的可靠性保障是一个挑战，因其通过非结构化自然语言协调易发生语义故障且难以察觉。现有端到端任务成功评估缺乏对故障原因和恢复能力的深入洞察。为解决此问题，本文提出了MAS-FIRE，一个系统性故障注入和可靠性评估框架，定义了包含15种故障类型的分类法，涵盖智能体内认知错误和智能体间协调失败，并通过三种非侵入性机制注入故障。将MAS-FIRE应用于三种代表性MAS架构，揭示了四层故障容忍行为，并发现更强大的基础模型并不总是提高鲁棒性，而迭代、闭环架构能有效中和导致系统崩溃的故障。MAS-FIRE提供了过程级可观察性和可操作性指导，以系统改进MAS。"
      },
      {
        "paper_id": "2602.19823",
        "title": "Open-vocabulary 3D scene perception in industrial environments",
        "abstract": "Autonomous vision applications in production, intralogistics, or manufacturing environments require perception capabilities beyond a small, fixed set of classes. Recent open-vocabulary methods, leveraging 2D Vision-Language Foundation Models (VLFMs), target this task but often rely on class-agnostic segmentation models pre-trained on non-industrial datasets (e.g., household scenes). In this work, we first demonstrate that such models fail to generalize, performing poorly on common industrial objects. Therefore, we propose a training-free, open-vocabulary 3D perception pipeline that overcomes this limitation. Instead of using a pre-trained model to generate instance proposals, our method simply generates masks by merging pre-computed superpoints based on their semantic features. Following, we evaluate the domain-adapted VLFM \"IndustrialCLIP\" on a representative 3D industrial workshop scene for open-vocabulary querying. Our qualitative results demonstrate successful segmentation of industrial objects.",
        "authors_display": "Thorsten Schüppstuhl Team",
        "pdf_url": "http://arxiv.org/abs/2602.19823",
        "code_url": null,
        "date": "2026-02-23",
        "primary_category": "cs.CV",
        "chinese_summary": "生产、内部物流或制造环境中的自主视觉应用需要超越固定类别集之外的开放词汇感知能力。然而，现有开放词汇方法利用2D视觉-语言基础模型 (VLFMs)，但常依赖在非工业数据集上预训练的与类别无关的分割模型，导致在常见工业对象上性能不佳。为此，本研究提出了一种免训练的开放词汇3D感知管道，以克服这一局限。该方法不使用预训练模型生成实例提议，而是通过根据语义特征合并预先计算的超点来直接生成掩码。随后，将领域自适应的VLFM \"IndustrialCLIP\" 应用于代表性的3D工业车间场景进行开放词汇查询。定性结果表明，该管道能够成功分割工业对象，解决了现有方法在工业场景中的泛化性问题。"
      },
      {
        "paper_id": "2602.19810",
        "title": "OpenClaw, Moltbook, and ClawdLab: From Agent-Only Social Networks to Autonomous Scientific Research",
        "abstract": "In January 2026, the open-source agent framework OpenClaw and the agent-only social network Moltbook produced a large-scale dataset of autonomous AI-to-AI interaction, attracting six academic publications within fourteen days. This study conducts a multivocal literature review of that ecosystem and presents ClawdLab, an open-source platform for autonomous scientific research, as a design science response to the architectural failure modes identified. The literature documents emergent collective phenomena, security vulnerabilities spanning 131 agent skills and over 15,200 exposed control panels, and five recurring architectural patterns. ClawdLab addresses these failure modes through hard role restrictions, structured adversarial critique, PI-led governance, multi-model orchestration, and domain-specific evidence requirements encoded as protocol constraints that ground validation in computational tool outputs rather than social consensus; the architecture provides emergent Sybil resistance as a structural consequence. A three-tier taxonomy distinguishes single-agent pipelines, predetermined multi-agent workflows, and fully decentralised systems, analysing why leading AI co-scientist platforms remain confined to the first two tiers. ClawdLab's composable third-tier architecture, in which foundation models, capabilities, governance, and evidence requirements are independently modifiable, enables compounding improvement as the broader AI ecosystem advances.",
        "authors_display": "Aakaash Meduri Team",
        "pdf_url": "http://arxiv.org/abs/2602.19810",
        "code_url": null,
        "date": "2026-02-23",
        "primary_category": "cs.AI",
        "chinese_summary": "针对自主AI-to-AI交互生态系统中出现的集体现象、安全漏洞和反复出现的架构故障模式，本研究进行了一项多声部文献综述。背景是开源代理框架OpenClaw和社交网络Moltbook于2026年1月生成的大规模自主AI交互数据集引发了广泛关注。为响应这些架构故障模式，本研究提出了ClawdLab，一个用于自主科学研究的开源平台。ClawdLab通过硬性角色限制、结构化对抗性批评、PI主导的治理、多模型编排以及将领域特定证据要求编码为协议约束等方法来解决识别出的故障模式，将验证过程基于计算工具输出而非社会共识。该架构通过结构性设计实现了新兴的女巫攻击抵抗力，并提出了三层分类法来区分单智能体管道、预定多智能体工作流和完全去中心化系统。研究指出，ClawdLab的可组合三层架构，其基础模型、能力、治理和证据要求均可独立修改，能够随着更广泛AI生态系统的进步而实现复合改进。"
      },
      {
        "paper_id": "2602.19634",
        "title": "Compositional Planning with Jumpy World Models",
        "abstract": "The ability to plan with temporal abstractions is central to intelligent decision-making. Rather than reasoning over primitive actions, we study agents that compose pre-trained policies as temporally extended actions, enabling solutions to complex tasks that no constituent alone can solve. Such compositional planning remains elusive as compounding errors in long-horizon predictions make it challenging to estimate the visitation distribution induced by sequencing policies. Motivated by the geometric policy composition framework introduced in arXiv:2206.08736, we address these challenges by learning predictive models of multi-step dynamics -- so-called jumpy world models -- that capture state occupancies induced by pre-trained policies across multiple timescales in an off-policy manner. Building on Temporal Difference Flows (arXiv:2503.09817), we enhance these models with a novel consistency objective that aligns predictions across timescales, improving long-horizon predictive accuracy. We further demonstrate how to combine these generative predictions to estimate the value of executing arbitrary sequences of policies over varying timescales. Empirically, we find that compositional planning with jumpy world models significantly improves zero-shot performance across a wide range of base policies on challenging manipulation and navigation tasks, yielding, on average, a 200% relative improvement over planning with primitive actions on long-horizon tasks.",
        "authors_display": "Ahmed Touati Team",
        "pdf_url": "http://arxiv.org/abs/2602.19634",
        "code_url": null,
        "date": "2026-02-23",
        "primary_category": "cs.LG",
        "chinese_summary": "智能决策的核心在于利用时间抽象进行规划，即通过组合预训练策略来解决复杂任务。然而，由于长远预测中的复合误差，估计策略序列诱导的访问分布具有挑战性，使得这种组合规划难以实现。受几何策略组合框架启发，本研究提出了通过学习多步动态的预测模型，即“跳跃式世界模型”，来应对这些挑战。该模型以离线方式捕获预训练策略在多个时间尺度上诱导的状态占据。在Temporal Difference Flows的基础上，本研究通过新颖的一致性目标增强这些模型，对齐跨时间尺度的预测，从而提高长远预测精度。实验结果表明，使用跳跃式世界模型的组合规划在各种基本策略下的零样本性能显著提高，在挑战性的操作和导航任务上，对于长时任务，相对于原始动作规划平均提高了200%。"
      },
      {
        "paper_id": "2602.19615",
        "title": "Seeing Clearly, Reasoning Confidently: Plug-and-Play Remedies for Vision Language Model Blindness",
        "abstract": "Vision language models (VLMs) have achieved remarkable success in broad visual understanding, yet they remain challenged by object-centric reasoning on rare objects due to the scarcity of such instances in pretraining data. While prior efforts alleviate this issue by retrieving additional data or introducing stronger vision encoders, these methods are still computationally intensive during finetuning VLMs and don't fully exploit the original training data. In this paper, we introduce an efficient plug-and-play module that substantially improves VLMs' reasoning over rare objects by refining visual tokens and enriching input text prompts, without VLMs finetuning. Specifically, we propose to learn multi-modal class embeddings for rare objects by leveraging prior knowledge from vision foundation models and synonym-augmented text descriptions, compensating for limited training examples. These embeddings refine the visual tokens in VLMs through a lightweight attention-based enhancement module that improves fine-grained object details. In addition, we use the learned embeddings as object-aware detectors to generate informative hints, which are injected into the text prompts to help guide the VLM's attention toward relevant image regions. Experiments on two benchmarks show consistent and substantial gains for pretrained VLMs in rare object recognition and reasoning. Further analysis reveals how our method strengthens the VLM's ability to focus on and reason about rare objects.",
        "authors_display": "Zhengming Ding Team",
        "pdf_url": "http://arxiv.org/abs/2602.19615",
        "code_url": null,
        "date": "2026-02-23",
        "primary_category": "cs.CV",
        "chinese_summary": "视觉语言模型 (VLMs) 在广泛的视觉理解方面取得了显著成功，但在稀有对象的以对象为中心的推理方面仍面临挑战，原因在于预训练数据中此类实例稀缺。现有方法通过检索额外数据或引入更强的视觉编码器来缓解此问题，但这些方法在微调VLMs时计算密集，且未能充分利用原始训练数据。为此，本研究提出了一种高效的即插即用模块，通过精炼视觉token和丰富输入文本提示来显著改善VLMs对稀有对象的推理，而无需微调VLMs。具体而言，该方法通过利用视觉基础模型的先验知识和同义词增强的文本描述，学习稀有对象的多模态类别嵌入，以弥补训练示例的局限。这些嵌入通过轻量级基于注意力的增强模块精炼VLMs中的视觉token，改善细粒度对象细节。此外，学习到的嵌入还被用作对象感知检测器，生成信息提示注入文本提示中，以引导VLM关注相关图像区域。实验结果表明，该方法在稀有对象识别和推理方面为预训练的VLMs带来了持续且显著的提升。"
      },
      {
        "paper_id": "2602.19608",
        "title": "Satellite-Based Detection of Looted Archaeological Sites Using Machine Learning",
        "abstract": "Looting at archaeological sites poses a severe risk to cultural heritage, yet monitoring thousands of remote locations remains operationally difficult. We present a scalable and satellite-based pipeline to detect looted archaeological sites, using PlanetScope monthly mosaics (4.7m/pixel) and a curated dataset of 1,943 archaeological sites in Afghanistan (898 looted, 1,045 preserved) with multi-year imagery (2016--2023) and site-footprint masks. We compare (i) end-to-end CNN classifiers trained on raw RGB patches and (ii) traditional machine learning (ML) trained on handcrafted spectral/texture features and embeddings from recent remote-sensing foundation models. Results indicate that ImageNet-pretrained CNNs combined with spatial masking reach an F1 score of 0.926, clearly surpassing the strongest traditional ML setup, which attains an F1 score of 0.710 using SatCLIP-V+RF+Mean, i.e., location and vision embeddings fed into a Random Forest with mean-based temporal aggregation. Ablation studies demonstrate that ImageNet pretraining (even in the presence of domain shift) and spatial masking enhance performance. In contrast, geospatial foundation model embeddings perform competitively with handcrafted features, suggesting that looting signatures are extremely localized. The repository is available at https://github.com/microsoft/looted_site_detection.",
        "authors_display": "Juan Lavista Ferres Team",
        "pdf_url": "http://arxiv.org/abs/2602.19608",
        "code_url": null,
        "date": "2026-02-23",
        "primary_category": "cs.CV",
        "chinese_summary": "考古遗址盗掘对文化遗产构成严重威胁，但监测成千上万个偏远地点在操作上仍然困难。为解决这一问题，本研究提出了一个可扩展的基于卫星的考古遗址盗掘检测管道。该方法利用PlanetScope月度影像和阿富汗1943个考古遗址的整理数据集，比较了两种主要方法：(i) 在原始RGB补丁上训练的端到端CNN分类器，以及 (ii) 在手工制作的光谱/纹理特征和最新遥感基础模型嵌入上训练的传统机器学习 (ML) 方法。实验结果表明，ImageNet预训练的CNN结合空间掩模达到了0.926的F1分数，明显优于最强的传统ML设置（F1分数为0.710）。消融研究进一步证实，ImageNet预训练（即使存在领域偏移）和空间掩模均能显著提高性能，而地理空间基础模型嵌入与手工特征表现相当，暗示盗掘特征具有极强的局部性。"
      },
      {
        "paper_id": "2602.19571",
        "title": "HOCA-Bench: Beyond Semantic Perception to Predictive World Modeling via Hegelian Ontological-Causal Anomalies",
        "abstract": "Video-LLMs have improved steadily on semantic perception, but they still fall short on predictive world modeling, which is central to physically grounded intelligence. We introduce HOCA-Bench, a benchmark that frames physical anomalies through a Hegelian lens. HOCA-Bench separates anomalies into two types: ontological anomalies, where an entity violates its own definition or persistence, and causal anomalies, where interactions violate physical relations. Using state-of-the-art generative video models as adversarial simulators, we build a testbed of 1,439 videos (3,470 QA pairs). Evaluations on 17 Video-LLMs show a clear cognitive lag: models often identify static ontological violations (e.g., shape mutations) but struggle with causal mechanisms (e.g., gravity or friction), with performance dropping by more than 20% on causal tasks. System-2 \"Thinking\" modes improve reasoning, but they do not close the gap, suggesting that current architectures recognize visual patterns more readily than they apply basic physical laws.",
        "authors_display": "Zhiping Cai Team",
        "pdf_url": "http://arxiv.org/abs/2602.19571",
        "code_url": null,
        "date": "2026-02-23",
        "primary_category": "cs.CV",
        "chinese_summary": "视频大语言模型 (Video-LLMs) 在语义感知方面持续改进，但在预测性世界建模方面仍显不足，而这对于基于物理的智能至关重要。为评估这一能力，本研究引入了HOCA-Bench基准测试，该基准从黑格尔视角将物理异常分为本体论异常（实体违反自身定义或持久性）和因果异常（交互违反物理关系）。研究人员使用最先进的生成式视频模型作为对抗性模拟器，构建了一个包含1,439个视频和3,470对问答的测试平台。对17个Video-LLMs的评估结果显示出明显的认知滞后：模型通常能识别静态的本体论违规（如形状突变），但在处理因果机制（如重力或摩擦力）时却表现困难，因果任务的性能下降超过20%。虽然系统2“思维”模式能改善推理，但未能弥合这一差距，表明当前架构识别视觉模式比应用基本物理定律更容易。"
      },
      {
        "paper_id": "2602.19547",
        "title": "CIBER: A Comprehensive Benchmark for Security Evaluation of Code Interpreter Agents",
        "abstract": "LLM-based code interpreter agents are increasingly deployed in critical workflows, yet their robustness against risks introduced by their code execution capabilities remains underexplored. Existing benchmarks are limited to static datasets or simulated environments, failing to capture the security risks arising from dynamic code execution, tool interactions, and multi-turn context. To bridge this gap, we introduce CIBER, an automated benchmark that combines dynamic attack generation, isolated secure sandboxing, and state-aware evaluation to systematically assess the vulnerability of code interpreter agents against four major types of adversarial attacks: Direct/Indirect Prompt Injection, Memory Poisoning, and Prompt-based Backdoor.   We evaluate six foundation models across two representative code interpreter agents (OpenInterpreter and OpenCodeInterpreter), incorporating a controlled study of identical models. Our results reveal that Interpreter Architecture and Model Alignment Set the Security Baseline. Structural integration enables aligned specialized models to outperform generic SOTA models. Conversely, high intelligence paradoxically increases susceptibility to complex adversarial prompts due to stronger instruction adherence. Furthermore, we identify a \"Natural Language Disguise\" Phenomenon, where natural language functions as a significantly more effective input modality than explicit code snippets (+14.1% ASR), thereby bypassing syntax-based defenses. Finally, we expose an alarming Security Polarization, where agents exhibit robust defenses against explicit threats yet fail catastrophically against implicit semantic hazards, highlighting a fundamental blind spot in current pattern-matching protection approaches.",
        "authors_display": "Songze Li Team",
        "pdf_url": "http://arxiv.org/abs/2602.19547",
        "code_url": null,
        "date": "2026-02-23",
        "primary_category": "cs.CR",
        "chinese_summary": "LLM代码解释器代理在关键工作流中日益普及，但其代码执行能力引入的安全风险，尤其是在动态执行、工具交互和多轮上下文方面，尚未得到充分研究。为此，研究人员提出了CIBER，一个自动化的基准测试平台，通过动态攻击生成、隔离沙盒和状态感知评估，系统性地评估代理对抗直接/间接提示注入、内存投毒和基于提示的后门等四类主要对抗攻击的脆弱性。实验结果表明，解释器架构和模型对齐是安全基线，结构集成使专用模型优于通用SOTA模型；高智能反而因指令遵循性强而更易受复杂对抗提示影响；自然语言作为输入模式比显式代码更有效，能绕过语法防御；代理对显式威胁防御稳健，但对隐式语义危害却灾难性失败，揭示了当前模式匹配保护方法的盲点。"
      },
      {
        "paper_id": "2602.19503",
        "title": "A Text-Guided Vision Model for Enhanced Recognition of Small Instances",
        "abstract": "As drone-based object detection technology continues to evolve, the demand is shifting from merely detecting objects to enabling users to accurately identify specific targets. For example, users can input particular targets as prompts to precisely detect desired objects. To address this need, an efficient text-guided object detection model has been developed to enhance the detection of small objects. Specifically, an improved version of the existing YOLO-World model is introduced. The proposed method replaces the C2f layer in the YOLOv8 backbone with a C3k2 layer, enabling more precise representation of local features, particularly for small objects or those with clearly defined boundaries. Additionally, the proposed architecture improves processing speed and efficiency through parallel processing optimization, while also contributing to a more lightweight model design. Comparative experiments on the VisDrone dataset show that the proposed model outperforms the original YOLO-World model, with precision increasing from 40.6% to 41.6%, recall from 30.8% to 31%, F1 score from 35% to 35.5%, and mAP@0.5 from 30.4% to 30.7%, confirming its enhanced accuracy. Furthermore, the model demonstrates superior lightweight performance, with the parameter count reduced from 4 million to 3.8 million and FLOPs decreasing from 15.7 billion to 15.2 billion. These results indicate that the proposed approach provides a practical and effective solution for precise object detection in drone-based applications.",
        "authors_display": "Hyun-Ki Jung Team",
        "pdf_url": "http://arxiv.org/abs/2602.19503",
        "code_url": null,
        "date": "2026-02-23",
        "primary_category": "cs.CV",
        "chinese_summary": "随着无人机目标检测技术的发展，用户对精确识别特定目标的需求日益增长，尤其是在小目标检测方面。为满足此需求，本研究提出了一种改进的YOLO-World模型，旨在增强文本引导的小目标检测能力。具体方法是将YOLOv8骨干网络中的C2f层替换为C3k2层，以更精确地捕捉局部特征，并采用并行处理优化来提高速度和效率，同时实现轻量化设计。在VisDrone数据集上的比较实验表明，该模型在精度、召回率、F1分数和mAP@0.5等指标上均优于原始YOLO-World模型，且参数量和FLOPs有所减少，验证了其在准确性和轻量化方面的提升，为无人机精确目标检测提供了实用解决方案。"
      },
      {
        "paper_id": "2602.19471",
        "title": "Forgetting-Resistant and Lesion-Aware Source-Free Domain Adaptive Fundus Image Analysis with Vision-Language Model",
        "abstract": "Source-free domain adaptation (SFDA) aims to adapt a model trained in the source domain to perform well in the target domain, with only unlabeled target domain data and the source model. Taking into account that conventional SFDA methods are inevitably error-prone under domain shift, recently greater attention has been directed to SFDA assisted with off-the-shelf foundation models, e.g., vision-language (ViL) models. However, existing works of leveraging ViL models for SFDA confront two issues: (i) Although mutual information is exploited to consider the joint distribution between the predictions of ViL model and the target model, we argue that the forgetting of some superior predictions of the target model still occurs, as indicated by the decline of the accuracies of certain classes during adaptation; (ii) Prior research disregards the rich, fine-grained knowledge embedded in the ViL model, which offers detailed grounding for fundus image diagnosis. In this paper, we introduce a novel forgetting-resistant and lesion-aware (FRLA) method for SFDA of fundus image diagnosis with ViL model. Specifically, a forgetting-resistant adaptation module explicitly preserves the confident predictions of the target model, and a lesion-aware adaptation module yields patch-wise predictions from ViL model and employs them to help the target model be aware of the lesion areas and leverage the ViL model's fine-grained knowledge. Extensive experiments show that our method not only significantly outperforms the vision-language model, but also achieves consistent improvements over the state-of-the-art methods. Our code will be released.",
        "authors_display": "Xiaomeng Li Team",
        "pdf_url": "http://arxiv.org/abs/2602.19471",
        "code_url": null,
        "date": "2026-02-23",
        "primary_category": "cs.CV",
        "chinese_summary": "源域无关域适应（SFDA）旨在仅利用未标记目标域数据和源模型在目标域进行模型适应，但传统方法在域偏移下易出错。近期虽有研究尝试结合视觉-语言（ViL）基础模型，但存在目标模型优越预测遗忘和忽略ViL模型细粒度知识的问题。为此，本文提出了一种新型的抗遗忘和病灶感知（FRLA）方法，用于基于ViL模型的眼底图像诊断SFDA。该方法包含一个抗遗忘适应模块以保留模型置信预测，以及一个病灶感知适应模块利用ViL模型的像素级预测来引导目标模型感知病灶区域并利用其细粒度知识。大量实验证实，该方法不仅显著优于ViL模型，且超越了最先进的SFDA方法。"
      },
      {
        "paper_id": "2602.19411",
        "title": "MACE-POLAR-1: A Polarisable Electrostatic Foundation Model for Molecular Chemistry",
        "abstract": "Accurate modelling of electrostatic interactions and charge transfer is fundamental to computational chemistry, yet most machine learning interatomic potentials (MLIPs) rely on local atomic descriptors that cannot capture long-range electrostatic effects. We present a new electrostatic foundation model for molecular chemistry that extends the MACE architecture with explicit treatment of long-range interactions and electrostatic induction. Our approach combines local many-body geometric features with a non-self-consistent field formalism that updates learnable charge and spin densities through polarisable iterations to model induction, followed by global charge equilibration via learnable Fukui functions to control total charge and total spin. This design enables an accurate and physical description of systems with varying charge and spin states while maintaining computational efficiency. Trained on the OMol25 dataset of 100 million hybrid DFT calculations, our models achieve chemical accuracy across diverse benchmarks, with accuracy competitive with hybrid DFT on thermochemistry, reaction barriers, conformational energies, and transition metal complexes. Notably, we demonstrate that the inclusion of long-range electrostatics leads to a large improvement in the description of non-covalent interactions and supramolecular complexes over non-electrostatic models, including sub-kcal/mol prediction of molecular crystal formation energy in the X23-DMC dataset and a fourfold improvement over short-ranged models on protein-ligand interactions. The model's ability to handle variable charge and spin states, respond to external fields, provide interpretable spin-resolved charge densities, and maintain accuracy from small molecules to protein-ligand complexes positions it as a versatile tool for computational molecular chemistry and drug discovery.",
        "authors_display": "Gábor Csányi Team",
        "pdf_url": "http://arxiv.org/abs/2602.19411",
        "code_url": null,
        "date": "2026-02-23",
        "primary_category": "physics.chem-ph",
        "chinese_summary": "计算化学中，静电相互作用和电荷转移的精确建模至关重要，但大多数机器学习原子间势（MLIPs）难以捕捉长程静电效应。本研究提出一种新的分子化学静电基础模型，通过明确处理长程相互作用和静电感应，扩展了MACE架构。该方法结合局部多体几何特征与非自洽场形式，通过可极化迭代更新可学习的电荷和自旋密度，并通过可学习的Fukui函数进行全局电荷平衡。模型在OMol25数据集上训练，实现了化学精度，并在热化学、反应势垒、构象能量和过渡金属络合物方面与混合DFT相当。引入长程静电作用显著改善了非共价相互作用和超分子络合物的描述，在分子晶体形成能和蛋白质-配体相互作用上表现出色。该模型因其处理可变电荷和自旋状态、响应外部场及提供可解释电荷密度的能力，成为计算分子化学和药物发现的通用工具。"
      },
      {
        "paper_id": "2602.19400",
        "title": "Hilbert-Augmented Reinforcement Learning for Scalable Multi-Robot Coverage and Exploration",
        "abstract": "We present a coverage framework that integrates Hilbert space-filling priors into decentralized multi-robot learning and execution. We augment DQN and PPO with Hilbert-based spatial indices to structure exploration and reduce redundancy in sparse-reward environments, and we evaluate scalability in multi-robot grid coverage. We further describe a waypoint interface that converts Hilbert orderings into curvature-bounded, time-parameterized SE(2) trajectories (planar (x, y, θ)), enabling onboard feasibility on resource-constrained robots. Experiments show improvements in coverage efficiency, redundancy, and convergence speed over DQN/PPO baselines. In addition, we validate the approach on a Boston Dynamics Spot legged robot, executing the generated trajectories in indoor environments and observing reliable coverage with low redundancy. These results indicate that geometric priors improve autonomy and scalability for swarm and legged robotics.",
        "authors_display": "Aryya Gangopadhyay Team",
        "pdf_url": "http://arxiv.org/abs/2602.19400",
        "code_url": null,
        "date": "2026-02-23",
        "primary_category": "cs.RO",
        "chinese_summary": "在分散式多机器人系统中，稀疏奖励环境下的覆盖探索效率和冗余是一个重要挑战。本研究提出一个覆盖框架，将Hilbert空间填充先验整合到分散式多机器人学习和执行中。该方法通过Hilbert-based空间索引增强DQN和PPO算法，以结构化探索并减少冗余。此外，还设计了一个路径点接口，将Hilbert排序转换为曲率受限、时间参数化的SE(2)轨迹，以确保在资源受限机器人上的板载可行性。实验结果表明，该方法在覆盖效率、冗余和收敛速度上均优于DQN/PPO基线。在Boston Dynamics Spot机器人的实际验证中，生成的轨迹实现了可靠且低冗余的室内覆盖，证明了几何先验能够显著提升群体和腿式机器人的自主性和可扩展性。"
      },
      {
        "paper_id": "2602.20307",
        "title": "In-context Pre-trained Time-Series Foundation Models adapt to Unseen Tasks",
        "abstract": "Time-series foundation models (TSFMs) have demonstrated strong generalization capabilities across diverse datasets and tasks. However, existing foundation models are typically pre-trained to enhance performance on specific tasks and often struggle to generalize to unseen tasks without fine-tuning. To address this limitation, we propose augmenting TSFMs with In-Context Learning (ICL) capabilities, enabling them to perform test-time inference by dynamically adapting to input-output relationships provided within the context. Our framework, In-Context Time-series Pre-training (ICTP), restructures the original pre-training data to equip the backbone TSFM with ICL capabilities, enabling adaptation to unseen tasks. Experiments demonstrate that ICT improves the performance of state-of-the-art TSFMs by approximately 11.4% on unseen tasks without requiring fine-tuning.",
        "authors_display": "B. Aditya Prakash Team",
        "pdf_url": "http://arxiv.org/abs/2602.20307",
        "code_url": null,
        "date": "2026-02-23",
        "primary_category": "cs.LG",
        "chinese_summary": "时间序列基础模型（TSFMs）虽然泛化能力强，但在未经微调的情况下难以适应未见任务。为解决此问题，本文提出了情境时间序列预训练（ICTP）框架，通过情境学习（ICL）增强TSFMs的能力。ICTP重构原始预训练数据，使主干TSFM能够通过动态适应情境中提供的输入-输出关系进行测试时推理，从而适应未见任务。实验结果表明，ICTP在未见任务上将最先进的TSFMs性能提高了约11.4%，且无需进行微调，显著提升了模型的泛化能力。"
      },
      {
        "paper_id": "2602.19385",
        "title": "Adaptive Data Augmentation with Multi-armed Bandit: Sample-Efficient Embedding Calibration for Implicit Pattern Recognition",
        "abstract": "Recognizing implicit visual and textual patterns is essential in many real-world applications of modern AI. However, tackling long-tail pattern recognition tasks remains challenging for current pre-trained foundation models such as LLMs and VLMs. While finetuning pre-trained models can improve accuracy in recognizing implicit patterns, it is usually infeasible due to a lack of training data and high computational overhead. In this paper, we propose ADAMAB, an efficient embedding calibration framework for few-shot pattern recognition. To maximally reduce the computational costs, ADAMAB trains embedder-agnostic light-weight calibrators on top of fixed embedding models without accessing their parameters. To mitigate the need for large-scale training data, we introduce an adaptive data augmentation strategy based on the Multi-Armed Bandit (MAB) mechanism. With a modified upper confidence bound algorithm, ADAMAB diminishes the gradient shifting and offers theoretically guaranteed convergence in few-shot training. Our multi-modal experiments justify the superior performance of ADAMAB, with up to 40% accuracy improvement when training with less than 5 initial data samples of each class.",
        "authors_display": "Taha Belkhouja Yujia Bao Team",
        "pdf_url": "http://arxiv.org/abs/2602.19385",
        "code_url": null,
        "date": "2026-02-22",
        "primary_category": "cs.CV",
        "chinese_summary": "在现代AI应用中，识别隐式视觉和文本模式至关重要，但长尾模式识别任务对LLMs和VLMs等预训练基础模型仍具挑战。虽然微调可提高精度，但通常因训练数据稀缺和计算开销大而不可行。针对此问题，本文提出了ADAMAB，一个高效的嵌入校准框架，用于少样本模式识别。该框架在固定嵌入模型上训练嵌入器无关的轻量级校准器，无需访问模型参数以降低计算成本。同时，引入基于多臂老虎机（MAB）机制的自适应数据增强策略，通过改进的UPPER CONFIDENCE BOUND算法减少梯度偏移，并在少样本训练中提供理论上的收敛保证。多模态实验证明，当每个类别仅用少于5个初始数据样本进行训练时，ADAMAB可将准确率提升高达40%，显示出其优越性能。"
      },
      {
        "paper_id": "2602.19380",
        "title": "Detector-in-the-Loop Tracking: Active Memory Rectification for Stable Glottic Opening Localization",
        "abstract": "Temporal stability in glottic opening localization remains challenging due to the complementary weaknesses of single-frame detectors and foundation-model trackers: the former lacks temporal context, while the latter suffers from memory drift. Specifically, in video laryngoscopy, rapid tissue deformation, occlusions, and visual ambiguities in emergency settings require a robust, temporally aware solution that can prevent progressive tracking errors. We propose Closed-Loop Memory Correction (CL-MC), a detector-in-the-loop framework that supervises Segment Anything Model 2(SAM2) through confidence-aligned state decisions and active memory rectification. High-confidence detections trigger semantic resets that overwrite corrupted tracker memory, effectively mitigating drift accumulation with a training-free foundation tracker in complex endoscopic scenes. On emergency intubation videos, CL-MC achieves state-of-the-art performance, significantly reducing drift and missing rate compared with the SAM2 variants and open loop based methods. Our results establish memory correction as a crucial component for reliable clinical video tracking. Our code will be available in https://github.com/huayuww/CL-MR.",
        "authors_display": "Jenq-Neng Hwang Team",
        "pdf_url": "http://arxiv.org/abs/2602.19380",
        "code_url": null,
        "date": "2026-02-22",
        "primary_category": "cs.CV",
        "chinese_summary": "在视频喉镜检查中，由于单帧检测器缺乏时间上下文和基础模型跟踪器存在记忆漂移，声门开口定位的瞬时稳定性面临挑战，特别是在紧急情况下，快速组织变形、遮挡和视觉模糊要求鲁棒、时间感知的解决方案。为此，研究团队提出了闭环记忆校正（CL-MC）框架，这是一个检测器在环的框架，它通过置信度对齐的状态决策和主动记忆修正来监督Segment Anything Model 2 (SAM2)。高置信度检测能触发语义重置，覆盖损坏的跟踪器记忆，有效减轻复杂内窥镜场景中无训练基础跟踪器的漂移累积。在紧急插管视频上的实验表明，CL-MC实现了最先进的性能，与SAM2变体和开环方法相比，显著降低了漂移和漏检率，证实了记忆校正对可靠临床视频跟踪的重要性。"
      },
      {
        "paper_id": "2602.19359",
        "title": "Vid2Sid: Videos Can Help Close the Sim2Real Gap",
        "abstract": "Calibrating a robot simulator's physics parameters (friction, damping, material stiffness) to match real hardware is often done by hand or with black-box optimizers that reduce error but cannot explain which physical discrepancies drive the error. When sensing is limited to external cameras, the problem is further compounded by perception noise and the absence of direct force or state measurements. We present Vid2Sid, a video-driven system identification pipeline that couples foundation-model perception with a VLM-in-the-loop optimizer that analyzes paired sim-real videos, diagnoses concrete mismatches, and proposes physics parameter updates with natural language rationales. We evaluate our approach on a tendon-actuated finger (rigid-body dynamics in MuJoCo) and a deformable continuum tentacle (soft-body dynamics in PyElastica). On sim2real holdout controls unseen during training, Vid2Sid achieves the best average rank across all settings, matching or exceeding black-box optimizers while uniquely providing interpretable reasoning at each iteration. Sim2sim validation confirms that Vid2Sid recovers ground-truth parameters most accurately (mean relative error under 13\\% vs. 28--98\\%), and ablation analysis reveals three calibration regimes. VLM-guided optimization excels when perception is clean and the simulator is expressive, while model-class limitations bound performance in more challenging settings.",
        "authors_display": "Josie Hughes Team",
        "pdf_url": "http://arxiv.org/abs/2602.19359",
        "code_url": null,
        "date": "2026-02-22",
        "primary_category": "cs.RO",
        "chinese_summary": "校准机器人模拟器物理参数以匹配真实硬件通常依赖手动或黑盒优化器，这些方法虽能减少误差但无法解释物理差异来源，且在仅限于外部摄像头的感知下，问题因感知噪声和缺乏直接测量而更复杂。本研究提出Vid2Sid，一个视频驱动的系统识别流程，它将基础模型感知与循环中的VLM优化器相结合，通过分析模拟与真实视频，诊断具体失配，并提供带自然语言解释的物理参数更新建议。在腱驱动手指和可变形连续触手上的实验表明，Vid2Sid在未见过的sim2real控制中取得最佳平均排名，媲美或超越黑盒优化器，并独特地提供可解释推理。Sim2sim验证证实Vid2Sid最准确地恢复了真实参数，且消融分析揭示，VLM引导优化在感知清晰且模拟器表达力强时表现卓越，但在挑战性设置中受模型类别限制。"
      },
      {
        "paper_id": "2602.19322",
        "title": "US-JEPA: A Joint Embedding Predictive Architecture for Medical Ultrasound",
        "abstract": "Ultrasound (US) imaging poses unique challenges for representation learning due to its inherently noisy acquisition process. The low signal-to-noise ratio and stochastic speckle patterns hinder standard self-supervised learning methods relying on a pixel-level reconstruction objective. Joint-Embedding Predictive Architectures (JEPAs) address this drawback by predicting masked latent representations rather than raw pixels. However, standard approaches depend on hyperparameter-brittle and computationally expensive online teachers updated via exponential moving average. We propose US-JEPA, a self-supervised framework that adopts the Static-teacher Asymmetric Latent Training (SALT) objective. By using a frozen, domain-specific teacher to provide stable latent targets, US-JEPA decouples student-teacher optimization and pushes the student to expand upon the semantic priors of the teacher. In addition, we provide the first rigorous comparison of all publicly available state-of-the-art ultrasound foundation models on UltraBench, a public dataset benchmark spanning multiple organs and pathological conditions. Under linear probing for diverse classification tasks, US-JEPA achieves performance competitive with or superior to domain-specific and universal vision foundation model baselines. Our results demonstrate that masked latent prediction provides a stable and efficient path toward robust ultrasound representations.",
        "authors_display": "William Speier Team",
        "pdf_url": "http://arxiv.org/abs/2602.19322",
        "code_url": null,
        "date": "2026-02-22",
        "primary_category": "cs.CV",
        "chinese_summary": "超声（US）成像因其固有的噪声采集过程，对表征学习构成独特挑战，低信噪比和随机散斑模式阻碍了依赖像素级重建的标准自监督学习方法。联合嵌入预测架构（JEPAs）通过预测掩码潜表示而非原始像素来解决此问题，但标准方法依赖于超参数敏感且计算成本高的在线教师模型。本研究提出US-JEPA，一个自监督框架，采用静态教师非对称潜训练（SALT）目标，通过使用冻结的、领域特定教师模型提供稳定的潜在目标，解耦学生-教师优化并促使学生扩展教师的语义先验。此外，本研究首次在UltraBench上严格比较了所有公开的最先进超声基础模型。在各种分类任务的线性探测下，US-JEPA实现了与领域特定和通用视觉基础模型基线相当或更优的性能，表明掩码潜在预测为稳健超声表征提供了一条稳定高效的途径。"
      },
      {
        "paper_id": "2602.19308",
        "title": "WildOS: Open-Vocabulary Object Search in the Wild",
        "abstract": "Autonomous navigation in complex, unstructured outdoor environments requires robots to operate over long ranges without prior maps and limited depth sensing. In such settings, relying solely on geometric frontiers for exploration is often insufficient. In such settings, the ability to reason semantically about where to go and what is safe to traverse is crucial for robust, efficient exploration. This work presents WildOS, a unified system for long-range, open-vocabulary object search that combines safe geometric exploration with semantic visual reasoning. WildOS builds a sparse navigation graph to maintain spatial memory, while utilizing a foundation-model-based vision module, ExploRFM, to score frontier nodes of the graph. ExploRFM simultaneously predicts traversability, visual frontiers, and object similarity in image space, enabling real-time, onboard semantic navigation tasks. The resulting vision-scored graph enables the robot to explore semantically meaningful directions while ensuring geometric safety. Furthermore, we introduce a particle-filter-based method for coarse localization of the open-vocabulary target query, that estimates candidate goal positions beyond the robot's immediate depth horizon, enabling effective planning toward distant goals. Extensive closed-loop field experiments across diverse off-road and urban terrains demonstrate that WildOS enables robust navigation, significantly outperforming purely geometric and purely vision-based baselines in both efficiency and autonomy. Our results highlight the potential of vision foundation models to drive open-world robotic behaviors that are both semantically informed and geometrically grounded. Project Page: https://leggedrobotics.github.io/wildos/",
        "authors_display": "Patrick Spieler Team",
        "pdf_url": "http://arxiv.org/abs/2602.19308",
        "code_url": null,
        "date": "2026-02-22",
        "primary_category": "cs.RO",
        "chinese_summary": "在复杂、非结构化的室外环境中进行自主导航时，机器人需要在无预设地图和有限深度感知下进行远距离操作。在这种情况下，仅依靠几何边界进行探索往往不足，语义推理对于确定安全可遍历区域和目的地至关重要。本文提出了WildOS，一个用于长距离、开放词汇对象搜索的统一系统，它结合了安全几何探索与语义视觉推理。WildOS通过构建稀疏导航图来维护空间记忆，并利用基于基础模型的视觉模块ExploRFM对图的边界节点进行评分，ExploRFM能实时预测图像空间中的可遍历性、视觉边界和对象相似性，从而实现实时的板载语义导航。此外，系统还引入基于粒子滤波器的方法，用于开放词汇目标查询的粗略定位，以估计超出机器人即时深度范围的候选目标位置，实现对远距离目标的有效规划。在多种越野和城市地形的闭环现场实验表明，WildOS在效率和自主性方面显著优于纯几何和纯视觉基线，突出了视觉基础模型在驱动语义信息和几何基础的开放世界机器人行为方面的潜力。"
      },
      {
        "paper_id": "2602.18422",
        "title": "Generated Reality: Human-centric World Simulation using Interactive Video Generation with Hand and Camera Control",
        "abstract": "Extended reality (XR) demands generative models that respond to users' tracked real-world motion, yet current video world models accept only coarse control signals such as text or keyboard input, limiting their utility for embodied interaction. We introduce a human-centric video world model that is conditioned on both tracked head pose and joint-level hand poses. For this purpose, we evaluate existing diffusion transformer conditioning strategies and propose an effective mechanism for 3D head and hand control, enabling dexterous hand--object interactions. We train a bidirectional video diffusion model teacher using this strategy and distill it into a causal, interactive system that generates egocentric virtual environments. We evaluate this generated reality system with human subjects and demonstrate improved task performance as well as a significantly higher level of perceived amount of control over the performed actions compared with relevant baselines.",
        "authors_display": "Gordon Wetzstein Team",
        "pdf_url": "http://arxiv.org/abs/2602.18422",
        "code_url": "https://codeysun.github.io/generated-reality",
        "date": "2026-02-20",
        "primary_category": "cs.CV",
        "chinese_summary": "现有扩展现实（XR）中的视频世界模型缺乏对用户精确实时动作的响应能力，限制了其在具身交互中的应用。为此，本研究提出了一个以人为中心的视频世界模型，该模型以跟踪到的头部姿态和关节级手部姿态为条件，并引入了一种有效的3D头手控制机制以实现灵巧的手物交互。通过训练和蒸馏双向视频扩散模型，最终形成一个可交互的因果系统，生成第一人称视角的虚拟环境。实验结果表明，该系统显著提升了任务执行性能，并提高了用户对其动作的感知控制水平，优于现有基线方法。"
      },
      {
        "paper_id": "2602.18308",
        "title": "JPmHC Dynamical Isometry via Orthogonal Hyper-Connections",
        "abstract": "Recent advances in deep learning, exemplified by Hyper-Connections (HC), have expanded the residual connection paradigm by introducing wider residual streams and diverse connectivity patterns. While these innovations yield significant performance gains, they compromise the identity mapping property of residual connections, leading to training instability, limited scalability, and increased memory overhead. To address these challenges, we propose JPmHC (Jacobian-spectrum Preserving manifold-constrained Hyper-Connections), a framework that replaces identity skips with a trainable linear mixer acting on n parallel streams while explicitly controlling gradient conditioning. By constraining the mixer M on operator-norm-bounded manifolds (e.g., bistochastic, Stiefel, Grassmann), JPmHC prevents gradient pathologies and enhances stability. JPmHC introduces three key contributions: (i) a free-probability analysis that predicts Jacobian spectra for structured skips, providing actionable design rules for mixer selection; (ii) memory-efficient implicit differentiation for fixed-point projections, reducing activation memory and synchronization overhead; and (iii) a Stiefel-constrained mixer via Cayley transforms, ensuring orthogonality without post-hoc normalization. Empirical evaluations on ARC-AGI demonstrate that JPmHC achieves faster convergence, higher accuracy, and lower computational cost compared to bistochastic baselines. As a flexible and scalable extension of HC, JPmHC advances spectrum-aware, stable, and efficient deep learning, offering insights into topological architecture design and foundational model evolution.",
        "authors_display": "Leo Brunswic Team",
        "pdf_url": "http://arxiv.org/abs/2602.18308",
        "code_url": null,
        "date": "2026-02-20",
        "primary_category": "cs.LG",
        "chinese_summary": "Hyper-Connections (HC) 虽提升了深度学习性能，但其牺牲了残差连接的恒等映射性质，导致训练不稳定和可扩展性问题。为解决此挑战，本文提出了JPmHC (Jacobian-spectrum Preserving manifold-constrained Hyper-Connections) 框架，用可训练的线性混合器替代恒等跳跃连接，并将其约束在算子范数有界流形上，以控制梯度条件、防止梯度病理并增强稳定性。该方法通过自由概率分析、内存高效的隐式微分和基于Cayley变换的Stiefel约束混合器实现。在ARC-AGI基准上的实验证明，JPmHC比现有双随机基线收敛更快，准确性更高，计算成本更低，为深度学习提供了频谱感知、稳定且高效的新途径。"
      },
      {
        "paper_id": "2602.18252",
        "title": "On the Adversarial Robustness of Discrete Image Tokenizers",
        "abstract": "Discrete image tokenizers encode visual inputs as sequences of tokens from a finite vocabulary and are gaining popularity in multimodal systems, including encoder-only, encoder-decoder, and decoder-only models. However, unlike CLIP encoders, their vulnerability to adversarial attacks has not been explored. Ours being the first work studying this topic, we first formulate attacks that aim to perturb the features extracted by discrete tokenizers, and thus change the extracted tokens. These attacks are computationally efficient, application-agnostic, and effective across classification, multimodal retrieval, and captioning tasks. Second, to defend against this vulnerability, inspired by recent work on robust CLIP encoders, we fine-tune popular tokenizers with unsupervised adversarial training, keeping all other components frozen. While unsupervised and task-agnostic, our approach significantly improves robustness to both unsupervised and end-to-end supervised attacks and generalizes well to unseen tasks and data. Unlike supervised adversarial training, our approach can leverage unlabeled images, making it more versatile. Overall, our work highlights the critical role of tokenizer robustness in downstream tasks and presents an important step in the development of safe multimodal foundation models.",
        "authors_display": "Francesco Croce Team",
        "pdf_url": "http://arxiv.org/abs/2602.18252",
        "code_url": null,
        "date": "2026-02-20",
        "primary_category": "cs.CV",
        "chinese_summary": "离散图像tokenizer在多模态系统中日益普及，但其对对抗性攻击的脆弱性尚未被深入研究。本研究首次探讨了这一问题，提出了一系列计算高效、与应用无关的攻击方法，这些攻击能有效扰动tokenizer提取的特征并改变生成的token，在分类、多模态检索和图像描述任务中均表现出效力。为增强防御，研究团队受鲁棒CLIP编码器启发，通过无监督对抗训练对现有tokenizer进行微调，且保持其他组件不变。实验结果显示，该无监督且与任务无关的方法显著提升了模型对无监督和端到端监督攻击的鲁棒性，并展现出良好的跨任务和跨数据泛化能力，其利用未标记图像的特性使其更具通用性。本研究强调了tokenizer鲁棒性在下游任务中的关键作用，并为开发安全的多模态基础模型迈出了重要一步。"
      },
      {
        "paper_id": "2602.18083",
        "title": "Comparative Assessment of Multimodal Earth Observation Data for Soil Moisture Estimation",
        "abstract": "Accurate soil moisture (SM) estimation is critical for precision agriculture, water resources management and climate monitoring. Yet, existing satellite SM products are too coarse (>1km) for farm-level applications. We present a high-resolution (10m) SM estimation framework for vegetated areas across Europe, combining Sentinel-1 SAR, Sentinel-2 optical imagery and ERA-5 reanalysis data through machine learning. Using 113 International Soil Moisture Network (ISMN) stations spanning diverse vegetated areas, we compare modality combinations with temporal parameterizations, using spatial cross-validation, to ensure geographic generalization. We also evaluate whether foundation model embeddings from IBM-NASA's Prithvi model improve upon traditional hand-crafted spectral features. Results demonstrate that hybrid temporal matching - Sentinel-2 current-day acquisitions with Sentinel-1 descending orbit - achieves R^2=0.514, with 10-day ERA5 lookback window improving performance to R^2=0.518. Foundation model (Prithvi) embeddings provide negligible improvement over hand-crafted features (R^2=0.515 vs. 0.514), indicating traditional feature engineering remains highly competitive for sparse-data regression tasks. Our findings suggest that domain-specific spectral indices combined with tree-based ensemble methods offer a practical and computationally efficient solution for operational pan-European field-scale soil moisture monitoring.",
        "authors_display": "Charalampos Kontoes Team",
        "pdf_url": "http://arxiv.org/abs/2602.18083",
        "code_url": null,
        "date": "2026-02-20",
        "primary_category": "cs.CV",
        "chinese_summary": "农场级高分辨率土壤湿度（SM）估算对于精准农业至关重要，但现有卫星产品分辨率不足。本研究提出了一个针对欧洲植被区的高分辨率（10米）SM估算框架，融合了Sentinel-1 SAR、Sentinel-2光学图像和ERA-5再分析数据，并运用机器学习方法。通过空间交叉验证，研究比较了不同模态组合与时间参数化的效果，并评估了IBM-NASA Prithvi模型的基础模型嵌入是否优于传统手工光谱特征。结果显示，Sentinel-2当日与Sentinel-1降轨数据结合的混合时间匹配方案取得了R^2=0.514，配合10天ERA5回溯窗口可提升至R^2=0.518。值得注意的是，Prithvi基础模型嵌入对性能的提升微乎其微，表明在稀疏数据回归任务中，领域特定的光谱指数结合基于树的集成方法仍是一种实用且计算高效的泛欧农场尺度SM监测方案。"
      },
      {
        "paper_id": "2602.18016",
        "title": "Towards LLM-centric Affective Visual Customization via Efficient and Precise Emotion Manipulating",
        "abstract": "Previous studies on visual customization primarily rely on the objective alignment between various control signals (e.g., language, layout and canny) and the edited images, which largely ignore the subjective emotional contents, and more importantly lack general-purpose foundation models for affective visual customization. With this in mind, this paper proposes an LLM-centric Affective Visual Customization (L-AVC) task, which focuses on generating images within modifying their subjective emotions via Multimodal LLM. Further, this paper contends that how to make the model efficiently align emotion conversion in semantics (named inter-emotion semantic conversion) and how to precisely retain emotion-agnostic contents (named exter-emotion semantic retaining) are rather important and challenging in this L-AVC task. To this end, this paper proposes an Efficient and Precise Emotion Manipulating approach for editing subjective emotions in images. Specifically, an Efficient Inter-emotion Converting (EIC) module is tailored to make the LLM efficiently align emotion conversion in semantics before and after editing, followed by a Precise Exter-emotion Retaining (PER) module to precisely retain the emotion-agnostic contents. Comprehensive experimental evaluations on our constructed L-AVC dataset demonstrate the great advantage of the proposed EPEM approach to the L-AVC task over several state-of-the-art baselines. This justifies the importance of emotion information for L-AVC and the effectiveness of EPEM in efficiently and precisely manipulating such information.",
        "authors_display": "Jiahong Lu Team",
        "pdf_url": "http://arxiv.org/abs/2602.18016",
        "code_url": null,
        "date": "2026-02-20",
        "primary_category": "cs.CV",
        "chinese_summary": "现有视觉定制研究多侧重客观内容对齐，忽视了图像的主观情感，并缺乏通用情感视觉定制的基础模型。本文提出了以LLM为中心的情感视觉定制（L-AVC）任务，旨在通过多模态LLM生成并修改图像的主观情感。针对情感语义转换的有效对齐和情感无关内容的精确保留这两个挑战，研究团队提出了一种高效精确情感操控（EPEM）方法，其中包含高效跨情感转换（EIC）模块以实现语义情感转换的对齐，以及精确情感无关内容保留（PER）模块以保留情感无关内容。在构建的L-AVC数据集上的全面实验证明，所提出的EPEM方法在L-AVC任务上显著优于现有SOTA基线，突显了情感信息对该任务的重要性及其操纵方法的有效性。"
      },
      {
        "paper_id": "2602.17951",
        "title": "ROCKET: Residual-Oriented Multi-Layer Alignment for Spatially-Aware Vision-Language-Action Models",
        "abstract": "Vision-Language-Action (VLA) models enable instruction-following robotic manipulation, but they are typically pretrained on 2D data and lack 3D spatial understanding. An effective approach is representation alignment, where a strong vision foundation model is used to guide a 2D VLA model. However, existing methods usually apply supervision at only a single layer, failing to fully exploit the rich information distributed across depth; meanwhile, naïve multi-layer alignment can cause gradient interference. We introduce ROCKET, a residual-oriented multi-layer representation alignment framework that formulates multi-layer alignment as aligning one residual stream to another. Concretely, ROCKET employs a shared projector to align multiple layers of the VLA backbone with multiple layers of a powerful 3D vision foundation model via a layer-invariant mapping, which reduces gradient conflicts. We provide both theoretical justification and empirical analyses showing that a shared projector is sufficient and outperforms prior designs, and further propose a Matryoshka-style sparse activation scheme for the shared projector to balance multiple alignment losses. Our experiments show that, combined with a training-free layer selection strategy, ROCKET requires only about 4% of the compute budget while achieving 98.5% state-of-the-art success rate on LIBERO. We further demonstrate the superior performance of ROCKET across LIBERO-Plus and RoboTwin, as well as multiple VLA models. The code and model weights can be found at https://github.com/CASE-Lab-UMD/ROCKET-VLA.",
        "authors_display": "Ang Li Team",
        "pdf_url": "http://arxiv.org/abs/2602.17951",
        "code_url": null,
        "date": "2026-02-20",
        "primary_category": "cs.CV",
        "chinese_summary": "视觉-语言-动作（VLA）模型在机器人操作中指令遵循表现出色，但由于主要在2D数据上预训练，其3D空间理解能力不足。尽管表征对齐是弥补这一缺陷的有效方法，但现有方法通常仅在单层应用监督，未能充分利用深层信息，且多层朴素对齐易引发梯度冲突。为解决此问题，本文提出了ROCKET框架，它将多层对齐表述为将一个残差流与另一个对齐，并利用共享投影仪通过层不变映射对齐VLA骨干网络与3D视觉基础模型的多个层，从而有效减少梯度冲突。理论分析与实验证明了共享投影仪的有效性，并进一步提出了Matryoshka风格的稀疏激活方案以平衡多重对齐损失。实验结果表明，ROCKET结合免训练层选择策略，仅需约4%的计算预算，却能在LIBERO上达到98.5%的SOTA成功率，并在多个VLA模型和基准上展现了卓越性能。"
      },
      {
        "paper_id": "2602.17634",
        "title": "Reverso: Efficient Time Series Foundation Models for Zero-shot Forecasting",
        "abstract": "Learning time series foundation models has been shown to be a promising approach for zero-shot time series forecasting across diverse time series domains. Insofar as scaling has been a critical driver of performance of foundation models in other modalities such as language and vision, much recent work on time series foundation modeling has focused on scaling. This has resulted in time series foundation models with hundreds of millions of parameters that are, while performant, inefficient and expensive to use in practice. This paper describes a simple recipe for learning efficient foundation models for zero-shot time series forecasting that are orders of magnitude smaller. We show that large-scale transformers are not necessary: small hybrid models that interleave long convolution and linear RNN layers (in particular DeltaNet layers) can match the performance of larger transformer-based models while being more than a hundred times smaller. We also describe several data augmentation and inference strategies that further improve performance. This recipe results in Reverso, a family of efficient time series foundation models for zero-shot forecasting that significantly push the performance-efficiency Pareto frontier.",
        "authors_display": "Yoon Kim Team",
        "pdf_url": "http://arxiv.org/abs/2602.17634",
        "code_url": null,
        "date": "2026-02-19",
        "primary_category": "cs.LG",
        "chinese_summary": "时间序列基础模型在零样本预测方面潜力巨大，但现有大型Transformer模型参数量庞大，导致实际应用效率和成本问题。本研究提出一种学习高效零样本时间序列预测基础模型的简单方法，其规模远小于现有模型。研究发现，通过结合长卷积层和线性RNN层（特别是DeltaNet层）的小型混合模型，性能可与大型Transformer模型媲美，但体积却小一百多倍。此外，论文还介绍了多种数据增强和推理策略以进一步提升性能。这些结合产生了Reverso系列高效时间序列零样本预测基础模型，显著提升了性能-效率的帕累托前沿。"
      },
      {
        "paper_id": "2602.17594",
        "title": "AI Gamestore: Scalable, Open-Ended Evaluation of Machine General Intelligence with Human Games",
        "abstract": "Rigorously evaluating machine intelligence against the broad spectrum of human general intelligence has become increasingly important and challenging in this era of rapid technological advance. Conventional AI benchmarks typically assess only narrow capabilities in a limited range of human activity. Most are also static, quickly saturating as developers explicitly or implicitly optimize for them. We propose that a more promising way to evaluate human-like general intelligence in AI systems is through a particularly strong form of general game playing: studying how and how well they play and learn to play \\textbf{all conceivable human games}, in comparison to human players with the same level of experience, time, or other resources. We define a \"human game\" to be a game designed by humans for humans, and argue for the evaluative suitability of this space of all such games people can imagine and enjoy -- the \"Multiverse of Human Games\". Taking a first step towards this vision, we introduce the AI GameStore, a scalable and open-ended platform that uses LLMs with humans-in-the-loop to synthesize new representative human games, by automatically sourcing and adapting standardized and containerized variants of game environments from popular human digital gaming platforms. As a proof of concept, we generated 100 such games based on the top charts of Apple App Store and Steam, and evaluated seven frontier vision-language models (VLMs) on short episodes of play. The best models achieved less than 10\\% of the human average score on the majority of the games, and especially struggled with games that challenge world-model learning, memory and planning. We conclude with a set of next steps for building out the AI GameStore as a practical way to measure and drive progress toward human-like general intelligence in machines.",
        "authors_display": "Joshua B. Tenenbaum Team",
        "pdf_url": "http://arxiv.org/abs/2602.17594",
        "code_url": null,
        "date": "2026-02-19",
        "primary_category": "cs.AI",
        "chinese_summary": "在快速发展的技术时代，严格评估机器智能的通用性变得愈发重要和困难，而传统AI基准往往过于狭隘且易饱和。为此，本研究提出通过通用游戏玩法来评估AI系统的类人通用智能，即衡量AI系统在“所有可想象的人类游戏”中与人类玩家的对比表现。作为实现此愿景的第一步，研究引入了AI GameStore，一个可扩展、开放式的平台，它利用LLM与人类协作，通过从流行数字游戏平台获取并改编标准化、容器化的游戏环境，合成新的代表性人类游戏。概念验证基于Apple App Store和Steam排行榜生成了100款游戏，并评估了七个前沿视觉-语言模型（VLM）。结果显示，即使是最好的VLM在大多数游戏上的人类平均得分也不足10%，尤其在世界模型学习、记忆和规划等挑战性任务中表现不佳，这为衡量和推动机器类人通用智能的发展提供了实践途径。"
      },
      {
        "paper_id": "2602.17532",
        "title": "Systematic Evaluation of Single-Cell Foundation Model Interpretability Reveals Attention Captures Co-Expression Rather Than Unique Regulatory Signal",
        "abstract": "We present a systematic evaluation framework - thirty-seven analyses, 153 statistical tests, four cell types, two perturbation modalities - for assessing mechanistic interpretability in single-cell foundation models. Applying this framework to scGPT and Geneformer, we find that attention patterns encode structured biological information with layer-specific organisation - protein-protein interactions in early layers, transcriptional regulation in late layers - but this structure provides no incremental value for perturbation prediction: trivial gene-level baselines outperform both attention and correlation edges (AUROC 0.81-0.88 versus 0.70), pairwise edge scores add zero predictive contribution, and causal ablation of regulatory heads produces no degradation. These findings generalise from K562 to RPE1 cells; the attention-correlation relationship is context-dependent, but gene-level dominance is universal. Cell-State Stratified Interpretability (CSSI) addresses an attention-specific scaling failure, improving GRN recovery up to 1.85x. The framework establishes reusable quality-control standards for the field.",
        "authors_display": "Ihor Kendiukhov Team",
        "pdf_url": "http://arxiv.org/abs/2602.17532",
        "code_url": null,
        "date": "2026-02-19",
        "primary_category": "q-bio.GN",
        "chinese_summary": "为了评估单细胞基础模型的机制可解释性，本研究提出了一个包含37项分析、153项统计测试的系统评估框架。通过该框架对scGPT和Geneformer进行评估，发现注意力模式编码了具有层级结构的生物信息，但这种结构对扰动预测没有额外价值，基因层面的基线表现更优。为解决注意力特有的扩展失效问题，研究引入了细胞状态分层可解释性（CSSI），成功将基因调控网络恢复能力提升了1.85倍，并为该领域建立了可重用的质量控制标准。"
      },
      {
        "paper_id": "2602.17385",
        "title": "Dataless Weight Disentanglement in Task Arithmetic via Kronecker-Factored Approximate Curvature",
        "abstract": "Task Arithmetic yields a modular, scalable way to adapt foundation models. Combining multiple task vectors, however, can lead to cross-task interference, causing representation drift and degraded performance. Representation drift regularization provides a natural remedy to disentangle task vectors; however, existing approaches typically require external task data, conflicting with modularity and data availability constraints (e.g., privacy requirements). We propose a dataless approach by framing regularization against representation drift as a curvature matrix approximation problem. This allows us to leverage well-established techniques; in particular, we adopt Kronecker-Factored Approximate Curvature and obtain a practical regularizer that achieves state-of-the-art results in task addition and negation. Our method has constant complexity in the number of tasks and promotes robustness to task vector rescaling, eliminating the need for held-out tuning.",
        "authors_display": "Simone Calderara Team",
        "pdf_url": "http://arxiv.org/abs/2602.17385",
        "code_url": null,
        "date": "2026-02-19",
        "primary_category": "cs.AI",
        "chinese_summary": "任务算术为基础模型提供了模块化、可扩展的适应方式，但组合任务向量可能导致跨任务干扰，引起表示漂移并降低性能。现有的正则化方法通常需要外部任务数据，与模块化原则和数据可用性相冲突。为此，本研究提出了一种无数据方法，将表示漂移正则化视为曲率矩阵近似问题，并采用Kronecker分解近似曲率技术。该方法在任务添加和否定方面取得了最先进的成果，且计算复杂度与任务数量无关，增强了对任务向量缩放的鲁棒性，无需额外的调优。"
      },
      {
        "paper_id": "2602.17365",
        "title": "Computer-Using World Model",
        "abstract": "Agents operating in complex software environments benefit from reasoning about the consequences of their actions, as even a single incorrect user interface (UI) operation can derail long, artifact-preserving workflows. This challenge is particularly acute for computer-using scenarios, where real execution does not support counterfactual exploration, making large-scale trial-and-error learning and planning impractical despite the environment being fully digital and deterministic. We introduce the Computer-Using World Model (CUWM), a world model for desktop software that predicts the next UI state given the current state and a candidate action. CUWM adopts a two-stage factorization of UI dynamics: it first predicts a textual description of agent-relevant state changes, and then realizes these changes visually to synthesize the next screenshot. CUWM is trained on offline UI transitions collected from agents interacting with real Microsoft Office applications, and further refined with a lightweight reinforcement learning stage that aligns textual transition predictions with the structural requirements of computer-using environments. We evaluate CUWM via test-time action search, where a frozen agent uses the world model to simulate and compare candidate actions before execution. Across a range of Office tasks, world-model-guided test-time scaling improves decision quality and execution robustness.",
        "authors_display": "Dongmei Zhang Team",
        "pdf_url": "http://arxiv.org/abs/2602.17365",
        "code_url": null,
        "date": "2026-02-19",
        "primary_category": "cs.SE",
        "chinese_summary": "在复杂的软件环境中，智能体需要对操作后果进行推理，因为即使是单一的错误UI操作也可能破坏长期的工作流。针对真实执行无法支持反事实探索的问题，本研究引入了计算机使用世界模型（CUWM），一个用于桌面软件的世界模型。CUWM采用UI动态的两阶段分解：首先预测与智能体相关的文本描述状态变化，然后将其可视化以合成下一个屏幕截图。CUWM在Microsoft Office真实应用程序的离线UI转换数据上进行训练，并通过轻量级强化学习进行微调，实验结果表明，在测试时行动搜索中，CUWM能够提高决策质量和执行鲁棒性。"
      },
      {
        "paper_id": "2602.17259",
        "title": "FRAPPE: Infusing World Modeling into Generalist Policies via Multiple Future Representation Alignment",
        "abstract": "Enabling VLA models to predict environmental dynamics, known as world modeling, has been recognized as essential for improving robotic reasoning and generalization. However, current approaches face two main issues: 1. The training objective forces models to over-emphasize pixel-level reconstruction, which constrains semantic learning and generalization 2. Reliance on predicted future observations during inference often leads to error accumulation. To address these challenges, we introduce Future Representation Alignment via Parallel Progressive Expansion (FRAPPE). Our method adopts a two-stage fine-tuning strategy: In the mid-training phase, the model learns to predict the latent representations of future observations; In the post-training phase, we expand the computational workload in parallel and align the representation simultaneously with multiple different visual foundation models. By significantly improving fine-tuning efficiency and reducing dependence on action-annotated data, FRAPPE provides a scalable and data-efficient pathway to enhance world-awareness in generalist robotic policies. Experiments on the RoboTwin benchmark and real-world tasks demonstrate that FRAPPE outperforms state-of-the-art approaches and shows strong generalization in long-horizon and unseen scenarios.",
        "authors_display": "Donglin Wang Team",
        "pdf_url": "http://arxiv.org/abs/2602.17259",
        "code_url": "https://h-zhao1997.github.io/frappe",
        "date": "2026-02-19",
        "primary_category": "cs.RO",
        "chinese_summary": "使视觉语言动作（VLA）模型能够预测环境动态（即世界建模）对于提高机器人推理和泛化至关重要。当前方法面临像素级重建过度强调导致语义学习受限，以及推理时依赖预测未来观测导致误差累积的问题。为此，本研究提出了通过并行渐进扩展的未来表示对齐（FRAPPE）方法，采用两阶段微调策略，模型在中期学习预测未来观测的潜在表示，后期并行扩展计算工作量并与多个视觉基础模型对齐。该方法显著提高了微调效率并减少了对动作标注数据的依赖，在RoboTwin基准和真实世界任务中均优于现有方法，展现出强大的长周期和未见场景泛化能力。"
      },
      {
        "paper_id": "2602.17251",
        "title": "Structured Prototype-Guided Adaptation for EEG Foundation Models",
        "abstract": "Electroencephalography (EEG) foundation models (EFMs) have achieved strong performance under full fine-tuning but exhibit poor generalization when subject-level supervision is limited, a common constraint in real-world clinical settings. We show that this failure stems not merely from limited supervision, but from a structural mismatch between noisy, limited supervision and the highly plastic parameter space of EFMs. To address this challenge, we propose SCOPE, a Structured COnfidence-aware Prototype-guided adaptation framework for EFM fine-tuning. SCOPE follows a two-stage pipeline. In the first stage, we construct reliable external supervision by learning geometry-regularized task priors, constructing balanced class-level prototypes over the resulting embeddings, and producing confidence-aware pseudo-labels from their agreement to filter unreliable signals on unlabeled data. In the second stage, we introduce ProAdapter, which adapts frozen EEG foundation models via a lightweight adapter conditioned on the structured prototypes. Experiments across three EEG tasks and five foundation model backbones demonstrate that SCOPE consistently achieves strong performance and efficiency under label-limited cross-subject settings.",
        "authors_display": "Mengling Feng Team",
        "pdf_url": "http://arxiv.org/abs/2602.17251",
        "code_url": null,
        "date": "2026-02-19",
        "primary_category": "cs.LG",
        "chinese_summary": "脑电图（EEG）基础模型（EFMs）在完全微调下表现出色，但在真实临床场景常见的受试者级别监督受限时泛化能力差，这源于噪声数据与EFMs高可塑性参数空间之间的结构不匹配。为解决此问题，本研究提出了SCOPE，一个结构化置信度感知原型引导的EFM微调框架。SCOPE采用两阶段流程：首先通过学习几何正则化任务先验、构建平衡的类级别原型并生成置信度感知伪标签来构建可靠的外部监督；其次引入ProAdapter，通过基于结构化原型的轻量级适配器来适应冻结的EFM。实验结果表明，SCOPE在标签受限的跨受试者设置下，在三项EEG任务和五种基础模型骨干上均能持续实现出色的性能和效率。"
      },
      {
        "paper_id": "2602.17222",
        "title": "Decoding the Human Factor: High Fidelity Behavioral Prediction for Strategic Foresight",
        "abstract": "Predicting human decision-making in high-stakes environments remains a central challenge for artificial intelligence. While large language models (LLMs) demonstrate strong general reasoning, they often struggle to generate consistent, individual-specific behavior, particularly when accurate prediction depends on complex interactions between psychological traits and situational constraints. Prompting-based approaches can be brittle in this setting, exhibiting identity drift and limited ability to leverage increasingly detailed persona descriptions. To address these limitations, we introduce the Large Behavioral Model (LBM), a behavioral foundation model fine-tuned to predict individual strategic choices with high fidelity. LBM shifts from transient persona prompting to behavioral embedding by conditioning on a structured, high-dimensional trait profile derived from a comprehensive psychometric battery. Trained on a proprietary dataset linking stable dispositions, motivational states, and situational constraints to observed choices, LBM learns to map rich psychological profiles to discrete actions across diverse strategic dilemmas. In a held-out scenario evaluation, LBM fine-tuning improves behavioral prediction relative to the unadapted Llama-3.1-8B-Instruct backbone and performs comparably to frontier baselines when conditioned on Big Five traits. Moreover, we find that while prompting-based baselines exhibit a complexity ceiling, LBM continues to benefit from increasingly dense trait profiles, with performance improving as additional trait dimensions are provided. Together, these results establish LBM as a scalable approach for high-fidelity behavioral simulation, enabling applications in strategic foresight, negotiation analysis, cognitive security, and decision support.",
        "authors_display": "Shula Grinapol Team",
        "pdf_url": "http://arxiv.org/abs/2602.17222",
        "code_url": null,
        "date": "2026-02-19",
        "primary_category": "cs.AI",
        "chinese_summary": "预测高风险环境中的人类决策是人工智能的核心挑战，大型语言模型难以生成一致且个性化的行为。本研究引入了大型行为模型（LBM），一个经过微调的行为基础模型，用于高保真预测个体战略选择。LBM通过将结构化、高维度的特质概况作为条件进行行为嵌入，而非瞬态提示。模型在一个连接稳定倾向、动机状态和情境约束与观察到的选择的专有数据集上训练。在评估中，LBM在行为预测方面优于未经调整的Llama-3.1-8B-Instruct，且在条件设定为大五人格特质时与前沿基线模型表现相当，且随着额外特质维度的提供，性能持续提升，证明了其高保真行为模拟的可扩展性。"
      },
      {
        "paper_id": "2602.17217",
        "title": "Continual learning and refinement of causal models through dynamic predicate invention",
        "abstract": "Efficiently navigating complex environments requires agents to internalize the underlying logic of their world, yet standard world modelling methods often struggle with sample inefficiency, lack of transparency, and poor scalability. We propose a framework for constructing symbolic causal world models entirely online by integrating continuous model learning and repair into the agent's decision loop, by leveraging the power of Meta-Interpretive Learning and predicate invention to find semantically meaningful and reusable abstractions, allowing an agent to construct a hierarchy of disentangled, high-quality concepts from its observations. We demonstrate that our lifted inference approach scales to domains with complex relational dynamics, where propositional methods suffer from combinatorial explosion, while achieving sample-efficiency orders of magnitude higher than the established PPO neural-network-based baseline.",
        "authors_display": "Peter Flach Team",
        "pdf_url": "http://arxiv.org/abs/2602.17217",
        "code_url": null,
        "date": "2026-02-19",
        "primary_category": "cs.AI",
        "chinese_summary": "智能体在复杂环境中高效导航需要内化世界的底层逻辑，然而传统的建立世界模型的方法常面临样本效率低下、缺乏透明度及扩展性差的问题。本研究提出了一种框架，通过将连续模型学习与修复集成到智能体的决策循环中，利用元解释学习和谓词发明来在线构建符号因果世界模型。该方法能够发现语义上有意义且可重用的抽象，使智能体能够从观测中构建一个分层的、解耦的高质量概念。实验证明，该提升推理方法在具有复杂关系动态的领域中能够有效扩展，且样本效率比基于PPO神经网络的基线方法高出几个数量级。"
      },
      {
        "paper_id": "2602.17162",
        "title": "JEPA-DNA: Grounding Genomic Foundation Models through Joint-Embedding Predictive Architectures",
        "abstract": "Genomic Foundation Models (GFMs) have largely relied on Masked Language Modeling (MLM) or Next Token Prediction (NTP) to learn the language of life. While these paradigms excel at capturing local genomic syntax and fine-grained motif patterns, they often fail to capture the broader functional context, resulting in representations that lack a global biological perspective. We introduce JEPA-DNA, a novel pre-training framework that integrates the Joint-Embedding Predictive Architecture (JEPA) with traditional generative objectives. JEPA-DNA introduces latent grounding by coupling token-level recovery with a predictive objective in the latent space by supervising a CLS token. This forces the model to predict the high-level functional embeddings of masked genomic segments rather than focusing solely on individual nucleotides. JEPA-DNA extends both NTP and MLM paradigms and can be deployed either as a standalone from-scratch objective or as a continual pre-training enhancement for existing GFMs. Our evaluations across a diverse suite of genomic benchmarks demonstrate that JEPA-DNA consistently yields superior performance in supervised and zero-shot tasks compared to generative-only baselines. By providing a more robust and biologically grounded representation, JEPA-DNA offers a scalable path toward foundation models that understand not only the genomic alphabet, but also the underlying functional logic of the sequence.",
        "authors_display": "Yoli Shavit Team",
        "pdf_url": "http://arxiv.org/abs/2602.17162",
        "code_url": null,
        "date": "2026-02-19",
        "primary_category": "cs.AI",
        "chinese_summary": "基因组基础模型（GFMs）主要依赖掩码语言建模（MLM）或下一令牌预测（NTP）来学习生命语言。这些范式擅长捕捉局部基因组语法和精细基序模式，但往往未能捕捉更广泛的功能上下文，导致其表示缺乏全局生物学视角。本研究引入了JEPA-DNA，一种结合了联合嵌入预测架构（JEPA）与传统生成目标的预训练框架。JEPA-DNA通过将令牌级恢复与潜在空间中的预测目标耦合，监督一个CLS令牌来预测掩码基因组片段的高级功能嵌入，从而引入了潜在接地。实验结果表明，JEPA-DNA在各种基因组基准测试中的监督和零样本任务中均表现优于纯生成基线模型，提供了更稳健且生物学上更接地气的表示，为理解基因组字母及其底层功能逻辑的基础模型提供了可扩展的路径。"
      },
      {
        "paper_id": "2602.17097",
        "title": "AudioChat: Unified Audio Storytelling, Editing, and Understanding with Transfusion Forcing",
        "abstract": "Despite recent breakthroughs, audio foundation models struggle in processing complex multi-source acoustic scenes. We refer to this challenging domain as audio stories, which can have multiple speakers and background/foreground sound effects. Compared to traditional audio processing tasks, audio stories introduce new layers of semantic, temporal, and physical complexity. To address this challenge, we propose AudioChat, a framework for developing audio foundation models that can generate, edit, and understand audio stories. AudioChat introduces a new paradigm in which LLM-based toolcalling agents simulate interactions between users and the system, and these simulated dialogues are used as training data. We also introduce a novel Audio Transfusion Forcing objective to train the AudioChat model, allowing it to simultaneously decompose high-level instructions via structured chain-of-thought reasoning and perform interactive multi-turn audio understanding/generation. To evaluate generation and editing performance, we develop three new metrics that directly measure task performance instead of relying upon distribution-based scoring. We highly encourage readers to visit our demo to better understand the capabilities of AudioChat: https://wanchichen.github.io/audiochat/.",
        "authors_display": "Zeyu Jin Team",
        "pdf_url": "http://arxiv.org/abs/2602.17097",
        "code_url": null,
        "date": "2026-02-19",
        "primary_category": "cs.SD",
        "chinese_summary": "尽管最近取得了突破，音频基础模型在处理复杂的多源声学场景（即音频故事，包含多说话者和背景/前景音效）时仍面临挑战。为解决这一问题，本研究提出了AudioChat框架，旨在开发能生成、编辑和理解音频故事的音频基础模型。AudioChat引入了一种新范式，利用基于LLM的工具调用智能体模拟用户与系统间的交互，生成训练数据，并引入了一种新颖的Audio Transfusion Forcing目标函数进行模型训练，使其能够同时通过结构化的思维链推理分解高级指令，并执行交互式多轮音频理解/生成。为评估生成和编辑性能，我们开发了三项新的直接衡量任务表现的指标。"
      },
      {
        "paper_id": "2602.17868",
        "title": "MantisV2: Closing the Zero-Shot Gap in Time Series Classification with Synthetic Data and Test-Time Strategies",
        "abstract": "Developing foundation models for time series classification is of high practical relevance, as such models can serve as universal feature extractors for diverse downstream tasks. Although early models such as Mantis have shown the promise of this approach, a substantial performance gap remained between frozen and fine-tuned encoders. In this work, we introduce methods that significantly strengthen zero-shot feature extraction for time series. First, we introduce Mantis+, a variant of Mantis pre-trained entirely on synthetic time series. Second, through controlled ablation studies, we refine the architecture and obtain MantisV2, an improved and more lightweight encoder. Third, we propose an enhanced test-time methodology that leverages intermediate-layer representations and refines output-token aggregation. In addition, we show that performance can be further improved via self-ensembling and cross-model embedding fusion. Extensive experiments on UCR, UEA, Human Activity Recognition (HAR) benchmarks, and EEG datasets show that MantisV2 and Mantis+ consistently outperform prior time series foundation models, achieving state-of-the-art zero-shot performance.",
        "authors_display": "Ievgen Redko Team",
        "pdf_url": "http://arxiv.org/abs/2602.17868",
        "code_url": null,
        "date": "2026-02-19",
        "primary_category": "cs.LG",
        "chinese_summary": "为时间序列分类开发基础模型具有高度实用价值，但现有模型（如Mantis）在零样本和微调编码器之间存在显著性能差距。本研究引入了多项创新以显著强化时间序列的零样本特征提取能力。首先，提出了Mantis+，一个完全基于合成时间序列预训练的Mantis变体。其次，通过架构精炼，得到了更轻量、性能更优的MantisV2。第三，开发了一种增强型测试时方法，利用中间层表示并改进输出token聚合。此外，研究还展示了通过自集成和跨模型嵌入融合可进一步提升性能。在UCR、UEA、HAR和EEG等多个基准数据集上的广泛实验证明，MantisV2和Mantis+均持续优于先前的时间序列基础模型，实现了最先进的零样本性能。"
      },
      {
        "paper_id": "2602.17799",
        "title": "Enabling Training-Free Text-Based Remote Sensing Segmentation",
        "abstract": "Recent advances in Vision Language Models (VLMs) and Vision Foundation Models (VFMs) have opened new opportunities for zero-shot text-guided segmentation of remote sensing imagery. However, most existing approaches still rely on additional trainable components, limiting their generalisation and practical applicability. In this work, we investigate to what extent text-based remote sensing segmentation can be achieved without additional training, by relying solely on existing foundation models. We propose a simple yet effective approach that integrates contrastive and generative VLMs with the Segment Anything Model (SAM), enabling a fully training-free or lightweight LoRA-tuned pipeline. Our contrastive approach employs CLIP as mask selector for SAM's grid-based proposals, achieving state-of-the-art open-vocabulary semantic segmentation (OVSS) in a completely zero-shot setting. In parallel, our generative approach enables reasoning and referring segmentation by generating click prompts for SAM using GPT-5 in a zero-shot setting and a LoRA-tuned Qwen-VL model, with the latter yielding the best results. Extensive experiments across 19 remote sensing benchmarks, including open-vocabulary, referring, and reasoning-based tasks, demonstrate the strong capabilities of our approach. Code will be released at https://github.com/josesosajs/trainfree-rs-segmentation.",
        "authors_display": "Djamila Aouada Team",
        "pdf_url": "http://arxiv.org/abs/2602.17799",
        "code_url": null,
        "date": "2026-02-19",
        "primary_category": "cs.CV",
        "chinese_summary": "现有视觉语言模型（VLM）和视觉基础模型（VFM）为遥感图像的零样本文本引导分割带来了机遇，但多数方法仍依赖额外的可训练组件。本研究旨在探讨在不进行额外训练的情况下，仅依赖现有基础模型实现文本基遥感分割的潜力。研究提出了一种简单有效的方法，将对比和生成式VLM与Segment Anything Model (SAM) 集成，从而实现完全免训练或轻量级LoRA微调的管道。其中，对比方法使用CLIP作为SAM网格提案的掩码选择器，在完全零样本设置下实现了最先进的开放词汇语义分割（OVSS）。生成方法则利用GPT-5在零样本设置下为SAM生成点击提示，或通过LoRA微调的Qwen-VL模型实现推理和参照分割，后者取得了最佳结果。在19个遥感基准测试（涵盖开放词汇、参照和基于推理的任务）上的广泛实验证明了该方法的强大能力。"
      },
      {
        "paper_id": "2602.17739",
        "title": "GeneZip: Region-Aware Compression for Long Context DNA Modeling",
        "abstract": "Genomic sequences span billions of base pairs (bp), posing a fundamental challenge for genome-scale foundation models. Existing approaches largely sidestep this barrier by either scaling relatively small models to long contexts or relying on heavy multi-GPU parallelism. Here we introduce GeneZip, a DNA compression model that leverages a key biological prior: genomic information is highly imbalanced. Coding regions comprise only a small fraction (about 2 percent) yet are information-dense, whereas most non-coding sequence is comparatively information-sparse. GeneZip couples HNet-style dynamic routing with a region-aware compression-ratio objective, enabling adaptive allocation of representation budget across genomic regions. As a result, GeneZip learns region-aware compression and achieves 137.6x compression with only 0.31 perplexity increase. On downstream long-context benchmarks, GeneZip achieves comparable or better performance on contact map prediction, expression quantitative trait loci prediction, and enhancer-target gene prediction. By reducing effective sequence length, GeneZip unlocks simultaneous scaling of context and capacity: compared to the prior state-of-the-art model JanusDNA, it enables training models 82.6x larger at 1M-bp context, supporting a 636M-parameter GeneZip model at 1M-bp context. All experiments in this paper can be trained on a single A100 80GB GPU.",
        "authors_display": "Jian Tang Team",
        "pdf_url": "http://arxiv.org/abs/2602.17739",
        "code_url": null,
        "date": "2026-02-19",
        "primary_category": "q-bio.GN",
        "chinese_summary": "基因组序列包含数十亿个碱基对，对基因组规模的基础模型构成了巨大挑战，现有方法多通过扩展小型模型或依赖多GPU并行来规避。本研究提出了GeneZip，一个利用基因组信息高度不平衡（编码区信息密集，非编码区稀疏）的DNA压缩模型。GeneZip将HNet风格的动态路由与区域感知压缩比目标相结合，实现了基因组区域间表示预算的自适应分配。结果显示，GeneZip实现了137.6倍的压缩率，困惑度仅增加0.31。在下游长上下文基准测试中，GeneZip在接触图预测、表达数量性状基因座预测和增强子-靶基因预测方面表现相当或更优，从而解锁了上下文和容量的同时扩展，使得在单个A100 80GB GPU上训练1M-bp上下文的636M参数模型成为可能。"
      },
      {
        "paper_id": "2602.16696",
        "title": "Parameter-free representations outperform single-cell foundation models on downstream benchmarks",
        "abstract": "Single-cell RNA sequencing (scRNA-seq) data exhibit strong and reproducible statistical structure. This has motivated the development of large-scale foundation models, such as TranscriptFormer, that use transformer-based architectures to learn a generative model for gene expression by embedding genes into a latent vector space. These embeddings have been used to obtain state-of-the-art (SOTA) performance on downstream tasks such as cell-type classification, disease-state prediction, and cross-species learning. Here, we ask whether similar performance can be achieved without utilizing computationally intensive deep learning-based representations. Using simple, interpretable pipelines that rely on careful normalization and linear methods, we obtain SOTA or near SOTA performance across multiple benchmarks commonly used to evaluate single-cell foundation models, including outperforming foundation models on out-of-distribution tasks involving novel cell types and organisms absent from the training data. Our findings highlight the need for rigorous benchmarking and suggest that the biology of cell identity can be captured by simple linear representations of single cell gene expression data.",
        "authors_display": "Pankaj Mehta Team",
        "pdf_url": "http://arxiv.org/abs/2602.16696",
        "code_url": null,
        "date": "2026-02-18",
        "primary_category": "q-bio.GN",
        "chinese_summary": "单细胞RNA测序（scRNA-seq）数据展现出可重现的统计结构，促使了基于Transformer的大规模基础模型（如TranscriptFormer）发展，用于下游任务。本研究探讨了是否无需计算密集型深度学习表示也能实现类似性能。通过采用依赖精心归一化和线性方法的简单、可解释的流水线，研究在多个单细胞基础模型常用基准测试中达到了SOTA或接近SOTA的性能，甚至在涉及训练数据中未出现的新细胞类型和生物体的域外任务中超越了基础模型。研究结果强调了严格基准测试的重要性，并表明细胞身份的生物学特性可通过单细胞基因表达数据的简单线性表示来捕获。"
      },
      {
        "paper_id": "2602.16689",
        "title": "Are Object-Centric Representations Better At Compositional Generalization?",
        "abstract": "Compositional generalization, the ability to reason about novel combinations of familiar concepts, is fundamental to human cognition and a critical challenge for machine learning. Object-centric (OC) representations, which encode a scene as a set of objects, are often argued to support such generalization, but systematic evidence in visually rich settings is limited. We introduce a Visual Question Answering benchmark across three controlled visual worlds (CLEVRTex, Super-CLEVR, and MOVi-C) to measure how well vision encoders, with and without object-centric biases, generalize to unseen combinations of object properties. To ensure a fair and comprehensive comparison, we carefully account for training data diversity, sample size, representation size, downstream model capacity, and compute. We use DINOv2 and SigLIP2, two widely used vision encoders, as the foundation models and their OC counterparts. Our key findings reveal that (1) OC approaches are superior in harder compositional generalization settings; (2) original dense representations surpass OC only on easier settings and typically require substantially more downstream compute; and (3) OC models are more sample efficient, achieving stronger generalization with fewer images, whereas dense encoders catch up or surpass them only with sufficient data and diversity. Overall, object-centric representations offer stronger compositional generalization when any one of dataset size, training data diversity, or downstream compute is constrained.",
        "authors_display": "Andrea Dittadi Team",
        "pdf_url": "http://arxiv.org/abs/2602.16689",
        "code_url": null,
        "date": "2026-02-18",
        "primary_category": "cs.CV",
        "chinese_summary": "组合泛化能力是人类认知的基本要素，也是机器学习的重大挑战，而以对象为中心的（OC）表示被认为能支持此能力，但视觉丰富环境中的系统性证据有限。本研究引入了跨三个受控视觉世界的视觉问答基准，以评估带有或不带OC偏置的视觉编码器在泛化到未见过的对象属性组合时的表现。研究通过严谨控制训练数据多样性、样本量和计算等因素，对DINOv2和SigLIP2及其OC版本进行了比较。结果表明，在更复杂的组合泛化任务中，OC方法表现更优；原始密集表示仅在更简单的任务中超越OC且需要更多计算；OC模型在样本效率上更高，在数据集大小、训练数据多样性或下游计算受限时提供了更强的组合泛化能力。"
      },
      {
        "paper_id": "2602.16687",
        "title": "Scaling Open Discrete Audio Foundation Models with Interleaved Semantic, Acoustic, and Text Tokens",
        "abstract": "Current audio language models are predominantly text-first, either extending pre-trained text LLM backbones or relying on semantic-only audio tokens, limiting general audio modeling. This paper presents a systematic empirical study of native audio foundation models that apply next-token prediction to audio at scale, jointly modeling semantic content, acoustic details, and text to support both general audio generation and cross-modal capabilities. We provide comprehensive empirical insights for building such models: (1) We systematically investigate design choices -- data sources, text mixture ratios, and token composition -- establishing a validated training recipe. (2) We conduct the first scaling law study for discrete audio models via IsoFLOP analysis on 64 models spanning $3{\\times}10^{18}$ to $3{\\times}10^{20}$ FLOPs, finding that optimal data grows 1.6$\\times$ faster than optimal model size. (3) We apply these lessons to train SODA (Scaling Open Discrete Audio), a suite of models from 135M to 4B parameters on 500B tokens, comparing against our scaling predictions and existing models. SODA serves as a flexible backbone for diverse audio/text tasks -- we demonstrate this by fine-tuning for voice-preserving speech-to-speech translation, using the same unified architecture.",
        "authors_display": "Diyi Yang Team",
        "pdf_url": "http://arxiv.org/abs/2602.16687",
        "code_url": null,
        "date": "2026-02-18",
        "primary_category": "cs.SD",
        "chinese_summary": "当前音频语言模型多以文本为先，限制了通用音频建模。本文提出并系统研究了原生音频基础模型，通过大规模的下一令牌预测，联合建模语义内容、声学细节和文本，以支持通用音频生成和跨模态能力。研究者系统探索了数据源、文本混合比例等设计选择，并首次对离散音频模型进行了缩放定律研究。基于这些发现，他们训练了SODA模型套件，并在保留说话者声音的语音-语音翻译等任务中展示了其作为灵活骨干网络的有效性。"
      },
      {
        "paper_id": "2602.16684",
        "title": "Retrieval-Augmented Foundation Models for Matched Molecular Pair Transformations to Recapitulate Medicinal Chemistry Intuition",
        "abstract": "Matched molecular pairs (MMPs) capture the local chemical edits that medicinal chemists routinely use to design analogs, but existing ML approaches either operate at the whole-molecule level with limited edit controllability or learn MMP-style edits from restricted settings and small models. We propose a variable-to-variable formulation of analog generation and train a foundation model on large-scale MMP transformations (MMPTs) to generate diverse variables conditioned on an input variable. To enable practical control, we develop prompting mechanisms that let the users specify preferred transformation patterns during generation. We further introduce MMPT-RAG, a retrieval-augmented framework that uses external reference analogs as contextual guidance to steer generation and generalize from project-specific series. Experiments on general chemical corpora and patent-specific datasets demonstrate improved diversity, novelty, and controllability, and show that our method recovers realistic analog structures in practical discovery scenarios.",
        "authors_display": "Liang Zhao Team",
        "pdf_url": "http://arxiv.org/abs/2602.16684",
        "code_url": null,
        "date": "2026-02-18",
        "primary_category": "cs.LG",
        "chinese_summary": "针对药物化学中类似物设计缺乏高效、可控的分子编辑方法，现有机器学习方法存在局限性的问题，本文提出了一种基于大规模匹配分子对转换（MMPTs）的变异到变异（variable-to-variable）类似物生成基础模型。该模型通过提示机制实现用户对转换模式的精确控制，并引入MMPT-RAG框架利用外部参考类似物进行上下文指导。实验证明，该方法在通用化学语料库和专利数据集上显著提升了生成多样性、新颖性和可控性，能在实际发现场景中恢复出真实的类似物结构。"
      },
      {
        "paper_id": "2602.16682",
        "title": "Learning Situated Awareness in the Real World",
        "abstract": "A core aspect of human perception is situated awareness, the ability to relate ourselves to the surrounding physical environment and reason over possible actions in context. However, most existing benchmarks for multimodal foundation models (MFMs) emphasize environment-centric spatial relations (relations among objects in a scene), while largely overlooking observer-centric relationships that require reasoning relative to agent's viewpoint, pose, and motion. To bridge this gap, we introduce SAW-Bench (Situated Awareness in the Real World), a novel benchmark for evaluating egocentric situated awareness using real-world videos. SAW-Bench comprises 786 self-recorded videos captured with Ray-Ban Meta (Gen 2) smart glasses spanning diverse indoor and outdoor environments, and over 2,071 human-annotated question-answer pairs. It probes a model's observer-centric understanding with six different awareness tasks. Our comprehensive evaluation reveals a human-model performance gap of 37.66%, even with the best-performing MFM, Gemini 3 Flash. Beyond this gap, our in-depth analysis uncovers several notable findings; for example, while models can exploit partial geometric cues in egocentric videos, they often fail to infer a coherent camera geometry, leading to systematic spatial reasoning errors. We position SAW-Bench as a benchmark for situated spatial intelligence, moving beyond passive observation to understanding physically grounded, observer-centric dynamics.",
        "authors_display": "Xin Eric Wang Team",
        "pdf_url": "http://arxiv.org/abs/2602.16682",
        "code_url": null,
        "date": "2026-02-18",
        "primary_category": "cs.CV",
        "chinese_summary": "多模态基础模型（MFMs）现有基准多侧重环境中心空间关系，忽视了以观察者为中心的、需推理智能体视角和动作的情境感知。为填补此空白，研究者引入了SAW-Bench，一个基于Ray-Ban Meta智能眼镜录制真实世界视频的新型基准，用于评估自我中心情境感知。该基准包含786个视频和2071个问答对，通过六项感知任务探测模型的观察者中心理解。评估显示，即使是最佳MFM，与人类仍有显著差距，模型常未能推断出连贯的相机几何导致空间推理错误，突显了超越被动观察、理解以观察者为中心的物理动态的重要性。"
      },
      {
        "paper_id": "2602.16681",
        "title": "VETime: Vision Enhanced Zero-Shot Time Series Anomaly Detection",
        "abstract": "Time-series anomaly detection (TSAD) requires identifying both immediate Point Anomalies and long-range Context Anomalies. However, existing foundation models face a fundamental trade-off: 1D temporal models provide fine-grained pointwise localization but lack a global contextual perspective, while 2D vision-based models capture global patterns but suffer from information bottlenecks due to a lack of temporal alignment and coarse-grained pointwise detection. To resolve this dilemma, we propose VETime, the first TSAD framework that unifies temporal and visual modalities through fine-grained visual-temporal alignment and dynamic fusion. VETime introduces a Reversible Image Conversion and a Patch-Level Temporal Alignment module to establish a shared visual-temporal timeline, preserving discriminative details while maintaining temporal sensitivity. Furthermore, we design an Anomaly Window Contrastive Learning mechanism and a Task-Adaptive Multi-Modal Fusion to adaptively integrate the complementary perceptual strengths of both modalities. Extensive experiments demonstrate that VETime significantly outperforms state-of-the-art models in zero-shot scenarios, achieving superior localization precision with lower computational overhead than current vision-based approaches. Code available at: https://github.com/yyyangcoder/VETime.",
        "authors_display": "Chen Zhang Team",
        "pdf_url": "http://arxiv.org/abs/2602.16681",
        "code_url": null,
        "date": "2026-02-18",
        "primary_category": "cs.CV",
        "chinese_summary": "时间序列异常检测（TSAD）需要识别点异常和上下文异常，但现有基础模型在精细点定位和全局上下文理解之间存在权衡。为解决这一困境，本文提出了VETime，首个通过细粒度视觉-时间对齐和动态融合统一时间与视觉模态的TSAD框架。VETime引入可逆图像转换和补丁级时间对齐模块，以建立共享的视觉-时间轴并保留细节。此外，它设计了异常窗口对比学习和任务自适应多模态融合机制。大量实验表明，VETime在零样本场景中显著优于先进模型，实现了更高的定位精度和更低的计算开销。"
      },
      {
        "paper_id": "2602.16675",
        "title": "Learning to unfold cloth: Scaling up world models to deformable object manipulation",
        "abstract": "Learning to manipulate cloth is both a paradigmatic problem for robotic research and a problem of immediate relevance to a variety of applications ranging from assistive care to the service industry. The complex physics of the deformable object makes this problem of cloth manipulation nontrivial. In order to create a general manipulation strategy that addresses a variety of shapes, sizes, fold and wrinkle patterns, in addition to the usual problems of appearance variations, it becomes important to carefully consider model structure and their implications for generalisation performance. In this paper, we present an approach to in-air cloth manipulation that uses a variation of a recently proposed reinforcement learning architecture, DreamerV2. Our implementation modifies this architecture to utilise surface normals input, in addition to modiying the replay buffer and data augmentation procedures. Taken together these modifications represent an enhancement to the world model used by the robot, addressing the physical complexity of the object being manipulated by the robot. We present evaluations both in simulation and in a zero-shot deployment of the trained policies in a physical robot setup, performing in-air unfolding of a variety of different cloth types, demonstrating the generalisation benefits of our proposed architecture.",
        "authors_display": "Subramanian Ramamoorthy Team",
        "pdf_url": "http://arxiv.org/abs/2602.16675",
        "code_url": null,
        "date": "2026-02-18",
        "primary_category": "cs.RO",
        "chinese_summary": "机器人布料操作因其复杂的物理特性而极具挑战性，需要通用策略以适应不同布料特性。本文提出一种改进的DreamerV2强化学习架构应用于空中布料操作，通过引入表面法线作为输入，并优化回放缓冲区及数据增强程序，增强了机器人使用的世界模型来应对物理复杂性。在仿真和物理机器人的零样本部署实验中，该方法成功实现了多种布料的空中展开，证明了所提出架构在泛化性能上的显著优势。"
      },
      {
        "paper_id": "2602.16626",
        "title": "A Systematic Evaluation of Sample-Level Tokenization Strategies for MEG Foundation Models",
        "abstract": "Recent success in natural language processing has motivated growing interest in large-scale foundation models for neuroimaging data. Such models often require discretization of continuous neural time series data, a process referred to as 'tokenization'. However, the impact of different tokenization strategies for neural data is currently poorly understood. In this work, we present a systematic evaluation of sample-level tokenization strategies for transformer-based large neuroimaging models (LNMs) applied to magnetoencephalography (MEG) data. We compare learnable and non-learnable tokenizers by examining their signal reconstruction fidelity and their impact on subsequent foundation modeling performance (token prediction, biological plausibility of generated data, preservation of subject-specific information, and performance on downstream tasks). For the learnable tokenizer, we introduce a novel approach based on an autoencoder. Experiments were conducted on three publicly available MEG datasets spanning different acquisition sites, scanners, and experimental paradigms. Our results show that both learnable and non-learnable discretization schemes achieve high reconstruction accuracy and broadly comparable performance across most evaluation criteria, suggesting that simple fixed sample-level tokenization strategies can be used in the development of neural foundation models. The code is available at https://github.com/OHBA-analysis/Cho2026_Tokenizer.",
        "authors_display": "Mark W. Woolrich Team",
        "pdf_url": "http://arxiv.org/abs/2602.16626",
        "code_url": null,
        "date": "2026-02-18",
        "primary_category": "cs.LG",
        "chinese_summary": "神经影像数据的基础模型需将连续神经时间序列数据“令牌化”，但不同令牌化策略的影响尚不明确。本文对应用于脑磁图（MEG）数据的基于Transformer的大型神经影像模型（LNMs）的样本级令牌化策略进行了系统评估。通过比较可学习（引入基于自编码器的新方法）和不可学习令牌器在信号重建保真度、基础建模性能及下游任务上的表现，研究发现在多个MEG数据集上，两者均实现了高重建精度和大致相当的性能，表明简单的固定样本级令牌化策略足以支持神经基础模型的开发。"
      },
      {
        "paper_id": "2602.16587",
        "title": "Why Thinking Hurts? Diagnosing and Rectifying the Reasoning Shift in Foundation Recommender Models",
        "abstract": "Integrating Chain-of-Thought (CoT) reasoning into Semantic ID-based recommendation foundation models (such as OpenOneRec) often paradoxically degrades recommendation performance. We identify the root cause as textual inertia from the General Subspace, where verbose reasoning dominates inference and causes the model to neglect critical Semantic ID. To address this, we propose a training-free Inference-Time Subspace Alignment framework. By compressing reasoning chains and applying bias-subtracted contrastive decoding, our approach mitigates ungrounded textual drift. Experiments show this effectively calibrates inference, allowing foundation models to leverage reasoning without sacrificing ID-grounded accuracy.",
        "authors_display": "Enhong Chen Team",
        "pdf_url": "http://arxiv.org/abs/2602.16587",
        "code_url": null,
        "date": "2026-02-18",
        "primary_category": "cs.IR",
        "chinese_summary": "将思维链（CoT）推理整合到语义ID推荐基础模型中，常导致推荐性能下降，究其原因在于“通用子空间”中冗长推理的文本惯性，使得模型忽视关键语义ID。为解决此问题，本文提出一个训练无关的“推理时子空间对齐”框架。该方法通过压缩推理链和应用偏差减去的对比解码，有效缓解了无根据的文本漂移。实验证明，此框架能有效校准推理过程，使基础模型在利用推理能力的同时，不牺牲基于ID的推荐准确性。"
      },
      {
        "paper_id": "2602.16569",
        "title": "Arc2Morph: Identity-Preserving Facial Morphing with Arc2Face",
        "abstract": "Face morphing attacks are widely recognized as one of the most challenging threats to face recognition systems used in electronic identity documents. These attacks exploit a critical vulnerability in passport enrollment procedures adopted by many countries, where the facial image is often acquired without a supervised live capture process. In this paper, we propose a novel face morphing technique based on Arc2Face, an identity-conditioned face foundation model capable of synthesizing photorealistic facial images from compact identity representations. We demonstrate the effectiveness of the proposed approach by comparing the morphing attack potential metric on two large-scale sequestered face morphing attack detection datasets against several state-of-the-art morphing methods, as well as on two novel morphed face datasets derived from FEI and ONOT. Experimental results show that the proposed deep learning-based approach achieves a morphing attack potential comparable to that of landmark-based techniques, which have traditionally been regarded as the most challenging. These findings confirm the ability of the proposed method to effectively preserve and manage identity information during the morph generation process.",
        "authors_display": "Davide Maltoni Team",
        "pdf_url": "http://arxiv.org/abs/2602.16569",
        "code_url": null,
        "date": "2026-02-18",
        "primary_category": "cs.CV",
        "chinese_summary": "人脸融合攻击对电子身份文档中的人脸识别系统构成严峻威胁，尤其是在护照注册过程中缺乏实时监督采集时。本文提出一种基于身份条件化人脸基础模型Arc2Face的新型人脸融合技术，能够从紧凑的身份表示合成逼真的人脸图像。通过在多个大规模数据集上与现有先进方法进行比较，实验结果表明，所提出的深度学习方法在融合攻击潜力方面达到了与传统上最具挑战性的基于地标技术相当的水平，证实了其在融合生成过程中有效保留和管理身份信息的能力。"
      },
      {
        "paper_id": "2602.16493",
        "title": "MMA: Multimodal Memory Agent",
        "abstract": "Long-horizon multimodal agents depend on external memory; however, similarity-based retrieval often surfaces stale, low-credibility, or conflicting items, which can trigger overconfident errors. We propose Multimodal Memory Agent (MMA), which assigns each retrieved memory item a dynamic reliability score by combining source credibility, temporal decay, and conflict-aware network consensus, and uses this signal to reweight evidence and abstain when support is insufficient. We also introduce MMA-Bench, a programmatically generated benchmark for belief dynamics with controlled speaker reliability and structured text-vision contradictions. Using this framework, we uncover the \"Visual Placebo Effect\", revealing how RAG-based agents inherit latent visual biases from foundation models. On FEVER, MMA matches baseline accuracy while reducing variance by 35.2% and improving selective utility; on LoCoMo, a safety-oriented configuration improves actionable accuracy and reduces wrong answers; on MMA-Bench, MMA reaches 41.18% Type-B accuracy in Vision mode, while the baseline collapses to 0.0% under the same protocol. Code: https://github.com/AIGeeksGroup/MMA.",
        "authors_display": "Hao Tang Team",
        "pdf_url": "http://arxiv.org/abs/2602.16493",
        "code_url": null,
        "date": "2026-02-18",
        "primary_category": "cs.CV",
        "chinese_summary": "长时程多模态智能体在依赖外部记忆时，常因检索到过时、低可信或冲突信息而产生过度自信的错误。针对此问题，本文提出了多模态记忆智能体（MMA），该智能体结合来源可信度、时间衰减和冲突感知网络共识，动态评估每个记忆项的可靠性，并利用此信号加权证据或在支持不足时弃权。同时，引入了MMA-Bench基准来研究信念动态。实验结果表明，在FEVER数据集上，MMA在保持基线准确率的同时，将方差降低了35.2%；在LoCoMo上，提高了操作准确性并减少了错误答案；在MMA-Bench上，视觉模式下MMA的准确率显著优于基线，并揭示了RAG智能体中潜在的“视觉安慰剂效应”。"
      },
      {
        "paper_id": "2602.16444",
        "title": "RoboGene: Boosting VLA Pre-training via Diversity-Driven Agentic Framework for Real-World Task Generation",
        "abstract": "The pursuit of general-purpose robotic manipulation is hindered by the scarcity of diverse, real-world interaction data. Unlike data collection from web in vision or language, robotic data collection is an active process incurring prohibitive physical costs. Consequently, automated task curation to maximize data value remains a critical yet under-explored challenge. Existing manual methods are unscalable and biased toward common tasks, while off-the-shelf foundation models often hallucinate physically infeasible instructions. To address this, we introduce RoboGene, an agentic framework designed to automate the generation of diverse, physically plausible manipulation tasks across single-arm, dual-arm, and mobile robots. RoboGene integrates three core components: diversity-driven sampling for broad task coverage, self-reflection mechanisms to enforce physical constraints, and human-in-the-loop refinement for continuous improvement. We conduct extensive quantitative analysis and large-scale real-world experiments, collecting datasets of 18k trajectories and introducing novel metrics to assess task quality, feasibility, and diversity. Results demonstrate that RoboGene significantly outperforms state-of-the-art foundation models (e.g., GPT-4o, Gemini 2.5 Pro). Furthermore, real-world experiments show that VLA models pre-trained with RoboGene achieve higher success rates and superior generalization, underscoring the importance of high-quality task generation. Our project is available at https://robogene-boost-vla.github.io.",
        "authors_display": "Jian Tang Team",
        "pdf_url": "http://arxiv.org/abs/2602.16444",
        "code_url": null,
        "date": "2026-02-18",
        "primary_category": "cs.RO",
        "chinese_summary": "通用机器人操作受限于稀缺且成本高昂的真实世界交互数据，且现有任务策划方法不可扩展或易产生不可行指令。为解决这一问题，本文提出了RoboGene框架，旨在自动化生成多样化且物理上可行的单臂、双臂和移动机器人操作任务。RoboGene包含多样性驱动采样、自反思机制以强制物理约束以及人机协作精炼等核心组件。实验结果表明，RoboGene在定量分析和大规模真实世界实验中显著优于现有基础模型，且使用RoboGene预训练的VLA模型展现出更高的成功率和泛化能力，强调了高质量任务生成的重要性。"
      },
      {
        "paper_id": "2602.16422",
        "title": "Automated Histopathology Report Generation via Pyramidal Feature Extraction and the UNI Foundation Model",
        "abstract": "Generating diagnostic text from histopathology whole slide images (WSIs) is challenging due to the gigapixel scale of the input and the requirement for precise, domain specific language. We propose a hierarchical vision language framework that combines a frozen pathology foundation model with a Transformer decoder for report generation. To make WSI processing tractable, we perform multi resolution pyramidal patch selection (downsampling factors 2^3 to 2^6) and remove background and artifacts using Laplacian variance and HSV based criteria. Patch features are extracted with the UNI Vision Transformer and projected to a 6 layer Transformer decoder that generates diagnostic text via cross attention. To better represent biomedical terminology, we tokenize the output using BioGPT. Finally, we add a retrieval based verification step that compares generated reports with a reference corpus using Sentence BERT embeddings; if a high similarity match is found, the generated report is replaced with the retrieved ground truth reference to improve reliability.",
        "authors_display": "Serkan Sokmen Team",
        "pdf_url": "http://arxiv.org/abs/2602.16422",
        "code_url": null,
        "date": "2026-02-18",
        "primary_category": "eess.IV",
        "chinese_summary": "从千兆像素级的组织病理学全玻片图像（WSIs）生成精确的诊断文本面临巨大挑战。本文提出了一个分层视觉语言框架，将一个冻结的病理学基础模型与Transformer解码器相结合进行报告生成。该方法通过多分辨率金字塔补丁选择和背景伪影去除技术处理WSI，并利用UNI Vision Transformer提取特征，随后由Transformer解码器生成文本，并使用BioGPT进行分词。为提高可靠性，该框架还引入了一个基于检索的验证步骤，通过比较生成报告与参考语料库来修正报告内容。"
      },
      {
        "paper_id": "2602.16317",
        "title": "CADEvolve: Creating Realistic CAD via Program Evolution",
        "abstract": "Computer-Aided Design (CAD) delivers rapid, editable modeling for engineering and manufacturing. Recent AI progress now makes full automation feasible for various CAD tasks. However, progress is bottlenecked by data: public corpora mostly contain sketch-extrude sequences, lack complex operations, multi-operation composition and design intent, and thus hinder effective fine-tuning. Attempts to bypass this with frozen VLMs often yield simple or invalid programs due to limited 3D grounding in current foundation models. We present CADEvolve, an evolution-based pipeline and dataset that starts from simple primitives and, via VLM-guided edits and validations, incrementally grows CAD programs toward industrial-grade complexity. The result is 8k complex parts expressed as executable CadQuery parametric generators. After multi-stage post-processing and augmentation, we obtain a unified dataset of 1.3m scripts paired with rendered geometry and exercising the full CadQuery operation set. A VLM fine-tuned on CADEvolve achieves state-of-the-art results on the Image2CAD task across the DeepCAD, Fusion 360, and MCB benchmarks.",
        "authors_display": "Dmitrii Zhemchuzhnikov Team",
        "pdf_url": "http://arxiv.org/abs/2602.16317",
        "code_url": null,
        "date": "2026-02-18",
        "primary_category": "cs.GR",
        "chinese_summary": "计算机辅助设计（CAD）的AI自动化受限于缺乏复杂操作和设计意图的数据集，导致现有方法难以生成工业级程序。为此，本文提出了CADEvolve，一个基于进化的管道和数据集，它从简单基元开始，通过VLM引导的编辑和验证，逐步生成工业级复杂性的CAD程序。该方法生成了8k个复杂零件作为可执行的CadQuery参数化生成器，并经过处理和增强后，形成了一个包含130万个脚本的统一数据集。实验结果表明，在CADEvolve上微调的VLM在DeepCAD、Fusion 360和MCB基准测试的Image2CAD任务上均达到了最先进的性能。"
      },
      {
        "paper_id": "2602.16249",
        "title": "AFFMAE: Scalable and Efficient Vision Pretraining for Desktop Graphics Cards",
        "abstract": "Self-supervised pretraining has transformed computer vision by enabling data-efficient fine-tuning, yet high-resolution training typically requires server-scale infrastructure, limiting in-domain foundation model development for many research laboratories. Masked Autoencoders (MAE) reduce computation by encoding only visible tokens, but combining MAE with hierarchical downsampling architectures remains structurally challenging due to dense grid priors and mask-aware design compromises. We introduce AFFMAE, a masking-friendly hierarchical pretraining framework built on adaptive, off-grid token merging. By discarding masked tokens and performing dynamic merging exclusively over visible tokens, AFFMAE removes dense-grid assumptions while preserving hierarchical scalability. We developed numerically stable mixed-precision Flash-style cluster attention kernels, and mitigate sparse-stage representation collapse via deep supervision. On high-resolution electron microscopy segmentation, AFFMAE matches ViT-MAE performance at equal parameter count while reducing FLOPs by up to 7x, halving memory usage, and achieving faster training on a single RTX 5090. Code available at https://github.com/najafian-lab/affmae.",
        "authors_display": "Behzad Najafian Team",
        "pdf_url": "http://arxiv.org/abs/2602.16249",
        "code_url": null,
        "date": "2026-02-18",
        "primary_category": "cs.CV",
        "chinese_summary": "自监督预训练虽提高了计算机视觉效率，但高分辨率训练仍需服务器级基础设施，限制了基础模型的开发。传统MAE与分层架构结合时面临密集网格和掩码感知设计挑战。本文提出了AFFMAE，一个基于自适应、非网格token合并的掩码友好型分层预训练框架，通过丢弃被掩码的token并仅对可见token执行动态合并，消除了密集网格假设并保持了分层可扩展性。实验结果表明，在相同参数量下，AFFMAE在高分辨率电子显微镜分割任务上匹配了ViT-MAE的性能，同时将FLOPs减少高达7倍，内存使用减半，并在单个RTX 5090上实现了更快的训练。"
      },
      {
        "paper_id": "2602.16238",
        "title": "EasyControlEdge: A Foundation-Model Fine-Tuning for Edge Detection",
        "abstract": "We propose EasyControlEdge, adapting an image-generation foundation model to edge detection. In real-world edge detection (e.g., floor-plan walls, satellite roads/buildings, and medical organ boundaries), crispness and data efficiency are crucial, yet producing crisp raw edge maps with limited training samples remains challenging. Although image-generation foundation models perform well on many downstream tasks, their pretrained priors for data-efficient transfer and iterative refinement for high-frequency detail preservation remain underexploited for edge detection. To enable crisp and data-efficient edge detection using these capabilities, we introduce an edge-specialized adaptation of image-generation foundation models. To better specialize the foundation model for edge detection, we incorporate an edge-oriented objective with an efficient pixel-space loss. At inference, we introduce guidance based on unconditional dynamics, enabling a single model to control the edge density through a guidance scale. Experiments on BSDS500, NYUDv2, BIPED, and CubiCasa compare against state-of-the-art methods and show consistent gains, particularly under no-post-processing crispness evaluation and with limited training data.",
        "authors_display": "Tadahiro Taniguchi Team",
        "pdf_url": "http://arxiv.org/abs/2602.16238",
        "code_url": null,
        "date": "2026-02-18",
        "primary_category": "cs.CV",
        "chinese_summary": "在实际边缘检测中，清晰度和数据效率至关重要，但用有限数据生成清晰边缘图极具挑战，且图像生成基础模型在边缘检测领域的潜力尚未充分挖掘。本文提出了EasyControlEdge，旨在将图像生成基础模型适应于边缘检测任务，以实现高清晰度和数据高效性。该方法通过引入边缘导向目标和高效像素空间损失来专门化基础模型，并在推理时利用基于无条件动力学的引导，实现通过引导尺度控制边缘密度。实验结果显示，EasyControlEdge在多个基准测试中持续优于现有方法，特别是在无后处理清晰度评估和有限训练数据条件下表现突出。"
      },
      {
        "paper_id": "2602.16229",
        "title": "Factored Latent Action World Models",
        "abstract": "Learning latent actions from action-free video has emerged as a powerful paradigm for scaling up controllable world model learning. Latent actions provide a natural interface for users to iteratively generate and manipulate videos. However, most existing approaches rely on monolithic inverse and forward dynamics models that learn a single latent action to control the entire scene, and therefore struggle in complex environments where multiple entities act simultaneously. This paper introduces Factored Latent Action Model (FLAM), a factored dynamics framework that decomposes the scene into independent factors, each inferring its own latent action and predicting its own next-step factor value. This factorized structure enables more accurate modeling of complex multi-entity dynamics and improves video generation quality in action-free video settings compared to monolithic models. Based on experiments on both simulation and real-world multi-entity datasets, we find that FLAM outperforms prior work in prediction accuracy and representation quality, and facilitates downstream policy learning, demonstrating the benefits of factorized latent action models.",
        "authors_display": "Peter Stone Team",
        "pdf_url": "http://arxiv.org/abs/2602.16229",
        "code_url": null,
        "date": "2026-02-18",
        "primary_category": "cs.LG",
        "chinese_summary": "从无动作视频中学习潜在动作是构建可控世界模型的强大范式，但现有方法通常依赖单一动力学模型控制整个场景，难以应对多实体复杂环境。为解决此问题，本文提出了因子化潜在动作模型（FLAM），该框架将场景分解为独立因子，每个因子推断其自身潜在动作并预测下一时刻状态。这种因子化结构能够更准确地建模复杂的多实体动力学。实验结果表明，FLAM在模拟和真实世界多实体数据集上的预测准确性和表示质量均优于现有方法，改善了视频生成质量，并促进了下游策略学习。"
      },
      {
        "paper_id": "2602.16182",
        "title": "World Model Failure Classification and Anomaly Detection for Autonomous Inspection",
        "abstract": "Autonomous inspection robots for monitoring industrial sites can reduce costs and risks associated with human-led inspection. However, accurate readings can be challenging due to occlusions, limited viewpoints, or unexpected environmental conditions. We propose a hybrid framework that combines supervised failure classification with anomaly detection, enabling classification of inspection tasks as a success, known failure, or anomaly (i.e., out-of-distribution) case. Our approach uses a world model backbone with compressed video inputs. This policy-agnostic, distribution-free framework determines classifications based on two decision functions set by conformal prediction (CP) thresholds before a human observer does. We evaluate the framework on gauge inspection feeds collected from office and industrial sites and demonstrate real-time deployment on a Boston Dynamics Spot. Experiments show over 90% accuracy in distinguishing between successes, failures, and OOD cases, with classifications occurring earlier than a human observer. These results highlight the potential for robust, anticipatory failure detection in autonomous inspection tasks or as a feedback signal for model training to assess and improve the quality of training data. Project website: https://autoinspection-classification.github.io",
        "authors_display": "Shayegan Omidshafiei Team",
        "pdf_url": "http://arxiv.org/abs/2602.16182",
        "code_url": null,
        "date": "2026-02-18",
        "primary_category": "cs.RO",
        "chinese_summary": "自主检查机器人可降低工业现场监测的成本与风险，但遮挡、视角受限等问题使准确读数充满挑战。本文提出了一个混合框架，结合监督式故障分类与异常检测，将检查任务分类为成功、已知故障或异常情况。该方法以带有压缩视频输入的世界模型为骨干，并通过共形预测阈值确定的两个决策函数在人类观察者之前进行分类。在办公室和工业现场仪表检查上的实验结果显示，该框架在区分成功、故障和OOD情况方面准确率超过90%，且分类发生时间早于人类观察者，展现了其在自主检查中实现鲁棒、预见性故障检测的潜力。"
      },
      {
        "paper_id": "2602.16951",
        "title": "BrainRVQ: A High-Fidelity EEG Foundation Model via Dual-Domain Residual Quantization and Hierarchical Autoregression",
        "abstract": "Developing foundation models for electroencephalography (EEG) remains challenging due to the signal's low signal-to-noise ratio and complex spectro-temporal non-stationarity. Existing approaches often overlook the hierarchical latent structure inherent in neural dynamics, leading to suboptimal reconstruction of fine-grained information. In this work, we propose BrainRVQ, a general-purpose EEG foundation model pre-trained on a large-scale corpus of clinical EEG data. Unlike standard masked modeling, BrainRVQ features a Dual-Domain Residual Vector Quantization (DD-RVQ) tokenizer that disentangles temporal waveforms and spectral patterns into hierarchical discrete codes. We further introduce a hierarchical autoregressive pre-training objective that learns to reconstruct these codes in a coarse-to-fine manner, utilizing an importance-guided curriculum masking strategy to prioritize information-rich neural events over background noise. Extensive experiments across 8 diverse downstream datasets demonstrate that BrainRVQ consistently outperforms state-of-the-art baselines, validating its effectiveness in learning robust and generalizable neural representations. Our code and model weights are available:https://github.com/keqicmz/BrainRVQ",
        "authors_display": "Luca Mainardi Team",
        "pdf_url": "http://arxiv.org/abs/2602.16951",
        "code_url": null,
        "date": "2026-02-18",
        "primary_category": "eess.SP",
        "chinese_summary": "由于低信噪比和复杂的时频谱非平稳性，开发脑电图（EEG）基础模型并重建精细信息仍面临挑战。为此，本研究提出了BrainRVQ，一个通用的EEG基础模型，在大规模临床EEG数据上预训练。该模型采用双域残差向量量化（DD-RVQ）分词器将时域波形和频谱模式解耦为分层离散代码，并通过分层自回归预训练目标和“重要性引导的课程掩码策略”从粗到细地重建这些代码。在8个下游数据集上的广泛实验表明，BrainRVQ持续优于现有先进基线，验证了其学习鲁棒和可泛化神经表示的有效性。"
      },
      {
        "paper_id": "2602.16949",
        "title": "How should AI knowledge be governed? Epistemic authority, structural transparency, and the case for open cognitive graphs",
        "abstract": "Through widespread use in formative assessment and self-directed learning, educational AI systems exercise de facto epistemic authority. Unlike human educators, however, these systems are not embedded in institutional mechanisms of accountability, review, and correction, creating a structural governance challenge that cannot be resolved through application-level regulation or model transparency alone. This paper reconceptualizes educational AI as public educational cognitive infrastructure and argues that its governance must address the epistemic authority such systems exert. We propose the Open Cognitive Graph (OCG) as a technical interface that externalizes pedagogical structure in forms aligned with human educational reasoning. By explicitly representing concepts, prerequisite relations, misconceptions, and scaffolding, OCGs make the cognitive logic governing AI behaviour inspectable and revisable. Building on this foundation, we introduce the trunk-branch governance model, which organizes epistemic authority across layers of consensus and pluralism. A case study of a community-governed educational foundation model demonstrates how distributed expertise can be integrated through institutionalized processes of validation, correction, and propagation. The paper concludes by discussing implications for educational equity, AI policy, and sustainability. By shifting attention from access to governance conditions, the proposed framework offers a structural approach to aligning educational AI with democratic accountability and public responsibility.",
        "authors_display": "Yi Hua Team",
        "pdf_url": "http://arxiv.org/abs/2602.16949",
        "code_url": null,
        "date": "2026-02-18",
        "primary_category": "cs.CY",
        "chinese_summary": "教育AI系统在实际应用中拥有认知权威，但缺乏类似人类教育者的制度化问责机制，这构成了结构性治理难题。本研究将教育AI重新定义为公共教育认知基础设施，并提出了“开放认知图谱（OCG）”作为技术接口，通过显式表示概念、前提、错误认知和支架，使AI行为的认知逻辑可被审查和修改。在此基础上，研究引入了“主干-分支”治理模型来组织认知权威，并通过社区治理的教育基础模型案例研究，展示了如何通过制度化流程整合分布式专业知识，从而实现教育AI与民主问责制的对齐。"
      },
      {
        "paper_id": "2602.16926",
        "title": "BEMEval-Doc2Schema: Benchmarking Large Language Models for Structured Data Extraction in Building Energy Modeling",
        "abstract": "Recent advances in foundation models, including large language models (LLMs), have created new opportunities to automate building energy modeling (BEM). However, systematic evaluation has remained challenging due to the absence of publicly available, task-specific datasets and standardized performance metrics. We present BEMEval, a benchmark framework designed to assess foundation models' performance across BEM tasks. The first benchmark in this suite, BEMEval-Doc2Schema, focuses on structured data extraction from building documentation, a foundational step toward automated BEM processes. BEMEval-Doc2Schema introduces the Key-Value Overlap Rate (KVOR), a metric that quantifies the alignment between LLM-generated structured outputs and ground-truth schema references. Using this framework, we evaluate two leading models (GPT-5 and Gemini 2.5) under zero-shot and few-shot prompting strategies across three datasets: HERS L100, NREL iUnit, and NIST NZERTF. Results show that Gemini 2.5 consistently outperforms GPT-5, and that few-shot prompts improve accuracy for both models. Performance also varies by schema: the EPC schema yields significantly higher KVOR scores than HPXML, reflecting its simpler and reduced hierarchical depth. By combining curated datasets, reproducible metrics, and cross-model comparisons, BEMEval-Doc2Schema establishes the first community-driven benchmark for evaluating LLMs in performing building energy modeling tasks, laying the groundwork for future research on AI-assisted BEM workflows.",
        "authors_display": "Liang Zhang Team",
        "pdf_url": "http://arxiv.org/abs/2602.16926",
        "code_url": null,
        "date": "2026-02-18",
        "primary_category": "cs.CE",
        "chinese_summary": "大语言模型（LLM）等基础模型为自动化建筑能耗建模（BEM）提供了新机遇，但缺乏公开数据集和标准化评估指标限制了系统性评估。本研究提出了BEMEval基准框架，其中BEMEval-Doc2Schema专注于从建筑文档中提取结构化数据，并引入“键值重叠率（KVOR）”指标以量化模型输出与真实图式的一致性。利用该框架，在三个数据集上评估了GPT-5和Gemini 2.5，结果显示Gemini 2.5持续优于GPT-5，且少样本提示均能提高准确性，同时简单图式表现更佳。该研究为AI辅助BEM工作流奠定了首个社区驱动的评估基准。"
      },
      {
        "paper_id": "2602.16921",
        "title": "Beyond the Flag: A Framework for Integrating Cybersecurity Competitions into K-12 Education for Cognitive Apprenticeship and Ethical Skill Development",
        "abstract": "Capture the Flag (CTF) competitions are powerful pedagogical tools for addressing the global cybersecurity workforce gap, yet their effective K-12 implementation is often undermined by significant barriers, including educator preparedness gaps and equity concerns. This paper addresses these challenges by proposing the Ethical-Cognitive Apprenticeship in Cybersecurity (ECAC) framework, a new model derived from a systematic Framework Synthesis of existing literature and empirical evidence. ECAC systematically integrates cognitive apprenticeship theory with embedded ethical development across five phases: (1) Foundational Modeling, (2) Scaffolding the Arena, (3) Coaching and Articulation, (4) Ethical Dilemma Injections, and (5) Reflective Exploration. The framework provides a \"low floor, high ceiling\" learning pathway designed to broaden participation among diverse student groups, including underrepresented minorities and women, while fostering deep, transferable skills. By reframing the educator role as a lead learner,\" ECAC also offers a sustainable solution to the teacher expertise gap. Ultimately, this framework provides a practical roadmap for transforming CTFs from standalone competitions into integral learning experiences that cultivate a more skilled, ethical, and diverse generation of cybersecurity professionals.",
        "authors_display": "Nam Son Nguyen Team",
        "pdf_url": "http://arxiv.org/abs/2602.16921",
        "code_url": null,
        "date": "2026-02-18",
        "primary_category": "cs.CY",
        "chinese_summary": "夺旗（CTF）竞赛是弥补网络安全人才缺口的有效教学工具，但在K-12教育中面临教师准备不足和公平性等实施障碍。本研究提出了“网络安全伦理认知学徒（ECAC）”框架，该模型整合了认知学徒理论与嵌入式伦理发展，分为五个阶段，旨在提供“低门槛，高上限”的学习路径，以扩大包括少数族裔和女性在内的不同学生群体的参与。通过将教育者重新定义为“首席学习者”，ECAC还提供了解决教师专业知识差距的可持续方案，旨在将CTF转变为培养更专业、更具伦理素养和更多样化网络安全人才的综合学习体验。"
      },
      {
        "paper_id": "2602.16915",
        "title": "StereoAdapter-2: Globally Structure-Consistent Underwater Stereo Depth Estimation",
        "abstract": "Stereo depth estimation is fundamental to underwater robotic perception, yet suffers from severe domain shifts caused by wavelength-dependent light attenuation, scattering, and refraction. Recent approaches leverage monocular foundation models with GRU-based iterative refinement for underwater adaptation; however, the sequential gating and local convolutional kernels in GRUs necessitate multiple iterations for long-range disparity propagation, limiting performance in large-disparity and textureless underwater regions. In this paper, we propose StereoAdapter-2, which replaces the conventional ConvGRU updater with a novel ConvSS2D operator based on selective state space models. The proposed operator employs a four-directional scanning strategy that naturally aligns with epipolar geometry while capturing vertical structural consistency, enabling efficient long-range spatial propagation within a single update step at linear computational complexity. Furthermore, we construct UW-StereoDepth-80K, a large-scale synthetic underwater stereo dataset featuring diverse baselines, attenuation coefficients, and scattering parameters through a two-stage generative pipeline combining semantic-aware style transfer and geometry-consistent novel view synthesis. Combined with dynamic LoRA adaptation inherited from StereoAdapter, our framework achieves state-of-the-art zero-shot performance on underwater benchmarks with 17% improvement on TartanAir-UW and 7.2% improvment on SQUID, with real-world validation on the BlueROV2 platform demonstrates the robustness of our approach. Code: https://github.com/AIGeeksGroup/StereoAdapter-2. Website: https://aigeeksgroup.github.io/StereoAdapter-2.",
        "authors_display": "Hao Tang Team",
        "pdf_url": "http://arxiv.org/abs/2602.16915",
        "code_url": null,
        "date": "2026-02-18",
        "primary_category": "cs.CV",
        "chinese_summary": "水下立体深度估计受光衰减、散射和折射导致的严重域偏移影响，现有GRU迭代精修方法在处理大视差和无纹理区域时性能受限。本研究提出了StereoAdapter-2，将传统ConvGRU更新器替换为基于选择性状态空间模型的新型ConvSS2D算子，该算子通过四向扫描策略与极线几何对齐，从而在一个更新步骤内以线性计算复杂度实现高效长距离空间传播。此外，研究构建了大规模合成水下立体数据集UW-StereoDepth-80K。结合动态LoRA适应，该框架在TartanAir-UW和SQUID基准测试中实现了17%和7.2%的性能提升，并通过真实机器人平台验证了其鲁棒性。"
      },
      {
        "paper_id": "2602.16911",
        "title": "SparTa: Sparse Graphical Task Models from a Handful of Demonstrations",
        "abstract": "Learning long-horizon manipulation tasks efficiently is a central challenge in robot learning from demonstration. Unlike recent endeavors that focus on directly learning the task in the action domain, we focus on inferring what the robot should achieve in the task, rather than how to do so. To this end, we represent evolving scene states using a series of graphical object relationships. We propose a demonstration segmentation and pooling approach that extracts a series of manipulation graphs and estimates distributions over object states across task phases. In contrast to prior graph-based methods that capture only partial interactions or short temporal windows, our approach captures complete object interactions spanning from the onset of control to the end of the manipulation. To improve robustness when learning from multiple demonstrations, we additionally perform object matching using pre-trained visual features. In extensive experiments, we evaluate our method's demonstration segmentation accuracy and the utility of learning from multiple demonstrations for finding a desired minimal task model. Finally, we deploy the fitted models both in simulation and on a real robot, demonstrating that the resulting task representations support reliable execution across environments.",
        "authors_display": "Abhinav Valada Team",
        "pdf_url": "http://arxiv.org/abs/2602.16911",
        "code_url": null,
        "date": "2026-02-18",
        "primary_category": "cs.RO",
        "chinese_summary": "机器人学习中高效学习长时程操作任务是一项核心挑战，现有方法多关注学习动作而非任务目标。本研究提出通过一系列图形化的对象关系来表示不断变化的场景状态，并引入一种演示分割和池化方法，以提取操作图并估计跨任务阶段的对象状态分布。与以往仅捕获局部交互的基于图的方法不同，本方法能捕获从控制开始到操作结束的完整对象交互，并通过使用预训练视觉特征进行对象匹配以提高学习的鲁棒性。实验证明了方法在演示分割准确性上的有效性，以及学习到任务模型支持在仿真和真实机器人上跨环境可靠执行的能力。"
      },
      {
        "paper_id": "2602.16864",
        "title": "Position: Why a Dynamical Systems Perspective is Needed to Advance Time Series Modeling",
        "abstract": "Time series (TS) modeling has come a long way from early statistical, mainly linear, approaches to the current trend in TS foundation models. With a lot of hype and industrial demand in this field, it is not always clear how much progress there really is. To advance TS forecasting and analysis to the next level, here we argue that the field needs a dynamical systems (DS) perspective. TS of observations from natural or engineered systems almost always originate from some underlying DS, and arguably access to its governing equations would yield theoretically optimal forecasts. This is the promise of DS reconstruction (DSR), a class of ML/AI approaches that aim to infer surrogate models of the underlying DS from data. But models based on DS principles offer other profound advantages: Beyond short-term forecasts, they enable to predict the long-term statistics of an observed system, which in many practical scenarios may be the more relevant quantities. DS theory furthermore provides domain-independent theoretical insight into mechanisms underlying TS generation, and thereby will inform us, e.g., about upper bounds on performance of any TS model, generalization into unseen regimes as in tipping points, or potential control strategies. After reviewing some of the central concepts, methods, measures, and models in DS theory and DSR, we will discuss how insights from this field can advance TS modeling in crucial ways, enabling better forecasting with much lower computational and memory footprints. We conclude with a number of specific suggestions for translating insights from DSR into TS modeling.",
        "authors_display": "Lukas Eisenmann Team",
        "pdf_url": "http://arxiv.org/abs/2602.16864",
        "code_url": null,
        "date": "2026-02-18",
        "primary_category": "cs.LG",
        "chinese_summary": "时间序列（TS）建模已发展至TS基础模型，但其进展尚不完全明朗，领域需要动力系统（DS）视角以实现进一步突破。本研究强调了动力系统重建（DSR）的潜力，该方法旨在从数据中推断底层动力系统模型，从而不仅能实现短期预测，还能预测系统的长期统计行为。DS理论还能提供领域无关的见解，揭示TS生成机制、性能上限、未知区域的泛化能力及潜在控制策略。本研究回顾了DS理论和DSR的核心概念，并提出了将这些见解转化为TS建模的具体建议，以期实现更优预测和更低的计算与内存开销。"
      },
      {
        "paper_id": "2602.16019",
        "title": "MedProbCLIP: Probabilistic Adaptation of Vision-Language Foundation Model for Reliable Radiograph-Report Retrieval",
        "abstract": "Vision-language foundation models have emerged as powerful general-purpose representation learners with strong potential for multimodal understanding, but their deterministic embeddings often fail to provide the reliability required for high-stakes biomedical applications. This work introduces MedProbCLIP, a probabilistic vision-language learning framework for chest X-ray and radiology report representation learning and bidirectional retrieval. MedProbCLIP models image and text representations as Gaussian embeddings through a probabilistic contrastive objective that explicitly captures uncertainty and many-to-many correspondences between radiographs and clinical narratives. A variational information bottleneck mitigates overconfident predictions, while MedProbCLIP employs multi-view radiograph encoding and multi-section report encoding during training to provide fine-grained supervision for clinically aligned correspondence, yet requires only a single radiograph and a single report at inference. Evaluated on the MIMIC-CXR dataset, MedProbCLIP outperforms deterministic and probabilistic baselines, including CLIP, CXR-CLIP, and PCME++, in both retrieval and zero-shot classification. Beyond accuracy, MedProbCLIP demonstrates superior calibration, risk-coverage behavior, selective retrieval reliability, and robustness to clinically relevant corruptions, underscoring the value of probabilistic vision-language modeling for improving the trustworthiness and safety of radiology image-text retrieval systems.",
        "authors_display": "Gongbo Liang Team",
        "pdf_url": "http://arxiv.org/abs/2602.16019",
        "code_url": null,
        "date": "2026-02-17",
        "primary_category": "cs.CV",
        "chinese_summary": "视觉-语言基础模型在多模态理解方面潜力巨大，但其确定性嵌入难以满足高风险生物医学应用对可靠性的要求。本文提出了MedProbCLIP，一个用于胸部X射线和放射学报告表示学习及双向检索的概率视觉-语言学习框架。MedProbCLIP通过概率对比目标将图像和文本表示建模为高斯嵌入，显式捕获不确定性和多对多对应关系，并利用变分信息瓶颈减轻过度自信预测。实验结果表明，在MIMIC-CXR数据集上，MedProbCLIP在检索和零样本分类方面均优于现有基线，并展现出卓越的校准性、风险-覆盖行为、选择性检索可靠性以及对临床相关损坏的鲁棒性，提升了放射学图像-文本检索系统的可信度和安全性。"
      },
      {
        "paper_id": "2602.15781",
        "title": "Neural Scaling Laws for Boosted Jet Tagging",
        "abstract": "The success of Large Language Models (LLMs) has established that scaling compute, through joint increases in model capacity and dataset size, is the primary driver of performance in modern machine learning. While machine learning has long been an integral component of High Energy Physics (HEP) data analysis workflows, the compute used to train state-of-the-art HEP models remains orders of magnitude below that of industry foundation models. With scaling laws only beginning to be studied in the field, we investigate neural scaling laws for boosted jet classification using the public JetClass dataset. We derive compute optimal scaling laws and identify an effective performance limit that can be consistently approached through increased compute. We study how data repetition, common in HEP where simulation is expensive, modifies the scaling yielding a quantifiable effective dataset size gain. We then study how the scaling coefficients and asymptotic performance limits vary with the choice of input features and particle multiplicity, demonstrating that increased compute reliably drives performance toward an asymptotic limit, and that more expressive, lower-level features can raise the performance limit and improve results at fixed dataset size.",
        "authors_display": "Lukas Heinrich Team",
        "pdf_url": "http://arxiv.org/abs/2602.15781",
        "code_url": null,
        "date": "2026-02-17",
        "primary_category": "hep-ex",
        "chinese_summary": "大型语言模型（LLMs）的成功表明计算规模扩展是性能提升的关键，然而高能物理（HEP）领域最先进模型的训练计算量远低于工业界。针对该背景，本文研究了使用公共JetClass数据集进行增压射流分类的神经网络扩展定律。研究推导了计算最优的扩展定律，并识别出一个可通过增加计算持续接近的有效性能极限。研究结果表明，增加计算能可靠地推动性能接近渐近极限，且更具表达能力的低级特征可以提高性能极限并在固定数据集大小下改善结果，同时量化了数据重复对有效数据集大小的增益影响。"
      },
      {
        "paper_id": "2602.15031",
        "title": "EditCtrl: Disentangled Local and Global Control for Real-Time Generative Video Editing",
        "abstract": "High-fidelity generative video editing has seen significant quality improvements by leveraging pre-trained video foundation models. However, their computational cost is a major bottleneck, as they are often designed to inefficiently process the full video context regardless of the inpainting mask's size, even for sparse, localized edits. In this paper, we introduce EditCtrl, an efficient video inpainting control framework that focuses computation only where it is needed. Our approach features a novel local video context module that operates solely on masked tokens, yielding a computational cost proportional to the edit size. This local-first generation is then guided by a lightweight temporal global context embedder that ensures video-wide context consistency with minimal overhead. Not only is EditCtrl 10 times more compute efficient than state-of-the-art generative editing methods, it even improves editing quality compared to methods designed with full-attention. Finally, we showcase how EditCtrl unlocks new capabilities, including multi-region editing with text prompts and autoregressive content propagation.",
        "authors_display": "Caleb Leak Team",
        "pdf_url": "http://arxiv.org/abs/2602.15031",
        "code_url": "https://yehonathanlitman.github.io/edit_ctrl",
        "date": "2026-02-16",
        "primary_category": "cs.CV",
        "chinese_summary": "背景：高保真生成式视频编辑依赖预训练视频基础模型，但其计算成本高昂，即使是局部编辑也需处理整个视频上下文。方法：本文提出EditCtrl，一个高效的视频修复控制框架。它引入了新颖的局部视频上下文模块，仅对掩码标记进行操作，使计算成本与编辑区域大小成正比。同时，一个轻量级的时间全局上下文嵌入器确保视频整体上下文的一致性。结果：EditCtrl比现有先进方法计算效率高10倍，并提升了编辑质量。该方法还支持多区域文本提示编辑和自回归内容传播。"
      },
      {
        "paper_id": "2602.15021",
        "title": "Generalization from Low- to Moderate-Resolution Spectra with Neural Networks for Stellar Parameter Estimation: A Case Study with DESI",
        "abstract": "Cross-survey generalization is a critical challenge in stellar spectral analysis, particularly in cases such as transferring from low- to moderate-resolution surveys. We investigate this problem using pre-trained models, focusing on simple neural networks such as multilayer perceptrons (MLPs), with a case study transferring from LAMOST low-resolution spectra (LRS) to DESI medium-resolution spectra (MRS). Specifically, we pre-train MLPs on either LRS or their embeddings and fine-tune them for application to DESI stellar spectra. We compare MLPs trained directly on spectra with those trained on embeddings derived from transformer-based models (self-supervised foundation models pre-trained for multiple downstream tasks). We also evaluate different fine-tuning strategies, including residual-head adapters, LoRA, and full fine-tuning. We find that MLPs pre-trained on LAMOST LRS achieve strong performance, even without fine-tuning, and that modest fine-tuning with DESI spectra further improves the results. For iron abundance, embeddings from a transformer-based model yield advantages in the metal-rich ([Fe/H] > -1.0) regime, but underperform in the metal-poor regime compared to MLPs trained directly on LRS. We also show that the optimal fine-tuning strategy depends on the specific stellar parameter under consideration. These results highlight that simple pre-trained MLPs can provide competitive cross-survey generalization, while the role of spectral foundation models for cross-survey stellar parameter estimation requires further exploration.",
        "authors_display": "Viska Wei Team",
        "pdf_url": "http://arxiv.org/abs/2602.15021",
        "code_url": null,
        "date": "2026-02-16",
        "primary_category": "astro-ph.SR",
        "chinese_summary": "背景：在恒星光谱分析中，跨巡天泛化能力（特别是从低分辨率到中分辨率光谱的迁移）是一个关键挑战。方法：本文研究了使用预训练多层感知机（MLPs）解决此问题，以LAMOST低分辨率光谱到DESI中分辨率光谱的迁移为例。作者在LAMOST低分辨率光谱或其嵌入上预训练MLPs，并在DESI光谱上进行微调，比较了直接在光谱上训练的MLPs与基于Transformer模型嵌入训练的MLPs，并评估了不同的微调策略。结果：预训练在LAMOST低分辨率光谱上的MLPs表现出色，即便不微调也能获得良好性能，适度微调可进一步提升。研究表明，简单预训练MLPs能提供有竞争力的跨巡天泛化能力，但光谱基础模型在跨巡天恒星参数估计中的作用仍需深入探索。"
      },
      {
        "paper_id": "2602.15012",
        "title": "Cold-Start Personalization via Training-Free Priors from Structured World Models",
        "abstract": "Cold-start personalization requires inferring user preferences through interaction when no user-specific historical data is available. The core challenge is a routing problem: each task admits dozens of preference dimensions, yet individual users care about only a few, and which ones matter depends on who is asking. With a limited question budget, asking without structure will miss the dimensions that matter. Reinforcement learning is the natural formulation, but in multi-turn settings its terminal reward fails to exploit the factored, per-criterion structure of preference data, and in practice learned policies collapse to static question sequences that ignore user responses. We propose decomposing cold-start elicitation into offline structure learning and online Bayesian inference. Pep (Preference Elicitation with Priors) learns a structured world model of preference correlations offline from complete profiles, then performs training-free Bayesian inference online to select informative questions and predict complete preference profiles, including dimensions never asked about. The framework is modular across downstream solvers and requires only simple belief models. Across medical, mathematical, social, and commonsense reasoning, Pep achieves 80.8% alignment between generated responses and users' stated preferences versus 68.5% for RL, with 3-5x fewer interactions. When two users give different answers to the same question, Pep changes its follow-up 39-62% of the time versus 0-28% for RL. It does so with ~10K parameters versus 8B for RL, showing that the bottleneck in cold-start elicitation is the capability to exploit the factored structure of preference data.",
        "authors_display": "Asli Celikyilmaz Team",
        "pdf_url": "http://arxiv.org/abs/2602.15012",
        "code_url": null,
        "date": "2026-02-16",
        "primary_category": "cs.CL",
        "chinese_summary": "背景：冷启动个性化（即在无用户历史数据时推断用户偏好）是一个挑战，因为用户只关心少数偏好维度，且关键维度因人而异。现有强化学习方法在多轮交互中难以有效利用偏好数据的分因子结构。方法：本文提出Pep（Preference Elicitation with Priors）框架，将冷启动偏好获取分解为离线结构学习和在线贝叶斯推理。Pep离线从完整用户档案中学习偏好相关性的结构化世界模型，然后在线进行免训练的贝叶斯推理，以选择信息丰富的提问并预测完整的偏好档案。结果：Pep在生成响应与用户偏好的一致性方面达到80.8%，远高于强化学习的68.5%，且交互次数减少3-5倍。它仅用约1万参数就实现此效果，而强化学习需80亿参数，突显了利用偏好数据分因子结构的重要性。"
      },
      {
        "paper_id": "2602.15004",
        "title": "PDE foundation models are skillful AI weather emulators for the Martian atmosphere",
        "abstract": "We show that AI foundation models that are pretrained on numerical solutions to a diverse corpus of partial differential equations can be adapted and fine-tuned to obtain skillful predictive weather emulators for the Martian atmosphere. We base our work on the Poseidon PDE foundation model for two-dimensional systems. We develop a method to extend Poseidon from two to three dimensions while keeping the pretraining information. Moreover, we investigate the performance of the model in the presence of sparse initial conditions. Our results make use of four Martian years (approx.~34 GB) of training data and a median compute budget of 13 GPU hours. We find that the combination of pretraining and model extension yields a performance increase of 34.4\\% on a held-out year. This shows that PDEs-FMs can not only approximate solutions to (other) PDEs but also anchor models for real-world problems with complex interactions that lack a sufficient amount of training data or a suitable compute budget.",
        "authors_display": "Juan Bernabe-Moreno Team",
        "pdf_url": "http://arxiv.org/abs/2602.15004",
        "code_url": null,
        "date": "2026-02-16",
        "primary_category": "cs.LG",
        "chinese_summary": "背景：为火星大气层构建熟练的预测天气模拟器，面临训练数据和计算资源不足的挑战。方法：本文展示了如何将预训练在多源偏微分方程数值解上的AI基础模型（Poseidon PDE基础模型）适配并微调，以构建火星大气的预测天气模拟器。研究扩展了Poseidon模型从二维到三维的方法，同时保留了预训练信息，并探讨了在稀疏初始条件下的模型性能。结果：通过预训练与模型扩展的结合，模型在独立验证年份的性能提升了34.4%。这表明偏微分方程基础模型不仅能近似其他偏微分方程的解，还能作为解决实际世界复杂交互问题的锚定模型，尤其是在训练数据或计算预算有限的情况下。"
      },
      {
        "paper_id": "2602.14972",
        "title": "Use What You Know: Causal Foundation Models with Partial Graphs",
        "abstract": "Estimating causal quantities traditionally relies on bespoke estimators tailored to specific assumptions. Recently proposed Causal Foundation Models (CFMs) promise a more unified approach by amortising causal discovery and inference in a single step. However, in their current state, they do not allow for the incorporation of any domain knowledge, which can lead to suboptimal predictions. We bridge this gap by introducing methods to condition CFMs on causal information, such as the causal graph or more readily available ancestral information. When access to complete causal graph information is too strict a requirement, our approach also effectively leverages partial causal information. We systematically evaluate conditioning strategies and find that injecting learnable biases into the attention mechanism is the most effective method to utilise full and partial causal information. Our experiments show that this conditioning allows a general-purpose CFM to match the performance of specialised models trained on specific causal structures. Overall, our approach addresses a central hurdle on the path towards all-in-one causal foundation models: the capability to answer causal queries in a data-driven manner while effectively leveraging any amount of domain expertise.",
        "authors_display": "Bernhard Schölkopf Team",
        "pdf_url": "http://arxiv.org/abs/2602.14972",
        "code_url": null,
        "date": "2026-02-16",
        "primary_category": "cs.LG",
        "chinese_summary": "背景：传统的因果量估计依赖于为特定假设定制的估计器。新兴的因果基础模型（CFMs）提供统一方法，但目前无法融入领域知识，导致预测次优。方法：本文提出将因果信息（如完整因果图或部分祖先信息）条件化到CFMs中的方法。研究系统评估了不同的条件化策略，发现将可学习偏差注入注意力机制是利用完整和部分因果信息最有效的方法。结果：通过条件化，通用CFM的性能可以与针对特定因果结构训练的专用模型相媲美。这一方法克服了构建一体化因果基础模型的核心障碍，使其能够以数据驱动的方式回答因果查询，同时有效利用任何程度的领域专业知识。"
      },
      {
        "paper_id": "2602.14878",
        "title": "Model Context Protocol (MCP) Tool Descriptions Are Smelly! Towards Improving AI Agent Efficiency with Augmented MCP Tool Descriptions",
        "abstract": "The Model Context Protocol (MCP) standardizes how Foundation Model (FM)-based agents interact with external systems by invoking tools. However, to understand a tool's purpose and features, FMs rely on natural-language tool descriptions, making these descriptions a critical component in guiding FMs to select the optimal tool for a given (sub)task and to pass the right arguments to the tool. While defects or smells in these descriptions can misguide FM-based agents, their prevalence and consequences in the MCP ecosystem remain unclear.   To address this, we conduct the first large-scale empirical study of 856 tools spread across 103 MCP servers, assessing their description quality and their impact on agent performance. We identify six components of tool descriptions from the literature, develop a scoring rubric utilizing these components, then formalize tool description smells based on this rubric. By operationalizing this rubric through an FM-based scanner, we find that 97.1% of the analyzed tool descriptions contain at least one smell, with 56% failing to state their purpose clearly. While augmenting these descriptions for all components improves task success rates by a median of 5.85 percentage points and improves partial goal completion by 15.12%, it also increases the number of execution steps by 67.46% and regresses performance in 16.67% of cases. These findings highlight a trade-off between agent performance and cost, as well as the context sensitivity of the performance gain. Furthermore, component ablations show that compact variants of different component combinations often preserve behavioral reliability while reducing unnecessary token overhead, enabling more efficient use of the FM context window and lower execution costs.",
        "authors_display": "Ahmed E. Hassan Team",
        "pdf_url": "http://arxiv.org/abs/2602.14878",
        "code_url": null,
        "date": "2026-02-16",
        "primary_category": "cs.SE",
        "chinese_summary": "背景：基于基础模型（FM）的智能体通过工具描述与外部系统交互，但这些自然语言描述中的缺陷可能误导FM，其普遍性和影响尚不明确。方法：本文对103个MCP服务器上的856个工具进行了大规模实证研究。通过识别工具描述的六个组成部分，开发了评分标准并据此形式化了“工具描述异味”。利用FM-based扫描器进行操作化评估，并增强描述以评估其对智能体性能的影响。结果：97.1%的工具描述至少含有一种“异味”，其中56%未能清晰阐明目的。尽管增强所有组件的描述可使任务成功率中位数提升5.85个百分点，但执行步骤也增加了67.46%，并导致性能下降，揭示了性能与成本之间的权衡。"
      },
      {
        "paper_id": "2602.14857",
        "title": "World Models for Policy Refinement in StarCraft II",
        "abstract": "Large Language Models (LLMs) have recently shown strong reasoning and generalization capabilities, motivating their use as decision-making policies in complex environments. StarCraft II (SC2), with its massive state-action space and partial observability, is a challenging testbed. However, existing LLM-based SC2 agents primarily focus on improving the policy itself and overlook integrating a learnable, action-conditioned transition model into the decision loop. To bridge this gap, we propose StarWM, the first world model for SC2 that predicts future observations under partial observability. To facilitate learning SC2's hybrid dynamics, we introduce a structured textual representation that factorizes observations into five semantic modules, and construct SC2-Dynamics-50k, the first instruction-tuning dataset for SC2 dynamics prediction. We further develop a multi-dimensional offline evaluation framework for predicted structured observations. Offline results show StarWM's substantial gains over zero-shot baselines, including nearly 60% improvements in resource prediction accuracy and self-side macro-situation consistency. Finally, we propose StarWM-Agent, a world-model-augmented decision system that integrates StarWM into a Generate--Simulate--Refine decision loop for foresight-driven policy refinement. Online evaluation against SC2's built-in AI demonstrates consistent improvements, yielding win-rate gains of 30%, 15%, and 30% against Hard (LV5), Harder (LV6), and VeryHard (LV7), respectively, alongside improved macro-management stability and tactical risk assessment.",
        "authors_display": "Bo Xu Team",
        "pdf_url": "http://arxiv.org/abs/2602.14857",
        "code_url": null,
        "date": "2026-02-16",
        "primary_category": "cs.AI",
        "chinese_summary": "背景：尽管大型语言模型（LLMs）展现出强大的推理能力，但现有基于LLM的《星际争霸II》（SC2）智能体主要关注策略优化，缺乏可学习的、动作条件化的转移模型来辅助决策。方法：本文提出StarWM，这是首个在部分可观测环境下预测SC2未来观测的世界模型。为学习SC2的混合动态，作者引入了一种将观测分解为五个语义模块的结构化文本表示，并构建了首个用于SC2动态预测的指令微调数据集SC2-Dynamics-50k。StarWM被集成到“生成-模拟-细化”决策循环中，形成StarWM-Agent。结果：StarWM在资源预测准确性等指标上比零样本基线提升近60%。在线评估中，StarWM-Agent对不同难度级别（Hard、Harder、VeryHard）的胜率分别提升了30%、15%和30%，同时改善了宏观管理稳定性和战术风险评估。"
      },
      {
        "paper_id": "2602.14767",
        "title": "SAILS: Segment Anything with Incrementally Learned Semantics for Task-Invariant and Training-Free Continual Learning",
        "abstract": "Continual learning remains constrained by the need for repeated retraining, high computational costs, and the persistent challenge of forgetting. These factors significantly limit the applicability of continuous learning in real-world settings, as iterative model updates require significant computational resources and inherently exacerbate forgetting. We present SAILS -- Segment Anything with Incrementally Learned Semantics, a training-free framework for Class-Incremental Semantic Segmentation (CISS) that sidesteps these challenges entirely. SAILS leverages foundational models to decouple CISS into two stages: Zero-shot region extraction using Segment Anything Model (SAM), followed by semantic association through prototypes in a fixed feature space. SAILS incorporates selective intra-class clustering, resulting in multiple prototypes per class to better model intra-class variability. Our results demonstrate that, despite requiring no incremental training, SAILS typically surpasses the performance of existing training-based approaches on standard CISS datasets, particularly in long and challenging task sequences where forgetting tends to be most severe. By avoiding parameter updates, SAILS completely eliminates forgetting and maintains consistent, task-invariant performance. Furthermore, SAILS exhibits positive backward transfer, where the introduction of new classes can enhance performance on previous classes.",
        "authors_display": "René Schuster Team",
        "pdf_url": "http://arxiv.org/abs/2602.14767",
        "code_url": null,
        "date": "2026-02-16",
        "primary_category": "cs.CV",
        "chinese_summary": "背景：持续学习在类增量语义分割（CISS）中面临重复训练、高计算成本和灾难性遗忘的限制，制约了其实际应用。方法：本文提出SAILS（Segment Anything with Incrementally Learned Semantics），一个免训练的CISS框架，它将CISS解耦为两个阶段：首先利用Segment Anything Model (SAM)进行零样本区域提取，然后通过固定特征空间中的原型进行语义关联。SAILS通过选择性类内聚类，为每个类生成多个原型以更好地建模类内变异性。结果：SAILS无需增量训练，但在标准CISS数据集上通常超越了现有的基于训练的方法，特别是在遗忘问题严重的长期和挑战性任务序列中。SAILS完全消除了遗忘，保持了任务不变的一致性能，并展现出正向反向迁移。"
      },
      {
        "paper_id": "2602.14751",
        "title": "Depth Completion as Parameter-Efficient Test-Time Adaptation",
        "abstract": "We introduce CAPA, a parameter-efficient test-time optimization framework that adapts pre-trained 3D foundation models (FMs) for depth completion, using sparse geometric cues. Unlike prior methods that train task-specific encoders for auxiliary inputs, which often overfit and generalize poorly, CAPA freezes the FM backbone. Instead, it updates only a minimal set of parameters using Parameter-Efficient Fine-Tuning (e.g. LoRA or VPT), guided by gradients calculated directly from the sparse observations available at inference time. This approach effectively grounds the foundation model's geometric prior in the scene-specific measurements, correcting distortions and misplaced structures. For videos, CAPA introduces sequence-level parameter sharing, jointly adapting all frames to exploit temporal correlations, improve robustness, and enforce multi-frame consistency. CAPA is model-agnostic, compatible with any ViT-based FM, and achieves state-of-the-art results across diverse condition patterns on both indoor and outdoor datasets. Project page: research.nvidia.com/labs/dvl/projects/capa.",
        "authors_display": "Shengyu Huang Team",
        "pdf_url": "http://arxiv.org/abs/2602.14751",
        "code_url": null,
        "date": "2026-02-16",
        "primary_category": "cs.CV",
        "chinese_summary": "背景：现有深度补全方法通常通过训练任务特定编码器来利用辅助输入，但易过拟合且泛化性差。而3D基础模型可提供更强的几何先验。方法：本文提出CAPA，一个参数高效的测试时优化框架，用于利用稀疏几何线索对预训练3D基础模型进行深度补全。CAPA冻结基础模型骨干，仅通过参数高效微调（如LoRA或VPT）更新少量参数，并利用推理时稀疏观测直接计算梯度进行指导。对于视频，CAPA引入序列级参数共享以利用时间相关性并强制多帧一致性。结果：CAPA与任何基于ViT的基础模型兼容，并在室内外数据集的各种条件下取得了最先进的结果。它有效地将基础模型的几何先验与场景特定测量相结合，修正了畸变。"
      },
      {
        "paper_id": "2602.14721",
        "title": "WebWorld: A Large-Scale World Model for Web Agent Training",
        "abstract": "Web agents require massive trajectories to generalize, yet real-world training is constrained by network latency, rate limits, and safety risks. We introduce \\textbf{WebWorld} series, the first open-web simulator trained at scale. While existing simulators are restricted to closed environments with thousands of trajectories, WebWorld leverages a scalable data pipeline to train on 1M+ open-web interactions, supporting reasoning, multi-format data, and long-horizon simulations of 30+ steps. For intrinsic evaluation, we introduce WebWorld-Bench with dual metrics spanning nine dimensions, where WebWorld achieves simulation performance comparable to Gemini-3-Pro. For extrinsic evaluation, Qwen3-14B trained on WebWorld-synthesized trajectories improves by +9.2\\% on WebArena, reaching performance comparable to GPT-4o. WebWorld enables effective inference-time search, outperforming GPT-5 as a world model. Beyond web simulation, WebWorld exhibits cross-domain generalization to code, GUI, and game environments, providing a replicable recipe for world model construction.",
        "authors_display": "Zuozhu Liu Team",
        "pdf_url": "http://arxiv.org/abs/2602.14721",
        "code_url": null,
        "date": "2026-02-16",
        "primary_category": "cs.AI",
        "chinese_summary": "背景：网页智能体需要大量轨迹以实现泛化，但真实世界训练受网络延迟、速率限制和安全风险制约。现有模拟器局限于封闭环境。方法：本文推出WebWorld系列，首个大规模训练的开放网络模拟器。它利用可扩展数据管道在超过100万次开放网络交互中进行训练，支持推理、多格式数据和超过30步的长周期模拟。结果：WebWorld在WebWorld-Bench上实现了与Gemini-3-Pro相当的模拟性能。在WebWorld合成轨迹上训练的Qwen3-14B在WebArena上性能提升9.2%，达到与GPT-4o相当的水平。WebWorld作为世界模型，在推理时搜索方面超越了GPT-5。此外，它还展现出跨领域泛化能力，为构建世界模型提供了可复现的方法。"
      },
      {
        "paper_id": "2602.14643",
        "title": "Arbor: A Framework for Reliable Navigation of Critical Conversation Flows",
        "abstract": "Large language models struggle to maintain strict adherence to structured workflows in high-stakes domains such as healthcare triage. Monolithic approaches that encode entire decision structures within a single prompt are prone to instruction-following degradation as prompt length increases, including lost-in-the-middle effects and context window overflow. To address this gap, we present Arbor, a framework that decomposes decision tree navigation into specialized, node-level tasks. Decision trees are standardized into an edge-list representation and stored for dynamic retrieval. At runtime, a directed acyclic graph (DAG)-based orchestration mechanism iteratively retrieves only the outgoing edges of the current node, evaluates valid transitions via a dedicated LLM call, and delegates response generation to a separate inference step. The framework is agnostic to the underlying decision logic and model provider. Evaluated against single-prompt baselines across 10 foundation models using annotated turns from real clinical triage conversations. Arbor improves mean turn accuracy by 29.4 percentage points, reduces per-turn latency by 57.1%, and achieves an average 14.4x reduction in per-turn cost. These results indicate that architectural decomposition reduces dependence on intrinsic model capability, enabling smaller models to match or exceed larger models operating under single-prompt baselines.",
        "authors_display": "Luís Ungaro Team",
        "pdf_url": "http://arxiv.org/abs/2602.14643",
        "code_url": null,
        "date": "2026-02-16",
        "primary_category": "cs.AI",
        "chinese_summary": "针对大型语言模型在医疗分诊等高风险领域难以遵循结构化工作流程的问题，以及单一提示方法在长提示下易导致指令依从性下降的挑战，本文提出了Arbor框架。该框架将决策树导航分解为节点级任务，通过基于DAG的编排机制动态检索和评估转换，并将响应生成解耦。实验结果表明，与单一提示基线相比，Arbor在真实临床分诊对话中将平均轮次准确率提高了29.4个百分点，同时显著降低了延迟和成本，证明了架构分解能有效提升模型性能并降低对模型固有能力的依赖。"
      },
      {
        "paper_id": "2602.14622",
        "title": "Tabular Foundation Models Can Learn Association Rules",
        "abstract": "Association Rule Mining (ARM) is a fundamental task for knowledge discovery in tabular data and is widely used in high-stakes decision-making. Classical ARM methods rely on frequent itemset mining, leading to rule explosion and poor scalability, while recent neural approaches mitigate these issues but suffer from degraded performance in low-data regimes. Tabular foundation models (TFMs), pretrained on diverse tabular data with strong in-context generalization, provide a basis for addressing these limitations. We introduce a model-agnostic association rule learning framework that extracts association rules from any conditional probabilistic model over tabular data, enabling us to leverage TFMs. We then introduce TabProbe, an instantiation of our framework that utilizes TFMs as conditional probability estimators to learn association rules out-of-the-box without frequent itemset mining. We evaluate our approach on tabular datasets of varying sizes based on standard ARM rule quality metrics and downstream classification performance. The results show that TFMs consistently produce concise, high-quality association rules with strong predictive performance and remain robust in low-data settings without task-specific training. Source code is available at https://github.com/DiTEC-project/tabprobe.",
        "authors_display": "Victoria Degeler Team",
        "pdf_url": "http://arxiv.org/abs/2602.14622",
        "code_url": null,
        "date": "2026-02-16",
        "primary_category": "cs.AI",
        "chinese_summary": "传统的关联规则挖掘（ARM）方法存在规则爆炸和可扩展性差的问题，而现有神经方法在低数据量下性能不佳，但表格基础模型（TFMs）为解决这些局限性提供了基础。为此，本文提出了一个模型无关的关联规则学习框架，能够利用TFMs从任何条件概率模型中提取关联规则，并实例化了TabProbe。实验结果显示，TabProbe利用TFMs作为条件概率估计器，无需频繁项集挖掘即可生成简洁、高质量的关联规则，在低数据设置下仍保持强大的预测性能和鲁棒性。"
      },
      {
        "paper_id": "2602.14589",
        "title": "MATEO: A Multimodal Benchmark for Temporal Reasoning and Planning in LVLMs",
        "abstract": "AI agents need to plan to achieve complex goals that involve orchestrating perception, sub-goal decomposition, and execution. These plans consist of ordered steps structured according to a Temporal Execution Order (TEO, a directed acyclic graph that ensures each step executes only after its preconditions are satisfied. Existing research on foundational models' understanding of temporal execution is limited to automatically derived annotations, approximations of the TEO as a linear chain, or text-only inputs. To address this gap, we introduce MATEO (MultimodAl Temporal Execution Order), a benchmark designed to assess and improve the temporal reasoning abilities of Large Vision Language Models (LVLMs) required for real-world planning. We acquire a high-quality professional multimodal recipe corpus, authored through a standardized editorial process that decomposes instructions into discrete steps, each paired with corresponding images. We collect TEO annotations as graphs by designing and using a scalable crowdsourcing pipeline. Using MATEO, we evaluate six state-of-the-art LVLMs across model scales, varying language context, multimodal input structure, and fine-tuning strategies.",
        "authors_display": "Giuseppe Riccardi Team",
        "pdf_url": "http://arxiv.org/abs/2602.14589",
        "code_url": null,
        "date": "2026-02-16",
        "primary_category": "cs.AI",
        "chinese_summary": "AI智能体在实现复杂目标时需要规划，但现有研究对基础模型时间执行顺序（TEO）的理解有限，多局限于线性近似或纯文本输入。为解决此问题，本文引入了MATEO（MultimodAl Temporal Execution Order）基准，旨在评估和提升大型视觉语言模型（LVLMs）的时间推理能力。通过收集高质量多模态食谱语料库及相应的TEO图注释，作者使用MATEO评估了六个最先进的LVLM，考察了不同模型规模、语言上下文、多模态输入结构和微调策略对时间推理能力的影响。"
      },
      {
        "paper_id": "2602.14512",
        "title": "MedVAR: Towards Scalable and Efficient Medical Image Generation via Next-scale Autoregressive Prediction",
        "abstract": "Medical image generation is pivotal in applications like data augmentation for low-resource clinical tasks and privacy-preserving data sharing. However, developing a scalable generative backbone for medical imaging requires architectural efficiency, sufficient multi-organ data, and principled evaluation, yet current approaches leave these aspects unresolved. Therefore, we introduce MedVAR, the first autoregressive-based foundation model that adopts the next-scale prediction paradigm to enable fast and scale-up-friendly medical image synthesis. MedVAR generates images in a coarse-to-fine manner and produces structured multi-scale representations suitable for downstream use. To support hierarchical generation, we curate a harmonized dataset of around 440,000 CT and MRI images spanning six anatomical regions. Comprehensive experiments across fidelity, diversity, and scalability show that MedVAR achieves state-of-the-art generative performance and offers a promising architectural direction for future medical generative foundation models.",
        "authors_display": "Yueming Jin Team",
        "pdf_url": "http://arxiv.org/abs/2602.14512",
        "code_url": null,
        "date": "2026-02-16",
        "primary_category": "cs.CV",
        "chinese_summary": "医学图像生成在数据增强和隐私保护中至关重要，但现有方法在架构效率、多器官数据和原则性评估方面存在不足。为此，本文提出了MedVAR，首个基于自回归的医学基础模型，采用“下一尺度预测”范式，实现快速可扩展的医学图像合成。MedVAR以粗到精的方式生成图像，并构建了一个包含约44万张CT和MRI图像的协调数据集。综合实验表明，MedVAR在图像保真度、多样性和可扩展性方面均达到最先进水平，为未来的医学生成基础模型提供了 promising 的架构方向。"
      },
      {
        "paper_id": "2602.14506",
        "title": "Covariance-Aware Transformers for Quadratic Programming and Decision Making",
        "abstract": "We explore the use of transformers for solving quadratic programs and how this capability benefits decision-making problems that involve covariance matrices. We first show that the linear attention mechanism can provably solve unconstrained QPs by tokenizing the matrix variables (e.g.~$A$ of the objective $\\frac{1}{2}x^\\top Ax+b^\\top x$) row-by-row and emulating gradient descent iterations. Furthermore, by incorporating MLPs, a transformer block can solve (i) $\\ell_1$-penalized QPs by emulating iterative soft-thresholding and (ii) $\\ell_1$-constrained QPs when equipped with an additional feedback loop. Our theory motivates us to introduce Time2Decide: a generic method that enhances a time series foundation model (TSFM) by explicitly feeding the covariance matrix between the variates. We empirically find that Time2Decide uniformly outperforms the base TSFM model for the classical portfolio optimization problem that admits an $\\ell_1$-constrained QP formulation. Remarkably, Time2Decide also outperforms the classical \"Predict-then-Optimize (PtO)\" procedure, where we first forecast the returns and then explicitly solve a constrained QP, in suitable settings. Our results demonstrate that transformers benefit from explicit use of second-order statistics, and this can enable them to effectively solve complex decision-making problems, like portfolio construction, in one forward pass.",
        "authors_display": "Samet Oymak Team",
        "pdf_url": "http://arxiv.org/abs/2602.14506",
        "code_url": null,
        "date": "2026-02-16",
        "primary_category": "cs.LG",
        "chinese_summary": "针对Transformer在涉及协方差矩阵的决策问题中的应用潜力，本文首先证明线性注意力机制可通过模拟梯度下降求解无约束二次规划（QP），并扩展至求解L1惩罚/约束QP。在此基础上，本文提出了Time2Decide，一种通过显式输入协方差矩阵来增强时间序列基础模型（TSFM）的通用方法。实验结果表明，Time2Decide在经典的投资组合优化问题上，其性能普遍优于基础TSFM模型，并在特定条件下甚至超越了传统的“预测-优化”流程，证明Transformer通过显式利用二阶统计量能有效解决复杂决策问题。"
      },
      {
        "paper_id": "2602.14434",
        "title": "A Soft Wrist with Anisotropic and Selectable Stiffness for Robust Robot Learning in Contact-rich Manipulation",
        "abstract": "Contact-rich manipulation tasks in unstructured environments pose significant robustness challenges for robot learning, where unexpected collisions can cause damage and hinder policy acquisition. Existing soft end-effectors face fundamental limitations: they either provide a limited deformation range, lack directional stiffness control, or require complex actuation systems that compromise practicality. This study introduces CLAW (Compliant Leaf-spring Anisotropic soft Wrist), a novel soft wrist mechanism that addresses these limitations through a simple yet effective design using two orthogonal leaf springs and rotary joints with a locking mechanism. CLAW provides large 6-degree-of-freedom deformation (40mm lateral, 20mm vertical), anisotropic stiffness that is tunable across three distinct modes, while maintaining lightweight construction (330g) at low cost ($550). Experimental evaluations using imitation learning demonstrate that CLAW achieves 76% success rate in benchmark peg-insertion tasks, outperforming both the Fin Ray gripper (43%) and rigid gripper alternatives (36%). CLAW successfully handles diverse contact-rich scenarios, including precision assembly with tight tolerances and delicate object manipulation, demonstrating its potential to enable robust robot learning in contact-rich domains. Project page: https://project-page-manager.github.io/CLAW/",
        "authors_display": "Masashi Hamaya Team",
        "pdf_url": "http://arxiv.org/abs/2602.14434",
        "code_url": null,
        "date": "2026-02-16",
        "primary_category": "cs.RO",
        "chinese_summary": "在非结构化环境中，机器人进行接触密集型操作任务时，现有软末端执行器因形变范围有限、缺乏定向刚度控制或系统复杂而面临挑战。本文介绍了一种名为CLAW（Compliant Leaf-spring Anisotropic soft Wrist）的新型软腕机构，它通过简单的板簧和锁定旋转关节设计，实现了大范围6自由度形变和可调的各向异性刚度，同时保持轻量和低成本。在模仿学习实验中，CLAW在插销任务中实现了76%的成功率，显著优于其他夹具，并在处理高精度装配和精细物体操作等接触密集型场景中表现出强大潜力，预示其能增强机器人学习的鲁棒性。"
      },
      {
        "paper_id": "2602.14351",
        "title": "WIMLE: Uncertainty-Aware World Models with IMLE for Sample-Efficient Continuous Control",
        "abstract": "Model-based reinforcement learning promises strong sample efficiency but often underperforms in practice due to compounding model error, unimodal world models that average over multi-modal dynamics, and overconfident predictions that bias learning. We introduce WIMLE, a model-based method that extends Implicit Maximum Likelihood Estimation (IMLE) to the model-based RL framework to learn stochastic, multi-modal world models without iterative sampling and to estimate predictive uncertainty via ensembles and latent sampling. During training, WIMLE weights each synthetic transition by its predicted confidence, preserving useful model rollouts while attenuating bias from uncertain predictions and enabling stable learning. Across $40$ continuous-control tasks spanning DeepMind Control, MyoSuite, and HumanoidBench, WIMLE achieves superior sample efficiency and competitive or better asymptotic performance than strong model-free and model-based baselines. Notably, on the challenging Humanoid-run task, WIMLE improves sample efficiency by over $50$\\% relative to the strongest competitor, and on HumanoidBench it solves $8$ of $14$ tasks (versus $4$ for BRO and $5$ for SimbaV2). These results highlight the value of IMLE-based multi-modality and uncertainty-aware weighting for stable model-based RL.",
        "authors_display": "Ke Li Team",
        "pdf_url": "http://arxiv.org/abs/2602.14351",
        "code_url": "https://openreview.net/forum?id=mzLOnTb3WH",
        "date": "2026-02-15",
        "primary_category": "cs.LG",
        "chinese_summary": "基于模型的强化学习（MBRL）常因模型误差累积、世界模型处理多模态动力学不佳及预测过度自信而表现受限。本文提出了WIMLE，一种将隐式最大似然估计（IMLE）扩展到MBRL框架的方法，旨在学习随机、多模态的世界模型，并利用集成和潜在采样估计预测不确定性。WIMLE在训练中根据预测置信度加权合成转换，以稳定学习。在40个连续控制任务上的实验结果表明，WIMLE实现了卓越的样本效率和有竞争力的渐近性能，尤其在挑战性任务上显著提升了样本效率，凸显了IMLE基多模态和不确定性感知加权对稳定MBRL的价值。"
      },
      {
        "paper_id": "2602.14251",
        "title": "Multi-Agent Debate: A Unified Agentic Framework for Tabular Anomaly Detection",
        "abstract": "Tabular anomaly detection is often handled by single detectors or static ensembles, even though strong performance on tabular data typically comes from heterogeneous model families (e.g., tree ensembles, deep tabular networks, and tabular foundation models) that frequently disagree under distribution shift, missingness, and rare-anomaly regimes. We propose MAD, a Multi-Agent Debating framework that treats this disagreement as a first-class signal and resolves it through a mathematically grounded coordination layer. Each agent is a machine learning (ML)-based detector that produces a normalized anomaly score, confidence, and structured evidence, augmented by a large language model (LLM)-based critic. A coordinator converts these messages into bounded per-agent losses and updates agent influence via an exponentiated-gradient rule, yielding both a final debated anomaly score and an auditable debate trace. MAD is a unified agentic framework that can recover existing approaches, such as mixture-of-experts gating and learning-with-expert-advice aggregation, by restricting the message space and synthesis operator. We establish regret guarantees for the synthesized losses and show how conformal calibration can wrap the debated score to control false positives under exchangeability. Experiments on diverse tabular anomaly benchmarks show improved robustness over baselines and clearer traces of model disagreement",
        "authors_display": "Sheng Li Team",
        "pdf_url": "http://arxiv.org/abs/2602.14251",
        "code_url": null,
        "date": "2026-02-15",
        "primary_category": "cs.LG",
        "chinese_summary": "表格异常检测常依赖单一或静态集成检测器，但异构模型在分布漂移、缺失数据和稀有异常下常出现分歧。本文提出了MAD（Multi-Agent Debating）框架，将这种分歧作为核心信号，通过数学协调层解决。框架中每个代理是一个ML检测器，提供异常分数、置信度和证据，并由LLM评论员增强。协调器将消息转换为损失并更新代理影响力，生成最终异常分数和可审计的辩论轨迹。实验表明，MAD在各种表格异常基准测试上提高了鲁棒性，并提供了更清晰的模型分歧追踪。"
      },
      {
        "paper_id": "2602.14177",
        "title": "Towards Spatial Transcriptomics-driven Pathology Foundation Models",
        "abstract": "Spatial transcriptomics (ST) provides spatially resolved measurements of gene expression, enabling characterization of the molecular landscape of human tissue beyond histological assessment as well as localized readouts that can be aligned with morphology. Concurrently, the success of multimodal foundation models that integrate vision with complementary modalities suggests that morphomolecular coupling between local expression and morphology can be systematically used to improve histological representations themselves. We introduce Spatial Expression-Aligned Learning (SEAL), a vision-omics self-supervised learning framework that infuses localized molecular information into pathology vision encoders. Rather than training new encoders from scratch, SEAL is designed as a parameter-efficient vision-omics finetuning method that can be flexibly applied to widely used pathology foundation models. We instantiate SEAL by training on over 700,000 paired gene expression spot-tissue region examples spanning tumor and normal samples from 14 organs. Tested across 38 slide-level and 15 patch-level downstream tasks, SEAL provides a drop-in replacement for pathology foundation models that consistently improves performance over widely used vision-only and ST prediction baselines on slide-level molecular status, pathway activity, and treatment response prediction, as well as patch-level gene expression prediction tasks. Additionally, SEAL encoders exhibit robust domain generalization on out-of-distribution evaluations and enable new cross-modal capabilities such as gene-to-image retrieval. Our work proposes a general framework for ST-guided finetuning of pathology foundation models, showing that augmenting existing models with localized molecular supervision is an effective and practical step for improving visual representations and expanding their cross-modal utility.",
        "authors_display": "Faisal Mahmood Team",
        "pdf_url": "http://arxiv.org/abs/2602.14177",
        "code_url": null,
        "date": "2026-02-15",
        "primary_category": "cs.CV",
        "chinese_summary": "空间转录组学（ST）能够提供超越组织学评估的分子景观，多模态基础模型也显示了形态分子耦合提升组织学表征的潜力。为整合局部分子信息到病理视觉编码器，本文提出了Spatial Expression-Aligned Learning (SEAL) 框架，作为一种参数高效的视觉-组学自监督微调方法，可应用于现有病理学基础模型。SEAL通过在涵盖14个器官的70多万个配对基因表达点-组织区域示例上进行训练，在38项幻灯片级和15项补丁级下游任务上，持续优于纯视觉和ST预测基线，并展示了强大的域泛化能力和基因到图像检索等跨模态能力，为病理学基础模型的ST引导微调提供了通用且实用的框架。"
      },
      {
        "paper_id": "2602.14153",
        "title": "ARport: An Augmented Reality System for Markerless Image-Guided Port Placement in Robotic Surgery",
        "abstract": "Purpose: Precise port placement is a critical step in robot-assisted surgery, where port configuration influences both visual access to the operative field and instrument maneuverability. To bridge the gap between preoperative planning and intraoperative execution, we present ARport, an augmented reality (AR) system that automatically maps pre-planned trocar layouts onto the patient's body surface, providing intuitive spatial guidance during surgical preparation. Methods: ARport, implemented on an optical see-through head-mounted display (OST-HMD), operates without any external sensors or markers, simplifying setup and enhancing workflow integration. It reconstructs the operative scene from RGB, depth, and pose data captured by the OST-HMD, extracts the patient's body surface using a foundation model, and performs surface-based markerless registration to align preoperative anatomical models to the extracted patient's body surface, enabling in-situ visualization of planned trocar layouts. A demonstration video illustrating the overall workflow is available online. Results: In full-scale human-phantom experiments, ARport accurately overlaid pre-planned trocar sites onto the physical phantom, achieving consistent spatial correspondence between virtual plans and real anatomy. Conclusion: ARport provides a fully marker-free and hardware-minimal solution for visualizing preoperative trocar plans directly on the patient's body surface. The system facilitates efficient intraoperative setup and demonstrates potential for seamless integration into routine clinical workflows.",
        "authors_display": "Qi Dou Team",
        "pdf_url": "http://arxiv.org/abs/2602.14153",
        "code_url": null,
        "date": "2026-02-15",
        "primary_category": "cs.CV",
        "chinese_summary": "精确的端口放置是机器人辅助手术的关键步骤，但术前规划与术中执行之间存在差距。本文提出了ARport，一个增强现实（AR）系统，旨在自动将预规划的套管布局映射到患者体表，提供直观的术中空间指导。ARport在光学透视头戴式显示器（OST-HMD）上实现，无需外部传感器或标记，通过基础模型提取患者体表并进行无标记配准，实现术前解剖模型与患者体表的对齐，从而现场可视化套管布局。全尺寸人体模型实验表明，ARport能够准确叠加预规划的套管位置，实现虚拟规划与真实解剖之间的一致空间对应，为临床工作流程的无缝集成提供了高效且极简的解决方案。"
      },
      {
        "paper_id": "2602.13197",
        "title": "Imitating What Works: Simulation-Filtered Modular Policy Learning from Human Videos",
        "abstract": "The ability to learn manipulation skills by watching videos of humans has the potential to unlock a new source of highly scalable data for robot learning. Here, we tackle prehensile manipulation, in which tasks involve grasping an object before performing various post-grasp motions. Human videos offer strong signals for learning the post-grasp motions, but they are less useful for learning the prerequisite grasping behaviors, especially for robots without human-like hands. A promising way forward is to use a modular policy design, leveraging a dedicated grasp generator to produce stable grasps. However, arbitrary stable grasps are often not task-compatible, hindering the robot's ability to perform the desired downstream motion. To address this challenge, we present Perceive-Simulate-Imitate (PSI), a framework for training a modular manipulation policy using human video motion data processed by paired grasp-trajectory filtering in simulation. This simulation step extends the trajectory data with grasp suitability labels, which allows for supervised learning of task-oriented grasping capabilities. We show through real-world experiments that our framework can be used to learn precise manipulation skills efficiently without any robot data, resulting in significantly more robust performance than using a grasp generator naively.",
        "authors_display": "Wei-Chiu Ma Team",
        "pdf_url": "http://arxiv.org/abs/2602.13197",
        "code_url": null,
        "date": "2026-02-13",
        "primary_category": "cs.RO",
        "chinese_summary": "针对机器人通过观察人类视频学习抓取操作技能时，传统方法难以有效学习与任务兼容的抓取行为的问题，本研究提出了Perceive-Simulate-Imitate (PSI) 框架。该框架利用仿真中的抓取轨迹过滤技术，对人类视频数据进行处理，并生成带有抓取适用性标签的扩展轨迹数据，从而实现面向任务的抓取能力监督学习。真实世界实验表明，PSI无需任何机器人数据即可高效学习精确的操纵技能，并且相比简单使用抓取生成器的方法，性能显著提升，鲁棒性更强。"
      },
      {
        "paper_id": "2602.13136",
        "title": "Order Matters in Retrosynthesis: Structure-aware Generation via Reaction-Center-Guided Discrete Flow Matching",
        "abstract": "Template-free retrosynthesis methods treat the task as black-box sequence generation, limiting learning efficiency, while semi-template approaches rely on rigid reaction libraries that constrain generalization. We address this gap with a key insight: atom ordering in neural representations matters. Building on this insight, we propose a structure-aware template-free framework that encodes the two-stage nature of chemical reactions as a positional inductive bias. By placing reaction center atoms at the sequence head, our method transforms implicit chemical knowledge into explicit positional patterns that the model can readily capture. The proposed RetroDiT backbone, a graph transformer with rotary position embeddings, exploits this ordering to prioritize chemically critical regions. Combined with discrete flow matching, our approach decouples training from sampling and enables generation in 20--50 steps versus 500 for prior diffusion methods. Our method achieves state-of-the-art performance on both USPTO-50k (61.2% top-1) and the large-scale USPTO-Full (51.3% top-1) with predicted reaction centers. With oracle centers, performance reaches 71.1% and 63.4% respectively, surpassing foundation models trained on 10 billion reactions while using orders of magnitude less data. Ablation studies further reveal that structural priors outperform brute-force scaling: a 280K-parameter model with proper ordering matches a 65M-parameter model without it.",
        "authors_display": "Tianshu Yu Team",
        "pdf_url": "http://arxiv.org/abs/2602.13136",
        "code_url": null,
        "date": "2026-02-13",
        "primary_category": "cs.LG",
        "chinese_summary": "为解决现有免模板逆合成方法学习效率低和半模板方法泛化受限的问题，本研究提出了一种结构感知的免模板框架，核心在于利用原子排序信息。该方法将反应中心原子置于序列头部，通过位置归纳偏差编码化学反应的两阶段特性，并采用RetroDiT骨干网络与离散流匹配相结合。实验结果表明，该方法在USPTO-50k和USPTO-Full数据集上取得了SOTA性能，且在预测反应中心下，性能超越了使用更多数据训练的基础模型，并验证了结构先验的重要性。"
      },
      {
        "paper_id": "2602.13066",
        "title": "A Calibrated Memorization Index (MI) for Detecting Training Data Leakage in Generative MRI Models",
        "abstract": "Image generative models are known to duplicate images from the training data as part of their outputs, which can lead to privacy concerns when used for medical image generation. We propose a calibrated per-sample metric for detecting memorization and duplication of training data. Our metric uses image features extracted using an MRI foundation model, aggregates multi-layer whitened nearest-neighbor similarities, and maps them to a bounded \\emph{Overfit/Novelty Index} (ONI) and \\emph{Memorization Index} (MI) scores. Across three MRI datasets with controlled duplication percentages and typical image augmentations, our metric robustly detects duplication and provides more consistent metric values across datasets. At the sample level, our metric achieves near-perfect detection of duplicates.",
        "authors_display": "Ibrahim Habli Team",
        "pdf_url": "http://arxiv.org/abs/2602.13066",
        "code_url": null,
        "date": "2026-02-13",
        "primary_category": "cs.CV",
        "chinese_summary": "鉴于图像生成模型可能复制训练数据，尤其在医学图像生成中引发隐私问题，本研究提出了一种校准的逐样本度量方法来检测训练数据的记忆化和重复。该方法利用MRI基础模型提取图像特征，聚合多层白化最近邻相似度，并映射为有界的“过拟合/新颖性指数”（ONI）和“记忆化指数”（MI）分数。在多个MRI数据集上的实验结果表明，该度量能稳健检测重复数据，并提供一致的度量值，在样本级别实现了近乎完美的重复项检测。"
      },
      {
        "paper_id": "2602.12971",
        "title": "INHerit-SG: Incremental Hierarchical Semantic Scene Graphs with RAG-Style Retrieval",
        "abstract": "Driven by advancements in foundation models, semantic scene graphs have emerged as a prominent paradigm for high-level 3D environmental abstraction in robot navigation. However, existing approaches are fundamentally misaligned with the needs of embodied tasks. As they rely on either offline batch processing or implicit feature embeddings, the maps can hardly support interpretable human-intent reasoning in complex environments. To address these limitations, we present INHerit-SG. We redefine the map as a structured, RAG-ready knowledge base where natural-language descriptions are introduced as explicit semantic anchors to better align with human intent. An asynchronous dual-process architecture, together with a Floor-Room-Area-Object hierarchy, decouples geometric segmentation from time-consuming semantic reasoning. An event-triggered map update mechanism reorganizes the graph only when meaningful semantic events occur. This strategy enables our graph to maintain long-term consistency with relatively low computational overhead. For retrieval, we deploy multi-role Large Language Models (LLMs) to decompose queries into atomic constraints and handle logical negations, and employ a hard-to-soft filtering strategy to ensure robust reasoning. This explicit interpretability improves the success rate and reliability of complex retrievals, enabling the system to adapt to a broader spectrum of human interaction tasks. We evaluate INHerit-SG on a newly constructed dataset, HM3DSem-SQR, and in real-world environments. Experiments demonstrate that our system achieves state-of-the-art performance on complex queries, and reveal its scalability for downstream navigation tasks. Project Page: https://fangyuktung.github.io/INHeritSG.github.io/",
        "authors_display": "Yang Gao Team",
        "pdf_url": "http://arxiv.org/abs/2602.12971",
        "code_url": null,
        "date": "2026-02-13",
        "primary_category": "cs.RO",
        "chinese_summary": "针对现有语义场景图在机器人导航中难以支持可解释的人类意图推理的问题，本研究提出了INHerit-SG框架。该框架将地图定义为RAG-ready的知识库，通过引入自然语言描述作为语义锚点对齐人类意图，并采用异步双进程架构和分层结构解耦几何分割与语义推理，通过事件触发机制保持地图长期一致性。实验在新建数据集和真实世界环境中进行，结果表明INHerit-SG在复杂查询上达到了最先进性能，并提高了检索成功率和可靠性，展现了其在下游导航任务中的可扩展性。"
      },
      {
        "paper_id": "2602.12963",
        "title": "Information-theoretic analysis of world models in optimal reward maximizers",
        "abstract": "An important question in the field of AI is the extent to which successful behaviour requires an internal representation of the world. In this work, we quantify the amount of information an optimal policy provides about the underlying environment. We consider a Controlled Markov Process (CMP) with $n$ states and $m$ actions, assuming a uniform prior over the space of possible transition dynamics. We prove that observing a deterministic policy that is optimal for any non-constant reward function then conveys exactly $n \\log m$ bits of information about the environment. Specifically, we show that the mutual information between the environment and the optimal policy is $n \\log m$ bits. This bound holds across a broad class of objectives, including finite-horizon, infinite-horizon discounted, and time-averaged reward maximization. These findings provide a precise information-theoretic lower bound on the \"implicit world model'' necessary for optimality.",
        "authors_display": "Alex Altair Team",
        "pdf_url": "http://arxiv.org/abs/2602.12963",
        "code_url": null,
        "date": "2026-02-13",
        "primary_category": "cs.AI",
        "chinese_summary": "为量化最优行为对世界内部表示的需求，本研究考虑了一个具有n个状态和m个动作的受控马尔可夫过程，并假设转移动态存在均匀先验。研究证明，观察一个对任何非恒定奖励函数最优的确定性策略，可以精确地传达n log m比特关于环境的信息。具体来说，环境与最优策略之间的互信息为n log m比特。这些发现为实现最优性所需的“隐式世界模型”提供了精确的信息理论下限，适用于多种奖励最大化目标。"
      },
      {
        "paper_id": "2602.12734",
        "title": "Scaling Single Human Demonstrations for Imitation Learning using Generative Foundational Models",
        "abstract": "Imitation learning is a popular paradigm to teach robots new tasks, but collecting robot demonstrations through teleoperation or kinesthetic teaching is tedious and time-consuming. In contrast, directly demonstrating a task using our human embodiment is much easier and data is available in abundance, yet transfer to the robot can be non-trivial. In this work, we propose Real2Gen to train a manipulation policy from a single human demonstration. Real2Gen extracts required information from the demonstration and transfers it to a simulation environment, where a programmable expert agent can demonstrate the task arbitrarily many times, generating an unlimited amount of data to train a flow matching policy. We evaluate Real2Gen on human demonstrations from three different real-world tasks and compare it to a recent baseline. Real2Gen shows an average increase in the success rate of 26.6% and better generalization of the trained policy due to the abundance and diversity of training data. We further deploy our purely simulation-trained policy zero-shot in the real world. We make the data, code, and trained models publicly available at real2gen.cs.uni-freiburg.de.",
        "authors_display": "Abhinav Valada Team",
        "pdf_url": "http://arxiv.org/abs/2602.12734",
        "code_url": null,
        "date": "2026-02-13",
        "primary_category": "cs.RO",
        "chinese_summary": "鉴于机器人模仿学习中收集演示数据耗时且不易从人类演示直接迁移，本研究提出Real2Gen框架，旨在从单个“人类”演示中训练机器人操纵策略。该方法从人类演示中提取必要信息并传输至仿真环境，在仿真中利用可编程专家智能体生成无限数据来训练流匹配策略。实验结果显示，Real2Gen在三个真实世界任务上成功率平均提升26.6%，并且由于训练数据丰富多样，训练策略的泛化能力显著提高，纯仿真训练的策略还能零样本部署到真实世界。"
      },
      {
        "paper_id": "2602.12705",
        "title": "MedXIAOHE: A Comprehensive Recipe for Building Medical MLLMs",
        "abstract": "We present MedXIAOHE, a medical vision-language foundation model designed to advance general-purpose medical understanding and reasoning in real-world clinical applications. MedXIAOHE achieves state-of-the-art performance across diverse medical benchmarks and surpasses leading closed-source multimodal systems on multiple capabilities. To achieve this, we propose an entity-aware continual pretraining framework that organizes heterogeneous medical corpora to broaden knowledge coverage and reduce long-tail gaps (e.g., rare diseases). For medical expert-level reasoning and interaction, MedXIAOHE incorporates diverse medical reasoning patterns via reinforcement learning and tool-augmented agentic training, enabling multi-step diagnostic reasoning with verifiable decision traces. To improve reliability in real-world use, MedXIAOHE integrates user-preference rubrics, evidence-grounded reasoning, and low-hallucination long-form report generation, with improved adherence to medical instructions. We release this report to document our practical design choices, scaling insights, and evaluation framework, hoping to inspire further research.",
        "authors_display": "Zhixiong Yang Team",
        "pdf_url": "http://arxiv.org/abs/2602.12705",
        "code_url": null,
        "date": "2026-02-13",
        "primary_category": "cs.CL",
        "chinese_summary": "为提升真实世界临床应用中的通用医学理解和推理能力，本研究提出了医疗视觉-语言基础模型MedXIAOHE。该模型采用实体感知持续预训练框架，组织异构医学语料以拓宽知识覆盖并减少长尾问题；通过强化学习和工具增强的代理训练，整合多样化医学推理模式以支持带可验证决策轨迹的多步骤诊断推理；并融合用户偏好规则、证据推理和低幻觉长文本报告生成，提高真实世界使用的可靠性。MedXIAOHE在多项医学基准测试中取得了最先进的性能，并超越了领先的闭源多模态系统。"
      },
      {
        "paper_id": "2602.12606",
        "title": "RelBench v2: A Large-Scale Benchmark and Repository for Relational Data",
        "abstract": "Relational deep learning (RDL) has emerged as a powerful paradigm for learning directly on relational databases by modeling entities and their relationships across multiple interconnected tables. As this paradigm evolves toward larger models and relational foundation models, scalable and realistic benchmarks are essential for enabling systematic evaluation and progress. In this paper, we introduce RelBench v2, a major expansion of the RelBench benchmark for RDL. RelBench v2 adds four large-scale relational datasets spanning scholarly publications, enterprise resource planning, consumer platforms, and clinical records, increasing the benchmark to 11 datasets comprising over 22 million rows across 29 tables. We further introduce autocomplete tasks, a new class of predictive objectives that require models to infer missing attribute values directly within relational tables while respecting temporal constraints, expanding beyond traditional forecasting tasks constructed via SQL queries. In addition, RelBench v2 expands beyond its native datasets by integrating external benchmarks and evaluation frameworks: we translate event streams from the Temporal Graph Benchmark into relational schemas for unified relational-temporal evaluation, interface with ReDeLEx to provide uniform access to 70+ real-world databases suitable for pretraining, and incorporate 4DBInfer datasets and tasks to broaden multi-table prediction coverage. Experimental results demonstrate that RDL models consistently outperform single-table baselines across autocomplete, forecasting, and recommendation tasks, highlighting the importance of modeling relational structure explicitly.",
        "authors_display": "Jure Leskovec Team",
        "pdf_url": "http://arxiv.org/abs/2602.12606",
        "code_url": "https://relbench.stanford.edu",
        "date": "2026-02-13",
        "primary_category": "cs.LG",
        "chinese_summary": "为推动关系深度学习（RDL）的发展，并应对日益增长的模型规模需求，本研究引入了RelBench v2，一个大规模、真实的关系数据库基准扩展。RelBench v2新增了四个大型数据集和“自动完成任务”，旨在直接推理关系表中缺失的属性值，并整合了外部基准和评估框架以实现统一的关系-时间评估。实验结果表明，RDL模型在自动完成、预测和推荐任务中始终优于单表基线，突出了显式建模关系结构的重要性。"
      },
      {
        "paper_id": "2602.12563",
        "title": "The Constant Eye: Benchmarking and Bridging Appearance Robustness in Autonomous Driving",
        "abstract": "Despite rapid progress, autonomous driving algorithms remain notoriously fragile under Out-of-Distribution (OOD) conditions. We identify a critical decoupling failure in current research: the lack of distinction between appearance-based shifts, such as weather and lighting, and structural scene changes. This leaves a fundamental question unanswered: Is the planner failing because of complex road geometry, or simply because it is raining? To resolve this, we establish navdream, a high-fidelity robustness benchmark leveraging generative pixel-aligned style transfer. By creating a visual stress test with negligible geometric deviation, we isolate the impact of appearance on driving performance. Our evaluation reveals that existing planning algorithms often show significant degradation under OOD appearance conditions, even when the underlying scene structure remains consistent. To bridge this gap, we propose a universal perception interface leveraging a frozen visual foundation model (DINOv3). By extracting appearance-invariant features as a stable interface for the planner, we achieve exceptional zero-shot generalization across diverse planning paradigms, including regression-based, diffusion-based, and scoring-based models. Our plug-and-play solution maintains consistent performance across extreme appearance shifts without requiring further fine-tuning. The benchmark and code will be made available.",
        "authors_display": "Yiyi Liao Team",
        "pdf_url": "http://arxiv.org/abs/2602.12563",
        "code_url": null,
        "date": "2026-02-13",
        "primary_category": "cs.CV",
        "chinese_summary": "针对自动驾驶算法在OOD条件下易受外观变化影响，且难以区分外观与结构场景变化导致规划器失效的问题，本研究建立了Navdream，一个高保真鲁棒性基准。该基准利用生成式像素对齐风格迁移，隔离外观变化对驾驶性能的影响。为弥合这一差距，研究提出了一种通用感知接口，利用冻结的视觉基础模型（DINOv3）提取外观不变特征作为规划器的稳定接口。实验表明，现有规划算法在OOD外观下性能显著下降，而该即插即用解决方案在各种规划范式中实现了卓越的零样本泛化，在极端外观变化下仍保持一致性能。"
      },
      {
        "paper_id": "2602.12540",
        "title": "Self-Supervised JEPA-based World Models for LiDAR Occupancy Completion and Forecasting",
        "abstract": "Autonomous driving, as an agent operating in the physical world, requires the fundamental capability to build \\textit{world models} that capture how the environment evolves spatiotemporally in order to support long-term planning. At the same time, scalability demands learning such models in a self-supervised manner; \\textit{joint-embedding predictive architecture (JEPA)} enables learning world models via leveraging large volumes of unlabeled data without relying on expensive human annotations. In this paper, we propose \\textbf{AD-LiST-JEPA}, a self-supervised world model for autonomous driving that predicts future spatiotemporal evolution from LiDAR data using a JEPA framework. We evaluate the quality of the learned representations through a downstream LiDAR-based occupancy completion and forecasting (OCF) task, which jointly assesses perception and prediction. Proof of concept experiments show better OCF performance with pretrained encoder after JEPA-based world model learning.",
        "authors_display": "Anna Choromanska Team",
        "pdf_url": "http://arxiv.org/abs/2602.12540",
        "code_url": null,
        "date": "2026-02-13",
        "primary_category": "cs.CV",
        "chinese_summary": "鉴于自动驾驶需要世界模型来支持长期规划，且模型学习需具备自监督的可扩展性，本研究提出AD-LiST-JEPA，一个基于联合嵌入预测架构（JEPA）的自监督世界模型。该模型旨在利用JEPA框架从激光雷达数据预测未来时空演变。通过下游基于激光雷达的占用完成和预测（OCF）任务评估学习到的表示质量，概念验证实验表明，经过JEPA世界模型学习预训练后的编码器在OCF性能上有所提升，证明了该方法在感知和预测联合任务中的潜力。"
      },
      {
        "paper_id": "2602.12520",
        "title": "Multi-Agent Model-Based Reinforcement Learning with Joint State-Action Learned Embeddings",
        "abstract": "Learning to coordinate many agents in partially observable and highly dynamic environments requires both informative representations and data-efficient training. To address this challenge, we present a novel model-based multi-agent reinforcement learning framework that unifies joint state-action representation learning with imaginative roll-outs. We design a world model trained with variational auto-encoders and augment the model using the state-action learned embedding (SALE). SALE is injected into both the imagination module that forecasts plausible future roll-outs and the joint agent network whose individual action values are combined through a mixing network to estimate the joint action-value function. By coupling imagined trajectories with SALE-based action values, the agents acquire a richer understanding of how their choices influence collective outcomes, leading to improved long-term planning and optimization under limited real-environment interactions. Empirical studies on well-established multi-agent benchmarks, including StarCraft II Micro-Management, Multi-Agent MuJoCo, and Level-Based Foraging challenges, demonstrate consistent gains of our method over baseline algorithms and highlight the effectiveness of joint state-action learned embeddings within a multi-agent model-based paradigm.",
        "authors_display": "David Meger Team",
        "pdf_url": "http://arxiv.org/abs/2602.12520",
        "code_url": null,
        "date": "2026-02-13",
        "primary_category": "cs.LG",
        "chinese_summary": "在部分可观察和高度动态环境中，多智能体协调学习面临表示学习和数据效率挑战。为此，本文提出了一种新颖的基于模型的强化学习框架，该框架将联合状态-动作表示学习与想象式展开相结合。作者设计了一个使用变分自编码器训练的世界模型，并利用学习到的状态-动作嵌入（SALE）进行增强，将其注入到预测未来展开的想象模块和估计联合动作值函数的联合智能体网络中。在星际争霸II微管理、多智能体MuJoCo和基于级别的觅食挑战等基准测试中，该方法在有限真实环境交互下，通过将想象轨迹与基于SALE的动作值相结合，显著优于基线算法，验证了其在多智能体模型范式中学习联合状态-动作嵌入的有效性。"
      },
      {
        "paper_id": "2602.12218",
        "title": "The Observer Effect in World Models: Invasive Adaptation Corrupts Latent Physics",
        "abstract": "Determining whether neural models internalize physical laws as world models, rather than exploiting statistical shortcuts, remains challenging, especially under out-of-distribution (OOD) shifts. Standard evaluations often test latent capability via downstream adaptation (e.g., fine-tuning or high-capacity probes), but such interventions can change the representations being measured and thus confound what was learned during self-supervised learning (SSL). We propose a non-invasive evaluation protocol, PhyIP. We test whether physical quantities are linearly decodable from frozen representations, motivated by the linear representation hypothesis. Across fluid dynamics and orbital mechanics, we find that when SSL achieves low error, latent structure becomes linearly accessible. PhyIP recovers internal energy and Newtonian inverse-square scaling on OOD tests (e.g., $ρ> 0.90$). In contrast, adaptation-based evaluations can collapse this structure ($ρ\\approx 0.05$). These findings suggest that adaptation-based evaluation can obscure latent structures and that low-capacity probes offer a more accurate evaluation of physical world models.",
        "authors_display": "Barbara Hammer Team",
        "pdf_url": "http://arxiv.org/abs/2602.12218",
        "code_url": null,
        "date": "2026-02-12",
        "primary_category": "cs.LG",
        "chinese_summary": "判断神经网络模型是内化了物理定律还是仅利用统计捷径，尤其是在分布外（OOD）变化下，仍是一个难题。传统的适应性评估方法（如微调或高容量探针）可能改变被测量的表示，从而混淆自监督学习（SSL）期间的真实学习内容。为解决此问题，本文提出了一种非侵入性评估协议PhyIP，该协议基于线性表示假设，通过测试物理量能否从冻结表示中线性解码来评估。在流体动力学和轨道力学任务中，实验发现当SSL错误率较低时，潜在结构变得线性可访问，PhyIP在OOD测试中成功恢复了内能和牛顿反平方标度（ρ>0.90）。相比之下，基于适应性的评估可能使这种结构崩溃（ρ≈0.05）。这表明低容量探针能更准确地评估物理世界模型，而适应性评估可能掩盖潜在结构。"
      },
      {
        "paper_id": "2602.12215",
        "title": "LDA-1B: Scaling Latent Dynamics Action Model via Universal Embodied Data Ingestion",
        "abstract": "Recent robot foundation models largely rely on large-scale behavior cloning, which imitates expert actions but discards transferable dynamics knowledge embedded in heterogeneous embodied data. While the Unified World Model (UWM) formulation has the potential to leverage such diverse data, existing instantiations struggle to scale to foundation-level due to coarse data usage and fragmented datasets. We introduce LDA-1B, a robot foundation model that scales through universal embodied data ingestion by jointly learning dynamics, policy, and visual forecasting, assigning distinct roles to data of varying quality. To support this regime at scale, we assemble and standardize EI-30k, an embodied interaction dataset comprising over 30k hours of human and robot trajectories in a unified format. Scalable dynamics learning over such heterogeneous data is enabled by prediction in a structured DINO latent space, which avoids redundant pixel-space appearance modeling. Complementing this representation, LDA-1B employs a multi-modal diffusion transformer to handle asynchronous vision and action streams, enabling stable training at the 1B-parameter scale. Experiments in simulation and the real world show LDA-1B outperforms prior methods (e.g., $π_{0.5}$) by up to 21\\%, 48\\%, and 23\\% on contact-rich, dexterous, and long-horizon tasks, respectively. Notably, LDA-1B enables data-efficient fine-tuning, gaining 10\\% by leveraging 30\\% low-quality trajectories typically harmful and discarded.",
        "authors_display": "He Wang Team",
        "pdf_url": "http://arxiv.org/abs/2602.12215",
        "code_url": "https://pku-epic.github.io/LDA",
        "date": "2026-02-12",
        "primary_category": "cs.RO",
        "chinese_summary": "当前机器人基础模型多依赖大规模行为克隆，忽视了异构具身数据中可迁移的动力学知识，而现有统一世界模型（UWM）因粗糙数据使用和碎片化数据集难以扩展。为此，本文提出了LDA-1B，一个通过通用具身数据摄取进行扩展的机器人基础模型，它联合学习动力学、策略和视觉预测，并为不同质量数据分配不同角色。为支持大规模训练，作者构建并标准化了EI-30k数据集（超过3万小时的人类和机器人轨迹）。通过在结构化的DINO潜在空间中进行预测，实现了异构数据的可扩展动力学学习，避免了冗余的像素空间外观建模，并采用多模态扩散Transformer处理异步视觉和动作流，实现了10亿参数规模的稳定训练。实验结果表明，LDA-1B在接触密集型、灵巧型和长程任务上分别比现有方法（如π0.5）提高了21%、48%和23%，并能通过利用30%通常有害且被丢弃的低质量轨迹，实现数据高效微调，性能提升10%。"
      },
      {
        "paper_id": "2602.12160",
        "title": "DreamID-Omni: Unified Framework for Controllable Human-Centric Audio-Video Generation",
        "abstract": "Recent advancements in foundation models have revolutionized joint audio-video generation. However, existing approaches typically treat human-centric tasks including reference-based audio-video generation (R2AV), video editing (RV2AV) and audio-driven video animation (RA2V) as isolated objectives. Furthermore, achieving precise, disentangled control over multiple character identities and voice timbres within a single framework remains an open challenge. In this paper, we propose DreamID-Omni, a unified framework for controllable human-centric audio-video generation. Specifically, we design a Symmetric Conditional Diffusion Transformer that integrates heterogeneous conditioning signals via a symmetric conditional injection scheme. To resolve the pervasive identity-timbre binding failures and speaker confusion in multi-person scenarios, we introduce a Dual-Level Disentanglement strategy: Synchronized RoPE at the signal level to ensure rigid attention-space binding, and Structured Captions at the semantic level to establish explicit attribute-subject mappings. Furthermore, we devise a Multi-Task Progressive Training scheme that leverages weakly-constrained generative priors to regularize strongly-constrained tasks, preventing overfitting and harmonizing disparate objectives. Extensive experiments demonstrate that DreamID-Omni achieves comprehensive state-of-the-art performance across video, audio, and audio-visual consistency, even outperforming leading proprietary commercial models. We will release our code to bridge the gap between academic research and commercial-grade applications.",
        "authors_display": "Xiangwang Hou Team",
        "pdf_url": "http://arxiv.org/abs/2602.12160",
        "code_url": "https://guoxu1233.github.io/DreamID-Omni/",
        "date": "2026-02-12",
        "primary_category": "cs.CV",
        "chinese_summary": "尽管基础模型在音视频生成方面取得进展，但以人物为中心的多任务（如参考音视频生成、视频编辑、音频驱动动画）仍被孤立处理，且难以在单一框架内实现对多角色身份和音色的精确解耦控制。本文提出了DreamID-Omni，一个统一的可控人物中心音视频生成框架。作者设计了一个对称条件扩散Transformer，通过对称条件注入方案整合异构条件信号。为解决多人物场景中普遍存在的身份-音色绑定失败和说话人混淆问题，提出了双层解耦策略：在信号层面采用同步RoPE确保严格的注意力空间绑定，在语义层面采用结构化字幕建立显式属性-主题映射。此外，还设计了多任务渐进训练方案，利用弱约束生成先验来正则化强约束任务。大量实验证明，DreamID-Omni在视频、音频和音视频一致性方面均达到了全面的最先进性能，甚至超越了领先的商业模型。"
      },
      {
        "paper_id": "2602.12147",
        "title": "It's TIME: Towards the Next Generation of Time Series Forecasting Benchmarks",
        "abstract": "Time series foundation models (TSFMs) are revolutionizing the forecasting landscape from specific dataset modeling to generalizable task evaluation. However, we contend that existing benchmarks exhibit common limitations in four dimensions: constrained data composition dominated by reused legacy sources, compromised data integrity lacking rigorous quality assurance, misaligned task formulations detached from real-world contexts, and rigid analysis perspectives that obscure generalizable insights. To bridge these gaps, we introduce TIME, a next-generation task-centric benchmark comprising 50 fresh datasets and 98 forecasting tasks, tailored for strict zero-shot TSFM evaluation free from data leakage. Integrating large language models and human expertise, we establish a rigorous human-in-the-loop benchmark construction pipeline to ensure high data integrity and redefine task formulation by aligning forecasting configurations with real-world operational requirements and variate predictability. Furthermore, we propose a novel pattern-level evaluation perspective that moves beyond traditional dataset-level evaluations based on static meta labels. By leveraging structural time series features to characterize intrinsic temporal properties, this approach offers generalizable insights into model capabilities across diverse patterns. We evaluate 12 representative TSFMs and establish a multi-granular leaderboard to facilitate in-depth analysis and visualized inspection. The leaderboard is available at https://huggingface.co/spaces/Real-TSF/TIME-leaderboard.",
        "authors_display": "Chenghao Liu Team",
        "pdf_url": "http://arxiv.org/abs/2602.12147",
        "code_url": null,
        "date": "2026-02-12",
        "primary_category": "cs.LG",
        "chinese_summary": "时间序列基础模型（TSFMs）正在革新预测领域，但现有基准在数据构成、数据完整性、任务制定和分析视角上存在局限。为解决这些问题，本文提出了TIME，一个新一代任务中心基准，包含50个全新数据集和98个预测任务，专为严格的零样本TSFM评估而设计，避免数据泄露。通过整合大型语言模型和人类专业知识，建立了严格的人机协作基准构建流程，确保高数据完整性，并根据真实操作需求和变量可预测性重新定义任务。此外，作者提出了一种新颖的模式级评估视角，超越了基于静态元标签的传统数据集级评估，通过利用结构化时间序列特征表征内在时间属性，为模型能力提供了更具普适性的见解。对12个代表性TSFMs进行评估，并建立了一个多粒度排行榜，以促进深入分析和可视化检查。"
      },
      {
        "paper_id": "2602.12120",
        "title": "Commencing-Student Enrolment Forecasting Under Data Sparsity with Time Series Foundation Models",
        "abstract": "Many universities face increasing financial pressure and rely on accurate forecasts of commencing enrolments. However, enrolment forecasting in higher education is often data-sparse; annual series are short and affected by reporting changes and regime shifts. Popular classical approaches can be unreliable, as parameter estimation and model selection are unstable with short samples, and structural breaks degrade extrapolation. Recently, TSFMs have provided zero-shot priors, delivering strong gains in annual, data-sparse institutional forecasting under leakage-disciplined covariate construction. We benchmark multiple TSFM families in a zero-shot setting and test a compact, leakage-safe covariate set and introduce the Institutional Operating Conditions Index (IOCI), a transferable 0-100 regime covariate derived from time-stamped documentary evidence available at each forecast origin, alongside Google Trends demand proxies with stabilising feature engineering. Using an expanding-window backtest with strict vintage alignment, covariate-conditioned TSFMs perform on par with classical benchmarks without institution-specific training, with performance differences varying by cohort and model.",
        "authors_display": "Surangika Ranathunga Team",
        "pdf_url": "http://arxiv.org/abs/2602.12120",
        "code_url": null,
        "date": "2026-02-12",
        "primary_category": "cs.AI",
        "chinese_summary": "许多大学面临日益增长的财政压力，亟需准确预测新生入学人数，然而高等教育入学预测通常数据稀疏，年度序列短且受报告变化和体制转变影响。流行的经典方法因短样本导致参数估计和模型选择不稳定，以及结构性中断导致外推能力下降而不可靠。近期，TSFMs在泄漏受限的协变量构建下，为年度、数据稀疏的机构预测提供了强大的零样本先验。本文在零样本设置下，对多种TSFM家族进行了基准测试，并测试了一组紧凑、防泄漏的协变量集。作者引入了“机构运营状况指数”（IOCI），这是一个从时间戳文件证据中提取的可转移的0-100区间状态协变量，并结合了具有稳定特征工程的Google Trends需求代理。使用严格对齐的回溯测试，结果表明，在没有机构特定训练的情况下，条件化TSFMs的表现与经典基准相当，具体表现差异因群体和模型而异。"
      },
      {
        "paper_id": "2602.12108",
        "title": "The Pensieve Paradigm: Stateful Language Models Mastering Their Own Context",
        "abstract": "In the world of Harry Potter, when Dumbledore's mind is overburdened, he extracts memories into a Pensieve to be revisited later. In the world of AI, while we possess the Pensieve-mature databases and retrieval systems, our models inexplicably lack the \"wand\" to operate it. They remain like a Dumbledore without agency, passively accepting a manually engineered context as their entire memory. This work finally places the wand in the model's hand. We introduce StateLM, a new class of foundation models endowed with an internal reasoning loop to manage their own state. We equip our model with a suite of memory tools, such as context pruning, document indexing, and note-taking, and train it to actively manage these tools. By learning to dynamically engineering its own context, our model breaks free from the architectural prison of a fixed window. Experiments across various model sizes demonstrate StateLM's effectiveness across diverse scenarios. On long-document QA tasks, StateLMs consistently outperform standard LLMs across all model scales; on the chat memory task, they achieve absolute accuracy improvements of 10% to 20% over standard LLMs. On the deep research task BrowseComp-Plus, the performance gap becomes even more pronounced: StateLM achieves up to 52% accuracy, whereas standard LLM counterparts struggle around 5%. Ultimately, our approach shifts LLMs from passive predictors to state-aware agents where reasoning becomes a stateful and manageable process.",
        "authors_display": "Yan Wang Team",
        "pdf_url": "http://arxiv.org/abs/2602.12108",
        "code_url": null,
        "date": "2026-02-12",
        "primary_category": "cs.AI",
        "chinese_summary": "当前的大型语言模型（LLMs）虽有强大的数据库和检索系统，但缺乏主动管理自身状态和记忆的机制，被动接受手动提供的上下文，受限于固定窗口大小。本研究引入了StateLM，一种新型基础模型，它具备内部推理循环来管理自身状态。该模型配备了一系列记忆工具（如上下文剪枝、文档索引、笔记）并被训练主动管理这些工具，从而摆脱固定窗口的架构限制。实验结果表明，StateLM在所有模型规模的长文档问答任务中持续优于标准LLMs，在聊天记忆任务中绝对准确率提升10%至20%，在深度研究任务BrowseComp-Plus上更是达到52%的准确率，远超标准LLM的5%左右。这使得LLMs从被动预测器转变为状态感知代理，实现了状态化和可管理的推理过程。"
      },
      {
        "paper_id": "2602.12099",
        "title": "GigaBrain-0.5M*: a VLA That Learns From World Model-Based Reinforcement Learning",
        "abstract": "Vision-language-action (VLA) models that directly predict multi-step action chunks from current observations face inherent limitations due to constrained scene understanding and weak future anticipation capabilities. In contrast, video world models pre-trained on web-scale video corpora exhibit robust spatiotemporal reasoning and accurate future prediction, making them a natural foundation for enhancing VLA learning. Therefore, we propose \\textit{GigaBrain-0.5M*}, a VLA model trained via world model-based reinforcement learning. Built upon \\textit{GigaBrain-0.5}, which is pre-trained on over 10,000 hours of robotic manipulation data, whose intermediate version currently ranks first on the international RoboChallenge benchmark. \\textit{GigaBrain-0.5M*} further integrates world model-based reinforcement learning via \\textit{RAMP} (Reinforcement leArning via world Model-conditioned Policy) to enable robust cross-task adaptation. Empirical results demonstrate that \\textit{RAMP} achieves substantial performance gains over the RECAP baseline, yielding improvements of approximately 30\\% on challenging tasks including \\texttt{Laundry Folding}, \\texttt{Box Packing}, and \\texttt{Espresso Preparation}. Critically, \\textit{GigaBrain-0.5M$^*$} exhibits reliable long-horizon execution, consistently accomplishing complex manipulation tasks without failure as validated by real-world deployment videos on our \\href{https://gigabrain05m.github.io}{project page}.",
        "authors_display": "Zheng Zhu Team",
        "pdf_url": "http://arxiv.org/abs/2602.12099",
        "code_url": "https://gigabrain05m.github.io/",
        "date": "2026-02-12",
        "primary_category": "cs.CV",
        "chinese_summary": "直接从当前观察预测多步动作块的视觉-语言-动作（VLA）模型，因场景理解和未来预测能力受限而存在不足。相比之下，预训练于网络规模视频语料库的视频世界模型具有强大的时空推理和准确的未来预测能力，为增强VLA学习提供了基础。本研究提出了GigaBrain-0.5M*，一个通过世界模型驱动强化学习训练的VLA模型，其基于在超过1万小时机器人操作数据上预训练的GigaBrain-0.5。GigaBrain-0.5M*通过RAMP（通过世界模型条件策略进行强化学习）整合强化学习，以实现鲁棒的跨任务适应。实验结果表明，RAMP相对于RECAP基线取得了显著性能提升，在“叠衣服”、“装箱”和“制作浓缩咖啡”等挑战性任务上提升了约30%，并且在真实世界部署中展示了可靠的长时序复杂操作能力。"
      },
      {
        "paper_id": "2602.12065",
        "title": "Affordance-Graphed Task Worlds: Self-Evolving Task Generation for Scalable Embodied Learning",
        "abstract": "Training robotic policies directly in the real world is expensive and unscalable. Although generative simulation enables large-scale data synthesis, current approaches often fail to generate logically coherent long-horizon tasks and struggle with dynamic physical uncertainties due to open-loop execution. To address these challenges, we propose Affordance-Graphed Task Worlds (AGT-World), a unified framework that autonomously constructs interactive simulated environments and corresponding robot task policies based on real-world observations. Unlike methods relying on random proposals or static replication, AGT-World formalizes the task space as a structured graph, enabling the precise, hierarchical decomposition of complex goals into theoretically grounded atomic primitives. Furthermore, we introduce a Self-Evolution mechanism with hybrid feedback to autonomously refine policies, combining Vision-Language Model reasoning and geometric verification. Extensive experiments demonstrate that our method significantly outperforms in success rates and generalization, achieving a self-improving cycle of proposal, execution, and correction for scalable robot learning.",
        "authors_display": "Changshui Zhang Team",
        "pdf_url": "http://arxiv.org/abs/2602.12065",
        "code_url": null,
        "date": "2026-02-12",
        "primary_category": "cs.RO",
        "chinese_summary": "在现实世界中直接训练机器人策略成本高昂且难以扩展，而生成式仿真虽能合成大规模数据，但现有方法常难以生成逻辑连贯的长时序任务，且因开环执行难以应对动态物理不确定性。本研究提出了Affordance-Graphed Task Worlds (AGT-World)，一个统一框架，能够根据现实世界观察自主构建交互式仿真环境及相应的机器人任务策略。AGT-World通过将任务空间形式化为结构化图，实现复杂目标的精确分层分解为原子原语，并引入结合视觉-语言模型推理和几何验证的混合反馈自进化机制来自主优化策略。大量实验证明，AGT-World在成功率和泛化能力上显著优于现有方法，实现了提议、执行和纠错的自改进循环，从而促进可扩展的机器人学习。"
      },
      {
        "paper_id": "2602.12063",
        "title": "VLAW: Iterative Co-Improvement of Vision-Language-Action Policy and World Model",
        "abstract": "The goal of this paper is to improve the performance and reliability of vision-language-action (VLA) models through iterative online interaction. Since collecting policy rollouts in the real world is expensive, we investigate whether a learned simulator-specifically, an action-conditioned video generation model-can be used to generate additional rollout data. Unfortunately, existing world models lack the physical fidelity necessary for policy improvement: they are predominantly trained on demonstration datasets that lack coverage of many different physical interactions (particularly failure cases) and struggle to accurately model small yet critical physical details in contact-rich object manipulation. We propose a simple iterative improvement algorithm that uses real-world roll-out data to improve the fidelity of the world model, which can then, in turn, be used to generate supplemental synthetic data for improving the VLA model. In our experiments on a real robot, we use this approach to improve the performance of a state-of-the-art VLA model on multiple downstream tasks. We achieve a 39.2% absolute success rate improvement over the base policy and 11.6% improvement from training with the generated synthetic rollouts. Videos can be found at this anonymous website: https://sites.google.com/view/vla-w",
        "authors_display": "Chelsea Finn Team",
        "pdf_url": "http://arxiv.org/abs/2602.12063",
        "code_url": null,
        "date": "2026-02-12",
        "primary_category": "cs.RO",
        "chinese_summary": "视觉-语言-动作（VLA）模型的性能和可靠性可通过迭代在线交互提升，但现实世界策略回放数据收集昂贵。虽然学习到的模拟器（动作条件视频生成模型）有望生成额外回放数据，但现有世界模型缺乏物理保真度，难以精确建模接触密集物体操纵中的关键物理细节。本研究提出一种简单的迭代改进算法，利用真实世界回放数据提升世界模型的保真度，然后该世界模型再用于生成补充合成数据以改进VLA模型。在真实机器人实验中，该方法显著提升了最先进VLA模型在多个下游任务上的性能，相比基础策略，成功率绝对提升39.2%，相比使用生成合成回放数据进行训练，提升了11.6%。"
      },
      {
        "paper_id": "2602.12062",
        "title": "HoloBrain-0 Technical Report",
        "abstract": "In this work, we introduce HoloBrain-0, a comprehensive Vision-Language-Action (VLA) framework that bridges the gap between foundation model research and reliable real-world robot deployment. The core of our system is a novel VLA architecture that explicitly incorporates robot embodiment priors, including multi-view camera parameters and kinematic descriptions (URDF), to enhance 3D spatial reasoning and support diverse embodiments. We validate this design through a scalable ``pre-train then post-train\" paradigm, achieving state-of-the-art results on simulation benchmarks such as RoboTwin 2.0, LIBERO, and GenieSim, as well as strong results on challenging long-horizon real-world manipulation tasks. Notably, our efficient 0.2B-parameter variant rivals significantly larger baselines, enabling low-latency on-device deployment. To further accelerate research and practical adoption, we fully open-source the entire HoloBrain ecosystem, which includes: (1) powerful pre-trained VLA foundations; (2) post-trained checkpoints for multiple simulation suites and real-world tasks; and (3) RoboOrchard, a full-stack VLA infrastructure for data curation, model training and deployment. Together with standardized data collection protocols, this release provides the community with a complete, reproducible path toward high-performance robotic manipulation.",
        "authors_display": "Zhizhong Su Team",
        "pdf_url": "http://arxiv.org/abs/2602.12062",
        "code_url": null,
        "date": "2026-02-12",
        "primary_category": "cs.RO",
        "chinese_summary": "基础模型研究与可靠的真实世界机器人部署之间存在差距。为弥合此差距，本研究引入了HoloBrain-0，一个全面的视觉-语言-动作（VLA）框架。其核心是一个新颖的VLA架构，明确融入了机器人具身先验（如多视角相机参数和运动学描述），以增强3D空间推理并支持多样化的具身形态。该设计通过可扩展的“预训练-后训练”范式进行验证，在RoboTwin 2.0、LIBERO和GenieSim等仿真基准测试中取得了最先进的结果，并在具有挑战性的长时序真实世界操作任务中表现出色。值得一提的是，其高效的0.2B参数变体能与更大的基线模型相媲美，支持低延迟的设备端部署。为加速研究，HoloBrain生态系统已完全开源，包括预训练VLA基础模型、后训练检查点和全栈VLA基础设施RoboOrchard。"
      },
      {
        "paper_id": "2602.12014",
        "title": "FedGRPO: Privately Optimizing Foundation Models with Group-Relative Rewards from Domain Client",
        "abstract": "One important direction of Federated Foundation Models (FedFMs) is leveraging data from small client models to enhance the performance of a large server-side foundation model. Existing methods based on model level or representation level knowledge transfer either require expensive local training or incur high communication costs and introduce unavoidable privacy risks. We reformulate this problem as a reinforcement learning style evaluation process and propose FedGRPO, a privacy preserving framework comprising two modules. The first module performs competence-based expert selection by building a lightweight confidence graph from auxiliary data to identify the most suitable clients for each question. The second module leverages the \"Group Relative\" concept from the Group Relative Policy Optimization (GRPO) framework by packaging each question together with its solution rationale into candidate policies, dispatching these policies to a selected subset of expert clients, and aggregating solely the resulting scalar reward signals via a federated group-relative loss function. By exchanging reward values instead of data or model updates, FedGRPO reduces privacy risk and communication overhead while enabling parallel evaluation across heterogeneous devices. Empirical results on diverse domain tasks demonstrate that FedGRPO achieves superior downstream accuracy and communication efficiency compared to conventional FedFMs baselines.",
        "authors_display": "Yuxing Han Team",
        "pdf_url": "http://arxiv.org/abs/2602.12014",
        "code_url": null,
        "date": "2026-02-12",
        "primary_category": "cs.LG",
        "chinese_summary": "针对联邦基础模型(FedFMs)中现有知识迁移方法存在的训练成本高、通信开销大及隐私风险问题，本研究将其重构为强化学习式评估过程。提出了FedGRPO框架，通过构建轻量级置信图进行能力专家选择，并利用\"组相对\"概念将问题与解决方案打包为候选策略，仅聚合客户端返回的标量奖励信号，而非直接交换数据或模型更新。实验结果表明，FedGRPO在保证隐私和降低通信开销的同时，实现了优于传统FedFMs基线的下游任务精度和通信效率。"
      },
      {
        "paper_id": "2602.11978",
        "title": "Accelerating Robotic Reinforcement Learning with Agent Guidance",
        "abstract": "Reinforcement Learning (RL) offers a powerful paradigm for autonomous robots to master generalist manipulation skills through trial-and-error. However, its real-world application is stifled by severe sample inefficiency. Recent Human-in-the-Loop (HIL) methods accelerate training by using human corrections, yet this approach faces a scalability barrier. Reliance on human supervisors imposes a 1:1 supervision ratio that limits fleet expansion, suffers from operator fatigue over extended sessions, and introduces high variance due to inconsistent human proficiency. We present Agent-guided Policy Search (AGPS), a framework that automates the training pipeline by replacing human supervisors with a multimodal agent. Our key insight is that the agent can be viewed as a semantic world model, injecting intrinsic value priors to structure physical exploration. By using executable tools, the agent provides precise guidance via corrective waypoints and spatial constraints for exploration pruning. We validate our approach on two tasks, ranging from precision insertion to deformable object manipulation. Results demonstrate that AGPS outperforms HIL methods in sample efficiency. This automates the supervision pipeline, unlocking the path to labor-free and scalable robot learning. Project website: https://agps-rl.github.io/agps.",
        "authors_display": "Yaodong Yang Team",
        "pdf_url": "http://arxiv.org/abs/2602.11978",
        "code_url": null,
        "date": "2026-02-12",
        "primary_category": "cs.RO",
        "chinese_summary": "强化学习(RL)在机器人操作技能学习中面临样本效率低下，而现有的人在环(HIL)方法受限于可扩展性与人类监督的低效性。本研究提出了Agent-guided Policy Search (AGPS)框架，用多模态智能体取代人类监督者，将智能体视为语义世界模型，注入内在价值先验来指导物理探索。通过可执行工具，智能体提供精确的纠正航点和空间约束来修剪探索。实验结果表明，AGPS在精细插入和变形物体操作任务中，样本效率优于HIL方法，从而实现了自动化监督，为可扩展的机器人学习提供了新途径。"
      },
      {
        "paper_id": "2602.11882",
        "title": "Where Bits Matter in World Model Planning: A Paired Mixed-Bit Study for Efficient Spatial Reasoning",
        "abstract": "Efficient spatial reasoning requires world models that remain reliable under tight precision budgets. We study whether low-bit planning behavior is determined mostly by total bitwidth or by where bits are allocated across modules. Using DINO-WM on the Wall planning task, we run a paired-goal mixed-bit evaluation across uniform, mixed, asymmetric, and layerwise variants under two planner budgets. We observe a consistent three-regime pattern: 8-bit and 6-bit settings remain close to FP16, 3-bit settings collapse, and 4-bit settings are allocation-sensitive. In that transition region, preserving encoder precision improves planning relative to uniform quantization, and near-size asymmetric variants show the same encoder-side direction. In a later strict 22-cell replication with smaller per-cell episode count, the mixed-versus-uniform INT4 sign becomes budget-conditioned, which further highlights the sensitivity of this transition regime. These findings motivate module-aware, budget-aware quantization policies as a broader research direction for efficient spatial reasoning. Code and run artifacts are available at https://github.com/suraj-ranganath/DINO-MBQuant.",
        "authors_display": "Vaishak Menon Team",
        "pdf_url": "http://arxiv.org/abs/2602.11882",
        "code_url": null,
        "date": "2026-02-12",
        "primary_category": "cs.LG",
        "chinese_summary": "为实现高效空间推理，本研究探讨了低比特规划行为在有限精度预算下，是否主要由总比特位宽决定，或由比特位在模型模块间的分配方式决定。通过在DINO-WM上进行混合位评估，观察到8比特和6比特设置接近FP16，3比特设置性能崩溃，而4比特设置对位宽分配敏感。在4比特过渡区，保留编码器精度可提升规划性能。这些发现表明，模块感知、预算感知的量化策略对于高效空间推理具有重要研究意义。"
      },
      {
        "paper_id": "2602.11807",
        "title": "PuYun-LDM: A Latent Diffusion Model for High-Resolution Ensemble Weather Forecasts",
        "abstract": "Latent diffusion models (LDMs) suffer from limited diffusability in high-resolution (<=0.25°) ensemble weather forecasting, where diffusability characterizes how easily a latent data distribution can be modeled by a diffusion process. Unlike natural image fields, meteorological fields lack task-agnostic foundation models and explicit semantic structures, making VFM-based regularization inapplicable. Moreover, existing frequency-based approaches impose identical spectral regularization across channels under a homogeneity assumption, which leads to uneven regularization strength under the inter-variable spectral heterogeneity in multivariate meteorological data. To address these challenges, we propose a 3D Masked AutoEncoder (3D-MAE) that encodes weather-state evolution features as an additional conditioning for the diffusion model, together with a Variable-Aware Masked Frequency Modeling (VA-MFM) strategy that adaptively selects thresholds based on the spectral energy distribution of each variable. Together, we propose PuYun-LDM, which enhances latent diffusability and achieves superior performance to ENS at short lead times while remaining comparable to ENS at longer horizons. PuYun-LDM generates a 15-day global forecast with a 6-hour temporal resolution in five minutes on a single NVIDIA H200 GPU, while ensemble forecasts can be efficiently produced in parallel.",
        "authors_display": "Bin Wang Team",
        "pdf_url": "http://arxiv.org/abs/2602.11807",
        "code_url": null,
        "date": "2026-02-12",
        "primary_category": "cs.AI",
        "chinese_summary": "潜在扩散模型(LDMs)在 <=0.25° 的高分辨率集合天气预报中存在扩散性受限问题，且现有频率方法因缺乏对变量间频谱异质性的考虑而导致正则化强度不均。本研究提出了PuYun-LDM模型，结合3D掩码自编码器(3D-MAE)编码天气状态演化特征作为扩散模型的额外条件，并采用变量感知掩码频率建模(VA-MFM)策略自适应选择阈值。实验证明，PuYun-LDM增强了潜在扩散性，在短时效预报中表现优于ENS，长时效预报中与ENS相当，并能在短时间内高效生成全球预报。"
      },
      {
        "paper_id": "2602.11758",
        "title": "HAIC: Humanoid Agile Object Interaction Control via Dynamics-Aware World Model",
        "abstract": "Humanoid robots show promise for complex whole-body tasks in unstructured environments. Although Human-Object Interaction (HOI) has advanced, most methods focus on fully actuated objects rigidly coupled to the robot, ignoring underactuated objects with independent dynamics and non-holonomic constraints. These introduce control challenges from coupling forces and occlusions. We present HAIC, a unified framework for robust interaction across diverse object dynamics without external state estimation. Our key contribution is a dynamics predictor that estimates high-order object states (velocity, acceleration) solely from proprioceptive history. These predictions are projected onto static geometric priors to form a spatially grounded dynamic occupancy map, enabling the policy to infer collision boundaries and contact affordances in blind spots. We use asymmetric fine-tuning, where a world model continuously adapts to the student policy's exploration, ensuring robust state estimation under distribution shifts. Experiments on a humanoid robot show HAIC achieves high success rates in agile tasks (skateboarding, cart pushing/pulling under various loads) by proactively compensating for inertial perturbations, and also masters multi-object long-horizon tasks like carrying a box across varied terrain by predicting the dynamics of multiple objects.",
        "authors_display": "Renjing Xu Team",
        "pdf_url": "http://arxiv.org/abs/2602.11758",
        "code_url": "https://haic-humanoid.github.io/",
        "date": "2026-02-12",
        "primary_category": "cs.RO",
        "chinese_summary": "类人机器人在处理非结构化环境中复杂全身任务时，现有方法难以应对具有独立动力学和非完整约束的欠驱动物体，且缺乏外部状态估计。本研究提出了HAIC统一框架，通过仅利用本体感受历史数据估计高阶物体状态的动力学预测器，并将预测投影至静态几何先验形成空间接地动态占用图，使策略能在盲区推断碰撞边界和接触可供性。通过非对称微调，世界模型持续适应学生策略。实验表明，HAIC在敏捷任务和多物体长时序任务中均取得了高成功率，有效应对惯性扰动并预测多物体动力学。"
      },
      {
        "paper_id": "2602.11598",
        "title": "ABot-N0: Technical Report on the VLA Foundation Model for Versatile Embodied Navigation",
        "abstract": "Embodied navigation has long been fragmented by task-specific architectures. We introduce ABot-N0, a unified Vision-Language-Action (VLA) foundation model that achieves a ``Grand Unification'' across 5 core tasks: Point-Goal, Object-Goal, Instruction-Following, POI-Goal, and Person-Following. ABot-N0 utilizes a hierarchical ``Brain-Action'' architecture, pairing an LLM-based Cognitive Brain for semantic reasoning with a Flow Matching-based Action Expert for precise, continuous trajectory generation.   To support large-scale learning, we developed the ABot-N0 Data Engine, curating 16.9M expert trajectories and 5.0M reasoning samples across 7,802 high-fidelity 3D scenes (10.7 $\\text{km}^2$). ABot-N0 achieves new SOTA performance across 7 benchmarks, significantly outperforming specialized models. Furthermore, our Agentic Navigation System integrates a planner with hierarchical topological memory, enabling robust, long-horizon missions in dynamic real-world environments.",
        "authors_display": "Mu Xu Team",
        "pdf_url": "http://arxiv.org/abs/2602.11598",
        "code_url": "https://amap-cvlab.github.io/ABot-Navigation/ABot-N0/",
        "date": "2026-02-12",
        "primary_category": "cs.RO",
        "chinese_summary": "具身导航领域长期受限于任务特定的架构。本研究引入了ABot-N0，一个统一的视觉-语言-动作(VLA)基础模型，旨在实现点目标、物体目标、指令跟随、POI目标和人物跟随五项核心导航任务的“大一统”。模型采用分层“大脑-行动”架构，将基于大型语言模型的认知大脑与基于流匹配的动作专家结合。为支持大规模学习，研究团队开发了ABot-N0数据引擎， curating 16.9M专家轨迹和5.0M推理样本。ABot-N0在7个基准测试中取得了新的SOTA性能，并整合规划器与分层拓扑记忆，实现动态真实世界环境中的鲁棒长时序任务。"
      },
      {
        "paper_id": "2602.11558",
        "title": "Brain4FMs: A Benchmark of Foundation Models for Electrical Brain Signal",
        "abstract": "Brain Foundation Models (BFMs) are transforming neuroscience by enabling scalable and transferable learning from neural signals, advancing both clinical diagnostics and cutting-edge neuroscience exploration. Their emergence is powered by large-scale clinical recordings, particularly electroencephalography (EEG) and intracranial EEG, which provide rich temporal and spatial representations of brain dynamics. However, despite their rapid proliferation, the field lacks a unified understanding of existing methodologies and a standardized evaluation framework. To fill this gap, we map the benchmark design space along two axes: (i) from the model perspective, we organize BFMs under a self-supervised learning (SSL) taxonomy; and (ii) from the dataset perspective, we summarize common downstream tasks and curate representative public datasets across clinical and human-centric neurotechnology applications. Building on this consolidation, we introduce Brain4FMs, an open evaluation platform with plug-and-play interfaces that integrates 15 representative BFMs and 18 public datasets. It enables standardized comparisons and analysis of how pretraining data, SSL strategies, and architectures affect generalization and downstream performance, guiding more accurate and transferable BFMs. The code is available at https://anonymous.4open.science/r/Brain4FMs-85B8.",
        "authors_display": "Yang Yang Team",
        "pdf_url": "http://arxiv.org/abs/2602.11558",
        "code_url": null,
        "date": "2026-02-12",
        "primary_category": "cs.LG",
        "chinese_summary": "脑基础模型(BFMs)在神经科学领域快速发展，但缺乏统一的方法理解和标准化评估框架。本研究旨在填补这一空白，通过自监督学习(SSL)分类法组织BFMs，并从数据集角度总结常见下游任务，整理临床和以人为中心神经技术应用的代表性公共数据集。在此基础上，推出了Brain4FMs开放评估平台，集成了15个代表性BFM和18个公共数据集。该平台支持标准化比较和分析预训练数据、SSL策略和架构对泛化和下游性能的影响，以指导更准确、可迁移的BFMs开发。"
      },
      {
        "paper_id": "2602.11550",
        "title": "TS-Memory: Plug-and-Play Memory for Time Series Foundation Models",
        "abstract": "Time Series Foundation Models (TSFMs) achieve strong zero-shot forecasting through large-scale pre-training, but adapting them to downstream domains under distribution shift remains challenging. Existing solutions face a trade-off: Parametric Adaptation can cause catastrophic forgetting and requires costly multi-domain maintenance, while Non-Parametric Retrieval improves forecasts but incurs high inference latency due to datastore search. We propose Parametric Memory Distillation and implement it as TS-Memory, a lightweight memory adapter that augments frozen TSFMs. TS-Memory is trained in two stages. First, we construct an offline, leakage-safe kNN teacher that synthesizes confidence-aware quantile targets from retrieved futures. Second, we distill this retrieval-induced distributional correction into a lightweight memory adapter via confidence-gated supervision. During inference, TS-Memory fuses memory and backbone predictions with constant-time overhead, enabling retrieval-free deployment. Experiments across diverse TSFMs and benchmarks demonstrate consistent improvements in both point and probabilistic forecasting over representative adaptation methods, with efficiency comparable to the frozen backbone.",
        "authors_display": "Yuxuan Liang Team",
        "pdf_url": "http://arxiv.org/abs/2602.11550",
        "code_url": null,
        "date": "2026-02-12",
        "primary_category": "cs.LG",
        "chinese_summary": "时间序列基础模型(TSFMs)在分布偏移下的下游领域适应面临挑战，现有参数化适应易导致灾难性遗忘，而非参数化检索则引入高推理延迟。本研究提出了参数化记忆蒸馏方法并实现为TS-Memory，一个轻量级记忆适配器。TS-Memory分两阶段训练：首先构建离线、无信息泄露的kNN教师模型合成置信度感知的量化目标；其次通过置信度门控监督将检索诱导的分布校正蒸馏到记忆适配器中。实验表明，TS-Memory在多种TSFMs和基准测试上持续改进点预测和概率预测性能，且效率与冻结骨干网络相当，实现了无检索部署。"
      },
      {
        "paper_id": "2602.11541",
        "title": "Budget-Constrained Agentic Large Language Models: Intention-Based Planning for Costly Tool Use",
        "abstract": "We study budget-constrained tool-augmented agents, where a large language model must solve multi-step tasks by invoking external tools under a strict monetary budget. We formalize this setting as sequential decision making in context space with priced and stochastic tool executions, making direct planning intractable due to massive state-action spaces, high variance of outcomes and prohibitive exploration cost. To address these challenges, we propose INTENT, an inference-time planning framework that leverages an intention-aware hierarchical world model to anticipate future tool usage, risk-calibrated cost, and guide decisions online. Across cost-augmented StableToolBench, INTENT strictly enforces hard budget feasibility while substantially improving task success over baselines, and remains robust under dynamic market shifts such as tool price changes and varying budgets.",
        "authors_display": "Qi Qi Team",
        "pdf_url": "http://arxiv.org/abs/2602.11541",
        "code_url": null,
        "date": "2026-02-12",
        "primary_category": "cs.AI",
        "chinese_summary": "在预算约束下，大型语言模型调用外部工具解决多步骤任务的工具增强型智能体面临巨大状态-动作空间、高结果方差和过高探索成本导致直接规划难以处理的问题。本研究提出了INTENT，一个推理时规划框架，利用意图感知的层次世界模型来预测未来的工具使用和风险校准成本，并在线指导决策。实验证明，INTENT在成本增强型StableToolBench上严格遵循硬预算可行性，显著提高了任务成功率，并在工具价格和预算动态变化下表现出鲁棒性。"
      },
      {
        "paper_id": "2602.11536",
        "title": "Vascular anatomy-aware self-supervised pre-training for X-ray angiogram analysis",
        "abstract": "X-ray angiography is the gold standard imaging modality for cardiovascular diseases. However, current deep learning approaches for X-ray angiogram analysis are severely constrained by the scarcity of annotated data. While large-scale self-supervised learning (SSL) has emerged as a promising solution, its potential in this domain remains largely unexplored, primarily due to the lack of effective SSL frameworks and large-scale datasets. To bridge this gap, we introduce a vascular anatomy-aware masked image modeling (VasoMIM) framework that explicitly integrates domain-specific anatomical knowledge. Specifically, VasoMIM comprises two key designs: an anatomy-guided masking strategy and an anatomical consistency loss. The former strategically masks vessel-containing patches to compel the model to learn robust vascular semantics, while the latter preserves structural consistency of vessels between original and reconstructed images, enhancing the discriminability of the learned representations. In conjunction with VasoMIM, we curate XA-170K, the largest X-ray angiogram pre-training dataset to date. We validate VasoMIM on four downstream tasks across six datasets, where it demonstrates superior transferability and achieves state-of-the-art performance compared to existing methods. These findings highlight the significant potential of VasoMIM as a foundation model for advancing a wide range of X-ray angiogram analysis tasks. VasoMIM and XA-170K will be available at https://github.com/Dxhuang-CASIA/XA-SSL.",
        "authors_display": "Zeng-Guang Hou Team",
        "pdf_url": "http://arxiv.org/abs/2602.11536",
        "code_url": null,
        "date": "2026-02-12",
        "primary_category": "cs.CV",
        "chinese_summary": "X射线血管造影图像分析受限于带标注数据稀缺，大规模自监督学习(SSL)在该领域潜力未被充分挖掘。本研究引入了VasoMIM框架，通过解剖引导的掩码策略强制模型学习血管语义，并利用解剖一致性损失保留血管结构一致性，从而增强学习表征的判别力。同时，策展了迄今最大的X射线血管造影预训练数据集XA-170K。VasoMIM在多个下游任务中展现出卓越的可迁移性和领先的性能，凸显了其作为X射线血管造影分析基础模型的巨大潜力。"
      },
      {
        "paper_id": "2602.12461",
        "title": "Semantic-aware Adversarial Fine-tuning for CLIP",
        "abstract": "Recent studies have shown that CLIP model's adversarial robustness in zero-shot classification tasks can be enhanced by adversarially fine-tuning its image encoder with adversarial examples (AEs), which are generated by minimizing the cosine similarity between images and a hand-crafted template (e.g., ''A photo of a {label}''). However, it has been shown that the cosine similarity between a single image and a single hand-crafted template is insufficient to measure the similarity for image-text pairs. Building on this, in this paper, we find that the AEs generated using cosine similarity may fail to fool CLIP when the similarity metric is replaced with semantically enriched alternatives, making the image encoder fine-tuned with these AEs less robust. To overcome this issue, we first propose a semantic-ensemble attack to generate semantic-aware AEs by minimizing the average similarity between the original image and an ensemble of refined textual descriptions. These descriptions are initially generated by a foundation model to capture core semantic features beyond hand-crafted templates and are then refined to reduce hallucinations. To this end, we propose Semantic-aware Adversarial Fine-Tuning (SAFT), which fine-tunes CLIP's image encoder with semantic-aware AEs. Extensive experiments show that SAFT outperforms current methods, achieving substantial improvements in zero-shot adversarial robustness across 16 datasets. Our code is available at: https://github.com/tmlr-group/SAFT.",
        "authors_display": "Feng Liu Team",
        "pdf_url": "http://arxiv.org/abs/2602.12461",
        "code_url": null,
        "date": "2026-02-12",
        "primary_category": "cs.CV",
        "chinese_summary": "当前研究表明，通过对抗性微调CLIP图像编码器可增强其零样本分类的对抗鲁棒性，但生成对抗样本（AEs）时仅依赖图像与单一手动模板的余弦相似度，不足以衡量图文对的语义相似性，导致微调后的模型鲁棒性不足。为解决此问题，本文提出了一种语义集成攻击方法，通过最小化原始图像与一组精炼文本描述（由基础模型生成并去除了幻觉）之间的平均相似度来生成语义感知的AEs。在此基础上，作者提出了语义感知对抗微调（SAFT）框架。实验结果表明，SAFT在16个数据集上的零样本对抗鲁棒性方面显著优于现有方法，实现了实质性提升。"
      },
      {
        "paper_id": "2602.12429",
        "title": "Stabilizing Native Low-Rank LLM Pretraining",
        "abstract": "Foundation models have achieved remarkable success, yet their growing parameter counts pose significant computational and memory challenges. Low-rank factorization offers a promising route to reduce training and inference costs, but the community lacks a stable recipe for training models from scratch using exclusively low-rank weights while matching the performance of the dense model. We demonstrate that Large Language Models (LLMs) can be trained from scratch using exclusively low-rank factorized weights for all non-embedding matrices without auxiliary \"full-rank\" guidance required by prior methods. While native low-rank training often suffers from instability and loss spikes, we identify uncontrolled growth in the spectral norm (largest singular value) of the weight matrix update as the dominant factor. To address this, we introduce Spectron: Spectral renormalization with orthogonalization, which dynamically bounds the resultant weight updates based on the current spectral norms of the factors. Our method enables stable, end-to-end factorized training with negligible overhead. Finally, we establish compute-optimal scaling laws for natively low-rank transformers, demonstrating predictable power-law behavior and improved inference efficiency relative to dense models.",
        "authors_display": "Eugene Belilovsky Team",
        "pdf_url": "http://arxiv.org/abs/2602.12429",
        "code_url": null,
        "date": "2026-02-12",
        "primary_category": "cs.LG",
        "chinese_summary": "基础模型日益增长的参数量带来了巨大的计算和内存挑战，而低秩分解是降低成本的潜在途径，但从头开始仅使用低秩权重训练模型且性能匹配全秩模型仍缺乏稳定的方法。本文研究表明，无需先验方法的“全秩”辅助指导，大型语言模型（LLMs）可以从头开始仅使用低秩分解权重训练所有非嵌入矩阵。作者发现权重矩阵更新中谱范数（最大奇异值）的失控增长是导致原生低秩训练不稳定和损失尖峰的主要因素，并提出Spectron方法：通过正交化进行谱重归一化，根据因子当前的谱范数动态限制所得权重更新。实验证明，Spectron实现了稳定、端到端的低秩训练，开销可忽略不计，并为原生低秩Transformer建立了计算最优的缩放定律，展示了可预测的幂律行为和相对于全秩模型改进的推理效率。"
      },
      {
        "paper_id": "2602.12373",
        "title": "Policy4OOD: A Knowledge-Guided World Model for Policy Intervention Simulation against the Opioid Overdose Crisis",
        "abstract": "The opioid epidemic remains one of the most severe public health crises in the United States, yet evaluating policy interventions before implementation is difficult: multiple policies interact within a dynamic system where targeting one risk pathway may inadvertently amplify another. We argue that effective opioid policy evaluation requires three capabilities -- forecasting future outcomes under current policies, counterfactual reasoning about alternative past decisions, and optimization over candidate interventions -- and propose to unify them through world modeling. We introduce Policy4OOD, a knowledge-guided spatio-temporal world model that addresses three core challenges: what policies prescribe, where effects manifest, and when effects unfold.Policy4OOD jointly encodes policy knowledge graphs, state-level spatial dependencies, and socioeconomic time series into a policy-conditioned Transformer that forecasts future opioid outcomes.Once trained, the world model serves as a simulator: forecasting requires only a forward pass, counterfactual analysis substitutes alternative policy encodings in the historical sequence, and policy optimization employs Monte Carlo Tree Search over the learned simulator. To support this framework, we construct a state-level monthly dataset (2019--2024) integrating opioid mortality, socioeconomic indicators, and structured policy encodings. Experiments demonstrate that spatial dependencies and structured policy knowledge significantly improve forecasting accuracy, validating each architectural component and the potential of world modeling for data-driven public health decision support.",
        "authors_display": "Yanfang Ye Team",
        "pdf_url": "http://arxiv.org/abs/2602.12373",
        "code_url": null,
        "date": "2026-02-12",
        "primary_category": "cs.LG",
        "chinese_summary": "阿片类药物危机是美国严重的公共卫生问题，但由于政策互动复杂且系统动态，评估干预措施极具挑战。本文提出Policy4OOD，一个知识引导的时空世界模型，旨在整合预测、反事实推理和优化三种关键能力来有效评估阿片类政策。该模型通过策略知识图谱、州级空间依赖性及社会经济时间序列的联合编码，构建一个策略条件化的Transformer来预测阿片类药物相关结果。训练完成后，世界模型可作为模拟器，通过前向传播进行预测，通过替换历史策略编码进行反事实分析，并通过蒙特卡洛树搜索进行策略优化。实验结果表明，空间依赖性和结构化策略知识显著提高了预测准确性，验证了该模型在数据驱动的公共卫生决策支持中的潜力。"
      },
      {
        "paper_id": "2602.12317",
        "title": "Free Lunch in Medical Image Foundation Model Pre-training via Randomized Synthesis and Disentanglement",
        "abstract": "Medical image foundation models (MIFMs) have demonstrated remarkable potential for a wide range of clinical tasks, yet their development is constrained by the scarcity, heterogeneity, and high cost of large-scale annotated datasets. Here, we propose RaSD (Randomized Synthesis and Disentanglement), a scalable framework for pre-training MIFMs entirely on synthetic data. By modeling anatomical structures and appearance variations with randomized Gaussian distributions, RaSD exposes models to sufficient multi-scale structural and appearance perturbations, forcing them to rely on invariant and task-relevant anatomical cues rather than dataset-specific textures, thereby enabling robust and transferable representation learning. We pre-trained RaSD on 1.2 million 3D volumes and 9.6 million 2D images, and extensively evaluated the resulting models across 6 imaging modalities, 48 datasets, and 56 downstream tasks. Across all evaluated downstream tasks, RaSD consistently outperforms training-from-scratch models, achieves the best performance on 17 tasks, and remains comparable to models pre-trained on large real datasets in most others. These results demonstrate that the capacity of synthetic data alone to drive robust representation learning. Our findings establish a paradigm shift in medical AI, demonstrating that synthetic data can serve as a \"free lunch\" for scalable, privacy-preserving, and clinically generalizable foundation models.",
        "authors_display": "Hao Chen Team",
        "pdf_url": "http://arxiv.org/abs/2602.12317",
        "code_url": null,
        "date": "2026-02-12",
        "primary_category": "q-bio.QM",
        "chinese_summary": "医学图像基础模型（MIFMs）在临床任务中展现巨大潜力，但其发展受限于大规模标注数据集的稀缺、异质性和高成本。本文提出RaSD（Randomized Synthesis and Disentanglement），一个可扩展的框架，可完全利用合成数据预训练MIFMs。RaSD通过随机高斯分布模拟解剖结构和外观变异，使模型接触足够的多尺度结构和外观扰动，从而迫使其依赖不变和任务相关的解剖线索而非数据集特有纹理，实现鲁棒和可迁移的表示学习。在120万3D体和960万2D图像上进行预训练后，RaSD模型在6种成像模态、48个数据集和56个下游任务中，持续优于从零开始训练的模型，在17个任务上取得了最佳性能，并在大多数其他任务上与使用大型真实数据集预训练的模型表现相当。这些结果证明了仅合成数据即可驱动鲁棒表示学习的能力，为医学AI领域带来了范式转变。"
      }
    ],
    "VLM": [
      {
        "paper_id": "2602.23351",
        "title": "Scale Can't Overcome Pragmatics: The Impact of Reporting Bias on Vision-Language Reasoning",
        "abstract": "The lack of reasoning capabilities in Vision-Language Models (VLMs) has remained at the forefront of research discourse. We posit that this behavior stems from a reporting bias in their training data. That is, how people communicate about visual content by default omits tacit information needed to supervise some types of reasoning; e.g., \"at the game today!\" is a more likely caption than \"a photo of 37 people standing behind a field\". We investigate the data underlying the popular VLMs OpenCLIP, LLaVA-1.5 and Molmo through the lens of theories from pragmatics, and find that reporting bias results in insufficient representation of four reasoning skills (spatial, temporal, negation, and counting), despite the corpora being of web-scale, and/or synthetically generated. With a set of curated benchmarks, we demonstrate that: (i) VLMs perform poorly on the aforementioned types of reasoning suppressed in the training data by reporting bias; (ii) contrary to popular belief, scaling data size, model size, and to multiple languages does not result in emergence of these skills by default; but, promisingly, (iii) incorporating annotations specifically collected to obtain tacit information is effective. Our findings highlight the need for more intentional training data curation methods, rather than counting on scale for emergence of reasoning capabilities.",
        "authors_display": "Ranjay Krishna Team",
        "pdf_url": "http://arxiv.org/abs/2602.23351",
        "code_url": null,
        "date": "2026-02-26",
        "primary_category": "cs.CL",
        "chinese_summary": "当前视觉-语言模型（VLMs）在推理能力上的不足，源于训练数据中存在的“报告偏差”，即人们在描述视觉内容时常省略隐性信息，导致模型难以学习空间、时间、否定和计数等推理技能。研究通过分析OpenCLIP、LLaVA-1.5和Molmo等流行VLM的数据集，并构建专门的基准测试，发现这些模型在受报告偏差抑制的推理任务上表现不佳。结果表明，简单地扩大数据规模、模型尺寸或支持多语言并不能默认提升这些能力，但专门收集的带有隐性信息标注的数据能有效解决此问题。这强调了有意识的数据策划对于提升VLM推理能力的重要性，而非仅依赖规模。"
      },
      {
        "paper_id": "2602.23339",
        "title": "Retrieve and Segment: Are a Few Examples Enough to Bridge the Supervision Gap in Open-Vocabulary Segmentation?",
        "abstract": "Open-vocabulary segmentation (OVS) extends the zero-shot recognition capabilities of vision-language models (VLMs) to pixel-level prediction, enabling segmentation of arbitrary categories specified by text prompts. Despite recent progress, OVS lags behind fully supervised approaches due to two challenges: the coarse image-level supervision used to train VLMs and the semantic ambiguity of natural language. We address these limitations by introducing a few-shot setting that augments textual prompts with a support set of pixel-annotated images. Building on this, we propose a retrieval-augmented test-time adapter that learns a lightweight, per-image classifier by fusing textual and visual support features. Unlike prior methods relying on late, hand-crafted fusion, our approach performs learned, per-query fusion, achieving stronger synergy between modalities. The method supports continually expanding support sets, and applies to fine-grained tasks such as personalized segmentation. Experiments show that we significantly narrow the gap between zero-shot and supervised segmentation while preserving open-vocabulary ability.",
        "authors_display": "Giorgos Tolias Team",
        "pdf_url": "http://arxiv.org/abs/2602.23339",
        "code_url": null,
        "date": "2026-02-26",
        "primary_category": "cs.CV",
        "chinese_summary": "开放词汇分割（OVS）旨在将视觉-语言模型的零样本识别能力扩展到像素级别，但其性能受限于VLM训练中粗粒度的图像级监督和自然语言的语义模糊性。为解决这些挑战，本研究引入了一个少样本设定，通过带像素标注的支持图像来增强文本提示，并提出了一个检索增强的测试时适配器。该适配器通过融合文本和视觉支持特征学习一个轻量级的每图像分类器，实现了学习到的、每查询的模态融合，而非先前方法的晚期、手工融合。实验证明，该方法显著缩小了零样本分割与全监督分割之间的差距，同时保持了开放词汇能力，并支持持续扩展支持集和应用于个性化分割等细粒度任务。"
      },
      {
        "paper_id": "2602.23276",
        "title": "CXReasonAgent: Evidence-Grounded Diagnostic Reasoning Agent for Chest X-rays",
        "abstract": "Chest X-ray plays a central role in thoracic diagnosis, and its interpretation inherently requires multi-step, evidence-grounded reasoning. However, large vision-language models (LVLMs) often generate plausible responses that are not faithfully grounded in diagnostic evidence and provide limited visual evidence for verification, while also requiring costly retraining to support new diagnostic tasks, limiting their reliability and adaptability in clinical settings. To address these limitations, we present CXReasonAgent, a diagnostic agent that integrates a large language model (LLM) with clinically grounded diagnostic tools to perform evidence-grounded diagnostic reasoning using image-derived diagnostic and visual evidence. To evaluate these capabilities, we introduce CXReasonDial, a multi-turn dialogue benchmark with 1,946 dialogues across 12 diagnostic tasks, and show that CXReasonAgent produces faithfully grounded responses, enabling more reliable and verifiable diagnostic reasoning than LVLMs. These findings highlight the importance of integrating clinically grounded diagnostic tools, particularly in safety-critical clinical settings.",
        "authors_display": "Edward Choi Team",
        "pdf_url": "http://arxiv.org/abs/2602.23276",
        "code_url": null,
        "date": "2026-02-26",
        "primary_category": "cs.AI",
        "chinese_summary": "胸部X射线诊断需要多步骤、基于证据的推理，但现有大型视觉-语言模型（LVLMs）常生成缺乏诊断证据支持的“貌似合理”回应，且难以提供可验证的视觉证据，同时在新任务上适应性差。为解决这些限制，本研究提出了CXReasonAgent，一个将大型语言模型（LLM）与临床诊断工具相结合的诊断智能体，能够利用图像导出的诊断和视觉证据执行基于证据的诊断推理。为评估其能力，研究引入了CXReasonDial多轮对话基准，并在12项诊断任务上验证了CXReasonAgent能生成忠实于证据的回应，展现出比LVLMs更可靠和可验证的诊断推理能力，突显了在安全攸关的临床环境中集成临床诊断工具的重要性。"
      },
      {
        "paper_id": "2602.23229",
        "title": "Large Multimodal Models as General In-Context Classifiers",
        "abstract": "Which multimodal model should we use for classification? Previous studies suggest that the answer lies in CLIP-like contrastive Vision-Language Models (VLMs), due to their remarkable performance in zero-shot classification. In contrast, Large Multimodal Models (LMM) are more suitable for complex tasks. In this work, we argue that this answer overlooks an important capability of LMMs: in-context learning. We benchmark state-of-the-art LMMs on diverse datasets for closed-world classification and find that, although their zero-shot performance is lower than CLIP's, LMMs with a few in-context examples can match or even surpass contrastive VLMs with cache-based adapters, their \"in-context\" equivalent. We extend this analysis to the open-world setting, where the generative nature of LMMs makes them more suitable for the task. In this challenging scenario, LMMs struggle whenever provided with imperfect context information. To address this issue, we propose CIRCLE, a simple training-free method that assigns pseudo-labels to in-context examples, iteratively refining them with the available context itself. Through extensive experiments, we show that CIRCLE establishes a robust baseline for open-world classification, surpassing VLM counterparts and highlighting the potential of LMMs to serve as unified classifiers, and a flexible alternative to specialized models.",
        "authors_display": "Elisa Ricci Team",
        "pdf_url": "http://arxiv.org/abs/2602.23229",
        "code_url": "https://circle-lmm.github.io/",
        "date": "2026-02-26",
        "primary_category": "cs.CV",
        "chinese_summary": "现有研究在分类任务中倾向于对比学习的视觉-语言模型（VLM），而忽略了大型多模态模型（LMM）的上下文学习能力。本研究通过对最先进的LMMs在封闭世界分类数据集上进行基准测试，发现尽管其零样本性能低于CLIP，但通过少量上下文示例，LMMs能匹配甚至超越带有缓存适配器的对比VLM。针对开放世界场景，研究提出了CIRCLE，一种无需训练的方法，通过迭代地为上下文示例分配并精炼伪标签。实验证明，CIRCLE在开放世界分类中建立了稳健的基线，超越了VLM对应模型，突显了LMMs作为统一分类器和专用模型灵活替代品的潜力。"
      },
      {
        "paper_id": "2602.23228",
        "title": "MovieTeller: Tool-augmented Movie Synopsis with ID Consistent Progressive Abstraction",
        "abstract": "With the explosive growth of digital entertainment, automated video summarization has become indispensable for applications such as content indexing, personalized recommendation, and efficient media archiving. Automatic synopsis generation for long-form videos, such as movies and TV series, presents a significant challenge for existing Vision-Language Models (VLMs). While proficient at single-image captioning, these general-purpose models often exhibit critical failures in long-duration contexts, primarily a lack of ID-consistent character identification and a fractured narrative coherence. To overcome these limitations, we propose MovieTeller, a novel framework for generating movie synopses via tool-augmented progressive abstraction. Our core contribution is a training-free, tool-augmented, fact-grounded generation process. Instead of requiring costly model fine-tuning, our framework directly leverages off-the-shelf models in a plug-and-play manner. We first invoke a specialized face recognition model as an external \"tool\" to establish Factual Groundings--precise character identities and their corresponding bounding boxes. These groundings are then injected into the prompt to steer the VLM's reasoning, ensuring the generated scene descriptions are anchored to verifiable facts. Furthermore, our progressive abstraction pipeline decomposes the summarization of a full-length movie into a multi-stage process, effectively mitigating the context length limitations of current VLMs. Experiments demonstrate that our approach yields significant improvements in factual accuracy, character consistency, and overall narrative coherence compared to end-to-end baselines.",
        "authors_display": "Gaoang Wang Team",
        "pdf_url": "http://arxiv.org/abs/2602.23228",
        "code_url": null,
        "date": "2026-02-26",
        "primary_category": "cs.CV",
        "chinese_summary": "现有视觉-语言模型（VLMs）在长篇视频（如电影、电视剧）的自动摘要方面存在显著挑战，主要表现为角色识别不一致和叙事连贯性不足。为克服这些限制，本研究提出了MovieTeller框架，通过工具增强的渐进式抽象方法生成电影摘要。该方法无需模型微调，而是即插即用地利用现成模型，首先通过专业人脸识别工具获取精确的角色身份和边界框（事实依据），然后将其注入提示中以引导VLM的推理，确保生成场景描述的事实准确性。此外，渐进式抽象流水线将长电影摘要分解为多阶段过程，有效缓解了当前VLM的上下文长度限制。实验结果表明，MovieTeller在事实准确性、角色一致性和整体叙事连贯性方面显著优于端到端基线方法。"
      },
      {
        "paper_id": "2602.23088",
        "title": "Cytoarchitecture in Words: Weakly Supervised Vision-Language Modeling for Human Brain Microscopy",
        "abstract": "Foundation models increasingly offer potential to support interactive, agentic workflows that assist researchers during analysis and interpretation of image data. Such workflows often require coupling vision to language to provide a natural-language interface. However, paired image-text data needed to learn this coupling are scarce and difficult to obtain in many research and clinical settings. One such setting is microscopic analysis of cell-body-stained histological human brain sections, which enables the study of cytoarchitecture: cell density and morphology and their laminar and areal organization. Here, we propose a label-mediated method that generates meaningful captions from images by linking images and text only through a label, without requiring curated paired image-text data. Given the label, we automatically mine area descriptions from related literature and use them as synthetic captions reflecting canonical cytoarchitectonic attributes. An existing cytoarchitectonic vision foundation model (CytoNet) is then coupled to a large language model via an image-to-text training objective, enabling microscopy regions to be described in natural language. Across 57 brain areas, the resulting method produces plausible area-level descriptions and supports open-set use through explicit rejection of unseen areas. It matches the cytoarchitectonic reference label for in-scope patches with 90.6% accuracy and, with the area label masked, its descriptions remain discriminative enough to recover the area in an 8-way test with 68.6% accuracy. These results suggest that weak, label-mediated pairing can suffice to connect existing biomedical vision foundation models to language, providing a practical recipe for integrating natural-language in domains where fine-grained paired annotations are scarce.",
        "authors_display": "Christian Schiffer Team",
        "pdf_url": "http://arxiv.org/abs/2602.23088",
        "code_url": null,
        "date": "2026-02-26",
        "primary_category": "cs.CV",
        "chinese_summary": "在许多研究和临床环境中，特别是人脑细胞体染色组织学切片的显微分析中，用于训练视觉与语言耦合的配对图像-文本数据稀缺。为解决这一问题，本研究提出了一种标签介导的方法，通过仅利用标签将图像与文本关联起来，无需精心策划的配对图像-文本数据即可生成有意义的图像描述。具体而言，给定区域标签，方法自动从相关文献中挖掘区域描述作为反映细胞结构特征的合成标题，然后通过图像到文本的训练目标将现有的细胞结构视觉基础模型（CytoNet）与大型语言模型耦合。实验结果显示，该方法在57个大脑区域生成了合理的区域描述，并支持对未见区域的显式拒绝，对于范围内的图像块与参考标签匹配准确率达到90.6%，在隐藏区域标签的情况下，其描述仍能以68.6%的准确率在8个区域测试中恢复正确区域。这表明，弱的、标签介导的配对足以将现有的生物医学视觉基础模型与语言连接起来，为细粒度配对标注稀缺的领域提供了一种实用的自然语言集成方案。"
      },
      {
        "paper_id": "2602.23013",
        "title": "SubspaceAD: Training-Free Few-Shot Anomaly Detection via Subspace Modeling",
        "abstract": "Detecting visual anomalies in industrial inspection often requires training with only a few normal images per category. Recent few-shot methods achieve strong results employing foundation-model features, but typically rely on memory banks, auxiliary datasets, or multi-modal tuning of vision-language models. We therefore question whether such complexity is necessary given the feature representations of vision foundation models. To answer this question, we introduce SubspaceAD, a training-free method, that operates in two simple stages. First, patch-level features are extracted from a small set of normal images by a frozen DINOv2 backbone. Second, a Principal Component Analysis (PCA) model is fit to these features to estimate the low-dimensional subspace of normal variations. At inference, anomalies are detected via the reconstruction residual with respect to this subspace, producing interpretable and statistically grounded anomaly scores. Despite its simplicity, SubspaceAD achieves state-of-the-art performance across one-shot and few-shot settings without training, prompt tuning, or memory banks. In the one-shot anomaly detection setting, SubspaceAD achieves image-level and pixel-level AUROC of 98.0% and 97.6% on the MVTec-AD dataset, and 93.3% and 98.3% on the VisA dataset, respectively, surpassing prior state-of-the-art results. Code and demo are available at https://github.com/CLendering/SubspaceAD.",
        "authors_display": "Egor Bondarev Team",
        "pdf_url": "http://arxiv.org/abs/2602.23013",
        "code_url": null,
        "date": "2026-02-26",
        "primary_category": "cs.CV",
        "chinese_summary": "工业检测中的视觉异常检测通常仅需少量正常图像进行训练，而现有少样本方法虽性能强大，但常依赖记忆库、辅助数据集或多模态微调，增加了复杂性。本研究提出SubspaceAD，一种无需训练的方法，旨在探究视觉基础模型的特征表示是否足以解决此问题。该方法分两步：首先，使用冻结的DINOv2骨干网络从少量正常图像中提取补丁级特征；其次，对这些特征进行主成分分析（PCA）以估计正常变化的低维子空间。在推理时，通过与该子空间的重建残差来检测异常，生成可解释且具有统计学依据的异常分数。尽管方法简单，SubspaceAD在单样本和少样本设置下均取得了最先进的性能，例如在MVTec-AD数据集上，单样本图像和像素AUROC分别达到98.0%和97.6%，超越了现有技术水平。"
      },
      {
        "paper_id": "2602.22918",
        "title": "Where Vision Becomes Text: Locating the OCR Routing Bottleneck in Vision-Language Models",
        "abstract": "Vision-language models (VLMs) can read text from images, but where does this optical character recognition (OCR) information enter the language processing stream? We investigate the OCR routing mechanism across three architecture families (Qwen3-VL, Phi-4, InternVL3.5) using causal interventions. By computing activation differences between original images and text-inpainted versions, we identify architecture-specific OCR bottlenecks whose dominant location depends on the vision-language integration strategy: DeepStack models (Qwen) show peak sensitivity at mid-depth (about 50%) for scene text, while single-stage projection models (Phi-4, InternVL) peak at early layers (6-25%), though the exact layer of maximum effect varies across datasets. The OCR signal is remarkably low-dimensional: PC1 captures 72.9% of variance. Crucially, principal component analysis (PCA) directions learned on one dataset transfer to others, demonstrating shared text-processing pathways. Surprisingly, in models with modular OCR circuits (notably Qwen3-VL-4B), OCR removal can improve counting performance (up to +6.9 percentage points), suggesting OCR interferes with other visual processing in sufficiently modular architectures.",
        "authors_display": "Oren Gal Team",
        "pdf_url": "http://arxiv.org/abs/2602.22918",
        "code_url": null,
        "date": "2026-02-26",
        "primary_category": "cs.CL",
        "chinese_summary": "本研究通过因果干预手段，探究了光学字符识别（OCR）信息如何进入Vision-Language Models (VLMs) 的语言处理流程。研究分析了Qwen3-VL、Phi-4和InternVL3.5三种不同架构家族，通过比较原始图像和文本内嵌版本之间的激活差异，识别出特定于架构的OCR瓶颈。结果显示，OCR信号的进入位置取决于视觉-语言集成策略：DeepStack模型（如Qwen）的敏感度峰值出现在中间层（约50%），而单阶段投影模型（如Phi-4、InternVL）则在早期层（6-25%）达到峰值。OCR信号维度极低，第一主成分（PC1）捕获了72.9%的方差，且PCA方向可跨数据集迁移，表明存在共享的文本处理路径。令人惊讶的是，在具有模块化OCR电路的模型（特别是Qwen3-VL-4B）中，移除OCR反而能提高计数性能（最高+6.9个百分点），暗示OCR可能在高度模块化的架构中干扰其他视觉处理。"
      },
      {
        "paper_id": "2602.22779",
        "title": "TrajTok: Learning Trajectory Tokens enables better Video Understanding",
        "abstract": "Tokenization in video models, typically through patchification, generates an excessive and redundant number of tokens. This severely limits video efficiency and scalability. While recent trajectory-based tokenizers offer a promising solution by decoupling video duration from token count, they rely on complex external segmentation and tracking pipelines that are slow and task-agnostic. We propose TrajTok, an end-to-end video tokenizer module that is fully integrated and co-trained with video models for a downstream objective, dynamically adapting its token granularity to semantic complexity, independent of video duration. TrajTok contains a unified segmenter that performs implicit clustering over pixels in both space and time to directly produce object trajectories in a single forward pass. By prioritizing downstream adaptability over pixel-perfect segmentation fidelity, TrajTok is lightweight and efficient, yet empirically improves video understanding performance. With TrajTok, we implement a video CLIP model trained from scratch (TrajViT2). It achieves the best accuracy at scale across both classification and retrieval benchmarks, while maintaining efficiency comparable to the best token-merging methods. TrajTok also proves to be a versatile component beyond its role as a tokenizer. We show that it can be seamlessly integrated as either a probing head for pretrained visual features (TrajAdapter) or an alignment connector in vision-language models (TrajVLM) with especially strong performance in long-video reasoning.",
        "authors_display": "Ranjay Krishna Team",
        "pdf_url": "http://arxiv.org/abs/2602.22779",
        "code_url": null,
        "date": "2026-02-26",
        "primary_category": "cs.CV",
        "chinese_summary": "视频模型中常见的令牌化（如补丁化）会生成过多冗余令牌，严重限制了视频处理效率和可扩展性。虽然基于轨迹的令牌化方法有望解耦视频时长与令牌数量，但它们依赖于复杂且缓慢的外部分割和跟踪流程。本研究提出了TrajTok，一个端到端视频令牌化模块，其与视频模型完全集成并协同训练以实现下游目标，能够根据语义复杂性动态调整令牌粒度，且独立于视频时长。TrajTok包含一个统一的分割器，通过在空间和时间上对像素进行隐式聚类，直接在单次前向传播中生成对象轨迹。TrajTok在下游适应性上优先于像素级分割精度，因此轻量高效，并能提升视频理解性能。基于TrajTok实现的视频CLIP模型（TrajViT2）在分类和检索基准上均达到了最佳精度，同时保持了与最佳令牌合并方法相当的效率。TrajTok还可作为探针头或视觉-语言模型中的对齐连接器，在长视频推理中表现尤为出色。"
      },
      {
        "paper_id": "2602.22727",
        "title": "HulluEdit: Single-Pass Evidence-Consistent Subspace Editing for Mitigating Hallucinations in Large Vision-Language Models",
        "abstract": "Object hallucination in Large Vision-Language Models (LVLMs) significantly hinders their reliable deployment. Existing methods struggle to balance efficiency and accuracy: they often require expensive reference models and multiple forward passes, or apply static edits that risk suppressing genuine visual evidence. To address this, we introduce HulluEdit, a single-pass, reference-free intervention framework. Our core innovation is orthogonal subspace editing: we decompose the hidden states of the model into orthogonal subspaces - visual evidence, conflicting priors, and residual uncertainty - enabling selective suppression of hallucinatory patterns without interfering with visual grounding. This approach mathematically guarantees that edits applied to the prior subspace leave the visual component entirely unaffected. Extensive experiments show that HulluEdit achieves state-of-the-art hallucination reduction on benchmarks including POPE and CHAIR across diverse architectures, while preserving general capabilities on MME and maintaining efficient inference. Our method consistently outperforms contrastive decoding and static subspace editing baselines, offering a new pathway toward more trustworthy LVLMs.",
        "authors_display": "Jitao Sang Team",
        "pdf_url": "http://arxiv.org/abs/2602.22727",
        "code_url": null,
        "date": "2026-02-26",
        "primary_category": "cs.CV",
        "chinese_summary": "大型视觉-语言模型（LVLMs）中的对象幻觉问题严重阻碍了其可靠部署。现有方法在效率与准确性之间难以平衡，常需昂贵的参考模型和多轮推理，或采用可能抑制真实视觉证据的静态编辑。本研究提出了HulluEdit，一个单次前向传播、无参考的干预框架，其核心创新是正交子空间编辑。该方法将模型的隐藏状态分解为视觉证据、冲突先验和残余不确定性等正交子空间，从而能够在不干扰视觉基础的情况下选择性地抑制先验子空间中的幻觉模式。大量实验表明，HulluEdit在POPE和CHAIR等基准测试上实现了最先进的幻觉减少效果，并在MME上保持了通用能力，同时保持高效推理。该方法始终优于对比解码和静态子空间编辑基线，为构建更值得信赖的LVLMs提供了新途径。"
      },
      {
        "paper_id": "2602.22716",
        "title": "SoPE: Spherical Coordinate-Based Positional Embedding for Enhancing Spatial Perception of 3D LVLMs",
        "abstract": "3D Large Vision-Language Models (3D LVLMs) built upon Large Language Models (LLMs) have achieved remarkable progress across various multimodal tasks. However, their inherited position-dependent modeling mechanism, Rotary Position Embedding (RoPE), remains suboptimal for 3D multimodal understanding. The vanilla RoPE formulation fails to preserve essential three-dimensional spatial structures when encoding 3D tokens, and its relative distance computation overlooks angular dependencies, hindering the model's ability to capture directional variations in visual representations. To overcome these limitations, we introduce Spherical Coordinate-based Positional Embedding (SoPE). Our method maps point-cloud token indices into a 3D spherical coordinate space, enabling unified modeling of spatial locations and directional angles. This formulation preserves the inherent geometric structure of point-cloud data, enhances spatial awareness, and yields more consistent and expressive geometric representations for multimodal learning. In addition, we introduce a multi-scale frequency mixing strategy to fuse feature information across different frequency domains. Experimental results on multiple 3D scene benchmarks validate the effectiveness of our approach, while real-world deployment experiments further demonstrate its strong generalization capability.",
        "authors_display": "Ka-Veng Yuen Team",
        "pdf_url": "http://arxiv.org/abs/2602.22716",
        "code_url": null,
        "date": "2026-02-26",
        "primary_category": "cs.CV",
        "chinese_summary": "当前3D大型视觉-语言模型（3D LVLMs）中使用的旋转位置嵌入（RoPE）机制在编码3D tokens时未能有效保留三维空间结构和角度依赖性，从而限制了模型对3D多模态数据的理解能力。为此，本研究提出了基于球坐标的位置嵌入（SoPE），将点云token索引映射到3D球坐标空间，以统一建模空间位置和方向角度，并辅以多尺度频率混合策略融合特征。实验结果表明，SoPE在多个3D场景基准测试上显著提升了模型的性能和泛化能力，生成了更一致、富有表现力的几何表示。"
      },
      {
        "paper_id": "2602.22703",
        "title": "Enhancing Geometric Perception in VLMs via Translator-Guided Reinforcement Learning",
        "abstract": "Vision-language models (VLMs) often struggle with geometric reasoning due to their limited perception of fundamental diagram elements. To tackle this challenge, we introduce GeoPerceive, a benchmark comprising diagram instances paired with domain-specific language (DSL) representations, along with an efficient automatic data generation pipeline. This design enables the isolated evaluation of geometric perception independently from reasoning. To exploit the data provided by GeoPerceive for enhancing the geometric perception capabilities of VLMs, we propose GeoDPO, a translator-guided reinforcement learning (RL) framework. GeoDPO employs an NL-to-DSL translator, which is trained on synthetic pairs generated by the data engine of GeoPerceive, to bridge natural language and DSL. This translator facilitates the computation of fine-grained, DSL-level scores, which serve as reward signals in reinforcement learning. We assess GeoDPO on both in-domain and out-of-domain datasets, spanning tasks in geometric perception as well as downstream reasoning. Experimental results demonstrate that, while supervised fine-tuning (SFT) offers only marginal improvements and may even impair performance in out-of-domain scenarios, GeoDPO achieves substantial gains: $+26.5\\%$ on in-domain data, $+8.0\\%$ on out-of-domain data, and $+39.0\\%$ on downstream reasoning tasks. These findings underscore the superior performance and generalization ability of GeoDPO over SFT. All codes are released at https://github.com/Longin-Yu/GeoPerceive   to ensure reproducibility.",
        "authors_display": "Chun Yuan Team",
        "pdf_url": "http://arxiv.org/abs/2602.22703",
        "code_url": null,
        "date": "2026-02-26",
        "primary_category": "cs.LG",
        "chinese_summary": "视觉-语言模型（VLMs）在几何推理方面存在对图表元素感知不足的问题。为解决此挑战，本研究提出了GeoPerceive基准，包含图表实例与领域特定语言（DSL）表示，并设计了高效的自动数据生成流水线。在此基础上，引入了GeoDPO框架，一个翻译器引导的强化学习模型，通过NL-to-DSL翻译器将自然语言与DSL桥接，并利用细粒度的DSL级分数作为RL奖励信号。实验证明，GeoDPO在域内、域外数据及下游推理任务上均取得显著性能提升（分别达+26.5%、+8.0%和+39.0%），远超监督微调方法，展现出卓越的泛化能力。"
      },
      {
        "paper_id": "2602.22689",
        "title": "No Caption, No Problem: Caption-Free Membership Inference via Model-Fitted Embeddings",
        "abstract": "Latent diffusion models have achieved remarkable success in high-fidelity text-to-image generation, but their tendency to memorize training data raises critical privacy and intellectual property concerns. Membership inference attacks (MIAs) provide a principled way to audit such memorization by determining whether a given sample was included in training. However, existing approaches assume access to ground-truth captions. This assumption fails in realistic scenarios where only images are available and their textual annotations remain undisclosed, rendering prior methods ineffective when substituted with vision-language model (VLM) captions. In this work, we propose MoFit, a caption-free MIA framework that constructs synthetic conditioning inputs that are explicitly overfitted to the target model's generative manifold. Given a query image, MoFit proceeds in two stages: (i) model-fitted surrogate optimization, where a perturbation applied to the image is optimized to construct a surrogate in regions of the model's unconditional prior learned from member samples, and (ii) surrogate-driven embedding extraction, where a model-fitted embedding is derived from the surrogate and then used as a mismatched condition for the query image. This embedding amplifies conditional loss responses for member samples while leaving hold-outs relatively less affected, thereby enhancing separability in the absence of ground-truth captions. Our comprehensive experiments across multiple datasets and diffusion models demonstrate that MoFit consistently outperforms prior VLM-conditioned baselines and achieves performance competitive with caption-dependent methods.",
        "authors_display": "Sung-Eui Yoon Team",
        "pdf_url": "http://arxiv.org/abs/2602.22689",
        "code_url": null,
        "date": "2026-02-26",
        "primary_category": "cs.CV",
        "chinese_summary": "潜在扩散模型在文本到图像生成方面表现出色，但也存在记忆训练数据的隐私风险，而现有成员推断攻击（MIAs）方法通常依赖于真实标题，在仅有图像的场景下失效。为此，本研究提出了MoFit，一个无需标题的MIA框架，通过构建显式过拟合目标模型生成流形的合成条件输入来工作。MoFit包含模型拟合的替代优化和替代驱动的嵌入提取两个阶段，旨在放大成员样本的条件损失响应。实验结果表明，MoFit在多个数据集和扩散模型上均优于现有的VLM条件基线，并达到了与依赖标题的方法相媲美的性能。"
      },
      {
        "paper_id": "2602.22683",
        "title": "SUPERGLASSES: Benchmarking Vision Language Models as Intelligent Agents for AI Smart Glasses",
        "abstract": "The rapid advancement of AI-powered smart glasses, one of the hottest wearable devices, has unlocked new frontiers for multimodal interaction, with Visual Question Answering (VQA) over external knowledge sources emerging as a core application. Existing Vision Language Models (VLMs) adapted to smart glasses are typically trained and evaluated on traditional multimodal datasets; however, these datasets lack the variety and realism needed to reflect smart glasses usage scenarios and diverge from their specific challenges, where accurately identifying the object of interest must precede any external knowledge retrieval. To bridge this gap, we introduce SUPERGLASSES, the first comprehensive VQA benchmark built on real-world data entirely collected by smart glasses devices. SUPERGLASSES comprises 2,422 egocentric image-question pairs spanning 14 image domains and 8 query categories, enriched with full search trajectories and reasoning annotations. We evaluate 26 representative VLMs on this benchmark, revealing significant performance gaps. To address the limitations of existing models, we further propose SUPERLENS, a multimodal smart glasses agent that enables retrieval-augmented answer generation by integrating automatic object detection, query decoupling, and multimodal web search. Our agent achieves state-of-the-art performance, surpassing GPT-4o by 2.19 percent, and highlights the need for task-specific solutions in smart glasses VQA scenarios.",
        "authors_display": "Qing Li Team",
        "pdf_url": "http://arxiv.org/abs/2602.22683",
        "code_url": null,
        "date": "2026-02-26",
        "primary_category": "cs.CV",
        "chinese_summary": "AI智能眼镜领域的视觉问答（VQA）应用面临挑战，现有VLM在传统数据集上训练，未能反映智能眼镜场景中先识别对象再检索外部知识的特点。本研究为此引入了SUPERGLASSES，首个基于智能眼镜真实数据构建的综合VQA基准，包含2,422对第一视角图像-问题。为解决现有模型的不足，提出了SUPERLENS，一个多模态智能眼镜代理，通过集成自动对象检测、查询解耦和多模态网络搜索实现检索增强的答案生成。实验证明，SUPERLENS超越GPT-4o 2.19%，强调了智能眼镜VQA场景中任务特定解决方案的重要性。"
      },
      {
        "paper_id": "2602.22678",
        "title": "ViCLIP-OT: The First Foundation Vision-Language Model for Vietnamese Image-Text Retrieval with Optimal Transport",
        "abstract": "Image-text retrieval has become a fundamental component in intelligent multimedia systems; however, most existing vision-language models are optimized for highresource languages and remain suboptimal for low-resource settings such as Vietnamese. This work introduces ViCLIP-OT, a foundation vision-language model specifically designed for Vietnamese image-text retrieval. The proposed framework integrates CLIP-style contrastive learning with a Similarity-Graph Regularized Optimal Transport (SIGROT) loss to enhance global cross-modal consistency and mitigate modality gap issues. Extensive experiments on three Vietnamese benchmarks (UITOpenViIC, KTVIC, and Crossmodal-3600) demonstrate that ViCLIP-OT consistently outperforms CLIP and SigLIP baselines in both in-domain and zero-shot settings. On UIT-OpenViIC, the model achieves an average Recall@K of 67.34%, improving upon CLIP by 5.75 percentage points. In zero-shot evaluation on Crossmodal-3600, ViCLIPOT surpasses CLIP by 11.72 percentage points. Embedding-space analysis further confirms improved alignment and reduced modality gap. The results indicate that integrating SIGROT provides an effective and scalable strategy for cross-modal retrieval in low-resource languages, offering practical implications for intelligent multimedia retrieval systems in Vietnamese and other underrepresented linguistic contexts.",
        "authors_display": "Nguyen-Khang Pham Team",
        "pdf_url": "http://arxiv.org/abs/2602.22678",
        "code_url": null,
        "date": "2026-02-26",
        "primary_category": "cs.CV",
        "chinese_summary": "图像-文本检索在智能多媒体系统中至关重要，但现有视觉-语言模型（VLMs）主要针对高资源语言优化，对越南语等低资源语言效果不佳。为此，本研究提出了ViCLIP-OT，一个专为越南语图像-文本检索设计的视觉-语言基础模型。该模型结合了CLIP风格的对比学习与相似图正则化最优传输（SIGROT）损失，以增强跨模态一致性并弥补模态鸿沟。在三个越南语基准上的实验结果显示，ViCLIP-OT在域内和零样本设置下均持续超越CLIP和SigLIP基线，平均Recall@K提高了5.75至11.72个百分点，证明了SIGROT集成在低资源语言跨模态检索中的有效性。"
      },
      {
        "paper_id": "2602.22549",
        "title": "DrivePTS: A Progressive Learning Framework with Textual and Structural Enhancement for Driving Scene Generation",
        "abstract": "Synthesis of diverse driving scenes serves as a crucial data augmentation technique for validating the robustness and generalizability of autonomous driving systems. Current methods aggregate high-definition (HD) maps and 3D bounding boxes as geometric conditions in diffusion models for conditional scene generation. However, implicit inter-condition dependency causes generation failures when control conditions change independently. Additionally, these methods suffer from insufficient details in both semantic and structural aspects. Specifically, brief and view-invariant captions restrict semantic contexts, resulting in weak background modeling. Meanwhile, the standard denoising loss with uniform spatial weighting neglects foreground structural details, causing visual distortions and blurriness. To address these challenges, we propose DrivePTS, which incorporates three key innovations. Firstly, our framework adopts a progressive learning strategy to mitigate inter-dependency between geometric conditions, reinforced by an explicit mutual information constraint. Secondly, a Vision-Language Model is utilized to generate multi-view hierarchical descriptions across six semantic aspects, providing fine-grained textual guidance. Thirdly, a frequency-guided structure loss is introduced to strengthen the model's sensitivity to high-frequency elements, improving foreground structural fidelity. Extensive experiments demonstrate that our DrivePTS achieves state-of-the-art fidelity and controllability in generating diverse driving scenes. Notably, DrivePTS successfully generates rare scenes where prior methods fail, highlighting its strong generalization ability.",
        "authors_display": "Cheng Lu Team",
        "pdf_url": "http://arxiv.org/abs/2602.22549",
        "code_url": null,
        "date": "2026-02-26",
        "primary_category": "cs.CV",
        "chinese_summary": "自动驾驶系统中多样化驾驶场景的合成面临挑战，现有方法在扩散模型中因隐式条件间依赖性、简短描述限制语义上下文及统一空间加权忽略前景细节，导致生成失败或细节不足。本研究提出了DrivePTS框架，通过渐进式学习策略与显式互信息约束减轻几何条件间的相互依赖；利用视觉-语言模型生成多视图分层描述提供细粒度文本指导；并引入频率引导结构损失以增强对高频元素的敏感性。大量实验结果表明，DrivePTS在生成多样化驾驶场景方面实现了最先进的保真度和可控性，成功生成了现有方法失败的罕见场景，展现出强大的泛化能力。"
      },
      {
        "paper_id": "2602.22144",
        "title": "NoLan: Mitigating Object Hallucinations in Large Vision-Language Models via Dynamic Suppression of Language Priors",
        "abstract": "Object hallucination is a critical issue in Large Vision-Language Models (LVLMs), where outputs include objects that do not appear in the input image. A natural question arises from this phenomenon: Which component of the LVLM pipeline primarily contributes to object hallucinations? The vision encoder to perceive visual information, or the language decoder to generate text responses? In this work, we strive to answer this question through designing a systematic experiment to analyze the roles of the vision encoder and the language decoder in hallucination generation. Our observations reveal that object hallucinations are predominantly associated with the strong priors from the language decoder. Based on this finding, we propose a simple and training-free framework, No-Language-Hallucination Decoding, NoLan, which refines the output distribution by dynamically suppressing language priors, modulated based on the output distribution difference between multimodal and text-only inputs. Experimental results demonstrate that NoLan effectively reduces object hallucinations across various LVLMs on different tasks. For instance, NoLan achieves substantial improvements on POPE, enhancing the accuracy of LLaVA-1.5 7B and Qwen-VL 7B by up to 6.45 and 7.21, respectively. The code is publicly available at: https://github.com/lingfengren/NoLan.",
        "authors_display": "Xinchao Wang Team",
        "pdf_url": "http://arxiv.org/abs/2602.22144",
        "code_url": "https://github.com/lingfengren/NoLan",
        "date": "2026-02-25",
        "primary_category": "cs.CV",
        "chinese_summary": "大型视觉语言模型（LVLMs）面临物体幻觉问题，即模型生成图像中不存在的物体。为探究此问题来源，本研究设计系统实验，发现物体幻觉主要源于语言解码器强大的先验知识。基于此，提出了一种无训练的NoLan框架，通过动态抑制语言先验来精炼输出分布。实验结果表明，NoLan有效减少了多种LVLM在不同任务上的物体幻觉，例如在POPE数据集上，LLaVA-1.5 7B和Qwen-VL 7B的准确率分别提升了高达6.45和7.21。"
      },
      {
        "paper_id": "2602.22120",
        "title": "GeoDiv: Framework For Measuring Geographical Diversity In Text-To-Image Models",
        "abstract": "Text-to-image (T2I) models are rapidly gaining popularity, yet their outputs often lack geographical diversity, reinforce stereotypes, and misrepresent regions. Given their broad reach, it is critical to rigorously evaluate how these models portray the world. Existing diversity metrics either rely on curated datasets or focus on surface-level visual similarity, limiting interpretability. We introduce GeoDiv, a framework leveraging large language and vision-language models to assess geographical diversity along two complementary axes: the Socio-Economic Visual Index (SEVI), capturing economic and condition-related cues, and the Visual Diversity Index (VDI), measuring variation in primary entities and backgrounds. Applied to images generated by models such as Stable Diffusion and FLUX.1-dev across $10$ entities and $16$ countries, GeoDiv reveals a consistent lack of diversity and identifies fine-grained attributes where models default to biased portrayals. Strikingly, depictions of countries like India, Nigeria, and Colombia are disproportionately impoverished and worn, reflecting underlying socio-economic biases. These results highlight the need for greater geographical nuance in generative models. GeoDiv provides the first systematic, interpretable framework for measuring such biases, marking a step toward fairer and more inclusive generative systems. Project page: https://abhipsabasu.github.io/geodiv",
        "authors_display": "R. Venkatesh Babu Team",
        "pdf_url": "http://arxiv.org/abs/2602.22120",
        "code_url": null,
        "date": "2026-02-25",
        "primary_category": "cs.CV",
        "chinese_summary": "当前文本到图像（T2I）模型输出缺乏地理多样性，常强化刻板印象，且现有多样性评估方法存在局限。为此，本研究引入GeoDiv框架，利用大型语言模型和视觉语言模型，从社会经济视觉指数（SEVI）和视觉多样性指数（VDI）两个维度评估地理多样性。将GeoDiv应用于Stable Diffusion和FLUX.1-dev等模型生成的图像，结果揭示了模型普遍缺乏多样性，并存在细粒度属性上的偏见描绘，例如印度、尼日利亚和哥伦比亚的图像被过度描绘为贫困和破旧，反映了深层的社会经济偏见。"
      },
      {
        "paper_id": "2602.22098",
        "title": "Brain3D: Brain Report Automation via Inflated Vision Transformers in 3D",
        "abstract": "Current medical vision-language models (VLMs) process volumetric brain MRI using 2D slice-based approximations, fragmenting the spatial context required for accurate neuroradiological interpretation. We developed \\textbf{Brain3D}, a staged vision-language framework for automated radiology report generation from 3D brain tumor MRI. Our approach inflates a pretrained 2D medical encoder into a native 3D architecture and progressively aligns it with a causal language model through three stages: contrastive grounding, supervised projector warmup, and LoRA-based linguistic specialization. Unlike generalist 3D medical VLMs, \\textbf{Brain3D} is tailored to neuroradiology, where hemispheric laterality, tumor infiltration patterns, and anatomical localization are critical. Evaluated on 468 subjects (BraTS pathological cases plus healthy controls), our model achieves a Clinical Pathology F1 of 0.951 versus 0.413 for a strong 2D baseline while maintaining perfect specificity on healthy scans. The staged alignment proves essential: contrastive grounding establishes visual-textual correspondence, projector warmup stabilizes conditioning, and LoRA adaptation shifts output from verbose captions to structured clinical reports\\footnote{Our code is publicly available for transparency and reproducibility",
        "authors_display": "Vincenzo Moscato Team",
        "pdf_url": "http://arxiv.org/abs/2602.22098",
        "code_url": null,
        "date": "2026-02-25",
        "primary_category": "cs.CV",
        "chinese_summary": "现有医学视觉语言模型处理体三维脑部MRI时，采用2D切片近似方法，导致空间上下文丢失，影响神经放射学解释的准确性。本研究开发了Brain3D，一个分阶段的视觉语言框架，用于从3D脑肿瘤MRI自动生成放射学报告。该方法将预训练的2D医学编码器扩展为3D架构，并通过对比接地、监督投影器预热和基于LoRA的语言专业化三个阶段与语言模型对齐。在468名受试者上的评估显示，Brain3D的临床病理F1分数高达0.951，远超2D基线（0.413），并在健康扫描上保持完美特异性，证明了分阶段对齐的有效性。"
      },
      {
        "paper_id": "2602.22013",
        "title": "RobustVisRAG: Causality-Aware Vision-Based Retrieval-Augmented Generation under Visual Degradations",
        "abstract": "Vision-based Retrieval-Augmented Generation (VisRAG) leverages vision-language models (VLMs) to jointly retrieve relevant visual documents and generate grounded answers based on multimodal evidence. However, existing VisRAG models degrade in performance when visual inputs suffer from distortions such as blur, noise, low light, or shadow, where semantic and degradation factors become entangled within pretrained visual encoders, leading to errors in both retrieval and generation stages. To address this limitation, we introduce RobustVisRAG, a causality-guided dual-path framework that improves VisRAG robustness while preserving efficiency and zero-shot generalization. RobustVisRAG uses a non-causal path to capture degradation signals through unidirectional attention and a causal path to learn purified semantics guided by these signals. Together with the proposed Non-Causal Distortion Modeling and Causal Semantic Alignment objectives, the framework enforces a clear separation between semantics and degradations, enabling stable retrieval and generation under challenging visual conditions. To evaluate robustness under realistic conditions, we introduce the Distortion-VisRAG dataset, a large-scale benchmark containing both synthetic and real-world degraded documents across seven domains, with 12 synthetic and 5 real distortion types that comprehensively reflect practical visual degradations. Experimental results show that RobustVisRAG improves retrieval, generation, and end-to-end performance by 7.35%, 6.35%, and 12.40%, respectively, on real-world degradations, while maintaining comparable accuracy on clean inputs.",
        "authors_display": "Wei-Ting Chen Team",
        "pdf_url": "http://arxiv.org/abs/2602.22013",
        "code_url": "https://robustvisrag.github.io",
        "date": "2026-02-25",
        "primary_category": "cs.CV",
        "chinese_summary": "基于视觉的检索增强生成（VisRAG）模型在视觉输入存在失真（如模糊、噪声、低光照）时性能会显著下降，原因是语义和退化因素在视觉编码器中相互纠缠。为解决此问题，本研究提出了RobustVisRAG，一个因果指导的双路径框架，通过非因果路径捕捉退化信号，再由因果路径在此信号指导下学习纯净语义，从而将语义与退化因素清晰分离。在引入的包含合成和真实世界退化图像的Distortion-VisRAG数据集上进行实验，结果表明RobustVisRAG在真实世界退化条件下，检索、生成和端到端性能分别提升了7.35%、6.35%和12.40%，同时在干净输入上保持了相近的准确性。"
      },
      {
        "paper_id": "2602.21992",
        "title": "PanoEnv: Exploring 3D Spatial Intelligence in Panoramic Environments with Reinforcement Learning",
        "abstract": "360 panoramic images are increasingly used in virtual reality, autonomous driving, and robotics for holistic scene understanding. However, current Vision-Language Models (VLMs) struggle with 3D spatial reasoning on Equirectangular Projection (ERP) images due to geometric distortion and limited 3D supervision. We introduce PanoEnv, a large-scale VQA benchmark built from synthetic 3D environments, containing 14.8K questions across five categories (e.g., relative position, volume comparison) grounded in accurate 3D annotations including depth, segmentation, and bounding boxes. Benchmarking 14 state-of-the-art VLMs reveals limited 3D understanding, achieving only 49.34% overall accuracy and 8.36% on open-ended (OE) questions. To enhance 3D reasoning, we propose a reinforcement learning post-training framework based on Group Relative Policy Optimization (GRPO) with a ground-truth-guided reward that incorporates five geometry-aware strategies such as distance tolerance and spatial consistency. A two-stage curriculum further mitigates catastrophic forgetting: Stage 1 trains on structured tasks (true/false and multiple choice), and Stage 2 fine-tunes on mixed open-ended data to improve generalization. Our 7B model achieves new state-of-the-art performance, improving overall accuracy to 52.93% (+3.59%) and open-ended accuracy to 14.83% while maintaining structured-task performance. It also achieves top semantic evaluation scores (Q-Score 6.24, P-Score 5.95), surpassing 32B models. These results demonstrate that PanoEnv-QA and our curriculum-based RL framework effectively instill 3D spatial intelligence in VLMs for omnidirectional perception.",
        "authors_display": "Xu Zheng Team",
        "pdf_url": "http://arxiv.org/abs/2602.21992",
        "code_url": null,
        "date": "2026-02-25",
        "primary_category": "cs.CV",
        "chinese_summary": "当前视觉语言模型（VLMs）在360度全景图像上的3D空间推理能力有限，主要受几何失真和3D监督不足影响。为解决此问题，本研究构建了PanoEnv，一个包含14.8K个问题的3D环境大规模VQA基准，并提出了一个基于群体相对策略优化（GRPO）的强化学习后训练框架，该框架引入了真值引导的奖励机制和几何感知策略，以增强3D推理。通过两阶段课程学习缓解灾难性遗忘。实验结果显示，本文提出的7B模型在整体准确率上提升了3.59%达到52.93%，开放式问题准确率提升至14.83%，并取得了领先的语义评估分数，证明了PanoEnv和课程化RL框架在提升VLM全向感知3D空间智能方面的有效性。"
      },
      {
        "paper_id": "2602.21983",
        "title": "Humanizing Robot Gaze Shifts: A Framework for Natural Gaze Shifts in Humanoid Robots",
        "abstract": "Leveraging auditory and visual feedback for attention reorientation is essential for natural gaze shifts in social interaction. However, enabling humanoid robots to perform natural and context-appropriate gaze shifts in unconstrained human--robot interaction (HRI) remains challenging, as it requires the coupling of cognitive attention mechanisms and biomimetic motion generation. In this work, we propose the Robot Gaze-Shift (RGS) framework, which integrates these two components into a unified pipeline. First, RGS employs a vision--language model (VLM)-based gaze reasoning pipeline to infer context-appropriate gaze targets from multimodal interaction cues, ensuring consistency with human gaze-orienting regularities. Second, RGS introduces a conditional Vector Quantized-Variational Autoencoder (VQ-VAE) model for eye--head coordinated gaze-shift motion generation, producing diverse and human-like gaze-shift behaviors. Experiments validate that RGS effectively replicates human-like target selection and generates realistic, diverse gaze-shift motions.",
        "authors_display": "Zhouping Yin Team",
        "pdf_url": "http://arxiv.org/abs/2602.21983",
        "code_url": null,
        "date": "2026-02-25",
        "primary_category": "cs.RO",
        "chinese_summary": "在人机交互中，机器人实现自然、符合语境的凝视转移是一个挑战，这要求认知注意力机制与仿生运动生成相结合。本研究提出了机器人凝视转移（RGS）框架，将这两个组件整合为统一的流程。RGS首先利用VLM驱动的凝视推理流水线，从多模态交互线索中推断凝视目标；其次，引入条件向量量化变分自编码器（VQ-VAE）模型生成眼-头协调的凝视转移运动。实验验证了RGS能够有效复制类人的目标选择，并生成逼真、多样的凝视转移行为。"
      },
      {
        "paper_id": "2602.21952",
        "title": "MindDriver: Introducing Progressive Multimodal Reasoning for Autonomous Driving",
        "abstract": "Vision-Language Models (VLM) exhibit strong reasoning capabilities, showing promise for end-to-end autonomous driving systems. Chain-of-Thought (CoT), as VLM's widely used reasoning strategy, is facing critical challenges. Existing textual CoT has a large gap between text semantic space and trajectory physical space. Although the recent approach utilizes future image to replace text as CoT process, it lacks clear planning-oriented objective guidance to generate images with accurate scene evolution. To address these, we innovatively propose MindDriver, a progressive multimodal reasoning framework that enables VLM to imitate human-like progressive thinking for autonomous driving. MindDriver presents semantic understanding, semantic-to-physical space imagination, and physical-space trajectory planning. To achieve aligned reasoning processes in MindDriver, we develop a feedback-guided automatic data annotation pipeline to generate aligned multimodal reasoning training data. Furthermore, we develop a progressive reinforcement fine-tuning method to optimize the alignment through progressive high- level reward-based learning. MindDriver demonstrates superior performance in both nuScences open-loop and Bench2Drive closed-loop evaluation. Codes are available at https://github.com/hotdogcheesewhite/MindDriver.",
        "authors_display": "Mu Xu Team",
        "pdf_url": "http://arxiv.org/abs/2602.21952",
        "code_url": null,
        "date": "2026-02-25",
        "primary_category": "cs.CV",
        "chinese_summary": "视觉语言模型（VLM）在自动驾驶领域展现出推理潜力，但其思维链（CoT）策略面临挑战：文本语义与轨迹物理空间存在鸿沟，且现有图像CoT缺乏明确的规划导向。为此，本研究提出MindDriver，一个渐进式多模态推理框架，模仿人类思维过程进行自动驾驶，涵盖语义理解、语义到物理空间的想象以及物理空间轨迹规划。为实现推理过程对齐，设计了反馈引导的自动数据标注管道和渐进式强化微调方法。MindDriver在nuScences开环和Bench2Drive闭环评估中均表现出卓越性能。"
      },
      {
        "paper_id": "2602.21864",
        "title": "DynamicGTR: Leveraging Graph Topology Representation Preferences to Boost VLM Capabilities on Graph QAs",
        "abstract": "Vision-Language Models (VLMs) have emerged as versatile solutions for zero-shot question answering (QA) across various domains. However, enabling VLMs to effectively comprehend structured graphs and perform accurate, efficient QA remains challenging. Existing approaches typically rely on one single graph topology representation (GTR), such as fixed-style visual images or unified text descriptions. This ``one-size-fits-all'' strategy often neglects model-specific and task-specific preferences, resulting in inaccurate or over-lengthy responses to graph-related queries. To address this, we propose the $\\mbox{DynamicGTR}$ framework, which dynamically selects the optimal GTR for each query during inference, thereby enhancing the zero-shot graph QA capabilities of VLMs with a customizable accuracy and brevity trade-off. Extensive experiments show that DynamicGTR not only improves VLM-based graph algorithm QA performance but also successfully transfers the experience trained from synthetic graph algorithm tasks to real-world applications like link prediction and node classification, without any additional training. Additionally, DynamicGTR demonstrates strong transferability across tasks, domains, and models, suggesting its potential as a flexible solution for broad graph scenarios.",
        "authors_display": "Yu Zhang Team",
        "pdf_url": "http://arxiv.org/abs/2602.21864",
        "code_url": null,
        "date": "2026-02-25",
        "primary_category": "cs.CV",
        "chinese_summary": "尽管视觉语言模型（VLMs）在零样本问答中表现出色，但其在理解结构化图并进行高效问答方面仍面临挑战，现有方法常采用单一图拓扑表示（GTR），忽略了模型和任务特定偏好。为解决此问题，本研究提出了DynamicGTR框架，该框架在推理过程中动态选择每个查询的最优GTR，从而在准确性和简洁性之间实现权衡，提升VLMs的零样本图问答能力。大量实验表明，DynamicGTR不仅提升了基于VLM的图算法问答性能，还能将学习到的经验零成本迁移到链接预测和节点分类等真实世界应用，并展现出强大的跨任务、领域和模型可迁移性。"
      },
      {
        "paper_id": "2602.21824",
        "title": "DocDjinn: Controllable Synthetic Document Generation with VLMs and Handwriting Diffusion",
        "abstract": "Effective document intelligence models rely on large amounts of annotated training data. However, procuring sufficient and high-quality data poses significant challenges due to the labor-intensive and costly nature of data acquisition. Additionally, leveraging language models to annotate real documents raises concerns about data privacy. Synthetic document generation has emerged as a promising, privacy-preserving alternative. We propose DocDjinn, a novel framework for controllable synthetic document generation using Vision-Language Models (VLMs) that produces annotated documents from unlabeled seed samples. Our approach generates visually plausible and semantically consistent synthetic documents that follow the distribution of an existing source dataset through clustering-based seed selection with parametrized sampling. By enriching documents with realistic diffusion-based handwriting and contextual visual elements via semantic-visual decoupling, we generate diverse, high-quality annotated synthetic documents. We evaluate across eleven benchmarks spanning key information extraction, question answering, document classification, and document layout analysis. To our knowledge, this is the first work demonstrating that VLMs can generate faithful annotated document datasets at scale from unlabeled seeds that can effectively enrich or approximate real, manually annotated data for diverse document understanding tasks. We show that with only 100 real training samples, our framework achieves on average $87\\%$ of the performance of the full real-world dataset. We publicly release our code and 140k+ synthetic document samples.",
        "authors_display": "Adrian Ulges Team",
        "pdf_url": "http://arxiv.org/abs/2602.21824",
        "code_url": null,
        "date": "2026-02-25",
        "primary_category": "cs.LG",
        "chinese_summary": "文档智能模型依赖大量标注数据，但数据获取成本高昂且存在隐私问题，合成文档生成成为有前景的替代方案。本研究提出了DocDjinn框架，利用视觉语言模型（VLMs）从未标注的种子样本可控生成带标注的合成文档。该方法通过聚类种子选择和参数化采样，生成在视觉上合理、语义一致且遵循源数据集分布的文档，并结合扩散模型和语义-视觉解耦技术丰富文档。在11个基准上的评估结果表明，DocDjinn是首个能从无标注种子大规模生成忠实带标注文档数据集的工作，该数据集能有效增强或近似真实标注数据，且仅用100个真实训练样本即可达到完整真实数据集平均87%的性能。"
      },
      {
        "paper_id": "2602.21779",
        "title": "Beyond Static Artifacts: A Forensic Benchmark for Video Deepfake Reasoning in Vision Language Models",
        "abstract": "Current Vision-Language Models (VLMs) for deepfake detection excel at identifying spatial artifacts but overlook a critical dimension: temporal inconsistencies in video forgeries. Adapting VLMs to reason about these dynamic cues remains a distinct challenge. To bridge this gap, we propose Forensic Answer-Questioning (FAQ), a large-scale benchmark that formulates temporal deepfake analysis as a multiple-choice task. FAQ introduces a three-level hierarchy to progressively evaluate and equip VLMs with forensic capabilities: (1) Facial Perception, testing the ability to identify static visual artifacts; (2) Temporal Deepfake Grounding, requiring the localization of dynamic forgery artifacts across frames; and (3) Forensic Reasoning, challenging models to synthesize evidence for final authenticity verdicts. We evaluate a range of VLMs on FAQ and generate a corresponding instruction-tuning set, FAQ-IT. Extensive experiments show that models fine-tuned on FAQ-IT achieve advanced performance on both in-domain and cross-dataset detection benchmarks. Ablation studies further validate the impact of our key design choices, confirming that FAQ is the driving force behind the temporal reasoning capabilities of these VLMs.",
        "authors_display": "Xuelong Li Team",
        "pdf_url": "http://arxiv.org/abs/2602.21779",
        "code_url": null,
        "date": "2026-02-25",
        "primary_category": "cs.CV",
        "chinese_summary": "当前视觉语言模型（VLM）在深度伪造检测中擅长识别空间伪造痕迹，但对视频伪造中的时间不一致性推理能力不足。为弥补这一差距，本研究提出了Forensic Answer-Questioning (FAQ)，一个将时间深度伪造分析构建为多项选择任务的大规模基准。FAQ设计了三级层次结构，逐步评估VLM的法医能力：面部感知、时间深度伪造定位和法医推理。通过在FAQ上评估并基于其构建FAQ-IT指令微调集，实验证明，在FAQ-IT上微调的模型在域内和跨数据集检测基准上均实现了先进性能，验证了FAQ在赋予VLM时间推理能力方面的关键作用。"
      },
      {
        "paper_id": "2602.21735",
        "title": "SigVLP: Sigmoid Volume-Language Pre-Training for Self-Supervised CT-Volume Adaptive Representation Learning",
        "abstract": "Large-scale, volumetric medical imaging datasets typically aggregate scans from different vendors and devices, resulting in highly variable resolution, slice thicknesses, and numbers of slices per study. Consequently, training representation models usually requires cropping or interpolating along the z-axis to obtain fixed-size blocks, which inevitably causes information loss. We propose a new training approach to overcome this limitation. Instead of absolute position embeddings, we interpret volumes as sequences of 3D chunks and adopt Rotary Position Embeddings, allowing us to treat the z-axis as an unconstrained temporal dimensions. Building on this idea, we introduce a new vision-language model: SigVLP. In SigVLP, we implement Rotary Position Embedding as the positional encoding method, which is applied directly within the attention operation, generating input-conditioned sine and cosine weights on the fly. This design ensures consistent alignment between query and key projections and adapts to any input sizes. To allow for variable input size during training, we sample Computed Tomography volumes in chunks and pair them with localized organ-wise textual observations. Compared to using entire reports for conditioning, chunkwise alignment provides finer-grained supervision, enabling the model to establish stronger correlations between the text and volume representations, thereby improving the precision of text-to-volume alignment. Our models are trained with the Muon optimizer and evaluated on a diverse set of downstream tasks, including zero-shot abnormality and organ classification, segmentation, and retrieval tasks.",
        "authors_display": "Bernhard Kainz Team",
        "pdf_url": "http://arxiv.org/abs/2602.21735",
        "code_url": null,
        "date": "2026-02-25",
        "primary_category": "cs.CV",
        "chinese_summary": "针对大规模医学图像数据在分辨率、层厚和层数上的高度可变性导致现有表示模型训练时信息丢失的问题，本研究提出了一种新的训练方法和SigVLP视觉语言模型。该方法将体数据视为三维块序列，并采用旋转位置嵌入（Rotary Position Embeddings）将z轴视为非受限的时间维度，直接在注意力操作中生成输入条件的正弦余弦权重，以适应任意输入大小。通过将CT体数据分块并与局部器官文本观察配对，SigVLP实现了更细粒度的监督和更强的文本-体素关联。实验结果表明，该模型在零样本异常和器官分类、分割以及检索等多样化下游任务上表现出色。"
      },
      {
        "paper_id": "2602.21706",
        "title": "SurGo-R1: Benchmarking and Modeling Contextual Reasoning for Operative Zone in Surgical Video",
        "abstract": "Minimally invasive surgery has dramatically improved patient operative outcomes, yet identifying safe operative zones remains challenging in critical phases, requiring surgeons to integrate visual cues, procedural phase, and anatomical context under high cognitive load. Existing AI systems offer binary safety verification or static detection, ignoring the phase-dependent nature of intraoperative reasoning. We introduce ResGo, a benchmark of laparoscopic frames annotated with Go Zone bounding boxes and clinician-authored rationales covering phase, exposure quality reasoning, next action and risk reminder. We introduce evaluation metrics that treat correct grounding under incorrect phase as failures, revealing that most vision-language models cannot handle such tasks and perform poorly. We then present SurGo-R1, a model optimized via RLHF with a multi-turn phase-then-go architecture where the model first identifies the surgical phase, then generates reasoning and Go Zone coordinates conditioned on that context. On unseen procedures, SurGo-R1 achieves 76.6% phase accuracy, 32.7 mIoU, and 54.8% hardcore accuracy, a 6.6$\\times$ improvement over the mainstream generalist VLMs. Code, model and benchmark will be available at https://github.com/jinlab-imvr/SurGo-R1",
        "authors_display": "Yueming Jin Team",
        "pdf_url": "http://arxiv.org/abs/2602.21706",
        "code_url": null,
        "date": "2026-02-25",
        "primary_category": "cs.CV",
        "chinese_summary": "针对微创手术中安全操作区识别的挑战以及现有AI系统缺乏对手术阶段依赖性推理能力的问题，本研究引入了ResGo基准，该基准包含腹腔镜帧、Go Zone边界框以及临床医生撰写的推理信息。在此基础上，提出并优化了SurGo-R1模型，该模型通过RLHF优化，采用多轮“先识别阶段再区域”架构，能够首先识别手术阶段，然后在此上下文下生成推理和Go Zone坐标。实验结果显示，SurGo-R1在未见过的手术中取得了76.6%的阶段准确率和54.8%的硬核准确率，相对于主流通用VLM提升了6.6倍。"
      },
      {
        "paper_id": "2602.21704",
        "title": "Dynamic Multimodal Activation Steering for Hallucination Mitigation in Large Vision-Language Models",
        "abstract": "Large Vision-Language Models (LVLMs) exhibit outstanding performance on vision-language tasks but struggle with hallucination problems. Through in-depth analysis of LVLM activation patterns, we reveal two key findings: 1) truthfulness and visual perception capabilities predominantly engage different subsets of attention heads within the model architecture; and 2) truthfulness steering vectors vary significantly across different semantic contexts. Based on these observations, we propose Dynamic Multimodal Activation Steering, a training-free approach for hallucination mitigation. Our method constructs a semantic-based truthfulness steering vector database and computes visual perception steering vectors, enabling context-aware interventions during inference by dynamically selecting the most relevant steering vectors based on input semantic similarity and applying them to the most influential attention heads. We conduct comprehensive experiments across multiple models and datasets, demonstrating that our approach significantly enhances model performance, outperforming existing state-of-the-art methods.",
        "authors_display": "Liang He Team",
        "pdf_url": "http://arxiv.org/abs/2602.21704",
        "code_url": null,
        "date": "2026-02-25",
        "primary_category": "cs.CV",
        "chinese_summary": "鉴于大型视觉语言模型（LVLM）在出色性能的同时存在幻觉问题，本研究深入分析了LVLM的激活模式。研究发现真实性和视觉感知能力主要由不同的注意力头子集处理，且真实性引导向量在不同语义上下文中差异显著。基于此，提出了一种免训练的动态多模态激活引导方法，该方法通过构建语义引导向量数据库和视觉感知引导向量，在推理时根据输入语义相似性动态选择并应用于最具影响力的注意力头，实现上下文感知干预。广泛实验结果表明，该方法显著提升了模型性能，优于现有先进方法。"
      },
      {
        "paper_id": "2602.21619",
        "title": "When More Is Less: A Systematic Analysis of Spatial and Commonsense Information for Visual Spatial Reasoning",
        "abstract": "Visual spatial reasoning (VSR) remains challenging for modern vision-language models (VLMs), despite advances in multimodal architectures. A common strategy is to inject additional information at inference time, such as explicit spatial cues, external commonsense knowledge, or chain-of-thought (CoT) reasoning instructions. However, it remains unclear when such information genuinely improves reasoning and when it introduces noise. In this paper, we conduct a hypothesis-driven analysis of information injection for VSR across three representative VLMs and two public benchmarks. We examine (i) the type and number of spatial contexts, (ii) the amount and relevance of injected commonsense knowledge, and (iii) the interaction between spatial grounding and CoT prompting. Our results reveal a consistent pattern: more information does not necessarily yield better reasoning. Targeted single spatial cues outperform multi-context aggregation, excessive or weakly relevant commonsense knowledge degrades performance, and CoT prompting improves accuracy only when spatial grounding is sufficiently precise. These findings highlight the importance of selective, task-aligned information injection and provide practical guidance for designing reliable multimodal reasoning pipelines.",
        "authors_display": "Soyeon Caren Han Team",
        "pdf_url": "http://arxiv.org/abs/2602.21619",
        "code_url": null,
        "date": "2026-02-25",
        "primary_category": "cs.CL",
        "chinese_summary": "尽管多模态架构不断发展，视觉空间推理（VSR）对于现代视觉语言模型（VLM）仍是一项挑战。针对推理时注入额外信息（如空间线索、常识知识或CoT推理）何时真正有效、何时引入噪声这一问题，本研究对VSR的信息注入进行了假设驱动分析。通过在三个代表性VLM和两个公共基准上，检验空间上下文类型与数量、常识知识的数量与相关性以及空间接地与CoT提示的交互作用。结果一致表明，并非更多信息就能带来更好的推理；有针对性的单一空间线索优于多上下文聚合，过多或弱相关的常识知识会降低性能，且CoT提示仅在空间接地足够精确时才有效。"
      },
      {
        "paper_id": "2602.21613",
        "title": "Virtual Biopsy for Intracranial Tumors Diagnosis on MRI",
        "abstract": "Deep intracranial tumors situated in eloquent brain regions controlling vital functions present critical diagnostic challenges. Clinical practice has shifted toward stereotactic biopsy for pathological confirmation before treatment. Yet biopsy carries inherent risks of hemorrhage and neurological deficits and struggles with sampling bias due to tumor spatial heterogeneity, because pathological changes are typically region-selective rather than tumor-wide. Therefore, advancing non-invasive MRI-based pathology prediction is essential for holistic tumor assessment and modern clinical decision-making.   The primary challenge lies in data scarcity: low tumor incidence requires long collection cycles, and annotation demands biopsy-verified pathology from neurosurgical experts. Additionally, tiny lesion volumes lacking segmentation masks cause critical features to be overwhelmed by background noise. To address these challenges, we construct the ICT-MRI dataset - the first public biopsy-verified benchmark with 249 cases across four categories. We propose a Virtual Biopsy framework comprising: MRI-Processor for standardization; Tumor-Localizer employing vision-language models for coarse-to-fine localization via weak supervision; and Adaptive-Diagnoser with a Masked Channel Attention mechanism fusing local discriminative features with global contexts. Experiments demonstrate over 90% accuracy, outperforming baselines by more than 20%.",
        "authors_display": "Jianguo Zhang Team",
        "pdf_url": "http://arxiv.org/abs/2602.21613",
        "code_url": null,
        "date": "2026-02-25",
        "primary_category": "cs.CV",
        "chinese_summary": "面对深部颅内肿瘤诊断的挑战、活检的固有风险以及数据稀缺和微小病灶易被噪声淹没的问题，本研究构建了ICT-MRI数据集，这是首个包含249例活检验证的公开脑肿瘤病理预测基准。在此基础上，提出了一种名为虚拟活检（Virtual Biopsy）的框架，该框架包括MRI处理器进行标准化、利用视觉语言模型通过弱监督实现粗细粒度定位的肿瘤定位器，以及结合掩蔽通道注意力机制融合局部判别特征与全局上下文的自适应诊断器。实验结果表明，该框架的准确率超过90%，比现有基线方法提高了20%以上。"
      },
      {
        "paper_id": "2602.21497",
        "title": "See It, Say It, Sorted: An Iterative Training-Free Framework for Visually-Grounded Multimodal Reasoning in LVLMs",
        "abstract": "Recent large vision-language models (LVLMs) have demonstrated impressive reasoning ability by generating long chain-of-thought (CoT) responses. However, CoT reasoning in multimodal contexts is highly vulnerable to visual hallucination propagation: once an intermediate reasoning step becomes inconsistent with the visual evidence, subsequent steps-even if logically valid-can still lead to incorrect final answers. Existing solutions attempt to mitigate this issue by training models to \"think with images\" via reinforcement learning (RL). While effective, these methods are costly, model-specific, and difficult to generalize across architectures. Differently, we present a lightweight method that bypasses RL training and provides an iterative, training-free, plug-and-play framework for visually-grounded multimodal reasoning. Our key idea is to supervise each reasoning step at test time with visual evidence, ensuring that every decoded token is justified by corresponding visual cues. Concretely, we construct a textual visual-evidence pool that guides the model's reasoning generation. When existing evidence is insufficient, a visual decider module dynamically extracts additional relevant evidence from the image based on the ongoing reasoning context, expanding the pool until the model achieves sufficient visual certainty to terminate reasoning and produce the final answer. Extensive experiments on multiple LVLM backbones and benchmarks demonstrate the effectiveness of our approach. Our method achieves 16.5%-29.5% improvements on TreeBench and 13.7% RH-AUC gains on RH-Bench, substantially reducing hallucination rates while improving reasoning accuracy without additional training.",
        "authors_display": "Yang Chen Team",
        "pdf_url": "http://arxiv.org/abs/2602.21497",
        "code_url": null,
        "date": "2026-02-25",
        "primary_category": "cs.CV",
        "chinese_summary": "大型视觉语言模型（LVLM）在生成链式思维（CoT）推理时，易受视觉幻觉传播影响，而现有基于强化学习的解决方案成本高且通用性差。本研究提出了一种轻量级、免训练、即插即用的迭代框架，用于视觉接地的多模态推理。其核心思想是在测试时利用视觉证据监督每个推理步骤，确保每个解码的token都与视觉线索对应。通过构建文本视觉证据池，并在证据不足时动态提取额外相关视觉证据，模型能实现更可靠的推理。在多个LVLM和基准上的广泛实验表明，该方法显著降低了幻觉率，并提升了推理准确性，无需额外训练。"
      },
      {
        "paper_id": "2602.22474",
        "title": "When to Act, Ask, or Learn: Uncertainty-Aware Policy Steering",
        "abstract": "Policy steering is an emerging way to adapt robot behaviors at deployment-time: a learned verifier analyzes low-level action samples proposed by a pre-trained policy (e.g., diffusion policy) and selects only those aligned with the task. While Vision-Language Models (VLMs) are promising general-purpose verifiers due to their reasoning capabilities, existing frameworks often assume these models are well-calibrated. In practice, the overconfident judgment from VLM can degrade the steering performance under both high-level semantic uncertainty in task specifications and low-level action uncertainty or incapability of the pre-trained policy. We propose uncertainty-aware policy steering (UPS), a framework that jointly reasons about semantic task uncertainty and low-level action feasibility, and selects an uncertainty resolution strategy: execute a high-confidence action, clarify task ambiguity via natural language queries, or ask for action interventions to correct the low-level policy when it is deemed incapable at the task. We leverage conformal prediction to calibrate the composition of the VLM and the pre-trained base policy, providing statistical assurances that the verifier selects the correct strategy. After collecting interventions during deployment, we employ residual learning to improve the capability of the pre-trained policy, enabling the system to learn continually but with minimal expensive human feedback. We demonstrate our framework through experiments in simulation and on hardware, showing that UPS can disentangle confident, ambiguous, and incapable scenarios and minimizes expensive user interventions compared to uncalibrated baselines and prior human- or robot-gated continual learning approaches. Videos can be found at https://jessie-yuan.github.io/ups/",
        "authors_display": "Andrea Bajcsy Team",
        "pdf_url": "http://arxiv.org/abs/2602.22474",
        "code_url": null,
        "date": "2026-02-25",
        "primary_category": "cs.RO",
        "chinese_summary": "机器人策略引导在部署时适应行为时，VLM验证器的过度自信判断可能在语义任务不确定性和低级动作不确定性下降低性能。本研究提出了不确定性感知策略引导（UPS）框架，它联合推理语义任务不确定性和低级动作可行性，并选择执行高置信动作、通过自然语言澄清任务或请求动作干预。UPS利用共形预测校准VLM和基础策略的组合，并通过部署期间收集的干预进行残差学习，以持续改进策略。模拟和硬件实验表明，UPS能有效区分置信、模糊和无能场景，并最大限度地减少用户干预。"
      },
      {
        "paper_id": "2602.22469",
        "title": "Beyond Dominant Patches: Spatial Credit Redistribution For Grounded Vision-Language Models",
        "abstract": "Vision-language models (VLMs) frequently hallucinate objects absent from the input image. We trace this failure to spatial credit collapse: activation credit concentrating on sparse visual patches in early transformer layers, which suppresses contextual evidence and increases reliance on language priors. We introduce Spatial Credit Redistribution (SCR), a training-free inference-time intervention that redistributes hidden-state activation from high-attention source patches to their context, guided by low-entropy inputs. We evaluate six model families (Chameleon, LLaVA, and Qwen, including both Qwen-VL and Qwen2-VL) at scales of 7B, 13B, and 30B, on POPE and CHAIR benchmarks. SCR reduces hallucination by ~4.7-6.0 percentage points on POPE-Adversarial, cuts CHAIR-s by 3.7-5.2 percentage points (42-51 percent relative), and CHAIR-i by 2.7-4.4 percentage points (44-58 percent relative), and preserves CIDEr within 0.8 percentage points. Gains are largest for low-entropy inputs, consistent with the theoretical framework. SCR incurs only 43-56 ms overhead (small models: +43-46 ms; large models: +54-56 ms), roughly 3-6 times lower than OPERA and VCD and 1.3-1.7 times lower than OVCD (+72 ms), while Pareto-dominating all three on both hallucination rate and CIDEr, making it practical for real-time settings. A controlled ablation confirms that attention-guided source selection is essential: replacing it with uniform random selection reduces hallucination rate gains from ~4.7-6.0 percentage points to only ~2.6-3.4 percentage points, pointing to credit-collapse as the key driver.",
        "authors_display": "Md Ashikur Rahman Team",
        "pdf_url": "http://arxiv.org/abs/2602.22469",
        "code_url": null,
        "date": "2026-02-25",
        "primary_category": "cs.CV",
        "chinese_summary": "视觉-语言模型（VLMs）常出现幻觉问题，这源于早期Transformer层中激活信用集中于稀疏视觉补丁，抑制了上下文证据。为解决此问题，本研究提出了空间信用再分配（SCR），一种无需训练的推理时干预方法，通过低熵输入引导，将隐藏状态激活从高注意力源补丁重新分配到其上下文。在POPE和CHAIR基准上对多达六个VLM家族的评估显示，SCR在大幅降低幻觉率（POPE-Adversarial降低约4.7-6.0个百分点，CHAIR相对减少42-58%）的同时，仅带来极低的计算开销（43-56毫秒），并在各项指标上优于现有方法，使其适用于实时应用。"
      },
      {
        "paper_id": "2602.22462",
        "title": "MammoWise: Multi-Model Local RAG Pipeline for Mammography Report Generation",
        "abstract": "Screening mammography is high volume, time sensitive, and documentation heavy. Radiologists must translate subtle visual findings into consistent BI-RADS assessments, breast density categories, and structured narrative reports. While recent Vision Language Models (VLMs) enable image-to-text reporting, many rely on closed cloud systems or tightly coupled architectures that limit privacy, reproducibility, and adaptability. We present MammoWise, a local multi-model pipeline that transforms open source VLMs into mammogram report generators and multi-task classifiers. MammoWise supports any Ollama-hosted VLM and mammography dataset, and enables zero-shot, few-shot, and Chain-of-Thought prompting, with optional multimodal Retrieval Augmented Generation (RAG) using a vector database for case-specific context. We evaluate MedGemma, LLaVA-Med, and Qwen2.5-VL on VinDr-Mammo and DMID datasets, assessing report quality (BERTScore, ROUGE-L), BI-RADS classification, breast density, and key findings. Report generation is consistently strong and improves with few-shot prompting and RAG. Classification is feasible but sensitive to model and dataset choice. Parameter-efficient fine-tuning (QLoRA) of MedGemma improves reliability, achieving BI-RADS accuracy of 0.7545, density accuracy of 0.8840, and calcification accuracy of 0.9341 while preserving report quality. MammoWise provides a practical and extensible framework for deploying local VLMs for mammography reporting within a unified and reproducible workflow.",
        "authors_display": "Vladimir Filkov Team",
        "pdf_url": "http://arxiv.org/abs/2602.22462",
        "code_url": null,
        "date": "2026-02-25",
        "primary_category": "cs.CV",
        "chinese_summary": "乳腺X线筛查报告生成任务复杂且对隐私要求高，现有视觉语言模型（VLMs）常依赖封闭系统，限制了其在实际应用中的隐私、可复现性和适应性。本研究提出了MammoWise，一个本地多模型管道，将开源VLM转化为乳腺X线报告生成器和多任务分类器。MammoWise支持多种VLM和数据集，并结合零样本、少样本提示和多模态检索增强生成（RAG）。实验表明，MammoWise的报告生成能力强劲，通过少样本提示和RAG可进一步提升；MedGemma的参数高效微调显著提高了BI-RADS、乳腺密度和钙化分类的准确性，同时保持报告质量。MammoWise提供了一个实用且可扩展的本地VLM乳腺X线报告框架。"
      },
      {
        "paper_id": "2602.22419",
        "title": "CLIP Is Shortsighted: Paying Attention Beyond the First Sentence",
        "abstract": "CLIP models learn transferable multi-modal features via image-text contrastive learning on internet-scale data. They are widely used in zero-shot classification, multi-modal retrieval, text-to-image diffusion, and as image encoders in large vision-language models. However, CLIP's pretraining is dominated by images paired with short captions, biasing the model toward encoding simple descriptions of salient objects and leading to coarse alignment on complex scenes and dense descriptions. While recent work mitigates this by fine-tuning on small-scale long-caption datasets, we identify an important common bias: both human- and LLM-generated long captions typically begin with a one-sentence summary followed by a detailed description. We show that this acts as a shortcut during training, concentrating attention on the opening sentence and early tokens and weakening alignment over the rest of the caption. To resolve this, we introduce DeBias-CLIP, which removes the summary sentence during training and applies sentence sub-sampling and text token padding to distribute supervision across all token positions. DeBias-CLIP achieves state-of-the-art long-text retrieval, improves short-text retrieval, and is less sensitive to sentence order permutations. It is a drop-in replacement for Long-CLIP with no additional trainable parameters.",
        "authors_display": "Steven L. Waslander Team",
        "pdf_url": "http://arxiv.org/abs/2602.22419",
        "code_url": null,
        "date": "2026-02-25",
        "primary_category": "cs.CV",
        "chinese_summary": "CLIP模型通过图像-文本对比学习获得多模态特征，但其预训练数据主要包含短标题，导致模型在复杂场景和长描述上对齐不足。研究发现，长标题通常以总结句开头，导致训练时注意力集中于开头句和早期tokens，削弱了对标题其余部分的对齐。为此，本研究提出了DeBias-CLIP，在训练中移除总结句，并采用句子子采样和文本token填充，以将监督分布到所有token位置。DeBias-CLIP在长文本检索方面取得了最先进的性能，同时改进了短文本检索，且对句子顺序排列的敏感性降低，作为现有Long-CLIP的即插即用替代品，无需额外参数。"
      },
      {
        "paper_id": "2602.21186",
        "title": "Spa3R: Predictive Spatial Field Modeling for 3D Visual Reasoning",
        "abstract": "While Vision-Language Models (VLMs) exhibit exceptional 2D visual understanding, their ability to comprehend and reason about 3D space--a cornerstone of spatial intelligence--remains superficial. Current methodologies attempt to bridge this domain gap either by relying on explicit 3D modalities or by augmenting VLMs with partial, view-conditioned geometric priors. However, such approaches hinder scalability and ultimately burden the language model with the ill-posed task of implicitly reconstructing holistic 3D geometry from sparse cues. In this paper, we argue that spatial intelligence can emerge inherently from 2D vision alone, rather than being imposed via explicit spatial instruction tuning. To this end, we introduce Spa3R, a self-supervised framework that learns a unified, view-invariant spatial representation directly from unposed multi-view images. Spa3R is built upon the proposed Predictive Spatial Field Modeling (PSFM) paradigm, where Spa3R learns to synthesize feature fields for arbitrary unseen views conditioned on a compact latent representation, thereby internalizing a holistic and coherent understanding of the underlying 3D scene. We further integrate the pre-trained Spa3R Encoder into existing VLMs via a lightweight adapter to form Spa3-VLM, effectively grounding language reasoning in a global spatial context. Experiments on the challenging VSI-Bench demonstrate that Spa3-VLM achieves state-of-the-art accuracy of 58.6% on 3D VQA, significantly outperforming prior methods. These results highlight PSFM as a scalable path toward advancing spatial intelligence. Code is available at https://github.com/hustvl/Spa3R.",
        "authors_display": "Xinggang Wang Team",
        "pdf_url": "http://arxiv.org/abs/2602.21186",
        "code_url": null,
        "date": "2026-02-24",
        "primary_category": "cs.CV",
        "chinese_summary": "针对视觉-语言模型（VLMs）在三维空间理解方面的局限性，现有方法依赖显式三维模态或部分几何先验，导致可扩展性受限。本文提出Spa3R框架，通过自监督方式从多视图图像中学习统一的、视图不变的空间表示，并引入预测空间场建模（PSFM）范式，实现对底层三维场景的整体理解。将预训练的Spa3R编码器集成到现有VLM中形成Spa3-VLM后，模型在VSI-Bench的3D VQA任务上达到了58.6%的最新准确率，显著优于现有方法，证明了PSFM在提升空间智能方面的可扩展性。"
      },
      {
        "paper_id": "2602.21175",
        "title": "Seeing Through Words: Controlling Visual Retrieval Quality with Language Models",
        "abstract": "Text-to-image retrieval is a fundamental task in vision-language learning, yet in real-world scenarios it is often challenged by short and underspecified user queries. Such queries are typically only one or two words long, rendering them semantically ambiguous, prone to collisions across diverse visual interpretations, and lacking explicit control over the quality of retrieved images. To address these issues, we propose a new paradigm of quality-controllable retrieval, which enriches short queries with contextual details while incorporating explicit notions of image quality. Our key idea is to leverage a generative language model as a query completion function, extending underspecified queries into descriptive forms that capture fine-grained visual attributes such as pose, scene, and aesthetics. We introduce a general framework that conditions query completion on discretized quality levels, derived from relevance and aesthetic scoring models, so that query enrichment is not only semantically meaningful but also quality-aware. The resulting system provides three key advantages: 1) flexibility, it is compatible with any pretrained vision-language model (VLMs) without modification; 2) transparency, enriched queries are explicitly interpretable by users; and 3) controllability, enabling retrieval results to be steered toward user-preferred quality levels. Extensive experiments demonstrate that our proposed approach significantly improves retrieval results and provides effective quality control, bridging the gap between the expressive capacity of modern VLMs and the underspecified nature of short user queries. Our code is available at https://github.com/Jianglin954/QCQC.",
        "authors_display": "Yun Fu Team",
        "pdf_url": "http://arxiv.org/abs/2602.21175",
        "code_url": null,
        "date": "2026-02-24",
        "primary_category": "cs.CV",
        "chinese_summary": "传统图文检索在处理用户简短、语义模糊的查询时面临挑战，导致检索质量不可控。为此，本文提出一种质量可控的检索新范式，利用生成式语言模型将简短查询扩展为包含姿态、场景、美学等细粒度视觉属性的描述性文本。该框架通过结合相关性和美学评分模型，将查询补全过程与离散的质量水平相关联，从而实现语义丰富且质量感知的检索。实验结果表明，所提方法显著提升了检索效果并提供了有效的质量控制能力，弥合了VLM表达能力与用户简短查询之间的差距。"
      },
      {
        "paper_id": "2602.21142",
        "title": "LUMEN: Longitudinal Multi-Modal Radiology Model for Prognosis and Diagnosis",
        "abstract": "Large vision-language models (VLMs) have evolved from general-purpose applications to specialized use cases such as in the clinical domain, demonstrating potential for decision support in radiology. One promising application is assisting radiologists in decision-making by the analysis of radiology imaging data such as chest X-rays (CXR) via a visual and natural language question-answering (VQA) interface. When longitudinal imaging is available, radiologists analyze temporal changes, which are essential for accurate diagnosis and prognosis. The manual longitudinal analysis is a time-consuming process, motivating the development of a training framework that can provide prognostic capabilities. We introduce a novel training framework LUMEN, that is optimized for longitudinal CXR interpretation, leveraging multi-image and multi-task instruction fine-tuning to enhance prognostic and diagnostic performance. We conduct experiments on the publicly available MIMIC-CXR and its associated Medical-Diff-VQA datasets. We further formulate and construct a novel instruction-following dataset incorporating longitudinal studies, enabling the development of a prognostic VQA task. Our method demonstrates significant improvements over baseline models in diagnostic VQA tasks, and more importantly, shows promising potential for prognostic capabilities. These results underscore the value of well-designed, instruction-tuned VLMs in enabling more accurate and clinically meaningful radiological interpretation of longitudinal radiological imaging data.",
        "authors_display": "Marius George Linguraru Team",
        "pdf_url": "http://arxiv.org/abs/2602.21142",
        "code_url": null,
        "date": "2026-02-24",
        "primary_category": "cs.CV",
        "chinese_summary": "大型视觉-语言模型（VLMs）在临床领域展现出决策支持潜力，尤其是在放射学胸部X光（CXR）图像分析中。然而，手动进行纵向CXR分析耗时且缺乏预后能力。本文提出了LUMEN，一个专为纵向CXR解读优化的新型训练框架，它利用多图像和多任务指令微调来增强模型的诊断和预后性能。通过在MIMIC-CXR等数据集上的实验，LUMEN显著提高了诊断VQA任务的性能，并展现出有前景的预后能力，突出了设计良好的指令微调VLM在实现更准确、临床意义更强的纵向放射学图像解读中的价值。"
      },
      {
        "paper_id": "2602.21054",
        "title": "VAUQ: Vision-Aware Uncertainty Quantification for LVLM Self-Evaluation",
        "abstract": "Large Vision-Language Models (LVLMs) frequently hallucinate, limiting their safe deployment in real-world applications. Existing LLM self-evaluation methods rely on a model's ability to estimate the correctness of its own outputs, which can improve deployment reliability; however, they depend heavily on language priors and are therefore ill-suited for evaluating vision-conditioned predictions. We propose VAUQ, a vision-aware uncertainty quantification framework for LVLM self-evaluation that explicitly measures how strongly a model's output depends on visual evidence. VAUQ introduces the Image-Information Score (IS), which captures the reduction in predictive uncertainty attributable to visual input, and an unsupervised core-region masking strategy that amplifies the influence of salient regions. Combining predictive entropy with this core-masked IS yields a training-free scoring function that reliably reflects answer correctness. Comprehensive experiments show that VAUQ consistently outperforms existing self-evaluation methods across multiple datasets.",
        "authors_display": "Sharon Li Team",
        "pdf_url": "http://arxiv.org/abs/2602.21054",
        "code_url": null,
        "date": "2026-02-24",
        "primary_category": "cs.CV",
        "chinese_summary": "大型视觉-语言模型（LVLMs）常出现幻觉问题，但现有的LLM自评估方法过度依赖语言先验，不适用于视觉条件下的预测评估。本文提出了VAUQ，一个视觉感知不确定性量化框架，用于LVLM自评估，它显式地衡量模型输出对视觉证据的依赖程度。VAUQ引入了图像信息分数（IS）来量化视觉输入带来的预测不确定性降低，并结合无监督核心区域掩蔽策略。实验证明，VAUQ在多个数据集上始终优于现有自评估方法，有效提升了LVLM的可靠性。"
      },
      {
        "paper_id": "2602.21053",
        "title": "OCR-Agent: Agentic OCR with Capability and Memory Reflection",
        "abstract": "Large Vision-Language Models (VLMs) have demonstrated significant potential on complex visual understanding tasks through iterative optimization methods.However, these models generally lack effective self-correction mechanisms, making it difficult for them to independently rectify cognitive biases. Consequently, during multi-turn revisions, they often fall into repetitive and ineffective attempts, failing to achieve stable improvements in answer quality.To address this issue, we propose a novel iterative self-correction framework that endows models with two key capabilities: Capability Reflection and Memory Reflection. This framework guides the model to first diagnose errors and generate a correction plan via Capability Reflection, then leverage Memory Reflection to review past attempts to avoid repetition and explore new solutions, and finally, optimize the answer through rigorous re-reasoning. Experiments on the challenging OCRBench v2 benchmark show that OCR-Agent outperforms the current open-source SOTA model InternVL3-8B by +2.0 on English and +1.2 on Chinese subsets, while achieving state-of-the-art results in Visual Understanding (79.9) and Reasoning (66.5) - surpassing even larger fine-tuned models. Our method demonstrates that structured, self-aware reflection can significantly enhance VLMs' reasoning robustness without additional training. Code: https://github.com/AIGeeksGroup/OCR-Agent.",
        "authors_display": "Ying Cai Team",
        "pdf_url": "http://arxiv.org/abs/2602.21053",
        "code_url": null,
        "date": "2026-02-24",
        "primary_category": "cs.CV",
        "chinese_summary": "大型视觉-语言模型（VLMs）在多轮修订中缺乏有效的自纠正机制，常陷入重复和低效的尝试，难以持续提升回答质量。本文提出一种新颖的迭代自纠正框架OCR-Agent，赋予模型能力反思和记忆反思两大核心能力。通过能力反思诊断错误并生成纠正方案，再利用记忆反思回顾过往尝试以避免重复并探索新解，最终通过严谨的重新推理优化答案。实验表明，OCR-Agent在OCRBench v2基准上显著超越了当前SOTA模型，并在视觉理解和推理任务中取得了领先结果，证明了结构化、自反思的机制可在无需额外训练的情况下显著增强VLM的推理鲁棒性。"
      },
      {
        "paper_id": "2602.21035",
        "title": "Not Just What's There: Enabling CLIP to Comprehend Negated Visual Descriptions Without Fine-tuning",
        "abstract": "Vision-Language Models (VLMs) like CLIP struggle to understand negation, often embedding affirmatives and negatives similarly (e.g., matching \"no dog\" with dog images). Existing methods refine negation understanding via fine-tuning CLIP's text encoder, risking overfitting. In this work, we propose CLIPGlasses, a plug-and-play framework that enhances CLIP's ability to comprehend negated visual descriptions. CLIPGlasses adopts a dual-stage design: a Lens module disentangles negated semantics from text embeddings, and a Frame module predicts context-aware repulsion strength, which is integrated into a modified similarity computation to penalize alignment with negated semantics, thereby reducing false positive matches. Experiments show that CLIP equipped with CLIPGlasses achieves competitive in-domain performance and outperforms state-of-the-art methods in cross-domain generalization. Its superiority is especially evident under low-resource conditions, indicating stronger robustness across domains.",
        "authors_display": "Zejiang He Team",
        "pdf_url": "http://arxiv.org/abs/2602.21035",
        "code_url": null,
        "date": "2026-02-24",
        "primary_category": "cs.CV",
        "chinese_summary": "视觉-语言模型（VLMs）如CLIP在理解否定句方面表现不佳，常将肯定和否定语义（如“没有狗”）嵌入得相似，而现有微调方法有过度拟合风险。本文提出了CLIPGlasses，一个即插即用的框架，旨在增强CLIP对否定视觉描述的理解能力。该框架采用双阶段设计：Lens模块负责解耦文本嵌入中的否定语义，而Frame模块预测与上下文相关的排斥强度，并将其整合到修正的相似性计算中，以惩罚与否定语义的对齐。实验证明，CLIP配备CLIPGlasses后在域内性能具有竞争力，并在跨域泛化方面优于SOTA方法，尤其在低资源条件下表现出更强的鲁棒性。"
      },
      {
        "paper_id": "2602.21015",
        "title": "From Perception to Action: An Interactive Benchmark for Vision Reasoning",
        "abstract": "Understanding the physical structure is essential for real-world applications such as embodied agents, interactive design, and long-horizon manipulation. Yet, prevailing Vision-Language Model (VLM) evaluations still center on structure-agnostic, single-turn setups (e.g., VQA), which fail to assess agents' ability to reason about how geometry, contact, and support relations jointly constrain what actions are possible in a dynamic environment. To address this gap, we introduce the Causal Hierarchy of Actions and Interactions (CHAIN) benchmark, an interactive 3D, physics-driven testbed designed to evaluate whether models can understand, plan, and execute structured action sequences grounded in physical constraints. CHAIN shifts evaluation from passive perception to active problem solving, spanning tasks such as interlocking mechanical puzzles and 3D stacking and packing. We conduct a comprehensive study of state-of-the-art VLMs and diffusion-based models under unified interactive settings. Our results show that top-performing models still struggle to internalize physical structure and causal constraints, often failing to produce reliable long-horizon plans and cannot robustly translate perceived structure into effective actions. The project is available at https://social-ai-studio.github.io/CHAIN/.",
        "authors_display": "Roy Ka-Wei Lee Team",
        "pdf_url": "http://arxiv.org/abs/2602.21015",
        "code_url": "https://social-ai-studio.github.io/CHAIN/",
        "date": "2026-02-24",
        "primary_category": "cs.CV",
        "chinese_summary": "现有视觉-语言模型（VLM）评估多集中于与结构无关的单轮任务，无法有效评估模型对几何、接触和支撑关系的推理能力，这对于理解物理结构至关重要。为弥补这一空白，本文引入CHAIN基准，一个交互式3D物理驱动测试平台，旨在评估模型理解、规划和执行基于物理约束的结构化动作序列的能力。CHAIN将评估从被动感知转向主动问题解决，涵盖机械拼图、3D堆叠等任务。实验结果表明，即使是最先进的VLM和扩散模型，仍难以内化物理结构和因果约束，在生成长程规划和有效行动方面表现不足。"
      },
      {
        "paper_id": "2602.20901",
        "title": "SpatiaLQA: A Benchmark for Evaluating Spatial Logical Reasoning in Vision-Language Models",
        "abstract": "Vision-Language Models (VLMs) have been increasingly applied in real-world scenarios due to their outstanding understanding and reasoning capabilities. Although VLMs have already demonstrated impressive capabilities in common visual question answering and logical reasoning, they still lack the ability to make reasonable decisions in complex real-world environments. We define this ability as spatial logical reasoning, which not only requires understanding the spatial relationships among objects in complex scenes, but also the logical dependencies between steps in multi-step tasks. To bridge this gap, we introduce Spatial Logical Question Answering (SpatiaLQA), a benchmark designed to evaluate the spatial logical reasoning capabilities of VLMs. SpatiaLQA consists of 9,605 question answer pairs derived from 241 real-world indoor scenes. We conduct extensive experiments on 41 mainstream VLMs, and the results show that even the most advanced models still struggle with spatial logical reasoning. To address this issue, we propose a method called recursive scene graph assisted reasoning, which leverages visual foundation models to progressively decompose complex scenes into task-relevant scene graphs, thereby enhancing the spatial logical reasoning ability of VLMs, outperforming all previous methods. Code and dataset are available at https://github.com/xieyc99/SpatiaLQA.",
        "authors_display": "Jie Song Team",
        "pdf_url": "http://arxiv.org/abs/2602.20901",
        "code_url": null,
        "date": "2026-02-24",
        "primary_category": "cs.CV",
        "chinese_summary": "尽管视觉-语言模型（VLMs）在通用视觉问答和逻辑推理方面表现出色，但在复杂真实环境中进行合理决策的能力（即空间逻辑推理）仍有欠缺。为弥补这一不足，本文提出了SpatiaLQA基准，专门用于评估VLM的空间逻辑推理能力，该基准包含来自241个真实室内场景的9605个问答对。针对现有模型在此任务上的不足，我们提出了一种递归场景图辅助推理方法，利用视觉基础模型逐步将复杂场景分解为与任务相关的场景图。实验证明，该方法显著优于所有现有方法，有效提升了VLMs的空间逻辑推理能力。"
      },
      {
        "paper_id": "2602.20878",
        "title": "Diagnosing Causal Reasoning in Vision-Language Models via Structured Relevance Graphs",
        "abstract": "Large Vision-Language Models (LVLMs) achieve strong performance on visual question answering benchmarks, yet often rely on spurious correlations rather than genuine causal reasoning. Existing evaluations primarily assess the correctness of the answers, making it unclear whether failures arise from limited reasoning capability or from misidentifying causally relevant information. We introduce Vision-Language Causal Graphs (VLCGs), a structured, query-conditioned representation that explicitly encodes causally relevant objects, attributes, relations, and scene-grounded assumptions. Building on this representation, we present ViLCaR, a diagnostic benchmark comprising tasks for Causal Attribution, Causal Inference, and Question Answering, along with graph-aligned evaluation metrics that assess relevance identification beyond final answer accuracy. Experiments in state-of-the-art LVLMs show that injecting structured relevance information significantly improves attribution and inference consistency compared to zero-shot and standard in-context learning. These findings suggest that current limitations in LVLM causal reasoning stem primarily from insufficient structural guidance rather than a lack of reasoning capacity.",
        "authors_display": "Yihao Ding Team",
        "pdf_url": "http://arxiv.org/abs/2602.20878",
        "code_url": null,
        "date": "2026-02-24",
        "primary_category": "cs.AI",
        "chinese_summary": "大型视觉-语言模型（LVLMs）在视觉问答任务中表现优异，但常依赖虚假关联而非真实的因果推理。现有评估主要关注答案正确性，未能区分失败原因是否源于推理能力限制或未能识别因果相关信息。本文引入了视觉-语言因果图（VLCGs），一种结构化、查询条件化的表示，明确编码因果相关的对象、属性、关系及场景假设。基于此，提出了ViLCaR诊断基准，包含因果归因、因果推理和问答任务，并采用与图对齐的评估指标。实验表明，注入结构化相关性信息能显著提升LVLMs的归因和推理一致性，暗示当前因果推理的局限性主要源于结构指导不足，而非推理能力缺乏。"
      },
      {
        "paper_id": "2602.20873",
        "title": "MUSE: Harnessing Precise and Diverse Semantics for Few-Shot Whole Slide Image Classification",
        "abstract": "In computational pathology, few-shot whole slide image classification is primarily driven by the extreme scarcity of expert-labeled slides. Recent vision-language methods incorporate textual semantics generated by large language models, but treat these descriptions as static class-level priors that are shared across all samples and lack sample-wise refinement. This limits both the diversity and precision of visual-semantic alignment, hindering generalization under limited supervision. To overcome this, we propose the stochastic MUlti-view Semantic Enhancement (MUSE), a framework that first refines semantic precision via sample-wise adaptation and then enhances semantic richness through retrieval-augmented multi-view generation. Specifically, MUSE introduces Sample-wise Fine-grained Semantic Enhancement (SFSE), which yields a fine-grained semantic prior for each sample through MoE-based adaptive visual-semantic interaction. Guided by this prior, Stochastic Multi-view Model Optimization (SMMO) constructs an LLM-generated knowledge base of diverse pathological descriptions per class, then retrieves and stochastically integrates multiple matched textual views during training. These dynamically selected texts serve as enriched semantic supervisions to stochastically optimize the vision-language model, promoting robustness and mitigating overfitting. Experiments on three benchmark WSI datasets show that MUSE consistently outperforms existing vision-language baselines in few-shot settings, demonstrating that effective few-shot pathology learning requires not only richer semantic sources but also their active and sample-aware semantic optimization. Our code is available at: https://github.com/JiahaoXu-god/CVPR2026_MUSE.",
        "authors_display": "Nankun Mu Team",
        "pdf_url": "http://arxiv.org/abs/2602.20873",
        "code_url": null,
        "date": "2026-02-24",
        "primary_category": "cs.CV",
        "chinese_summary": "计算病理学中，少量样本全玻片图像分类受限于专家标注稀缺，而现有视觉-语言方法将文本语义作为静态类级别先验，限制了视觉-语义对齐的多样性和精确性。为此，本文提出随机多视图语义增强（MUSE）框架，首先通过基于MoE的自适应视觉-语义交互实现样本级细粒度语义增强（SFSE），然后通过随机多视图模型优化（SMMO）构建LLM生成的丰富病理描述知识库，并在训练中随机整合多视图文本作为增强语义监督。实验证明，MUSE在多个WSI基准上持续优于现有VLM基线，强调了有效少量样本病理学习需更丰富的语义源和积极、样本感知的语义优化。"
      },
      {
        "paper_id": "2602.20853",
        "title": "On the Explainability of Vision-Language Models in Art History",
        "abstract": "Vision-Language Models (VLMs) transfer visual and textual data into a shared embedding space. In so doing, they enable a wide range of multimodal tasks, while also raising critical questions about the nature of machine 'understanding.' In this paper, we examine how Explainable Artificial Intelligence (XAI) methods can render the visual reasoning of a VLM - namely, CLIP - legible in art-historical contexts. To this end, we evaluate seven methods, combining zero-shot localization experiments with human interpretability studies. Our results indicate that, while these methods capture some aspects of human interpretation, their effectiveness hinges on the conceptual stability and representational availability of the examined categories.",
        "authors_display": "Stefanie Schneider Team",
        "pdf_url": "http://arxiv.org/abs/2602.20853",
        "code_url": null,
        "date": "2026-02-24",
        "primary_category": "cs.CV",
        "chinese_summary": "在问题背景方面，视觉-语言模型（VLMs）将视觉和文本数据整合到共享嵌入空间中，同时引发了对机器“理解”本质的质疑。本研究旨在通过可解释人工智能（XAI）方法，揭示VLM（特指CLIP）在艺术史背景下的视觉推理过程。研究方法上，论文评估了七种XAI方法，结合了零样本定位实验和人类可解释性研究。实验结果表明，尽管这些方法能够捕捉人类解释的某些方面，但其有效性取决于所检查类别的概念稳定性及其表征可用性。"
      },
      {
        "paper_id": "2602.20818",
        "title": "GatedCLIP: Gated Multimodal Fusion for Hateful Memes Detection",
        "abstract": "Detecting hateful content in multimodal memes presents unique challenges, as harmful messages often emerge from the complex interplay between benign images and text. We propose GatedCLIP, a Vision-Language model that enhances CLIP's multimodal capabilities with specialized architectural improvements for hateful memes detection. Our approach introduces learned projection heads that map CLIP embeddings to a task-optimized semantic space, a dynamic gated fusion mechanism that adaptively weights visual and textual features, and a contrastive learning objective that maintains cross-modal semantic alignment. Experiments on the Hateful Memes dataset demonstrate that GatedCLIP achieves an AUROC of 0.66, substantially outperforming the CLIP baseline (AUROC 0.49) while maintaining computational efficiency with only 350K trainable parameters.",
        "authors_display": "Zirong Zeng Team",
        "pdf_url": "http://arxiv.org/abs/2602.20818",
        "code_url": null,
        "date": "2026-02-24",
        "primary_category": "cs.CV",
        "chinese_summary": "针对多模态表情包中仇恨内容的检测难题（因图像与文本复杂交互而产生有害信息），本研究提出了一种名为GatedCLIP的视觉-语言模型。该模型通过专门的架构改进增强了CLIP的多模态能力，具体包括引入学习投影头以将CLIP嵌入映射到任务优化的语义空间、动态门控融合机制以自适应加权视觉和文本特征，以及对比学习目标以保持跨模态语义对齐。在Hateful Memes数据集上的实验结果表明，GatedCLIP的AUROC达到0.66，显著优于CLIP基线（AUROC 0.49），同时仅需35万个可训练参数，保持了计算效率。"
      },
      {
        "paper_id": "2602.20794",
        "title": "VGGDrive: Empowering Vision-Language Models with Cross-View Geometric Grounding for Autonomous Driving",
        "abstract": "The significance of cross-view 3D geometric modeling capabilities for autonomous driving is self-evident, yet existing Vision-Language Models (VLMs) inherently lack this capability, resulting in their mediocre performance. While some promising approaches attempt to mitigate this by constructing Q&A data for auxiliary training, they still fail to fundamentally equip VLMs with the ability to comprehensively handle diverse evaluation protocols. We thus chart a new course, advocating for the infusion of VLMs with the cross-view geometric grounding of mature 3D foundation models, closing this critical capability gap in autonomous driving. In this spirit, we propose a novel architecture, VGGDrive, which empowers Vision-language models with cross-view Geometric Grounding for autonomous Driving. Concretely, to bridge the cross-view 3D geometric features from the frozen visual 3D model with the VLM's 2D visual features, we introduce a plug-and-play Cross-View 3D Geometric Enabler (CVGE). The CVGE decouples the base VLM architecture and effectively empowers the VLM with 3D features through a hierarchical adaptive injection mechanism. Extensive experiments show that VGGDrive enhances base VLM performance across five autonomous driving benchmarks, including tasks like cross-view risk perception, motion prediction, and trajectory planning. It's our belief that mature 3D foundation models can empower autonomous driving tasks through effective integration, and we hope our initial exploration demonstrates the potential of this paradigm to the autonomous driving community.",
        "authors_display": "Long Chen Team",
        "pdf_url": "http://arxiv.org/abs/2602.20794",
        "code_url": null,
        "date": "2026-02-24",
        "primary_category": "cs.CV",
        "chinese_summary": "自动驾驶中跨视角3D几何建模能力至关重要，然而现有视觉-语言模型（VLMs）普遍缺乏此能力，导致性能平庸，且通过问答数据进行辅助训练的现有方法也未能根本解决此问题。为此，本研究提出了一种新范式，旨在将成熟3D基础模型的跨视角几何基础能力融入VLMs。具体方法是，论文提出了VGGDrive架构，并引入了即插即用的跨视角3D几何使能器（CVGE），通过分层自适应注入机制，将冻结的视觉3D模型的跨视角3D几何特征与VLM的2D视觉特征桥接起来，从而赋能VLM具备3D能力。大量实验表明，VGGDrive在包括跨视角风险感知、运动预测和轨迹规划在内的五个自动驾驶基准测试中显著提升了VLM的性能。"
      },
      {
        "paper_id": "2602.20700",
        "title": "NGL-Prompter: Training-Free Sewing Pattern Estimation from a Single Image",
        "abstract": "Estimating sewing patterns from images is a practical approach for creating high-quality 3D garments. Due to the lack of real-world pattern-image paired data, prior approaches fine-tune large vision language models (VLMs) on synthetic garment datasets generated by randomly sampling from a parametric garment model GarmentCode. However, these methods often struggle to generalize to in-the-wild images, fail to capture real-world correlations between garment parts, and are typically restricted to single-layer outfits. In contrast, we observe that VLMs are effective at describing garments in natural language, yet perform poorly when asked to directly regress GarmentCode parameters from images. To bridge this gap, we propose NGL (Natural Garment Language), a novel intermediate language that restructures GarmentCode into a representation more understandable to language models. Leveraging this language, we introduce NGL-Prompter, a training-free pipeline that queries large VLMs to extract structured garment parameters, which are then deterministically mapped to valid GarmentCode. We evaluate our method on the Dress4D, CloSe and a newly collected dataset of approximately 5,000 in-the-wild fashion images. Our approach achieves state-of-the-art performance on standard geometry metrics and is strongly preferred in both human and GPT-based perceptual evaluations compared to existing baselines. Furthermore, NGL-prompter can recover multi-layer outfits whereas competing methods focus mostly on single-layer garments, highlighting its strong generalization to real-world images even with occluded parts. These results demonstrate that accurate sewing pattern reconstruction is possible without costly model training. Our code and data will be released for research use.",
        "authors_display": "Michael Black Team",
        "pdf_url": "http://arxiv.org/abs/2602.20700",
        "code_url": null,
        "date": "2026-02-24",
        "primary_category": "cs.CV",
        "chinese_summary": "从图像中估计缝纫版型是创建高质量3D服装的实用方法，但缺乏真实世界数据导致现有VLM方法难以泛化，且多限于单层服装。本研究观察到VLM能有效描述服装，但直接回归参数表现不佳。为解决此问题，论文提出了NGL（Natural Garment Language），一种将GarmentCode重构为更易VLM理解的中间语言。在此基础上，引入了NGL-Prompter，一个无需训练的流程，通过查询大型VLM提取结构化服装参数，再确定性地映射到有效的GarmentCode。在Dress4D、CloSe和新收集的约5000张真实时尚图像数据集上的评估显示，该方法在几何度量上达到最先进水平，并在人类和GPT感性评估中获得显著偏好，同时能恢复多层服装并强泛化到真实世界图像，证明无需昂贵模型训练即可实现精确缝纫版型重建。"
      },
      {
        "paper_id": "2602.20696",
        "title": "PromptCD: Test-Time Behavior Enhancement via Polarity-Prompt Contrastive Decoding",
        "abstract": "Reliable AI systems require large language models (LLMs) to exhibit behaviors aligned with human preferences and values. However, most existing alignment approaches operate at training time and rely on additional high-quality data, incurring significant computational and annotation costs. While recent work has shown that contrastive decoding can leverage a model's internal distributions to improve specific capabilities, its applicability remains limited to narrow behavioral scopes and scenarios. In this work, we introduce Polarity-Prompt Contrastive Decoding (PromptCD), a test-time behavior control method that generalizes contrastive decoding to broader enhancement settings. PromptCD constructs paired positive and negative guiding prompts for a target behavior and contrasts model responses-specifically token-level probability distributions in LLMs and visual attention patterns in VLMs-to reinforce desirable outcomes. This formulation extends contrastive decoding to a wide range of enhancement objectives and is applicable to both LLMs and Vision-Language Models (VLMs) without additional training. For LLMs, experiments on the \"3H\" alignment objectives (helpfulness, honesty, and harmlessness) demonstrate consistent and substantial improvements, indicating that post-trained models can achieve meaningful self-enhancement purely at test time. For VLMs, we further analyze contrastive effects on visual attention, showing that PromptCD significantly improves VQA performance by reinforcing behavior-consistent visual grounding. Collectively, these results highlight PromptCD as a simple, general, and cost-efficient strategy for reliable behavior control across modalities.",
        "authors_display": "Xueqi Cheng Team",
        "pdf_url": "http://arxiv.org/abs/2602.20696",
        "code_url": null,
        "date": "2026-02-24",
        "primary_category": "cs.AI",
        "chinese_summary": "为构建可靠的AI系统，大型语言模型（LLMs）需与人类偏好和价值观对齐，但现有对齐方法依赖大量高质量数据和高昂成本，且对比解码方法应用范围有限。为此，本研究提出了一种名为极性提示对比解码（PromptCD）的测试时行为控制方法，旨在将对比解码推广到更广泛的增强设置。PromptCD通过构建针对目标行为的成对正向和负向引导提示，对比模型响应（LLM中的词元级概率分布和VLM中的视觉注意力模式）以增强期望结果，且无需额外训练。实验结果显示，PromptCD在LLM的“3H”对齐目标（有益性、诚实性和无害性）上持续取得显著改进，在VLM上通过强化行为一致的视觉基础显著提升了VQA性能，证实其作为跨模态行为控制的简单、通用且经济高效的策略。"
      },
      {
        "paper_id": "2602.20687",
        "title": "How Foundational Skills Influence VLM-based Embodied Agents:A Native Perspective",
        "abstract": "Recent advances in vision-language models (VLMs) have shown promise for human-level embodied intelligence. However, existing benchmarks for VLM-driven embodied agents often rely on high-level commands or discretized action spaces, which are non-native settings that differ markedly from real-world control. In addition, current benchmarks focus primarily on high-level tasks and lack joint evaluation and analysis at both low and high levels. To address these limitations, we present NativeEmbodied, a challenging benchmark for VLM-driven embodied agents that uses a unified, native low-level action space. Built on diverse simulated scenes, NativeEmbodied includes three representative high-level tasks in complex scenarios to evaluate overall performance. For more detailed analysis, we further decouple the skills required by complex tasks and construct four types of low-level tasks, each targeting a fundamental embodied skill. This joint evaluation across task and skill granularities enables fine-grained assessment of embodied agents. Experiments with state-of-the-art VLMs reveal clear deficiencies in several fundamental embodied skills, and further analysis shows that these bottlenecks significantly limit performance on high-level tasks. NativeEmbodied highlights key challenges for current VLM-driven embodied agents and provides insights to guide future research.",
        "authors_display": "Tong Xu Team",
        "pdf_url": "http://arxiv.org/abs/2602.20687",
        "code_url": null,
        "date": "2026-02-24",
        "primary_category": "cs.AI",
        "chinese_summary": "视觉-语言模型（VLMs）在具身智能方面展现了巨大潜力，然而现有VLM驱动的具身智能体基准测试常依赖高层命令或离散动作空间，与现实世界控制存在显著差异，且缺乏低层和高层任务的联合评估。为解决这些局限性，本研究提出了NativeEmbodied，一个采用统一、原生低层动作空间的VLM驱动具身智能体挑战性基准。该基准构建于多样化的模拟场景，包含三个复杂场景中的代表性高层任务以评估整体性能，并进一步解耦复杂任务所需的技能，构建了四种针对基本具身技能的低层任务，从而实现跨任务和技能粒度的联合评估。对最先进VLM的实验揭示了其在多个基本具身技能上的明显缺陷，并分析表明这些瓶颈显著限制了高层任务的性能，为未来研究指明了关键挑战和方向。"
      },
      {
        "paper_id": "2602.20659",
        "title": "Recursive Belief Vision Language Model",
        "abstract": "Current vision-language-action (VLA) models struggle with long-horizon manipulation under partial observability. Most existing approaches remain observation-driven, relying on short context windows or repeated queries to vision-language models (VLMs). This leads to loss of task progress, action repetition under perceptual aliasing, and high inference latency. Semantic reasoning alone is not the primary bottleneck in long-horizon manipulation. Instead, VLAs lack persistent, action-conditioned state representations and exhibit limited temporal and physical reasoning, making them ill-suited for multi-stage control. This paper introduces RB-VLA, a belief-centric architecture trained with self-supervised world-model objectives that maintains a compact latent state encoding task-relevant history, dynamics, and object interactions. Queried once for high-level intent, the VLM provides task specification, while the belief tracks task progress and enables phase-aware, causally grounded control under partial observability without storing raw observations or scaling memory with time. The belief and intent jointly condition a diffusion policy for robust closed-loop execution. RB-VLA outperforms prior VLAs on long-horizon benchmarks, achieving 52.5% and 37.5% higher success on multi-stage pick-and-place and stacking tasks, respectively, compared to π0. It also reduces inference latency by up to 5x relative to baselines and eliminates memory growth across timesteps observed in existing VLAs. Ablations show that the belief module is the primary driver of performance, increasing success rates from 32.5% to 77.5%. These results demonstrate the effectiveness of belief-based state representations for long-horizon VLA policies.",
        "authors_display": "Nirav Patel Team",
        "pdf_url": "http://arxiv.org/abs/2602.20659",
        "code_url": null,
        "date": "2026-02-24",
        "primary_category": "cs.AI",
        "chinese_summary": "当前视觉-语言-动作（VLA）模型在部分可观测的长期操纵任务中表现不佳，主要因依赖短期观察或反复查询VLM，导致任务进度丢失、感知混叠下重复动作以及高推理延迟，其根本在于缺乏持久的、动作条件化的状态表示以及有限的时空和物理推理能力。本研究提出RB-VLA，一种以信念为中心的架构，通过自监督世界模型目标进行训练，能够维持一个紧凑的潜在状态来编码任务相关的历史、动态和物体交互。该模型仅需一次VLM查询以获取高层意图，而信念模块则跟踪任务进度，并在不存储原始观测或随时间扩展内存的情况下，实现部分可观测条件下的阶段感知和因果接地控制。信念和意图共同条件化一个扩散策略以实现鲁棒的闭环执行。RB-VLA在长期基准测试中优于现有VLA模型，多阶段抓取放置和堆叠任务成功率分别比π0高出52.5%和37.5%，推理延迟降低高达5倍，并消除了现有VLA中随时间增长的内存问题。消融实验表明信念模块是性能提升的主要驱动因素，将成功率从32.5%提高到77.5%。"
      },
      {
        "paper_id": "2602.20658",
        "title": "Vision-Language Models for Ergonomic Assessment of Manual Lifting Tasks: Estimating Horizontal and Vertical Hand Distances from RGB Video",
        "abstract": "Manual lifting tasks are a major contributor to work-related musculoskeletal disorders, and effective ergonomic risk assessment is essential for quantifying physical exposure and informing ergonomic interventions. The Revised NIOSH Lifting Equation (RNLE) is a widely used ergonomic risk assessment tool for lifting tasks that relies on six task variables, including horizontal (H) and vertical (V) hand distances; such distances are typically obtained through manual measurement or specialized sensing systems and are difficult to use in real-world environments. We evaluated the feasibility of using innovative vision-language models (VLMs) to non-invasively estimate H and V from RGB video streams. Two multi-stage VLM-based pipelines were developed: a text-guided detection-only pipeline and a detection-plus-segmentation pipeline. Both pipelines used text-guided localization of task-relevant regions of interest, visual feature extraction from those regions, and transformer-based temporal regression to estimate H and V at the start and end of a lift. For a range of lifting tasks, estimation performance was evaluated using leave-one-subject-out validation across the two pipelines and seven camera view conditions. Results varied significantly across pipelines and camera view conditions, with the segmentation-based, multi-view pipeline consistently yielding the smallest errors, achieving mean absolute errors of approximately 6-8 cm when estimating H and 5-8 cm when estimating V. Across pipelines and camera view configurations, pixel-level segmentation reduced estimation error by approximately 20-30% for H and 35-40% for V relative to the detection-only pipeline. These findings support the feasibility of VLM-based pipelines for video-based estimation of RNLE distance parameters.",
        "authors_display": "Maury A. Nussbaum Team",
        "pdf_url": "http://arxiv.org/abs/2602.20658",
        "code_url": null,
        "date": "2026-02-24",
        "primary_category": "cs.CV",
        "chinese_summary": "人工搬运任务是工作相关肌肉骨骼疾病的主要原因，有效的工效学风险评估至关重要。修订后的NIOSH搬运方程（RNLE）是一种广泛使用的评估工具，依赖于手部水平（H）和垂直（V）距离等六个任务变量，这些变量通常通过手动测量或专业传感系统获取，难以在真实环境中应用。本研究评估了利用创新视觉-语言模型（VLMs）从RGB视频流中非侵入性估计H和V距离的可行性。为此，开发了两种多阶段VLM管道：文本引导的仅检测管道和检测加分割管道，两者均采用文本引导的兴趣区域定位、视觉特征提取和基于Transformer的时间回归来估计搬运开始和结束时的H和V。在多类搬运任务中，通过留一受试者交叉验证和七种摄像机视角条件进行性能评估。结果显示，基于分割的多视角管道持续产生最小误差（H约6-8厘米，V约5-8厘米的平均绝对误差），且像素级分割相比仅检测管道，将H和V的估计误差分别降低了约20-30%和35-40%。这些发现支持了基于VLM的管道在视频基础上估计RNLE距离参数的可行性。"
      },
      {
        "paper_id": "2602.20577",
        "title": "Efficient and Explainable End-to-End Autonomous Driving via Masked Vision-Language-Action Diffusion",
        "abstract": "Large Language Models (LLMs) and Vision-Language Models (VLMs) have emerged as promising candidates for end-to-end autonomous driving. However, these models typically face challenges in inference latency, action precision, and explainability. Existing autoregressive approaches struggle with slow token-by-token generation, while prior diffusion-based planners often rely on verbose, general-purpose language tokens that lack explicit geometric structure. In this work, we propose Masked Vision-Language-Action Diffusion for Autonomous Driving (MVLAD-AD), a novel framework designed to bridge the gap between efficient planning and semantic explainability via a masked vision-language-action diffusion model. Unlike methods that force actions into the language space, we introduce a discrete action tokenization strategy that constructs a compact codebook of kinematically feasible waypoints from real-world driving distributions. Moreover, we propose geometry-aware embedding learning to ensure that embeddings in the latent space approximate physical geometric metrics. Finally, an action-priority decoding strategy is introduced to prioritize trajectory generation. Extensive experiments on nuScenes and derived benchmarks demonstrate that MVLAD-AD achieves superior efficiency and outperforms state-of-the-art autoregressive and diffusion baselines in planning precision, while providing high-fidelity and explainable reasoning.",
        "authors_display": "Ziran Wang Team",
        "pdf_url": "http://arxiv.org/abs/2602.20577",
        "code_url": null,
        "date": "2026-02-24",
        "primary_category": "cs.CV",
        "chinese_summary": "大型语言模型（LLMs）和视觉-语言模型（VLMs）在端到端自动驾驶中展现潜力，但面临推理延迟、动作精度和可解释性挑战。现有自回归方法生成速度慢，而基于扩散的规划器常依赖缺乏几何结构的冗长通用语言标记。为此，本研究提出MVLAD-AD框架，旨在通过掩码视觉-语言-动作扩散模型弥合高效规划与语义可解释性之间的鸿沟。该方法引入离散动作标记策略，从真实驾驶分布构建运动学上可行的紧凑路点代码本；提出几何感知嵌入学习，确保潜在空间嵌入近似物理几何度量；并引入动作优先解码策略。在nuScenes及衍生基准上的大量实验表明，MVLAD-AD在效率和规划精度上优于最先进的自回归和扩散基线，同时提供高保真和可解释的推理。"
      },
      {
        "paper_id": "2602.20575",
        "title": "An interactive enhanced driving dataset for autonomous driving",
        "abstract": "The evolution of autonomous driving towards full automation demands robust interactive capabilities; however, the development of Vision-Language-Action (VLA) models is constrained by the sparsity of interactive scenarios and inadequate multimodal alignment in existing data. To this end, this paper proposes the Interactive Enhanced Driving Dataset (IEDD). We develop a scalable pipeline to mine million-level interactive segments from naturalistic driving data based on interactive trajectories, and design metrics to quantify the interaction processes. Furthermore, the IEDD-VQA dataset is constructed by generating synthetic Bird's Eye View (BEV) videos where semantic actions are strictly aligned with structured language. Benchmark results evaluating ten mainstream Vision Language Models (VLMs) are provided to demonstrate the dataset's reuse value in assessing and fine-tuning the reasoning capabilities of autonomous driving models.",
        "authors_display": "Lu Xiong Team",
        "pdf_url": "http://arxiv.org/abs/2602.20575",
        "code_url": null,
        "date": "2026-02-24",
        "primary_category": "cs.CV",
        "chinese_summary": "随着自动驾驶向完全自动化发展，强大的交互能力成为关键，但现有数据中稀疏的交互场景和不足的多模态对齐限制了视觉-语言-动作（VLA）模型的发展。为此，本研究提出了交互增强驾驶数据集（IEDD）。该方法开发了一个可扩展的管道，基于交互轨迹从自然驾驶数据中挖掘百万级的交互片段，并设计了量化交互过程的指标。此外，通过生成语义动作与结构化语言严格对齐的合成鸟瞰图（BEV）视频，构建了IEDD-VQA数据集。对十个主流视觉语言模型（VLMs）的基准测试结果表明，该数据集在评估和微调自动驾驶模型的推理能力方面具有很高的复用价值。"
      },
      {
        "paper_id": "2602.21435",
        "title": "Synergizing Understanding and Generation with Interleaved Analyzing-Drafting Thinking",
        "abstract": "Unified Vision-Language Models (UVLMs) aim to advance multimodal learning by supporting both understanding and generation within a single framework. However, existing approaches largely focus on architectural unification while overlooking the need for explicit interaction between the two capabilities during task solving. As a result, current models treat understanding and generation as parallel skills rather than synergistic processes. To achieve real synergy, we introduce the interleaved Analyzing-Drafting problem-solving loop (AD-Loop), a new think paradigm that dynamically alternates between analytic and drafting operations. By interleaving textual thoughts with visual thoughts, AD-Loop enables models to iteratively refine both comprehension and outputs, fostering genuine synergy. To train this mechanism, we design a two-stage strategy: supervised learning on interleaved thought data to initialize alternation, followed by reinforcement learning to promote adaptive and autonomous control. Extensive experiments demonstrate that AD-Loop consistently improves performance across standard benchmarks for both understanding and generation, with strong transferability to various UVLMs architectures. Visual analyses further validate the effectiveness of implicit visual thoughts. These results highlight AD-Loop as a principled and broadly applicable strategy for synergizing comprehension and creation. The project page is at https://sqwu.top/AD-Loop.",
        "authors_display": "Tat-seng Chua Team",
        "pdf_url": "http://arxiv.org/abs/2602.21435",
        "code_url": null,
        "date": "2026-02-24",
        "primary_category": "cs.CV",
        "chinese_summary": "现有统一视觉语言模型（UVLM）在整合理解与生成能力时，主要侧重架构统一，却忽略了这两种能力在任务解决中的显式交互，导致它们并行而非协同。为实现真正的协同，本研究引入了交错分析-起草问题解决循环（AD-Loop），这是一种动态切换分析和起草操作的新思维范式。通过交织文本和视觉思维，AD-Loop使模型能够迭代细化理解和输出，促进真实协同。其训练采用两阶段策略，先通过监督学习初始化交替过程，再通过强化学习实现自适应控制。实验证明，AD-Loop在理解和生成基准上持续提升性能，并具有良好的跨架构可迁移性。"
      },
      {
        "paper_id": "2602.21428",
        "title": "PSF-Med: Measuring and Explaining Paraphrase Sensitivity in Medical Vision Language Models",
        "abstract": "Medical Vision Language Models (VLMs) can change their answers when clinicians rephrase the same question, which raises deployment risks. We introduce Paraphrase Sensitivity Failure (PSF)-Med, a benchmark of 19,748 chest Xray questions paired with about 92,000 meaningpreserving paraphrases across MIMIC-CXR and PadChest. Across six medical VLMs, we measure yes/no flips for the same image and find flip rates from 8% to 58%. However, low flip rate does not imply visual grounding: text-only baselines show that some models stay consistent even when the image is removed, suggesting they rely on language priors. To study mechanisms in one model, we apply GemmaScope 2 Sparse Autoencoders (SAEs) to MedGemma 4B and analyze FlipBank, a curated set of 158 flip cases. We identify a sparse feature at layer 17 that correlates with prompt framing and predicts decision margin shifts. In causal patching, removing this feature's contribution recovers 45% of the yesminus-no logit margin on average and fully reverses 15% of flips. Acting on this finding, we show that clamping the identified feature at inference reduces flip rates by 31% relative with only a 1.3 percentage-point accuracy cost, while also decreasing text-prior reliance. These results suggest that flip rate alone is not enough; robustness evaluations should test both paraphrase stability and image reliance.",
        "authors_display": "Vahid Behzadan Team",
        "pdf_url": "http://arxiv.org/abs/2602.21428",
        "code_url": null,
        "date": "2026-02-24",
        "primary_category": "cs.CV",
        "chinese_summary": "医疗视觉语言模型（VLM）在面对相同问题不同表述时答案不一致，这带来了部署风险。本研究引入了Paraphrase Sensitivity Failure (PSF)-Med基准，包含大量胸部X光问题及其含义不变的复述，用以衡量VLM的复述敏感性。研究发现，尽管一些模型翻转率较低，但其决策可能依赖语言先验而非视觉信息。通过对MedGemma 4B应用GemmaScope 2稀疏自编码器分析，识别出一个与提示框架和决策边界位移相关的稀疏特征，并在因果修补中证实了其关键作用。结果表明，在推理时钳制该特征可显著降低翻转率（相对降低31%），同时减少对文本先验的依赖，强调了评估VLM鲁棒性时需同时考虑复述稳定性和图像依赖性。"
      },
      {
        "paper_id": "2602.21406",
        "title": "Exploring Vision-Language Models for Open-Vocabulary Zero-Shot Action Segmentation",
        "abstract": "Temporal Action Segmentation (TAS) requires dividing videos into action segments, yet the vast space of activities and alternative breakdowns makes collecting comprehensive datasets infeasible. Existing methods remain limited to closed vocabularies and fixed label sets. In this work, we explore the largely unexplored problem of Open-Vocabulary Zero-Shot Temporal Action Segmentation (OVTAS) by leveraging the strong zero-shot capabilities of Vision-Language Models (VLMs). We introduce a training-free pipeline that follows a segmentation-by-classification design: Frame-Action Embedding Similarity (FAES) matches video frames to candidate action labels, and Similarity-Matrix Temporal Segmentation (SMTS) enforces temporal consistency. Beyond proposing OVTAS, we present a systematic study across 14 diverse VLMs, providing the first broad analysis of their suitability for open-vocabulary action segmentation. Experiments on standard benchmarks show that OVTAS achieves strong results without task-specific supervision, underscoring the potential of VLMs for structured temporal understanding.",
        "authors_display": "Karthik Ramani Team",
        "pdf_url": "http://arxiv.org/abs/2602.21406",
        "code_url": null,
        "date": "2026-02-24",
        "primary_category": "cs.CV",
        "chinese_summary": "针对时间动作分割（TAS）数据收集困难且现有方法受限于封闭词汇和固定标签集的问题，本研究探索了开放词汇零样本时间动作分割（OVTAS）问题，并利用视觉语言模型（VLM）的零样本能力。研究提出了一种免训练流水线，通过帧-动作嵌入相似性（FAES）匹配视频帧与候选动作标签，并由相似性矩阵时间分割（SMTS）强制实现时间一致性。本研究对14个不同的VLM进行了系统性分析，首次广泛评估了它们在开放词汇动作分割任务上的适用性。实验结果显示，OVTAS在标准基准上无需任务特定监督即可取得良好效果，突显了VLM在结构化时间理解方面的巨大潜力。"
      },
      {
        "paper_id": "2602.21397",
        "title": "MMLoP: Multi-Modal Low-Rank Prompting for Efficient Vision-Language Adaptation",
        "abstract": "Prompt learning has become a dominant paradigm for adapting vision-language models (VLMs) such as CLIP to downstream tasks without modifying pretrained weights. While extending prompts to both vision and text encoders across multiple transformer layers significantly boosts performance, it dramatically increases the number of trainable parameters, with state-of-the-art methods requiring millions of parameters and abandoning the parameter efficiency that makes prompt tuning attractive. In this work, we propose \\textbf{MMLoP} (\\textbf{M}ulti-\\textbf{M}odal \\textbf{Lo}w-Rank \\textbf{P}rompting), a framework that achieves deep multi-modal prompting with only \\textbf{11.5K trainable parameters}, comparable to early text-only methods like CoOp. MMLoP parameterizes vision and text prompts at each transformer layer through a low-rank factorization, which serves as an implicit regularizer against overfitting on few-shot training data. To further close the accuracy gap with state-of-the-art methods, we introduce three complementary components: a self-regulating consistency loss that anchors prompted representations to frozen zero-shot CLIP features at both the feature and logit levels, a uniform drift correction that removes the global embedding shift induced by prompt tuning to preserve class-discriminative structure, and a shared up-projection that couples vision and text prompts through a common low-rank factor to enforce cross-modal alignment. Extensive experiments across three benchmarks and 11 diverse datasets demonstrate that MMLoP achieves a highly favorable accuracy-efficiency tradeoff, outperforming the majority of existing methods including those with orders of magnitude more parameters, while achieving a harmonic mean of 79.70\\% on base-to-novel generalization.",
        "authors_display": "Ramtin Pedarsani Team",
        "pdf_url": "http://arxiv.org/abs/2602.21397",
        "code_url": null,
        "date": "2026-02-24",
        "primary_category": "cs.CV",
        "chinese_summary": "当前VLM的Prompt学习范式在多层vision和text编码器中虽能提升性能，但大幅增加参数量，违背了Prompt Tuning的参数效率优势。为此，本研究提出了MMLoP（Multi-Modal Low-Rank Prompting）框架，该框架仅以11.5K可训练参数实现深度多模态Prompting。MMLoP通过低秩分解在每个transformer层参数化视觉和文本Prompt，并引入自调节一致性损失、统一漂移校正和共享上投影三个互补组件，以锚定表示、保留类别区分结构并强制跨模态对齐。广泛实验证明，MMLoP在准确性与效率之间取得了极佳平衡，优于大多数现有方法，并在base-to-novel泛化上实现了79.70%的调和平均值。"
      },
      {
        "paper_id": "2602.20119",
        "title": "NovaPlan: Zero-Shot Long-Horizon Manipulation via Closed-Loop Video Language Planning",
        "abstract": "Solving long-horizon tasks requires robots to integrate high-level semantic reasoning with low-level physical interaction. While vision-language models (VLMs) and video generation models can decompose tasks and imagine outcomes, they often lack the physical grounding necessary for real-world execution. We introduce NovaPlan, a hierarchical framework that unifies closed-loop VLM and video planning with geometrically grounded robot execution for zero-shot long-horizon manipulation. At the high level, a VLM planner decomposes tasks into sub-goals and monitors robot execution in a closed loop, enabling the system to recover from single-step failures through autonomous re-planning. To compute low-level robot actions, we extract and utilize both task-relevant object keypoints and human hand poses as kinematic priors from the generated videos, and employ a switching mechanism to choose the better one as a reference for robot actions, maintaining stable execution even under heavy occlusion or depth inaccuracy. We demonstrate the effectiveness of NovaPlan on three long-horizon tasks and the Functional Manipulation Benchmark (FMB). Our results show that NovaPlan can perform complex assembly tasks and exhibit dexterous error recovery behaviors without any prior demonstrations or training. Project page: https://nova-plan.github.io/",
        "authors_display": "George Konidaris Team",
        "pdf_url": "http://arxiv.org/abs/2602.20119",
        "code_url": "https://nova-plan.github.io/",
        "date": "2026-02-23",
        "primary_category": "cs.RO",
        "chinese_summary": "为解决机器人执行长周期任务时，视觉语言模型（VLM）和视频生成模型缺乏物理基础的问题，本文提出了NovaPlan，一个分层框架。该框架将闭环VLM与视频规划相结合，通过几何接地的机器人执行实现零样本长周期操作。高层VLM规划器分解子任务并闭环监控执行，实现自主重规划和单步失败恢复；低层则从生成视频中提取物体关键点和人手姿态作为运动学先验，并采用切换机制选择最优参考以计算机器人动作，确保稳定执行。实验结果表明，NovaPlan无需预训练或演示即可完成复杂装配任务，并展现出灵巧的错误恢复能力。"
      },
      {
        "paper_id": "2602.20089",
        "title": "StructXLIP: Enhancing Vision-language Models with Multimodal Structural Cues",
        "abstract": "Edge-based representations are fundamental cues for visual understanding, a principle rooted in early vision research and still central today. We extend this principle to vision-language alignment, showing that isolating and aligning structural cues across modalities can greatly benefit fine-tuning on long, detail-rich captions, with a specific focus on improving cross-modal retrieval. We introduce StructXLIP, a fine-tuning alignment paradigm that extracts edge maps (e.g., Canny), treating them as proxies for the visual structure of an image, and filters the corresponding captions to emphasize structural cues, making them \"structure-centric\". Fine-tuning augments the standard alignment loss with three structure-centric losses: (i) aligning edge maps with structural text, (ii) matching local edge regions to textual chunks, and (iii) connecting edge maps to color images to prevent representation drift. From a theoretical standpoint, while standard CLIP maximizes the mutual information between visual and textual embeddings, StructXLIP additionally maximizes the mutual information between multimodal structural representations. This auxiliary optimization is intrinsically harder, guiding the model toward more robust and semantically stable minima, enhancing vision-language alignment. Beyond outperforming current competitors on cross-modal retrieval in both general and specialized domains, our method serves as a general boosting recipe that can be integrated into future approaches in a plug-and-play manner. Code and pretrained models are publicly available at: https://github.com/intelligolabs/StructXLIP.",
        "authors_display": "Marco Cristani Team",
        "pdf_url": "http://arxiv.org/abs/2602.20089",
        "code_url": null,
        "date": "2026-02-23",
        "primary_category": "cs.CV",
        "chinese_summary": "针对现有视觉-语言对齐方法在处理长而细节丰富的字幕时，未能充分利用图像结构信息的问题，本文提出了StructXLIP。该方法通过提取图像边缘图作为视觉结构的代理，并过滤字幕以强调结构化线索，构建“以结构为中心”的对齐范式。在标准对齐损失的基础上，StructXLIP额外引入了三个结构中心损失，旨在对齐边缘图与结构化文本、匹配局部边缘区域与文本块、以及连接边缘图与彩色图像以防止表征漂移，从而最大化多模态结构表征间的互信息。实验证明，该方法在通用和专业领域的跨模态检索中超越了现有技术，并可作为即插即用的通用增强方案。"
      },
      {
        "paper_id": "2602.20084",
        "title": "Do Large Language Models Understand Data Visualization Principles?",
        "abstract": "Data visualization principles, derived from decades of research in design and perception, ensure proper visual communication. While prior work has shown that large language models (LLMs) can generate charts or flag misleading figures, it remains unclear whether they and their vision-language counterparts (VLMs) can reason about and enforce visualization principles directly. Constraint based systems encode these principles as logical rules for precise automated checks, but translating them into formal specifications demands expert knowledge. This motivates leveraging LLMs and VLMs as principle checkers that can reason about visual design directly, bypassing the need for symbolic rule specification. In this paper, we present the first systematic evaluation of both LLMs and VLMs on their ability to reason about visualization principles, using hard verification ground truth derived from Answer Set Programming (ASP). We compiled a set of visualization principles expressed as natural-language statements and generated a controlled dataset of approximately 2,000 Vega-Lite specifications annotated with explicit principle violations, complemented by over 300 real-world Vega-Lite charts. We evaluated both checking and fixing tasks, assessing how well models detect principle violations and correct flawed chart specifications. Our work highlights both the promise of large (vision-)language models as flexible validators and editors of visualization designs and the persistent gap with symbolic solvers on more nuanced aspects of visual perception. They also reveal an interesting asymmetry: frontier models tend to be more effective at correcting violations than at detecting them reliably.",
        "authors_display": "Emmanuel Iarussi Team",
        "pdf_url": "http://arxiv.org/abs/2602.20084",
        "code_url": null,
        "date": "2026-02-23",
        "primary_category": "cs.CV",
        "chinese_summary": "鉴于数据可视化原则对有效视觉传达的重要性，但将其编码为逻辑规则需要专业知识，本文旨在探究大型语言模型（LLM）和视觉语言模型（VLM）是否能直接推理和执行可视化原则，从而绕过符号规则规范的需求。研究首次系统评估了LLM和VLM在推理可视化原则上的能力，利用从Answer Set Programming (ASP) 推导的精确验证真值，构建了一个包含约2000个Vega-Lite规范和300多个真实世界图表的受控数据集。实验评估了模型在检测和修正原则违反方面的表现。结果表明，大型（视觉-）语言模型在可视化设计验证和编辑方面具有潜力，但在视觉感知更细微之处仍与符号求解器存在差距，并且模型在纠正违规方面的效果往往优于可靠地检测它们。"
      },
      {
        "paper_id": "2602.20066",
        "title": "HeatPrompt: Zero-Shot Vision-Language Modeling of Urban Heat Demand from Satellite Images",
        "abstract": "Accurate heat-demand maps play a crucial role in decarbonizing space heating, yet most municipalities lack detailed building-level data needed to calculate them. We introduce HeatPrompt, a zero-shot vision-language energy modeling framework that estimates annual heat demand using semantic features extracted from satellite images, basic Geographic Information System (GIS), and building-level features. We feed pretrained Large Vision Language Models (VLMs) with a domain-specific prompt to act as an energy planner and extract the visual attributes such as roof age, building density, etc, from the RGB satellite image that correspond to the thermal load. A Multi-Layer Perceptron (MLP) regressor trained on these captions shows an $R^2$ uplift of 93.7% and shrinks the mean absolute error (MAE) by 30% compared to the baseline model. Qualitative analysis shows that high-impact tokens align with high-demand zones, offering lightweight support for heat planning in data-scarce regions.",
        "authors_display": "Veit Hagenmeyer Team",
        "pdf_url": "http://arxiv.org/abs/2602.20066",
        "code_url": null,
        "date": "2026-02-23",
        "primary_category": "cs.CV",
        "chinese_summary": "鉴于大多数市政当局缺乏计算准确热需求地图所需详细建筑级数据的问题，本文提出了HeatPrompt，一个零样本视觉-语言能源建模框架。该框架利用卫星图像、基本地理信息系统（GIS）和建筑级特征中提取的语义特征来估算年度热需求。具体而言，它将预训练的大型视觉语言模型（VLM）与领域特定提示结合，使其充当能源规划器，并从RGB卫星图像中提取与热负荷相关的视觉属性（如屋顶年龄、建筑密度等）。实验结果显示，在此类描述上训练的多层感知器（MLP）回归器相比基线模型，其$R^2$提升了93.7%，平均绝对误差（MAE）降低了30%，并能通过高影响令牌识别高需求区域，为数据稀缺地区的热规划提供了轻量级支持。"
      },
      {
        "paper_id": "2602.19983",
        "title": "Contextual Safety Reasoning and Grounding for Open-World Robots",
        "abstract": "Robots are increasingly operating in open-world environments where safe behavior depends on context: the same hallway may require different navigation strategies when crowded versus empty, or during an emergency versus normal operations. Traditional safety approaches enforce fixed constraints in user-specified contexts, limiting their ability to handle the open-ended contextual variability of real-world deployment. We address this gap via CORE, a safety framework that enables online contextual reasoning, grounding, and enforcement without prior knowledge of the environment (e.g., maps or safety specifications). CORE uses a vision-language model (VLM) to continuously reason about context-dependent safety rules directly from visual observations, grounds these rules in the physical environment, and enforces the resulting spatially-defined safe sets via control barrier functions. We provide probabilistic safety guarantees for CORE that account for perceptual uncertainty, and we demonstrate through simulation and real-world experiments that CORE enforces contextually appropriate behavior in unseen environments, significantly outperforming prior semantic safety methods that lack online contextual reasoning. Ablation studies validate our theoretical guarantees and underscore the importance of both VLM-based reasoning and spatial grounding for enforcing contextual safety in novel settings. We provide additional resources at https://zacravichandran.github.io/CORE.",
        "authors_display": "George J. Pappas Team",
        "pdf_url": "http://arxiv.org/abs/2602.19983",
        "code_url": null,
        "date": "2026-02-23",
        "primary_category": "cs.RO",
        "chinese_summary": "针对机器人需在开放世界环境中操作，但传统安全方法无法处理情境可变性的问题，本文提出了CORE，一个无需环境先验知识即可实现在线上下文推理、接地和强制执行的安全框架。CORE利用视觉-语言模型（VLM）持续从视觉观测中推理上下文相关的安全规则，将这些规则在物理环境中接地，并通过控制障碍函数强制执行由此产生的空间定义安全集。该方法提供了考虑感知不确定性的概率安全保证。仿真和真实世界实验表明，CORE在未见过环境中能强制执行上下文适当的行为，显著优于缺乏在线上下文推理的语义安全方法，验证了VLM推理和空间接地对于新环境下上下文安全的重要性。"
      },
      {
        "paper_id": "2602.19929",
        "title": "BeamVLM for Low-altitude Economy: Generative Beam Prediction via Vision-language Models",
        "abstract": "For low-altitude economy (LAE), fast and accurate beam prediction between high-mobility unmanned aerial vehicles (UAVs) and ground base stations is of paramount importance, which ensures seamless coverage and reliable communications. However, existing deep learning-based beam prediction methods lack high-level semantic understanding of dynamic environments, resulting in poor generalization. On the other hand, the emerging large language model (LLM) based approaches show promise in enhancing generalization, but they typically lack rich environmental perception, thereby failing to capture fine-grained spatial semantics essential for precise beam alignment. To tackle these limitations, we propose in this correspondence a novel end-to-end generative framework for beam prediction, called BeamVLM, which treats beam prediction as a vision question answering task capitalizing on powerful existing vision-language models (VLMs). By projecting raw visual patches directly into the language domain and judiciously designing an instructional prompt, the proposed BeamVLM enables the VLM to jointly reason over UAV trajectories and environmental context. Last, experimental results on real-world datasets demonstrate that the proposed BeamVLM outperforms state-of-the-art methods in prediction accuracy and also exhibits superior generalization for other scenarios such as vehicle-to-infrastructure (V2I) beam prediction.",
        "authors_display": "Chengwen Xing Team",
        "pdf_url": "http://arxiv.org/abs/2602.19929",
        "code_url": null,
        "date": "2026-02-23",
        "primary_category": "cs.NI",
        "chinese_summary": "针对低空经济中高移动性无人机与地面基站之间波束预测现有方法泛化能力差、缺乏对动态环境高级语义理解或精细空间语义感知的问题，本文提出了BeamVLM。该框架是一个端到端生成式波束预测模型，将波束预测视为视觉问答任务，利用强大的视觉-语言模型（VLM）。通过将原始视觉补丁直接投射到语言域并巧妙设计指导性提示，BeamVLM使VLM能够联合推理无人机轨迹和环境上下文。在真实世界数据集上的实验结果表明，BeamVLM在预测精度上优于最先进方法，并对车对基础设施（V2I）波束预测等其他场景展现出卓越的泛化能力。"
      },
      {
        "paper_id": "2602.19910",
        "title": "Multi-Modal Representation Learning via Semi-Supervised Rate Reduction for Generalized Category Discovery",
        "abstract": "Generalized Category Discovery (GCD) aims to identify both known and unknown categories, with only partial labels given for the known categories, posing a challenging open-set recognition problem. State-of-the-art approaches for GCD task are usually built on multi-modality representation learning, which is heavily dependent upon inter-modality alignment. However, few of them cast a proper intra-modality alignment to generate a desired underlying structure of representation distributions. In this paper, we propose a novel and effective multi-modal representation learning framework for GCD via Semi-Supervised Rate Reduction, called SSR$^2$-GCD, to learn cross-modality representations with desired structural properties based on emphasizing to properly align intra-modality relationships. Moreover, to boost knowledge transfer, we integrate prompt candidates by leveraging the inter-modal alignment offered by Vision Language Models. We conduct extensive experiments on generic and fine-grained benchmark datasets demonstrating superior performance of our approach.",
        "authors_display": "Chun-Guang Li Team",
        "pdf_url": "http://arxiv.org/abs/2602.19910",
        "code_url": null,
        "date": "2026-02-23",
        "primary_category": "cs.CV",
        "chinese_summary": "针对广义类别发现（GCD）任务中，现有基于多模态表示学习的方法虽然依赖跨模态对齐，但往往忽视了模态内对齐以生成理想表示结构的问题，本文提出了SSR$^2$-GCD。该框架是一个新颖有效的多模态表示学习方法，通过半监督率降低机制，旨在强调正确对齐模态内关系，从而学习具有所需结构属性的跨模态表示。此外，为增强知识迁移，该方法利用视觉语言模型提供的模态间对齐，整合了提示候选。在通用和细粒度基准数据集上的广泛实验证明，该方法性能卓越。"
      },
      {
        "paper_id": "2602.19870",
        "title": "ApET: Approximation-Error Guided Token Compression for Efficient VLMs",
        "abstract": "Recent Vision-Language Models (VLMs) have demonstrated remarkable multimodal understanding capabilities, yet the redundant visual tokens incur prohibitive computational overhead and degrade inference efficiency. Prior studies typically relies on [CLS] attention or text-vision cross-attention to identify and discard redundant visual tokens. Despite promising results, such solutions are prone to introduce positional bias and, more critically, are incompatible with efficient attention kernels such as FlashAttention, limiting their practical deployment for VLM acceleration. In this paper, we step away from attention dependencies and revisit visual token compression from an information-theoretic perspective, aiming to maximally preserve visual information without any attention involvement. We present ApET, an Approximation-Error guided Token compression framework. ApET first reconstructs the original visual tokens with a small set of basis tokens via linear approximation, then leverages the approximation error to identify and drop the least informative tokens. Extensive experiments across multiple VLMs and benchmarks demonstrate that ApET retains 95.2% of the original performance on image-understanding tasks and even attains 100.4% on video-understanding tasks, while compressing the token budgets by 88.9% and 87.5%, respectively. Thanks to its attention-free design, ApET seamlessly integrates with FlashAttention, enabling further inference acceleration and making VLM deployment more practical. Code is available at https://github.com/MaQianKun0/ApET.",
        "authors_display": "Hairong Zheng Team",
        "pdf_url": "http://arxiv.org/abs/2602.19870",
        "code_url": null,
        "date": "2026-02-23",
        "primary_category": "cs.CV",
        "chinese_summary": "针对近期视觉-语言模型（VLMs）因冗余视觉令牌导致计算开销大和推理效率低的问题，且现有解决方案易引入位置偏差并与高效注意力内核不兼容的局限，本文提出了ApET（Approximation-Error guided Token compression）。该框架摆脱了注意力依赖，从信息论角度出发，旨在通过线性近似用少量基础令牌重建原始视觉令牌，然后利用近似误差识别并丢弃信息量最少的令牌，从而最大程度保留视觉信息。在多个VLM和基准测试上的实验表明，ApET在图像理解和视频理解任务中分别保留了95.2%和100.4%的原始性能，同时将令牌预算压缩了88.9%和87.5%，且其无注意力设计使其能与FlashAttention无缝集成，进一步加速推理，使VLM部署更具实用性。"
      },
      {
        "paper_id": "2602.19768",
        "title": "TraceVision: Trajectory-Aware Vision-Language Model for Human-Like Spatial Understanding",
        "abstract": "Recent Large Vision-Language Models (LVLMs) demonstrate remarkable capabilities in image understanding and natural language generation. However, current approaches focus predominantly on global image understanding, struggling to simulate human visual attention trajectories and explain associations between descriptions and specific regions. We propose TraceVision, a unified vision-language model integrating trajectory-aware spatial understanding in an end-to-end framework. TraceVision employs a Trajectory-aware Visual Perception (TVP) module for bidirectional fusion of visual features and trajectory information. We design geometric simplification to extract semantic keypoints from raw trajectories and propose a three-stage training pipeline where trajectories guide description generation and region localization. We extend TraceVision to trajectory-guided segmentation and video scene understanding, enabling cross-frame tracking and temporal attention analysis. We construct the Reasoning-based Interactive Localized Narratives (RILN) dataset to enhance logical reasoning and interpretability. Extensive experiments on trajectory-guided captioning, text-guided trajectory prediction, understanding, and segmentation demonstrate that TraceVision achieves state-of-the-art performance, establishing a foundation for intuitive spatial interaction and interpretable visual understanding.",
        "authors_display": "Jinqiao Wang Team",
        "pdf_url": "http://arxiv.org/abs/2602.19768",
        "code_url": null,
        "date": "2026-02-23",
        "primary_category": "cs.CV",
        "chinese_summary": "针对现有大型视觉-语言模型（LVLMs）主要关注全局图像理解，难以模拟人类视觉注意轨迹并解释描述与特定区域关联的问题，本文提出了TraceVision。该模型是一个统一的视觉-语言模型，通过轨迹感知视觉感知（TVP）模块实现视觉特征和轨迹信息的双向融合，从而将轨迹感知空间理解整合到端到端框架中。TraceVision设计了几何简化从原始轨迹中提取语义关键点，并提出三阶段训练流水线，以轨迹引导描述生成和区域定位，并可扩展至轨迹引导分割和视频场景理解。通过构建RILN数据集增强逻辑推理，实验证明TraceVision在轨迹引导字幕生成、文本引导轨迹预测、理解和分割任务上达到最先进性能，为直观空间交互和可解释视觉理解奠定了基础。"
      },
      {
        "paper_id": "2602.19615",
        "title": "Seeing Clearly, Reasoning Confidently: Plug-and-Play Remedies for Vision Language Model Blindness",
        "abstract": "Vision language models (VLMs) have achieved remarkable success in broad visual understanding, yet they remain challenged by object-centric reasoning on rare objects due to the scarcity of such instances in pretraining data. While prior efforts alleviate this issue by retrieving additional data or introducing stronger vision encoders, these methods are still computationally intensive during finetuning VLMs and don't fully exploit the original training data. In this paper, we introduce an efficient plug-and-play module that substantially improves VLMs' reasoning over rare objects by refining visual tokens and enriching input text prompts, without VLMs finetuning. Specifically, we propose to learn multi-modal class embeddings for rare objects by leveraging prior knowledge from vision foundation models and synonym-augmented text descriptions, compensating for limited training examples. These embeddings refine the visual tokens in VLMs through a lightweight attention-based enhancement module that improves fine-grained object details. In addition, we use the learned embeddings as object-aware detectors to generate informative hints, which are injected into the text prompts to help guide the VLM's attention toward relevant image regions. Experiments on two benchmarks show consistent and substantial gains for pretrained VLMs in rare object recognition and reasoning. Further analysis reveals how our method strengthens the VLM's ability to focus on and reason about rare objects.",
        "authors_display": "Zhengming Ding Team",
        "pdf_url": "http://arxiv.org/abs/2602.19615",
        "code_url": null,
        "date": "2026-02-23",
        "primary_category": "cs.CV",
        "chinese_summary": "为解决视觉语言模型（VLMs）在稀有物体上进行以物体为中心的推理时，由于预训练数据稀缺而面临挑战，且现有解决方案计算成本高昂的问题，本文提出了一个高效的即插即用模块。该模块在不微调VLM的情况下，通过精炼视觉令牌和丰富输入文本提示，显著提高VLM对稀有物体的推理能力。具体方法是利用视觉基础模型的先验知识和同义词增强文本描述，学习稀有物体的多模态类别嵌入；这些嵌入通过轻量级注意力增强模块精炼VLM的视觉令牌，并作为物体感知检测器生成信息提示，注入文本提示中以引导VLM关注相关图像区域。实验结果显示，预训练VLM在稀有物体识别和推理方面取得了显著提升，并增强了模型对稀有物体的关注和推理能力。"
      },
      {
        "paper_id": "2602.19570",
        "title": "VALD: Multi-Stage Vision Attack Detection for Efficient LVLM Defense",
        "abstract": "Large Vision-Language Models (LVLMs) can be vulnerable to adversarial images that subtly bias their outputs toward plausible yet incorrect responses. We introduce a general, efficient, and training-free defense that combines image transformations with agentic data consolidation to recover correct model behavior. A key component of our approach is a two-stage detection mechanism that quickly filters out the majority of clean inputs. We first assess image consistency under content-preserving transformations at negligible computational cost. For more challenging cases, we examine discrepancies in a text-embedding space. Only when necessary do we invoke a powerful LLM to resolve attack-induced divergences. A key idea is to consolidate multiple responses, leveraging both their similarities and their differences. We show that our method achieves state-of-the-art accuracy while maintaining notable efficiency: most clean images skip costly processing, and even in the presence of numerous adversarial examples, the overhead remains minimal.",
        "authors_display": "Ayellet Tal Team",
        "pdf_url": "http://arxiv.org/abs/2602.19570",
        "code_url": null,
        "date": "2026-02-23",
        "primary_category": "cs.CV",
        "chinese_summary": "大型视觉-语言模型（LVLMs）易受对抗性图像攻击，导致输出貌似合理但实则错误。为解决此问题，研究提出了一种通用、高效、免训练的防御机制，结合图像变换与主体数据整合来恢复模型正确行为。该方法包含两阶段检测：首先通过内容保留变换评估图像一致性，随后在文本嵌入空间检查差异；仅在必要时才调用大型语言模型解决分歧并整合多响应。实验证明，该方法在实现最先进准确性的同时保持了显著效率，多数干净图像跳过昂贵处理，即使在大量对抗性样本存在下开销也极小。"
      },
      {
        "paper_id": "2602.19539",
        "title": "Can a Teenager Fool an AI? Evaluating Low-Cost Cosmetic Attacks on Age Estimation Systems",
        "abstract": "Age estimation systems are increasingly deployed as gatekeepers for age-restricted online content, yet their robustness to cosmetic modifications has not been systematically evaluated. We investigate whether simple, household-accessible cosmetic changes, including beards, grey hair, makeup, and simulated wrinkles, can cause AI age estimators to classify minors as adults. To study this threat at scale without ethical concerns, we simulate these physical attacks on 329 facial images of individuals aged 10 to 21 using a VLM image editor (Gemini 2.5 Flash Image). We then evaluate eight models from our prior benchmark: five specialized architectures (MiVOLO, Custom-Best, Herosan, MiViaLab, DEX) and three vision-language models (Gemini 3 Flash, Gemini 2.5 Flash, GPT-5-Nano). We introduce the Attack Conversion Rate (ACR), defined as the fraction of images predicted as minor at baseline that flip to adult after attack, a population-agnostic metric that does not depend on the ratio of minors to adults in the test set. Our results reveal that a synthetic beard alone achieves 28 to 69 percent ACR across all eight models; combining all four attacks shifts predicted age by +7.7 years on average across all 329 subjects and reaches up to 83 percent ACR; and vision-language models exhibit lower ACR (59 to 71 percent) than specialized models (63 to 83 percent) under the full attack, although the ACR ranges overlap and the difference is not statistically tested. These findings highlight a critical vulnerability in deployed age-verification pipelines and call for adversarial robustness evaluation as a mandatory criterion for model selection.",
        "authors_display": "Simiao Ren Team",
        "pdf_url": "http://arxiv.org/abs/2602.19539",
        "code_url": null,
        "date": "2026-02-23",
        "primary_category": "cs.CV",
        "chinese_summary": "年龄估计系统作为在线内容守门人，其对化妆修改的鲁棒性未被充分评估。本研究系统性探究了胡须、白发、化妆和模拟皱纹等简单化妆是否会导致AI年龄估计器将未成年人错误分类为成年人。研究人员使用VLM图像编辑器在329张10-21岁面部图像上模拟攻击，并评估了八种模型，引入“攻击转换率”（ACR）衡量未成年预测转变为成年预测的比例。结果显示，合成胡须即可使28%至69%的图像被错误分类，四种攻击组合平均将预测年龄推高7.7岁，ACR高达83%，揭示了年龄验证系统的严重漏洞，强调对抗性鲁棒性评估的必要性。"
      },
      {
        "paper_id": "2602.19530",
        "title": "ORION: ORthonormal Text Encoding for Universal VLM AdaptatION",
        "abstract": "Vision language models (VLMs) have demonstrated remarkable generalization across diverse tasks, yet their performance remains constrained by the quality and geometry of the textual prototypes used to represent classes. Standard zero shot classifiers, derived from frozen text encoders and handcrafted prompts, may yield correlated or weakly separated embeddings that limit task specific discriminability. We introduce ORION, a text encoder fine tuning framework that improves pretrained VLMs using only class names. Our method optimizes, via low rank adaptation, a novel loss integrating two terms, one promoting pairwise orthogonality between the textual representations of the classes of a given task and the other penalizing deviations from the initial class prototypes. Furthermore, we provide a probabilistic interpretation of our orthogonality penalty, connecting it to the general maximum likelihood estimation (MLE) principle via Huygens theorem. We report extensive experiments on 11 benchmarks and three large VLM backbones, showing that the refined textual embeddings yield powerful replacements for the standard CLIP prototypes. Added as plug and play module on top of various state of the art methods, and across different prediction settings (zero shot, few shot and test time adaptation), ORION improves the performance consistently and significantly.",
        "authors_display": "Ismail Ben Ayed Team",
        "pdf_url": "http://arxiv.org/abs/2602.19530",
        "code_url": null,
        "date": "2026-02-23",
        "primary_category": "cs.CV",
        "chinese_summary": "视觉语言模型（VLMs）的性能受限于其类别的文本原型质量和几何结构，现有方法生成的嵌入可能存在相关或分离度弱的问题，限制了任务辨别能力。本文提出了ORION，一个仅使用类名即可改进预训练VLM的文本编码器微调框架。该方法通过低秩适应优化了一个新颖的损失函数，该函数整合了促进类文本表示间两两正交性和惩罚与初始原型偏差两项。实验在11个基准和三个大型VLM骨干网络上进行，结果表明ORION精炼后的文本嵌入能有效替代标准CLIP原型，作为即插即用模块在零样本、少样本及测试时间适应等多种设置下显著提升了各种SOTA方法的性能。"
      },
      {
        "paper_id": "2602.19471",
        "title": "Forgetting-Resistant and Lesion-Aware Source-Free Domain Adaptive Fundus Image Analysis with Vision-Language Model",
        "abstract": "Source-free domain adaptation (SFDA) aims to adapt a model trained in the source domain to perform well in the target domain, with only unlabeled target domain data and the source model. Taking into account that conventional SFDA methods are inevitably error-prone under domain shift, recently greater attention has been directed to SFDA assisted with off-the-shelf foundation models, e.g., vision-language (ViL) models. However, existing works of leveraging ViL models for SFDA confront two issues: (i) Although mutual information is exploited to consider the joint distribution between the predictions of ViL model and the target model, we argue that the forgetting of some superior predictions of the target model still occurs, as indicated by the decline of the accuracies of certain classes during adaptation; (ii) Prior research disregards the rich, fine-grained knowledge embedded in the ViL model, which offers detailed grounding for fundus image diagnosis. In this paper, we introduce a novel forgetting-resistant and lesion-aware (FRLA) method for SFDA of fundus image diagnosis with ViL model. Specifically, a forgetting-resistant adaptation module explicitly preserves the confident predictions of the target model, and a lesion-aware adaptation module yields patch-wise predictions from ViL model and employs them to help the target model be aware of the lesion areas and leverage the ViL model's fine-grained knowledge. Extensive experiments show that our method not only significantly outperforms the vision-language model, but also achieves consistent improvements over the state-of-the-art methods. Our code will be released.",
        "authors_display": "Xiaomeng Li Team",
        "pdf_url": "http://arxiv.org/abs/2602.19471",
        "code_url": null,
        "date": "2026-02-23",
        "primary_category": "cs.CV",
        "chinese_summary": "无源域适应（SFDA）在眼底图像诊断中，即使借助视觉-语言（ViL）基础模型，仍面临两个挑战：适应过程中目标模型优越预测的遗忘问题以及现有方法忽视了ViL模型中丰富的细粒度知识。针对此，本研究提出了一种新型的抗遗忘和病灶感知（FRLA）方法。具体而言，抗遗忘模块显式保留目标模型的置信预测，而病灶感知模块则利用ViL模型的逐块预测帮助目标模型感知病灶区域并利用其细粒度知识。大量实验证明，FRLA方法不仅显著优于独立的视觉-语言模型，而且相对于现有先进方法也实现了持续的性能提升。"
      },
      {
        "paper_id": "2602.19449",
        "title": "Decoupling Vision and Language: Codebook Anchored Visual Adaptation",
        "abstract": "Large Vision-Language Models (LVLMs) use their vision encoders to translate images into representations for downstream reasoning, but the encoders often underperform in domain-specific visual tasks such as medical image diagnosis or fine-grained classification, where representation errors can cascade through the language model, leading to incorrect responses. Existing adaptation methods modify the continuous feature interface between encoder and language model through projector tuning or other parameter-efficient updates, which still couples the two components and requires re-alignment whenever the encoder changes. We introduce CRAFT (Codebook RegulAted Fine-Tuning), a lightweight method that fine-tunes the encoder using a discrete codebook that anchors visual representations to a stable token space, achieving domain adaptation without modifying other parts of the model. This decoupled design allows the adapted encoder to seamlessly boost the performance of LVLMs with different language architectures, as long as they share the same codebook. Empirically, CRAFT achieves an average gain of 13.51% across 10 domain-specific benchmarks such as VQARAD and PlantVillage, while preserving the LLM's linguistic capabilities and outperforming peer methods that operate on continuous tokens.",
        "authors_display": "Jonathan Wu Team",
        "pdf_url": "http://arxiv.org/abs/2602.19449",
        "code_url": null,
        "date": "2026-02-23",
        "primary_category": "cs.CV",
        "chinese_summary": "大型视觉-语言模型（LVLMs）的视觉编码器在医学图像诊断或细粒度分类等领域特定任务中表现不佳，可能导致错误响应。现有适应方法通过调整编码器与语言模型之间的连续特征接口，但这种耦合设计限制了灵活性。本研究提出CRAFT（Codebook RegulAted Fine-Tuning），一种轻量级方法，通过离散码本微调视觉编码器，将视觉表示锚定到稳定的令牌空间，从而实现领域适应而不修改模型其他部分。这种解耦设计使得适应后的编码器能无缝提升共享相同码本的不同LVLM性能。实验在10个领域特定基准上平均实现了13.51%的性能提升，同时保留了LLM的语言能力，并优于其他处理连续令牌的方法。"
      },
      {
        "paper_id": "2602.19442",
        "title": "UrbanAlign: Post-hoc Semantic Calibration for VLM-Human Preference Alignment",
        "abstract": "Aligning vision-language model (VLM) outputs with human preferences in domain-specific tasks typically requires fine-tuning or reinforcement learning, both of which demand labelled data and GPU compute. We show that for subjective perception tasks, this alignment can be achieved without any model training: VLMs are already strong concept extractors but poor decision calibrators, and the gap can be closed externally. We propose a training-free post-hoc concept-bottleneck pipeline consisting of three tightly coupled stages: concept mining, multi-agent structured scoring, and geometric calibration, unified by an end-to-end dimension optimization loop. Interpretable evaluation dimensions are mined from a handful of human annotations; an Observer-Debater-Judge chain extracts robust continuous concept scores from a frozen VLM; and locally-weighted ridge regression on a hybrid visual-semantic manifold calibrates these scores against human ratings. Applied to urban perception as UrbanAlign, the framework achieves 72.2% accuracy ($κ=0.45$) on Place Pulse 2.0 across six categories, outperforming the best supervised baseline by +15.1 pp and uncalibrated VLM scoring by +16.3 pp, with full dimension-level interpretability and zero model-weight modification.",
        "authors_display": "Chunlei Shi Team",
        "pdf_url": "http://arxiv.org/abs/2602.19442",
        "code_url": null,
        "date": "2026-02-23",
        "primary_category": "cs.CV",
        "chinese_summary": "在领域特定任务中，使视觉-语言模型（VLM）输出与人类偏好对齐通常需要昂贵的微调或强化学习。本研究发现，对于主观感知任务，这种对齐无需模型训练即可实现，因为VLM是强大的概念提取器但校准决策能力较弱，可通过外部手段弥补。研究提出一个免训练的后处理概念瓶颈流程UrbanAlign，包含概念挖掘、多智能体结构化评分和几何校准三个阶段，由端到端维度优化循环统一。在城市感知任务Place Pulse 2.0上，该框架在六个类别中达到72.2%的准确率，超越了最佳监督基线15.1个百分点，且未校准的VLM评分16.3个百分点，同时提供全维度可解释性且无需修改模型权重。"
      },
      {
        "paper_id": "2602.19418",
        "title": "PA-Attack: Guiding Gray-Box Attacks on LVLM Vision Encoders with Prototypes and Attention",
        "abstract": "Large Vision-Language Models (LVLMs) are foundational to modern multimodal applications, yet their susceptibility to adversarial attacks remains a critical concern. Prior white-box attacks rarely generalize across tasks, and black-box methods depend on expensive transfer, which limits efficiency. The vision encoder, standardized and often shared across LVLMs, provides a stable gray-box pivot with strong cross-model transfer. Building on this premise, we introduce PA-Attack (Prototype-Anchored Attentive Attack). PA-Attack begins with a prototype-anchored guidance that provides a stable attack direction towards a general and dissimilar prototype, tackling the attribute-restricted issue and limited task generalization of vanilla attacks. Building on this, we propose a two-stage attention enhancement mechanism: (i) leverage token-level attention scores to concentrate perturbations on critical visual tokens, and (ii) adaptively recalibrate attention weights to track the evolving attention during the adversarial process. Extensive experiments across diverse downstream tasks and LVLM architectures show that PA-Attack achieves an average 75.1% score reduction rate (SRR), demonstrating strong attack effectiveness, efficiency, and task generalization in LVLMs. Code is available at https://github.com/hefeimei06/PA-Attack.",
        "authors_display": "Minjing Dong Team",
        "pdf_url": "http://arxiv.org/abs/2602.19418",
        "code_url": null,
        "date": "2026-02-23",
        "primary_category": "cs.CV",
        "chinese_summary": "大型视觉-语言模型（LVLMs）对对抗性攻击的脆弱性是一个关键问题，现有白盒攻击泛化性差，黑盒方法效率低下。鉴于视觉编码器在LVLMs间常被共享且稳定，本文提出了PA-Attack（Prototype-Anchored Attentive Attack）。该方法首先利用原型锚定指导，提供稳定的攻击方向以解决传统攻击的属性限制和任务泛化问题；其次，引入两阶段注意力增强机制，利用令牌级注意力分数集中扰动于关键视觉令牌，并自适应重新校准注意力权重以跟踪对抗过程中的注意力演变。大量实验表明，PA-Attack在各种下游任务和LVLM架构上实现了平均75.1%的分数降低率，展现了强大的攻击有效性、效率和任务泛化能力。"
      },
      {
        "paper_id": "2602.19372",
        "title": "Seeing Farther and Smarter: Value-Guided Multi-Path Reflection for VLM Policy Optimization",
        "abstract": "Solving complex, long-horizon robotic manipulation tasks requires a deep understanding of physical interactions, reasoning about their long-term consequences, and precise high-level planning. Vision-Language Models (VLMs) offer a general perceive-reason-act framework for this goal. However, previous approaches using reflective planning to guide VLMs in correcting actions encounter significant limitations. These methods rely on inefficient and often inaccurate implicit learning of state-values from noisy foresight predictions, evaluate only a single greedy future, and suffer from substantial inference latency. To address these limitations, we propose a novel test-time computation framework that decouples state evaluation from action generation. This provides a more direct and fine-grained supervisory signal for robust decision-making. Our method explicitly models the advantage of an action plan, quantified by its reduction in distance to the goal, and uses a scalable critic to estimate. To address the stochastic nature of single-trajectory evaluation, we employ beam search to explore multiple future paths and aggregate them during decoding to model their expected long-term returns, leading to more robust action generation. Additionally, we introduce a lightweight, confidence-based trigger that allows for early exit when direct predictions are reliable, invoking reflection only when necessary. Extensive experiments on diverse, unseen multi-stage robotic manipulation tasks demonstrate a 24.6% improvement in success rate over state-of-the-art baselines, while significantly reducing inference time by 56.5%.",
        "authors_display": "Dimitris N. Metaxas Team",
        "pdf_url": "http://arxiv.org/abs/2602.19372",
        "code_url": null,
        "date": "2026-02-22",
        "primary_category": "cs.RO",
        "chinese_summary": "解决复杂、长周期机器人操作任务需要深入理解物理交互和高层规划，但现有基于反射规划的VLM方法存在效率低下、预测不准确和推理延迟大等局限。本研究提出一种新颖的测试时计算框架，解耦状态评估与动作生成。该框架通过可扩展的评论器明确量化动作计划的优势（与目标距离的减少），并利用束搜索探索多条未来路径以聚合其预期长期回报，从而生成更稳健的动作。此外，引入了基于置信度的轻量级触发器，在直接预测可靠时提前退出，仅在必要时进行反思。在多样化的多阶段机器人操作任务中，该方法成功率提高了24.6%，推理时间减少了56.5%。"
      },
      {
        "paper_id": "2602.19357",
        "title": "MentalBlackboard: Evaluating Spatial Visualization via Mathematical Transformations",
        "abstract": "Spatial visualization is the mental ability to imagine, transform, and manipulate the spatial characteristics of objects and actions. This intelligence is a part of human cognition where actions and perception are connected on a mental level. To explore whether state-of-the-art Vision-Language Models (VLMs) exhibit this ability, we develop MentalBlackboard, an open-ended spatial visualization benchmark for Paper Folding and Hole Punching tests within two core tasks: prediction and planning. Our prediction experiments reveal that models struggle with applying symmetrical transformations, even when they predict the sequence of unfolding steps correctly. Also, rotations introduce a significant challenge to the physical situational awareness for models. The planning task reveals limitations of models in analyzing symmetrical relationships and in implementing the multi-stage symmetry process, with Claude Opus 4.1 achieving the highest planning score at an accuracy of 10\\%. The top-performing model, o3, attains a peak performance of 71.6\\% on the generalization task, which does not require spatial visualization but transfers spatial data; however, it achieves only 25\\% accuracy on text-based prediction tasks.",
        "authors_display": "Yezhou Yang Team",
        "pdf_url": "http://arxiv.org/abs/2602.19357",
        "code_url": null,
        "date": "2026-02-22",
        "primary_category": "cs.CV",
        "chinese_summary": "空间可视化是人类认知中将动作与感知在心理层面连接的重要能力。为探究现有视觉-语言模型（VLMs）是否具备此能力，本研究开发了MentalBlackboard基准，用于纸张折叠和打孔测试的预测和规划任务。预测实验显示，模型难以应用对称变换，即使展开步骤正确；旋转也对其物理情境感知构成显著挑战。规划任务揭示模型在分析对称关系和实施多阶段对称过程方面的局限性，其中Claude Opus 4.1的规划准确率最高，达到10%。表现最佳的模型o3在无需空间可视化但涉及空间数据传输的泛化任务上达到71.6%的峰值性能，但在基于文本的预测任务上仅达到25%的准确率，表明当前VLM在空间可视化能力方面仍存在局限。"
      },
      {
        "paper_id": "2602.19313",
        "title": "TOPReward: Token Probabilities as Hidden Zero-Shot Rewards for Robotics",
        "abstract": "While Vision-Language-Action (VLA) models have seen rapid progress in pretraining, their advancement in Reinforcement Learning (RL) remains hampered by low sample efficiency and sparse rewards in real-world settings. Developing generalizable process reward models is essential for providing the fine-grained feedback necessary to bridge this gap, yet existing temporal value functions often fail to generalize beyond their training domains. We introduce TOPReward, a novel, probabilistically grounded temporal value function that leverages the latent world knowledge of pretrained video Vision-Language Models (VLMs) to estimate robotic task progress. Unlike prior methods that prompt VLMs to directly output progress values, which are prone to numerical misrepresentation, TOPReward extracts task progress directly from the VLM's internal token logits. In zero-shot evaluations across 130+ distinct real-world tasks and multiple robot platforms (e.g., Franka, YAM, SO-100/101), TOPReward achieves 0.947 mean Value-Order Correlation (VOC) on Qwen3-VL, dramatically outperforming the state-of-the-art GVL baseline which achieves near-zero correlation on the same open-source model. We further demonstrate that TOPReward serves as a versatile tool for downstream applications, including success detection and reward-aligned behavior cloning.",
        "authors_display": "Ranjay Krishna Team",
        "pdf_url": "http://arxiv.org/abs/2602.19313",
        "code_url": null,
        "date": "2026-02-22",
        "primary_category": "cs.RO",
        "chinese_summary": "尽管视觉-语言-动作（VLA）模型在预训练方面取得了快速进展，但在真实世界场景中，其在强化学习（RL）方面的进步仍受低样本效率和稀疏奖励的阻碍。开发可泛化的过程奖励模型对于提供细粒度反馈至关重要，但现有时间价值函数通常无法泛化到训练域之外。本研究提出了TOPReward，一种新颖的、基于概率的时间价值函数，它利用预训练视频视觉-语言模型（VLMs）的潜在世界知识来估计机器人任务进度。与以往直接提示VLM输出进度值（易产生数值误表征）的方法不同，TOPReward直接从VLM的内部令牌逻辑中提取任务进度。在130多个真实世界任务和多个机器人平台的零样本评估中，TOPReward在Qwen3-VL上实现了0.947的平均价值-顺序相关性（VOC），显著优于最先进的GVL基线（在相同开源模型上相关性接近零）。此外，TOPReward可作为成功检测和奖励对齐行为克隆等下游应用的通用工具。"
      },
      {
        "paper_id": "2602.16898",
        "title": "MALLVI: A Multi-Agent Framework for Integrated Generalized Robotics Manipulation",
        "abstract": "Task planning for robotic manipulation with large language models (LLMs) is an emerging area. Prior approaches rely on specialized models, fine tuning, or prompt tuning, and often operate in an open loop manner without robust environmental feedback, making them fragile in dynamic settings.MALLVi present a Multi Agent Large Language and Vision framework that enables closed loop feedback driven robotic manipulation. Given a natural language instruction and an image of the environment, MALLVi generates executable atomic actions for a robot manipulator. After action execution, a Vision Language Model (VLM) evaluates environmental feedback and decides whether to repeat the process or proceed to the next step Rather than using a single model, MALLVi coordinates specialized agents, Decomposer, Localizer, Thinker, and Reflector, to manage perception, localization, reasoning, and high level planning. An optional Descriptor agent provides visual memory of the initial state. The Reflector supports targeted error detection and recovery by reactivating only relevant agents, avoiding full replanning.Experiments in simulation and real world settings show that iterative closed loop multi agent coordination improves generalization and increases success rates in zero shot manipulation tasks.Code available at https://github.com/iman1234ahmadi/MALLVI.",
        "authors_display": "Babak Khalaj Team",
        "pdf_url": "http://arxiv.org/abs/2602.16898",
        "code_url": null,
        "date": "2026-02-20",
        "primary_category": "cs.RO",
        "chinese_summary": "现有的基于大语言模型（LLMs）的机器人操作任务规划方法常以开环方式运行，缺乏环境反馈，在动态环境中显得脆弱。为解决此问题，本文提出了MALLVi（Multi Agent Large Language and Vision）框架，一个实现闭环反馈驱动的机器人操作的多智能体系统。MALLVi协调专门的智能体（如Decomposer、Localizer、Thinker、Reflector）来管理感知、定位、推理和高级规划。在动作执行后，VLM会评估环境反馈并决定后续步骤，Reflector智能体支持有针对性的错误检测和恢复。实验结果表明，这种迭代闭环多智能体协调在模拟和真实世界场景中提高了零样本操作任务的泛化能力和成功率。"
      },
      {
        "paper_id": "2602.18424",
        "title": "CapNav: Benchmarking Vision Language Models on Capability-conditioned Indoor Navigation",
        "abstract": "Vision-Language Models (VLMs) have shown remarkable progress in Vision-Language Navigation (VLN), offering new possibilities for navigation decision-making that could benefit both robotic platforms and human users. However, real-world navigation is inherently conditioned by the agent's mobility constraints. For example, a sweeping robot cannot traverse stairs, while a quadruped can. We introduce Capability-Conditioned Navigation (CapNav), a benchmark designed to evaluate how well VLMs can navigate complex indoor spaces given an agent's specific physical and operational capabilities. CapNav defines five representative human and robot agents, each described with physical dimensions, mobility capabilities, and environmental interaction abilities. CapNav provides 45 real-world indoor scenes, 473 navigation tasks, and 2365 QA pairs to test if VLMs can traverse indoor environments based on agent capabilities. We evaluate 13 modern VLMs and find that current VLM's navigation performance drops sharply as mobility constraints tighten, and that even state-of-the-art models struggle with obstacle types that require reasoning on spatial dimensions. We conclude by discussing the implications for capability-aware navigation and the opportunities for advancing embodied spatial reasoning in future VLMs. The benchmark is available at https://github.com/makeabilitylab/CapNav",
        "authors_display": "Jon Froehlich Team",
        "pdf_url": "http://arxiv.org/abs/2602.18424",
        "code_url": null,
        "date": "2026-02-20",
        "primary_category": "cs.CV",
        "chinese_summary": "现有视觉-语言模型（VLMs）在视觉-语言导航（VLN）方面表现出色，但尚未充分考虑代理在真实世界导航中固有的移动能力限制。为此，本研究引入了CapNav基准，旨在评估VLM在给定代理特定物理和操作能力的情况下导航复杂室内空间的能力。CapNav定义了五种代表性代理，并提供了45个真实世界室内场景、473个导航任务和2365个问答对。实验结果显示，随着移动限制的收紧，VLM的导航性能急剧下降，即使最先进的模型也难以处理需要空间维度推理的障碍物，这凸显了未来VLM在能力感知导航和具身空间推理方面的改进空间。"
      },
      {
        "paper_id": "2602.18374",
        "title": "Zero-shot Interactive Perception",
        "abstract": "Interactive perception (IP) enables robots to extract hidden information in their workspace and execute manipulation plans by physically interacting with objects and altering the state of the environment -- crucial for resolving occlusions and ambiguity in complex, partially observable scenarios. We present Zero-Shot IP (ZS-IP), a novel framework that couples multi-strategy manipulation (pushing and grasping) with a memory-driven Vision Language Model (VLM) to guide robotic interactions and resolve semantic queries. ZS-IP integrates three key components: (1) an Enhanced Observation (EO) module that augments the VLM's visual perception with both conventional keypoints and our proposed pushlines -- a novel 2D visual augmentation tailored to pushing actions, (2) a memory-guided action module that reinforces semantic reasoning through context lookup, and (3) a robotic controller that executes pushing, pulling, or grasping based on VLM output. Unlike grid-based augmentations optimized for pick-and-place, pushlines capture affordances for contact-rich actions, substantially improving pushing performance. We evaluate ZS-IP on a 7-DOF Franka Panda arm across diverse scenes with varying occlusions and task complexities. Our experiments demonstrate that ZS-IP outperforms passive and viewpoint-based perception techniques such as Mark-Based Visual Prompting (MOKA), particularly in pushing tasks, while preserving the integrity of non-target elements.",
        "authors_display": "Amir Ghalamzan Team",
        "pdf_url": "http://arxiv.org/abs/2602.18374",
        "code_url": "https://openreview.net/forum?id=7MhpFcr5Nx",
        "date": "2026-02-20",
        "primary_category": "cs.RO",
        "chinese_summary": "在复杂的、部分可观察的场景中，机器人通过物理交互（交互式感知，IP）来提取隐藏信息并执行操作至关重要。针对现有方法可能未能充分利用多策略操作并有效引导交互的问题，本研究提出了Zero-Shot IP (ZS-IP) 框架，该框架将多策略操作（推、抓）与记忆驱动的视觉语言模型 (VLM) 相结合。ZS-IP 集成了增强观测模块（引入了新颖的推线）、记忆引导动作模块和机器人控制器。实验结果表明，ZS-IP 在7自由度Franka Panda机械臂上，尤其是在推动任务中，表现优于被动和基于视角的感知技术，并能保持非目标元素的完整性。"
      },
      {
        "paper_id": "2602.18262",
        "title": "Simplifying Outcomes of Language Model Component Analyses with ELIA",
        "abstract": "While mechanistic interpretability has developed powerful tools to analyze the internal workings of Large Language Models (LLMs), their complexity has created an accessibility gap, limiting their use to specialists. We address this challenge by designing, building, and evaluating ELIA (Explainable Language Interpretability Analysis), an interactive web application that simplifies the outcomes of various language model component analyses for a broader audience. The system integrates three key techniques -- Attribution Analysis, Function Vector Analysis, and Circuit Tracing -- and introduces a novel methodology: using a vision-language model to automatically generate natural language explanations (NLEs) for the complex visualizations produced by these methods. The effectiveness of this approach was empirically validated through a mixed-methods user study, which revealed a clear preference for interactive, explorable interfaces over simpler, static visualizations. A key finding was that the AI-powered explanations helped bridge the knowledge gap for non-experts; a statistical analysis showed no significant correlation between a user's prior LLM experience and their comprehension scores, suggesting that the system reduced barriers to comprehension across experience levels. We conclude that an AI system can indeed simplify complex model analyses, but its true power is unlocked when paired with thoughtful, user-centered design that prioritizes interactivity, specificity, and narrative guidance.",
        "authors_display": "Nils Feldhus Team",
        "pdf_url": "http://arxiv.org/abs/2602.18262",
        "code_url": "https://github.com/aaron0eidt/ELIA",
        "date": "2026-02-20",
        "primary_category": "cs.CL",
        "chinese_summary": "尽管机制可解释性为分析大型语言模型 (LLM) 提供了强大工具，但其复杂性限制了非专业人员的使用。为了解决这一挑战，本研究设计并评估了ELIA，一个交互式网页应用，它简化了语言模型组件分析结果，使其更易于理解。ELIA集成了归因分析、功能向量分析和电路追踪，并创新性地使用视觉语言模型自动生成自然语言解释。用户研究表明，用户明显偏爱交互式界面，且AI驱动的解释有效弥合了非专业用户的知识鸿沟，表明AI系统结合用户中心设计能够有效简化复杂模型分析。"
      },
      {
        "paper_id": "2602.18154",
        "title": "FENCE: A Financial and Multimodal Jailbreak Detection Dataset",
        "abstract": "Jailbreaking poses a significant risk to the deployment of Large Language Models (LLMs) and Vision Language Models (VLMs). VLMs are particularly vulnerable because they process both text and images, creating broader attack surfaces. However, available resources for jailbreak detection are scarce, particularly in finance. To address this gap, we present FENCE, a bilingual (Korean-English) multimodal dataset for training and evaluating jailbreak detectors in financial applications. FENCE emphasizes domain realism through finance-relevant queries paired with image-grounded threats. Experiments with commercial and open-source VLMs reveal consistent vulnerabilities, with GPT-4o showing measurable attack success rates and open-source models displaying greater exposure. A baseline detector trained on FENCE achieves 99 percent in-distribution accuracy and maintains strong performance on external benchmarks, underscoring the dataset's robustness for training reliable detection models. FENCE provides a focused resource for advancing multimodal jailbreak detection in finance and for supporting safer, more reliable AI systems in sensitive domains. Warning: This paper includes example data that may be offensive.",
        "authors_display": "Youngjun Kwak Team",
        "pdf_url": "http://arxiv.org/abs/2602.18154",
        "code_url": null,
        "date": "2026-02-20",
        "primary_category": "cs.CL",
        "chinese_summary": "越狱攻击对大型语言模型 (LLMs) 和视觉语言模型 (VLMs) 构成严重威胁，VLM因其多模态特性而具有更广阔的攻击面，尤其在金融领域，越狱检测资源稀缺。为弥补这一空白，本研究提出了FENCE，一个双语（韩语-英语）多模态数据集，用于训练和评估金融应用中的越狱检测器，其通过金融相关查询和图像威胁确保领域真实性。实验结果显示，商业和开源VLM均存在漏洞，GPT-4o有可测量的攻击成功率，而开源模型暴露程度更高。基于FENCE训练的基线检测器实现了99%的分布内准确性，并在外部基准上表现强劲，证实了数据集在训练可靠检测模型方面的有效性。"
      },
      {
        "paper_id": "2602.18094",
        "title": "OODBench: Out-of-Distribution Benchmark for Large Vision-Language Models",
        "abstract": "Existing Visual-Language Models (VLMs) have achieved significant progress by being trained on massive-scale datasets, typically under the assumption that data are independent and identically distributed (IID). However, in real-world scenarios, it is often impractical to expect that all data processed by an AI system satisfy this assumption. Furthermore, failure to appropriately handle out-of-distribution (OOD) objects may introduce safety risks in real-world applications (e.g., autonomous driving or medical assistance). Unfortunately, current research has not yet provided valid benchmarks that can comprehensively assess the performance of VLMs in response to OOD data. Therefore, we propose OODBench, a predominantly automated method with minimal human verification, for constructing new benchmarks and evaluating the ability of VLMs to process OOD data. OODBench contains 40K instance-level OOD instance-category pairs, and we show that current VLMs still exhibit notable performance degradation on OODBench, even when the underlying image categories are common. In addition, we propose a reliable automated assessment metric that employs a Basic-to-Advanced Progression of prompted questions to assess the impact of OOD data on questions of varying difficulty more fully. Lastly, we summarize substantial findings and insights to facilitate future research in the acquisition and evaluation of OOD data.",
        "authors_display": "Jingrun Chen Team",
        "pdf_url": "http://arxiv.org/abs/2602.18094",
        "code_url": null,
        "date": "2026-02-20",
        "primary_category": "cs.CV",
        "chinese_summary": "现有视觉-语言模型 (VLMs) 假设数据为独立同分布 (IID)，但在现实世界中，分布外 (OOD) 对象常出现并带来安全风险。鉴于当前缺乏全面评估VLM处理OOD数据能力的有效基准，本研究提出了OODBench，一种主要自动化的方法来构建新基准并评估VLM处理OOD数据的能力。OODBench包含40K实例级的OOD实例-类别对，并提出了一种可靠的自动化评估指标。实验结果表明，当前VLM在OODBench上的性能显著下降，即使在常见图像类别上也是如此。研究总结了为未来OOD数据获取和评估提供指导的发现和见解。"
      },
      {
        "paper_id": "2602.18020",
        "title": "UAOR: Uncertainty-aware Observation Reinjection for Vision-Language-Action Models",
        "abstract": "Vision-Language-Action (VLA) models leverage pretrained Vision-Language Models (VLMs) as backbones to map images and instructions to actions, demonstrating remarkable potential for generalizable robotic manipulation. To enhance performance, existing methods often incorporate extra observation cues (e.g., depth maps, point clouds) or auxiliary modules (e.g., object detectors, encoders) to enable more precise and reliable task execution, yet these typically require costly data collection and additional training. Inspired by the finding that Feed-Forward Network (FFN) in language models can act as \"key-value memory\", we propose Uncertainty-aware Observation Reinjection (UAOR), an effective, training-free and plug-and-play module for VLA models. Specifically, when the current language model layer exhibits high uncertainty, measured by Action Entropy, it reinjects key observation information into the next layer's Feed-Forward Network (FFN) through attention retrieval. This mechanism helps VLAs better attend to observations during inference, enabling more confident and faithful action generation. Comprehensive experiments show that our method consistently improves diverse VLA models across simulation and real-world tasks with minimal overhead. Notably, UAOR eliminates the need for additional observation cues or modules, making it a versatile and practical plug-in for existing VLA pipelines. The project page is at https://uaor.jiabingyang.cn.",
        "authors_display": "Liang Wang Team",
        "pdf_url": "http://arxiv.org/abs/2602.18020",
        "code_url": null,
        "date": "2026-02-20",
        "primary_category": "cs.CV",
        "chinese_summary": "视觉-语言-动作 (VLA) 模型在通用机器人操作方面潜力巨大，但现有方法常需额外观测信息或辅助模块以提高性能，这增加了数据收集和训练成本。受语言模型中前馈网络 (FFN) 可作为“键值记忆”的启发，本研究提出了不确定性感知观测重注入 (UAOR)，一个无需训练、即插即用的模块。UAOR通过注意力检索，在当前语言模型层表现出高不确定性时，将关键观测信息重新注入到下一层的前馈网络中。全面实验表明，UAOR以最小开销持续改进了多种VLA模型在模拟和真实世界任务中的表现，且无需额外观测线索或模块，使其成为现有VLA流水线的通用实用插件。"
      },
      {
        "paper_id": "2602.17645",
        "title": "Pushing the Frontier of Black-Box LVLM Attacks via Fine-Grained Detail Targeting",
        "abstract": "Black-box adversarial attacks on Large Vision-Language Models (LVLMs) are challenging due to missing gradients and complex multimodal boundaries. While prior state-of-the-art transfer-based approaches like M-Attack perform well using local crop-level matching between source and target images, we find this induces high-variance, nearly orthogonal gradients across iterations, violating coherent local alignment and destabilizing optimization. We attribute this to (i) ViT translation sensitivity that yields spike-like gradients and (ii) structural asymmetry between source and target crops. We reformulate local matching as an asymmetric expectation over source transformations and target semantics, and build a gradient-denoising upgrade to M-Attack. On the source side, Multi-Crop Alignment (MCA) averages gradients from multiple independently sampled local views per iteration to reduce variance. On the target side, Auxiliary Target Alignment (ATA) replaces aggressive target augmentation with a small auxiliary set from a semantically correlated distribution, producing a smoother, lower-variance target manifold. We further reinterpret momentum as Patch Momentum, replaying historical crop gradients; combined with a refined patch-size ensemble (PE+), this strengthens transferable directions. Together these modules form M-Attack-V2, a simple, modular enhancement over M-Attack that substantially improves transfer-based black-box attacks on frontier LVLMs: boosting success rates on Claude-4.0 from 8% to 30%, Gemini-2.5-Pro from 83% to 97%, and GPT-5 from 98% to 100%, outperforming prior black-box LVLM attacks. Code and data are publicly available at: https://github.com/vila-lab/M-Attack-V2.",
        "authors_display": "Zhiqiang Shen Team",
        "pdf_url": "http://arxiv.org/abs/2602.17645",
        "code_url": "https://github.com/vila-lab/M-Attack-V2",
        "date": "2026-02-19",
        "primary_category": "cs.LG",
        "chinese_summary": "大型视觉语言模型 (LVLMs) 的黑盒对抗性攻击因梯度缺失和复杂多模态边界而极具挑战性。现有最先进的基于迁移方法M-Attack在局部裁剪级匹配时导致高方差梯度，造成优化不稳定。为此，本研究将局部匹配重新表述为源变换和目标语义上的非对称期望，并提出了M-Attack-V2，一个对M-Attack进行梯度去噪的升级模块。M-Attack-V2通过多裁剪对齐 (MCA) 降低源侧梯度方差，并通过辅助目标对齐 (ATA) 优化目标流形。实验结果表明，M-Attack-V2显著提升了对前沿LVLMs的黑盒攻击成功率，例如，Claude-4.0从8%升至30%，Gemini-2.5-Pro从83%升至97%，GPT-5从98%升至100%，全面超越了此前的黑盒LVLM攻击方法。"
      },
      {
        "paper_id": "2602.17625",
        "title": "Catastrophic Forgetting Resilient One-Shot Incremental Federated Learning",
        "abstract": "Modern big-data systems generate massive, heterogeneous, and geographically dispersed streams that are large-scale and privacy-sensitive, making centralization challenging. While federated learning (FL) provides a privacy-enhancing training mechanism, it assumes a static data flow and learns a collaborative model over multiple rounds, making learning with \\textit{incremental} data challenging in limited-communication scenarios. This paper presents One-Shot Incremental Federated Learning (OSI-FL), the first FL framework that addresses the dual challenges of communication overhead and catastrophic forgetting. OSI-FL communicates category-specific embeddings, devised by a frozen vision-language model (VLM) from each client in a single communication round, which a pre-trained diffusion model at the server uses to synthesize new data similar to the client's data distribution. The synthesized samples are used on the server for training. However, two challenges still persist: i) tasks arriving incrementally need to retrain the global model, and ii) as future tasks arrive, retraining the model introduces catastrophic forgetting. To this end, we augment training with Selective Sample Retention (SSR), which identifies and retains the top-p most informative samples per category and task pair based on sample loss. SSR bounds forgetting by ensuring that representative retained samples are incorporated into training in further iterations. The experimental results indicate that OSI-FL outperforms baselines, including traditional and one-shot FL approaches, in both class-incremental and domain-incremental scenarios across three benchmark datasets.",
        "authors_display": "Monowar Bhuyan Team",
        "pdf_url": "http://arxiv.org/abs/2602.17625",
        "code_url": null,
        "date": "2026-02-19",
        "primary_category": "cs.LG",
        "chinese_summary": "现代大数据系统中的联邦学习在处理大规模、隐私敏感的增量数据流时，面临通信开销和灾难性遗忘的挑战。为此，本文提出了One-Shot Incremental Federated Learning (OSI-FL) 框架，该框架通过客户端VLM提取类别特定嵌入并在单轮通信中发送给服务器，服务器利用扩散模型合成新数据进行训练。为解决灾难性遗忘，OSI-FL引入了选择性样本保留（SSR）机制，基于样本损失保留最具信息量的样本。实验结果显示，OSI-FL在类增量和域增量场景下，在三个基准数据集上均优于现有基线方法。"
      },
      {
        "paper_id": "2602.17594",
        "title": "AI Gamestore: Scalable, Open-Ended Evaluation of Machine General Intelligence with Human Games",
        "abstract": "Rigorously evaluating machine intelligence against the broad spectrum of human general intelligence has become increasingly important and challenging in this era of rapid technological advance. Conventional AI benchmarks typically assess only narrow capabilities in a limited range of human activity. Most are also static, quickly saturating as developers explicitly or implicitly optimize for them. We propose that a more promising way to evaluate human-like general intelligence in AI systems is through a particularly strong form of general game playing: studying how and how well they play and learn to play \\textbf{all conceivable human games}, in comparison to human players with the same level of experience, time, or other resources. We define a \"human game\" to be a game designed by humans for humans, and argue for the evaluative suitability of this space of all such games people can imagine and enjoy -- the \"Multiverse of Human Games\". Taking a first step towards this vision, we introduce the AI GameStore, a scalable and open-ended platform that uses LLMs with humans-in-the-loop to synthesize new representative human games, by automatically sourcing and adapting standardized and containerized variants of game environments from popular human digital gaming platforms. As a proof of concept, we generated 100 such games based on the top charts of Apple App Store and Steam, and evaluated seven frontier vision-language models (VLMs) on short episodes of play. The best models achieved less than 10\\% of the human average score on the majority of the games, and especially struggled with games that challenge world-model learning, memory and planning. We conclude with a set of next steps for building out the AI GameStore as a practical way to measure and drive progress toward human-like general intelligence in machines.",
        "authors_display": "Joshua B. Tenenbaum Team",
        "pdf_url": "http://arxiv.org/abs/2602.17594",
        "code_url": null,
        "date": "2026-02-19",
        "primary_category": "cs.AI",
        "chinese_summary": "为评估AI系统更接近人类的通用智能，本文指出传统基准测试的局限性，并提出通过AI玩和学习所有“人类游戏”来衡量通用智能。研究者定义了“人类游戏多重宇宙”作为评估空间，并介绍了AI GameStore平台，该平台利用LLM与人类协同生成新的代表性人类游戏。作为概念验证，团队基于热门榜单生成了100个游戏，并评估了七个视觉-语言模型（VLM）。结果表明，最佳模型在大多数游戏中的得分远低于人类平均水平，尤其在需要世界模型学习、记忆和规划的游戏中表现不佳。"
      },
      {
        "paper_id": "2602.17535",
        "title": "LATA: Laplacian-Assisted Transductive Adaptation for Conformal Uncertainty in Medical VLMs",
        "abstract": "Medical vision-language models (VLMs) are strong zero-shot recognizers for medical imaging, but their reliability under domain shift hinges on calibrated uncertainty with guarantees. Split conformal prediction (SCP) offers finite-sample coverage, yet prediction sets often become large (low efficiency) and class-wise coverage unbalanced-high class-conditioned coverage gap (CCV), especially in few-shot, imbalanced regimes; moreover, naively adapting to calibration labels breaks exchangeability and voids guarantees. We propose \\texttt{\\textbf{LATA}} (Laplacian-Assisted Transductive Adaptation), a \\textit{training- and label-free} refinement that operates on the joint calibration and test pool by smoothing zero-shot probabilities over an image-image k-NN graph using a small number of CCCP mean-field updates, preserving SCP validity via a deterministic transform. We further introduce a \\textit{failure-aware} conformal score that plugs into the vision-language uncertainty (ViLU) framework, providing instance-level difficulty and label plausibility to improve prediction set efficiency and class-wise balance at fixed coverage. \\texttt{\\textbf{LATA}} is black-box (no VLM updates), compute-light (windowed transduction, no backprop), and includes an optional prior knob that can run strictly label-free or, if desired, in a label-informed variant using calibration marginals once. Across \\textbf{three} medical VLMs and \\textbf{nine} downstream tasks, \\texttt{\\textbf{LATA}} consistently reduces set size and CCV while matching or tightening target coverage, outperforming prior transductive baselines and narrowing the gap to label-using methods, while using far less compute. Comprehensive ablations and qualitative analyses show that \\texttt{\\textbf{LATA}} sharpens zero-shot predictions without compromising exchangeability.",
        "authors_display": "Zongyuan Ge Team",
        "pdf_url": "http://arxiv.org/abs/2602.17535",
        "code_url": null,
        "date": "2026-02-19",
        "primary_category": "cs.CV",
        "chinese_summary": "医疗视觉-语言模型在零样本识别方面表现出色，但域偏移下可靠性不足，且传统保形预测方法在少样本不平衡场景下效率低、类别覆盖不均。为解决此问题，本文提出了\texttt{\textbf{LATA}} (Laplacian-Assisted Transductive Adaptation) 方法，这是一种无训练、无标签的精炼技术，通过在图像k-NN图上平滑零样本概率来优化预测集。此外，它引入了一种“失败感知”保形分数以提高效率和类别平衡。实验表明，\texttt{\textbf{LATA}}在三个医学VLM和九个下游任务上，在保持或收紧覆盖率的同时，显著减小了预测集大小和类别覆盖不均，并以更低的计算成本超越了现有转导基线。"
      },
      {
        "paper_id": "2602.17186",
        "title": "Selective Training for Large Vision Language Models via Visual Information Gain",
        "abstract": "Large Vision Language Models (LVLMs) have achieved remarkable progress, yet they often suffer from language bias, producing answers without relying on visual evidence. While prior work attempts to mitigate this issue through decoding strategies, architectural modifications, or curated instruction data, they typically lack a quantitative measure of how much individual training samples or tokens actually benefit from the image. In this work, we introduce Visual Information Gain (VIG), a perplexity-based metric that measures the reduction in prediction uncertainty provided by visual input. VIG enables fine-grained analysis at both sample and token levels, effectively highlighting visually grounded elements such as colors, spatial relations, and attributes. Leveraging this, we propose a VIG-guided selective training scheme that prioritizes high-VIG samples and tokens. This approach improves visual grounding and mitigates language bias, achieving superior performance with significantly reduced supervision by focusing exclusively on visually informative samples and tokens.",
        "authors_display": "Sangheum Hwang Team",
        "pdf_url": "http://arxiv.org/abs/2602.17186",
        "code_url": null,
        "date": "2026-02-19",
        "primary_category": "cs.CV",
        "chinese_summary": "大型视觉语言模型（LVLMs）常表现出语言偏见，即在缺乏视觉依据的情况下生成回答。为量化视觉信息对模型预测的贡献，本文引入了视觉信息增益（VIG），一个基于困惑度的指标，用于衡量视觉输入带来的预测不确定性降低。VIG支持细粒度的样本和token级别分析，能够突出视觉接地元素。基于此，研究者提出了一种VIG引导的选择性训练方案，优先训练高VIG的样本和token。该方法通过聚焦视觉信息丰富的元素，在显著减少监督的情况下，提高了模型的视觉接地能力并减轻了语言偏见。"
      },
      {
        "paper_id": "2602.17871",
        "title": "Understanding the Fine-Grained Knowledge Capabilities of Vision-Language Models",
        "abstract": "Vision-language models (VLMs) have made substantial progress across a wide range of visual question answering benchmarks, spanning visual reasoning, document understanding, and multimodal dialogue. These improvements are evident in a wide range of VLMs built on a variety of base models, alignment architectures, and training data. However, recent works show that these models trail behind in traditional image classification benchmarks, which test fine-grained visual knowledge. We test a large number of recent VLMs on fine-grained classification benchmarks and identify potential factors in the disconnect between fine-grained knowledge and other vision benchmarks. Through a series of ablation experiments, we find that using a better LLM improves all benchmark scores equally, while a better vision encoder disproportionately improves fine-grained classification performance. Furthermore, we find that the pretraining stage is also vital to fine-grained performance, particularly when the language model weights are unfrozen during pretraining. These insights pave the way for enhancing fine-grained visual understanding and vision-centric capabilities in VLMs.",
        "authors_display": "Ludwig Schmidt Team",
        "pdf_url": "http://arxiv.org/abs/2602.17871",
        "code_url": null,
        "date": "2026-02-19",
        "primary_category": "cs.CV",
        "chinese_summary": "视觉-语言模型 (VLM) 在视觉问答基准上进步显著，但在需要细粒度视觉知识的传统图像分类基准上仍有不足。本研究在细粒度分类基准上测试了大量最新的VLM，并探究了细粒度知识与其他视觉基准之间脱节的潜在因素。通过一系列消融实验，研究发现，更好的LLM能均匀提升所有基准分数，而更好的视觉编码器则能显著提升细粒度分类性能。此外，预训练阶段对细粒度性能至关重要，特别是在预训练期间不冻结语言模型权重时。这些见解为增强VLM的细粒度视觉理解和以视觉为中心的能力提供了方向。"
      },
      {
        "paper_id": "2602.17799",
        "title": "Enabling Training-Free Text-Based Remote Sensing Segmentation",
        "abstract": "Recent advances in Vision Language Models (VLMs) and Vision Foundation Models (VFMs) have opened new opportunities for zero-shot text-guided segmentation of remote sensing imagery. However, most existing approaches still rely on additional trainable components, limiting their generalisation and practical applicability. In this work, we investigate to what extent text-based remote sensing segmentation can be achieved without additional training, by relying solely on existing foundation models. We propose a simple yet effective approach that integrates contrastive and generative VLMs with the Segment Anything Model (SAM), enabling a fully training-free or lightweight LoRA-tuned pipeline. Our contrastive approach employs CLIP as mask selector for SAM's grid-based proposals, achieving state-of-the-art open-vocabulary semantic segmentation (OVSS) in a completely zero-shot setting. In parallel, our generative approach enables reasoning and referring segmentation by generating click prompts for SAM using GPT-5 in a zero-shot setting and a LoRA-tuned Qwen-VL model, with the latter yielding the best results. Extensive experiments across 19 remote sensing benchmarks, including open-vocabulary, referring, and reasoning-based tasks, demonstrate the strong capabilities of our approach. Code will be released at https://github.com/josesosajs/trainfree-rs-segmentation.",
        "authors_display": "Djamila Aouada Team",
        "pdf_url": "http://arxiv.org/abs/2602.17799",
        "code_url": null,
        "date": "2026-02-19",
        "primary_category": "cs.CV",
        "chinese_summary": "视觉语言模型 (VLM) 和视觉基础模型 (VFM) 的进展为遥感图像的零样本文本引导分割提供了新机遇，但现有方法通常依赖额外可训练组件。本研究提出一种简单有效的方法，整合对比式和生成式VLM与Segment Anything Model (SAM)，实现了完全无需训练或轻量级LoRA调优的流水线。对比式方法使用CLIP作为SAM提议的掩码选择器，实现零样本开放词汇语义分割。生成式方法通过GPT-5或LoRA调优的Qwen-VL模型生成点击提示。在19个遥感基准数据集上的广泛实验表明，该方法在开放词汇、指代和基于推理的任务中表现出强大能力，其中LoRA调优的Qwen-VL模型效果最佳。"
      },
      {
        "paper_id": "2602.17770",
        "title": "CLUTCH: Contextualized Language model for Unlocking Text-Conditioned Hand motion modelling in the wild",
        "abstract": "Hands play a central role in daily life, yet modeling natural hand motions remains underexplored. Existing methods that tackle text-to-hand-motion generation or hand animation captioning rely on studio-captured datasets with limited actions and contexts, making them costly to scale to \"in-the-wild\" settings. Further, contemporary models and their training schemes struggle to capture animation fidelity with text-motion alignment. To address this, we (1) introduce '3D Hands in the Wild' (3D-HIW), a dataset of 32K 3D hand-motion sequences and aligned text, and (2) propose CLUTCH, an LLM-based hand animation system with two critical innovations: (a) SHIFT, a novel VQ-VAE architecture to tokenize hand motion, and (b) a geometric refinement stage to finetune the LLM. To build 3D-HIW, we propose a data annotation pipeline that combines vision-language models (VLMs) and state-of-the-art 3D hand trackers, and apply it to a large corpus of egocentric action videos covering a wide range of scenarios. To fully capture motion in-the-wild, CLUTCH employs SHIFT, a part-modality decomposed VQ-VAE, which improves generalization and reconstruction fidelity. Finally, to improve animation quality, we introduce a geometric refinement stage, where CLUTCH is co-supervised with a reconstruction loss applied directly to decoded hand motion parameters. Experiments demonstrate state-of-the-art performance on text-to-motion and motion-to-text tasks, establishing the first benchmark for scalable in-the-wild hand motion modelling. Code, data and models will be released.",
        "authors_display": "Justus Thies Team",
        "pdf_url": "http://arxiv.org/abs/2602.17770",
        "code_url": "https://balamuruganthambiraja.github.io/CLUTCH/",
        "date": "2026-02-19",
        "primary_category": "cs.CV",
        "chinese_summary": "手部运动在日常生活中至关重要，但现有文本到手部动作生成方法依赖有限的摄影棚数据集，难以扩展到“野外”场景，且模型在动画保真度和文本-动作对齐方面表现不足。为解决此问题，本研究提出了“3D Hands in the Wild (3D-HIW)”数据集，包含32K 3D手部动作序列及对齐文本，并通过结合VLM和先进3D手部追踪器构建。在此基础上，引入了基于LLM的手部动画系统CLUTCH，其创新点包括：SHIFT（一种新颖的VQ-VAE架构用于手部动作分词）和几何精修阶段以微调LLM。实验证明，CLUTCH在文本到动作和动作到文本任务上均达到了最先进的性能，为可扩展的野外手部动作建模建立了首个基准。"
      },
      {
        "paper_id": "2602.16702",
        "title": "Saliency-Aware Multi-Route Thinking: Revisiting Vision-Language Reasoning",
        "abstract": "Vision-language models (VLMs) aim to reason by jointly leveraging visual and textual modalities. While allocating additional inference-time computation has proven effective for large language models (LLMs), achieving similar scaling in VLMs remains challenging. A key obstacle is that visual inputs are typically provided only once at the start of generation, while textual reasoning (e.g., early visual summaries) is generated autoregressively, causing reasoning to become increasingly text-dominated and allowing early visual grounding errors to accumulate. Moreover, vanilla guidance for visual grounding during inference is often coarse and noisy, making it difficult to steer reasoning over long texts. To address these challenges, we propose \\emph{Saliency-Aware Principle} (SAP) selection. SAP operates on high-level reasoning principles rather than token-level trajectories, which enable stable control over discrete generation under noisy feedback while allowing later reasoning steps to re-consult visual evidence when renewed grounding is required. In addition, SAP supports multi-route inference, enabling parallel exploration of diverse reasoning behaviors. SAP is model-agnostic and data-free, requiring no additional training. Empirical results show that SAP achieves competitive performance, especially in reducing object hallucination, under comparable token-generation budgets while yielding more stable reasoning and lower response latency than CoT-style long sequential reasoning.",
        "authors_display": "Jundong Li Team",
        "pdf_url": "http://arxiv.org/abs/2602.16702",
        "code_url": null,
        "date": "2026-02-18",
        "primary_category": "cs.CV",
        "chinese_summary": "视觉-语言模型（VLMs）的推理能力受限于视觉输入一次性提供、文本推理易受早期视觉接地错误累积及朴素视觉引导粗糙等问题。为解决这些挑战，本文提出了“显著性感知原则”（SAP）选择方法，其作用于高层推理原则而非token级轨迹，能在噪声反馈下稳定控制离散生成，并允许模型在需要时重新参考视觉证据。此外，SAP支持多路径推理以探索多样化行为。SAP是模型无关且无需额外训练。实验结果表明，SAP在可比的token生成预算下，特别是在减少物体幻觉方面，实现了竞争性性能，并提供了比CoT风格长序列推理更稳定的推理和更低的响应延迟。"
      },
      {
        "paper_id": "2602.16590",
        "title": "A Contrastive Learning Framework Empowered by Attention-based Feature Adaptation for Street-View Image Classification",
        "abstract": "Street-view image attribute classification is a vital downstream task of image classification, enabling applications such as autonomous driving, urban analytics, and high-definition map construction. It remains computationally demanding whether training from scratch, initialising from pre-trained weights, or fine-tuning large models. Although pre-trained vision-language models such as CLIP offer rich image representations, existing adaptation or fine-tuning methods often rely on their global image embeddings, limiting their ability to capture fine-grained, localised attributes essential in complex, cluttered street scenes. To address this, we propose CLIP-MHAdapter, a variant of the current lightweight CLIP adaptation paradigm that appends a bottleneck MLP equipped with multi-head self-attention operating on patch tokens to model inter-patch dependencies. With approximately 1.4 million trainable parameters, CLIP-MHAdapter achieves superior or competitive accuracy across eight attribute classification tasks on the Global StreetScapes dataset, attaining new state-of-the-art results while maintaining low computational cost. The code is available at https://github.com/SpaceTimeLab/CLIP-MHAdapter.",
        "authors_display": "James Haworth Team",
        "pdf_url": "http://arxiv.org/abs/2602.16590",
        "code_url": null,
        "date": "2026-02-18",
        "primary_category": "cs.CV",
        "chinese_summary": "街景图像属性分类是一项重要但计算量大的任务，而现有基于预训练视觉-语言模型（如CLIP）的适应方法多依赖全局图像嵌入，难以捕捉复杂街景中的细粒度局部属性。针对此问题，本文提出了CLIP-MHAdapter，这是对轻量级CLIP适应范式的一种改进。该方法通过在CLIP模型后附加一个配备多头自注意力机制的瓶颈MLP，使其能够作用于patch tokens，从而建模patch间的依赖关系。实验结果表明，CLIP-MHAdapter仅用约140万可训练参数，就在Global StreetScapes数据集的八项属性分类任务上实现了优异或具有竞争力的准确率，达到了新的SOTA，同时保持了较低的计算成本。"
      },
      {
        "paper_id": "2602.16502",
        "title": "DressWild: Feed-Forward Pose-Agnostic Garment Sewing Pattern Generation from In-the-Wild Images",
        "abstract": "Recent advances in garment pattern generation have shown promising progress. However, existing feed-forward methods struggle with diverse poses and viewpoints, while optimization-based approaches are computationally expensive and difficult to scale. This paper focuses on sewing pattern generation for garment modeling and fabrication applications that demand editable, separable, and simulation-ready garments. We propose DressWild, a novel feed-forward pipeline that reconstructs physics-consistent 2D sewing patterns and the corresponding 3D garments from a single in-the-wild image. Given an input image, our method leverages vision-language models (VLMs) to normalize pose variations at the image level, then extract pose-aware, 3D-informed garment features. These features are fused through a transformer-based encoder and subsequently used to predict sewing pattern parameters, which can be directly applied to physical simulation, texture synthesis, and multi-layer virtual try-on. Extensive experiments demonstrate that our approach robustly recovers diverse sewing patterns and the corresponding 3D garments from in-the-wild images without requiring multi-view inputs or iterative optimization, offering an efficient and scalable solution for realistic garment simulation and animation.",
        "authors_display": "Chenfanfu Jiang Team",
        "pdf_url": "http://arxiv.org/abs/2602.16502",
        "code_url": null,
        "date": "2026-02-18",
        "primary_category": "cs.CV",
        "chinese_summary": "现有服装图案生成方法在处理多样姿态和视角时面临挑战，且基于优化的方法计算成本高昂。本文针对需要可编辑、可分离、可模拟服装的缝纫图案生成，提出了一种名为DressWild的新型前馈流水线。该方法能够从单张“in-the-wild”图像重建物理一致的2D缝纫图案及对应的3D服装。DressWild利用视觉-语言模型（VLMs）规范姿态变化，并提取姿态感知和3D信息丰富的服装特征，通过Transformer编码器融合后预测缝纫图案参数。大量实验证明，该方法无需多视角输入或迭代优化，即可鲁棒地从真实图像中恢复多样的缝纫图案和3D服装，为服装模拟与动画提供了高效可扩展的方案。"
      },
      {
        "paper_id": "2602.16455",
        "title": "Visual Self-Refine: A Pixel-Guided Paradigm for Accurate Chart Parsing",
        "abstract": "While Large Vision-Language Models (LVLMs) have demonstrated remarkable capabilities for reasoning and self-correction at the textual level, these strengths provide minimal benefits for complex tasks centered on visual perception, such as Chart Parsing. Existing models often struggle with visually dense charts, leading to errors like data omission, misalignment, and hallucination. Inspired by the human strategy of using a finger as a ``visual anchor'' to ensure accuracy when reading complex charts, we propose a new paradigm named Visual Self-Refine (VSR). The core idea of VSR is to enable a model to generate pixel-level localization outputs, visualize them, and then feed these visualizations back to itself, allowing it to intuitively inspect and correct its own potential visual perception errors. We instantiate the VSR paradigm in the domain of Chart Parsing by proposing ChartVSR. This model decomposes the parsing process into two stages: a Refine Stage, where it iteratively uses visual feedback to ensure the accuracy of all data points' Pixel-level Localizations, and a Decode Stage, where it uses these verified localizations as precise visual anchors to parse the final structured data. To address the limitations of existing benchmarks, we also construct ChartP-Bench, a new and highly challenging benchmark for chart parsing. Our work also highlights VSR as a general-purpose visual feedback mechanism, offering a promising new direction for enhancing accuracy on a wide range of vision-centric tasks.",
        "authors_display": "Dahua Lin Team",
        "pdf_url": "http://arxiv.org/abs/2602.16455",
        "code_url": null,
        "date": "2026-02-18",
        "primary_category": "cs.CV",
        "chinese_summary": "针对大型视觉-语言模型（LVLMs）在图表解析等视觉感知密集任务中因视觉稠密导致数据遗漏、错位和幻觉等错误，本文提出了Visual Self-Refine（VSR）新范式。VSR受人类阅读复杂图表时使用“视觉锚点”的启发，使模型能生成像素级定位输出，并将其可视化反馈给自己进行错误修正。作者在图表解析领域实例化VSR为ChartVSR，将其解析过程分解为迭代修正像素级定位的“精炼阶段”和利用验证定位解析结构化数据的“解码阶段”。此外，还构建了高挑战性基准ChartP-Bench。这项工作展示了VSR作为通用视觉反馈机制在提升视觉中心任务准确性方面的潜力。"
      },
      {
        "paper_id": "2602.16430",
        "title": "Designing Production-Scale OCR for India: Multilingual and Domain-Specific Systems",
        "abstract": "Designing Optical Character Recognition (OCR) systems for India requires balancing linguistic diversity, document heterogeneity, and deployment constraints. In this paper, we study two training strategies for building multilingual OCR systems with Vision-Language Models through the Chitrapathak series. We first follow a popular multimodal approach, pairing a generic vision encoder with a strong multilingual language model and training the system end-to-end for OCR. Alternatively, we explore fine-tuning an existing OCR model, despite not being trained for the target languages. Through extensive evaluation on multilingual Indic OCR benchmarks and deployment-oriented metrics, we find that the second strategy consistently achieves better accuracy-latency trade-offs. Chitrapathak-2 achieves 3-6x speedup over its predecessor with being state-of-the-art (SOTA) in Telugu (6.69 char ANLS) and second best in the rest. In addition, we present Parichay, an independent OCR model series designed specifically for 9 Indian government documents to extract structured key fields, achieving 89.8% Exact Match score with a faster inference. Together, these systems achieve SOTA performance and provide practical guidance for building production-scale OCR pipelines in the Indian context.",
        "authors_display": "Shubham Agarwal Team",
        "pdf_url": "http://arxiv.org/abs/2602.16430",
        "code_url": null,
        "date": "2026-02-18",
        "primary_category": "cs.CV",
        "chinese_summary": "针对印度OCR系统在语言多样性、文档异构性和部署限制方面的挑战，本文通过Chitrapathak系列研究了两种多语言OCR训练策略。第一种是结合通用视觉编码器与多语言语言模型进行端到端训练；第二种是微调现有OCR模型，即使其未针对目标语言训练。实验结果表明，第二种策略在准确性与延迟权衡方面表现更优，Chitrapathak-2在泰卢固语上达到SOTA，并实现3-6倍加速。此外，还提出了Parichay系列模型，专门用于印度9种政府文档的关键字段提取，达到了89.8%的精确匹配率和更快的推理速度。这些系统为印度语境下的生产级OCR流水线提供了实践指导。"
      },
      {
        "paper_id": "2602.16157",
        "title": "Peeking Ahead of the Field Study: Exploring VLM Personas as Support Tools for Embodied Studies in HCI",
        "abstract": "Field studies are irreplaceable but costly, time-consuming, and error-prone, which need careful preparation. Inspired by rapid-prototyping in manufacturing, we propose a fast, low-cost evaluation method using Vision-Language Model (VLM) personas to simulate outcomes comparable to field results. While LLMs show human-like reasoning and language capabilities, autonomous vehicle (AV)-pedestrian interaction requires spatial awareness, emotional empathy, and behavioral generation. This raises our research question: To what extent can VLM personas mimic human responses in field studies? We conducted parallel studies: 1) one real-world study with 20 participants, and 2) one video-study using 20 VLM personas, both on a street-crossing task. We compared their responses and interviewed five HCI researchers on potential applications. Results show that VLM personas mimic human response patterns (e.g., average crossing times of 5.25 s vs. 5.07 s) lack the behavioral variability and depth. They show promise for formative studies, field study preparation, and human data augmentation.",
        "authors_display": "Takeo Igarashi Team",
        "pdf_url": "http://arxiv.org/abs/2602.16157",
        "code_url": null,
        "date": "2026-02-18",
        "primary_category": "cs.HC",
        "chinese_summary": "鉴于实地研究成本高昂、耗时且易出错，本文受快速原型启发，旨在探索VLM角色能否模拟实地研究结果。研究背景是尽管大型语言模型（LLMs）具有类人推理和语言能力，但自动驾驶车辆（AV）与行人交互需空间感知、情感共情和行为生成。为探究VLM角色模仿人类反应的程度，研究进行了平行实验：一项20人参与的真实街道穿越任务，以及一项20个VLM角色参与的视频研究。结果显示，VLM角色能模仿人类的反应模式（如平均穿越时间），但缺乏行为变异性和深度，它们在形成性研究、实地研究准备和人类数据增强方面展现了潜力。"
      },
      {
        "paper_id": "2602.16149",
        "title": "Evaluating Demographic Misrepresentation in Image-to-Image Portrait Editing",
        "abstract": "Demographic bias in text-to-image (T2I) generation is well studied, yet demographic-conditioned failures in instruction-guided image-to-image (I2I) editing remain underexplored. We examine whether identical edit instructions yield systematically different outcomes across subject demographics in open-weight I2I editors. We formalize two failure modes: Soft Erasure, where edits are silently weakened or ignored in the output image, and Stereotype Replacement, where edits introduce unrequested, stereotype-consistent attributes. We introduce a controlled benchmark that probes demographic-conditioned behavior by generating and editing portraits conditioned on race, gender, and age using a diagnostic prompt set, and evaluate multiple editors with vision-language model (VLM) scoring and human evaluation. Our analysis shows that identity preservation failures are pervasive, demographically uneven, and shaped by implicit social priors, including occupation-driven gender inference. Finally, we demonstrate that a prompt-level identity constraint, without model updates, can substantially reduce demographic change for minority groups while leaving majority-group portraits largely unchanged, revealing asymmetric identity priors in current editors. Together, our findings establish identity preservation as a central and demographically uneven failure mode in I2I editing and motivate demographic-robust editing systems. Project page: https://seochan99.github.io/i2i-demographic-bias",
        "authors_display": "Jean Oh Team",
        "pdf_url": "http://arxiv.org/abs/2602.16149",
        "code_url": null,
        "date": "2026-02-18",
        "primary_category": "cs.CV",
        "chinese_summary": "尽管文本到图像（T2I）生成中的人口偏见已广泛研究，但指令引导的图像到图像（I2I）编辑中与人口统计相关的失败模式仍未被充分探索。本文形式化了两种失败模式：软擦除（编辑被削弱或忽略）和刻板印象替换（引入非请求的刻板印象属性）。研究引入了一个受控基准，通过生成和编辑基于种族、性别和年龄的人像，并使用视觉-语言模型评分和人工评估来探究行为。分析表明，身份保留失败普遍存在，具有人口统计学上的不均衡性，并受隐含社会先验（如职业驱动的性别推断）的影响。研究还发现，提示级身份约束可在不更新模型的情况下显著减少少数群体的群体变化。这些发现确立了身份保留作为I2I编辑中一个核心且人口统计学上不均衡的失败模式，并推动了人口统计学鲁棒编辑系统的发展。"
      },
      {
        "paper_id": "2602.16138",
        "title": "IRIS: Intent Resolution via Inference-time Saccades for Open-Ended VQA in Large Vision-Language Models",
        "abstract": "We introduce IRIS (Intent Resolution via Inference-time Saccades), a novel training-free approach that uses eye-tracking data in real-time to resolve ambiguity in open-ended VQA. Through a comprehensive user study with 500 unique image-question pairs, we demonstrate that fixations closest to the time participants start verbally asking their questions are the most informative for disambiguation in Large VLMs, more than doubling the accuracy of responses on ambiguous questions (from 35.2% to 77.2%) while maintaining performance on unambiguous queries. We evaluate our approach across state-of-the-art VLMs, showing consistent improvements when gaze data is incorporated in ambiguous image-question pairs, regardless of architectural differences. We release a new benchmark dataset to use eye movement data for disambiguated VQA, a novel real-time interactive protocol, and an evaluation suite.",
        "authors_display": "Miguel P. Eckstein Team",
        "pdf_url": "http://arxiv.org/abs/2602.16138",
        "code_url": null,
        "date": "2026-02-18",
        "primary_category": "cs.CV",
        "chinese_summary": "针对开放式视觉问答（VQA）中大型视觉-语言模型（VLMs）在处理歧义时的挑战，本文引入了IRIS（Intent Resolution via Inference-time Saccades）——一种无需训练的方法，利用实时眼动追踪数据解决歧义。通过包含500个独特图像-问题对的综合用户研究，结果表明，参与者开始口头提问时最近的注视点对于大型VLM的消歧最为信息丰富。该方法使歧义问题的响应准确性提高了一倍以上（从35.2%到77.2%），同时保持了无歧义查询的性能。研究还在各种最先进的VLM上验证了其效果。这项工作发布了新的基准数据集、实时交互协议和评估套件，强调了眼动数据在增强VQA准确性方面的潜力。"
      },
      {
        "paper_id": "2602.16110",
        "title": "OmniCT: Towards a Unified Slice-Volume LVLM for Comprehensive CT Analysis",
        "abstract": "Computed Tomography (CT) is one of the most widely used and diagnostically information-dense imaging modalities, covering critical organs such as the heart, lungs, liver, and colon. Clinical interpretation relies on both slice-driven local features (e.g., sub-centimeter nodules, lesion boundaries) and volume-driven spatial representations (e.g., tumor infiltration, inter-organ anatomical relations). However, existing Large Vision-Language Models (LVLMs) remain fragmented in CT slice versus volumetric understanding: slice-driven LVLMs show strong generalization but lack cross-slice spatial consistency, while volume-driven LVLMs explicitly capture volumetric semantics but suffer from coarse granularity and poor compatibility with slice inputs. The absence of a unified modeling paradigm constitutes a major bottleneck for the clinical translation of medical LVLMs. We present OmniCT, a powerful unified slice-volume LVLM for CT scenarios, which makes three contributions: (i) Spatial Consistency Enhancement (SCE): volumetric slice composition combined with tri-axial positional embedding that introduces volumetric consistency, and an MoE hybrid projection enables efficient slice-volume adaptation; (ii) Organ-level Semantic Enhancement (OSE): segmentation and ROI localization explicitly align anatomical regions, emphasizing lesion- and organ-level semantics; (iii) MedEval-CT: the largest slice-volume CT dataset and hybrid benchmark integrates comprehensive metrics for unified evaluation. OmniCT consistently outperforms existing methods with a substantial margin across diverse clinical tasks and satisfies both micro-level detail sensitivity and macro-level spatial reasoning. More importantly, it establishes a new paradigm for cross-modal medical imaging understanding.",
        "authors_display": "Beng Chin Ooi Team",
        "pdf_url": "http://arxiv.org/abs/2602.16110",
        "code_url": null,
        "date": "2026-02-18",
        "primary_category": "cs.CV",
        "chinese_summary": "针对CT影像诊断中现有大型视觉-语言模型（LVLMs）在切片级和体积级理解上的碎片化问题，即切片驱动模型缺乏空间一致性而体积驱动模型粒度粗糙，本文提出了OmniCT，一个强大的统一切片-体积LVLM。该模型通过空间一致性增强（SCE）引入体积一致性，器官级语义增强（OSE）明确对齐解剖区域，并构建了最大规模的切片-体积CT数据集和混合基准MedEval-CT。实验证明，OmniCT在多样化的临床任务中显著优于现有方法，同时满足微观细节敏感性和宏观空间推理需求，为跨模态医学影像理解建立了新范式。"
      },
      {
        "paper_id": "2602.16931",
        "title": "Narrow fine-tuning erodes safety alignment in vision-language agents",
        "abstract": "Lifelong multimodal agents must continuously adapt to new tasks through post-training, but this creates fundamental tension between acquiring capabilities and preserving safety alignment. We demonstrate that fine-tuning aligned vision-language models on narrow-domain harmful datasets induces severe emergent misalignment that generalizes broadly across unrelated tasks and modalities. Through experiments on Gemma3-4B, we show that misalignment scales monotonically with LoRA rank, and that multimodal evaluation reveals substantially higher misalignment ($70.71 \\pm 1.22$ at $r=128$) than text-only evaluation ($41.19 \\pm 2.51$), suggesting that unimodal safety benchmarks may underestimate alignment degradation in vision-language models. Critically, even 10\\% harmful data in the training mixture induces substantial alignment degradation. Geometric analysis reveals that harmful behaviors occupy a remarkably low-dimensional subspace, with the majority of misalignment information captured in 10 principal components. To mitigate misalignment, we evaluate two strategies: benign narrow fine-tuning and activation-based steering. While both approaches substantially reduce misalignment, neither completely removes the learned harmful behaviors. Our findings highlight the need for robust continual learning frameworks, as current post-training paradigms may not sufficiently preserve alignment in post-deployment settings.",
        "authors_display": "Shivam Raval Team",
        "pdf_url": "http://arxiv.org/abs/2602.16931",
        "code_url": null,
        "date": "2026-02-18",
        "primary_category": "cs.AI",
        "chinese_summary": "终身多模态智能体在后训练过程中需适应新任务，但这与保持安全对齐存在冲突。本文研究发现，在狭窄领域的有害数据集上微调对齐的视觉-语言模型（VLM）会导致严重的突发性错位，且这种错位广泛泛化。实验在Gemma3-4B上进行，结果表明错位程度与LoRA秩单调递增，且多模态评估揭示的错位程度远高于纯文本评估。即使训练数据中只有10%的有害数据也会导致显著的对齐退化。几何分析显示有害行为占据低维子空间。虽然良性微调和基于激活的引导能减少错位，但均未能完全消除有害行为，强调了开发鲁棒持续学习框架的必要性。"
      },
      {
        "paper_id": "2602.16872",
        "title": "DODO: Discrete OCR Diffusion Models",
        "abstract": "Optical Character Recognition (OCR) is a fundamental task for digitizing information, serving as a critical bridge between visual data and textual understanding. While modern Vision-Language Models (VLM) have achieved high accuracy in this domain, they predominantly rely on autoregressive decoding, which becomes computationally expensive and slow for long documents as it requires a sequential forward pass for every generated token. We identify a key opportunity to overcome this bottleneck: unlike open-ended generation, OCR is a highly deterministic task where the visual input strictly dictates a unique output sequence, theoretically enabling efficient, parallel decoding via diffusion models. However, we show that existing masked diffusion models fail to harness this potential; those introduce structural instabilities that are benign in flexible tasks, like captioning, but catastrophic for the rigid, exact-match requirements of OCR. To bridge this gap, we introduce DODO, the first VLM to utilize block discrete diffusion and unlock its speedup potential for OCR. By decomposing generation into blocks, DODO mitigates the synchronization errors of global diffusion. Empirically, our method achieves near state-of-the-art accuracy while enabling up to 3x faster inference compared to autoregressive baselines.",
        "authors_display": "Niv Nayman Team",
        "pdf_url": "http://arxiv.org/abs/2602.16872",
        "code_url": null,
        "date": "2026-02-18",
        "primary_category": "cs.CV",
        "chinese_summary": "光学字符识别（OCR）作为信息数字化的基础任务，现代视觉-语言模型（VLM）通常采用自回归解码，但这对长文档而言计算成本高且速度慢。本文指出OCR的确定性特征使其理论上适合通过扩散模型进行高效并行解码，但现有掩码扩散模型存在结构不稳定性，不适用于OCR的精确匹配要求。为克服此限制，本文提出了DODO，这是首个将块离散扩散应用于OCR的VLM。DODO通过将生成分解为块来缓解全局扩散的同步错误。实验结果表明，DODO在实现接近最先进准确率的同时，推理速度比自回归基线快3倍。"
      },
      {
        "paper_id": "2602.16019",
        "title": "MedProbCLIP: Probabilistic Adaptation of Vision-Language Foundation Model for Reliable Radiograph-Report Retrieval",
        "abstract": "Vision-language foundation models have emerged as powerful general-purpose representation learners with strong potential for multimodal understanding, but their deterministic embeddings often fail to provide the reliability required for high-stakes biomedical applications. This work introduces MedProbCLIP, a probabilistic vision-language learning framework for chest X-ray and radiology report representation learning and bidirectional retrieval. MedProbCLIP models image and text representations as Gaussian embeddings through a probabilistic contrastive objective that explicitly captures uncertainty and many-to-many correspondences between radiographs and clinical narratives. A variational information bottleneck mitigates overconfident predictions, while MedProbCLIP employs multi-view radiograph encoding and multi-section report encoding during training to provide fine-grained supervision for clinically aligned correspondence, yet requires only a single radiograph and a single report at inference. Evaluated on the MIMIC-CXR dataset, MedProbCLIP outperforms deterministic and probabilistic baselines, including CLIP, CXR-CLIP, and PCME++, in both retrieval and zero-shot classification. Beyond accuracy, MedProbCLIP demonstrates superior calibration, risk-coverage behavior, selective retrieval reliability, and robustness to clinically relevant corruptions, underscoring the value of probabilistic vision-language modeling for improving the trustworthiness and safety of radiology image-text retrieval systems.",
        "authors_display": "Gongbo Liang Team",
        "pdf_url": "http://arxiv.org/abs/2602.16019",
        "code_url": null,
        "date": "2026-02-17",
        "primary_category": "cs.CV",
        "chinese_summary": "鉴于视觉-语言基础模型在生物医学应用中因确定性嵌入而缺乏可靠性，本文提出了MedProbCLIP，一个用于胸部X射线和放射学报告表示学习及双向检索的概率视觉-语言学习框架。MedProbCLIP通过概率对比目标将图像和文本表示建模为高斯嵌入，从而明确捕捉不确定性和多对多对应关系，并通过变分信息瓶颈减轻过度自信预测。模型在训练中采用多视图X射线编码和多节报告编码以提供细粒度监督，但推理时仅需单张图像和报告。在MIMIC-CXR数据集上的评估显示，MedProbCLIP在检索和零样本分类方面均优于现有基线，并展现出卓越的校准性、可靠性和鲁棒性，突显了概率建模在提升放射学图像-文本检索系统可信度和安全性方面的价值。"
      },
      {
        "paper_id": "2602.16006",
        "title": "BTReport: A Framework for Brain Tumor Radiology Report Generation with Clinically Relevant Features",
        "abstract": "Recent advances in radiology report generation (RRG) have been driven by large paired image-text datasets; however, progress in neuro-oncology has been limited due to a lack of open paired image-report datasets. Here, we introduce BTReport, an open-source framework for brain tumor RRG that constructs natural language radiology reports using deterministically extracted imaging features. Unlike existing approaches that rely on large general-purpose or fine-tuned vision-language models for both image interpretation and report composition, BTReport performs deterministic feature extraction for image analysis and uses large language models only for syntactic structuring and narrative formatting. By separating RRG into a deterministic feature extraction step and a report generation step, the generated reports are completely interpretable and less prone to hallucinations. We show that the features used for report generation are predictive of key clinical outcomes, including survival and IDH mutation status, and reports generated by BTReport are more closely aligned with reference clinical reports than existing baselines for RRG. Finally, we introduce BTReport-BraTS, a companion dataset that augments BraTS imaging with synthetically generated radiology reports produced with BTReport. Code for this project can be found at  https://github.com/KurtLabUW/BTReport.",
        "authors_display": "Mehmet Kurt Team",
        "pdf_url": "http://arxiv.org/abs/2602.16006",
        "code_url": null,
        "date": "2026-02-17",
        "primary_category": "cs.CV",
        "chinese_summary": "鉴于神经肿瘤学领域因缺乏开放的配对图像-报告数据集而限制了放射学报告生成（RRG）的进展，本文引入了BTReport，一个开源的脑肿瘤RRG框架。该框架通过确定性提取成像特征来构建自然语言放射学报告。与依赖大型视觉-语言模型进行图像解释和报告撰写的方法不同，BTReport将RRG分解为确定性特征提取和报告生成两个步骤，仅使用大型语言模型进行句法结构化和叙述格式化，从而提高了报告的可解释性并减少了幻觉。研究表明，用于报告生成的特征能预测关键临床结果，且BTReport生成的报告与参考临床报告更紧密对齐。此外，本文还发布了伴随数据集BTReport-BraTS。"
      },
      {
        "paper_id": "2602.15950",
        "title": "Can Vision-Language Models See Squares? Text-Recognition Mediates Spatial Reasoning Across Three Model Families",
        "abstract": "We present a simple experiment that exposes a fundamental limitation in vision-language models (VLMs): the inability to accurately localize filled cells in binary grids when those cells lack textual identity. We generate fifteen 15x15 grids with varying density (10.7%-41.8% filled cells) and render each as two image types -- text symbols (. and #) and filled squares without gridlines -- then ask three frontier VLMs (Claude Opus, ChatGPT 5.2, and Gemini 3 Thinking) to transcribe them. In the text-symbol condition, Claude and ChatGPT achieve approximately 91% cell accuracy and 84% F1, while Gemini achieves 84% accuracy and 63% F1. In the filled-squares condition, all three models collapse to 60-73% accuracy and 29-39% F1. Critically, all conditions pass through the same visual encoder -- the text symbols are images, not tokenized text. The text-vs-squares F1 gap ranges from 34 to 54 points across models, demonstrating that VLMs behave as if they possess a high-fidelity text-recognition pathway for spatial reasoning that dramatically outperforms their native visual pathway. Each model exhibits a distinct failure mode in the squares condition -- systematic under-counting (Claude), massive over-counting (ChatGPT), and template hallucination (Gemini) -- but all share the same underlying deficit: severely degraded spatial localization for non-textual visual elements.",
        "authors_display": "Yuval Levental Team",
        "pdf_url": "http://arxiv.org/abs/2602.15950",
        "code_url": null,
        "date": "2026-02-17",
        "primary_category": "cs.CV",
        "chinese_summary": "针对视觉-语言模型（VLMs）在二进制网格中无法准确本地化缺乏文本标识的填充单元格这一基本限制，本研究通过实验进行了探究。研究生成了两种类型的15x15网格图像（文本符号和无网格线填充方块），并要求三个前沿VLM进行转录。实验结果显示，在文本符号条件下，VLM表现良好（高准确率和F1分数），但在填充方块条件下，所有模型的准确率和F1分数均大幅下降。这表明，VLM似乎拥有一个高保真的文本识别路径用于空间推理，其性能远优于其原生视觉路径。每个模型在处理非文本视觉元素时均表现出严重的空间定位退化，且各有不同的失败模式（如欠计数、过计数和模板幻觉）。"
      },
      {
        "paper_id": "2602.15927",
        "title": "Visual Memory Injection Attacks for Multi-Turn Conversations",
        "abstract": "Generative large vision-language models (LVLMs) have recently achieved impressive performance gains, and their user base is growing rapidly. However, the security of LVLMs, in particular in a long-context multi-turn setting, is largely underexplored. In this paper, we consider the realistic scenario in which an attacker uploads a manipulated image to the web/social media. A benign user downloads this image and uses it as input to the LVLM. Our novel stealthy Visual Memory Injection (VMI) attack is designed such that on normal prompts the LVLM exhibits nominal behavior, but once the user gives a triggering prompt, the LVLM outputs a specific prescribed target message to manipulate the user, e.g. for adversarial marketing or political persuasion. Compared to previous work that focused on single-turn attacks, VMI is effective even after a long multi-turn conversation with the user. We demonstrate our attack on several recent open-weight LVLMs. This article thereby shows that large-scale manipulation of users is feasible with perturbed images in multi-turn conversation settings, calling for better robustness of LVLMs against these attacks. We release the source code at https://github.com/chs20/visual-memory-injection",
        "authors_display": "Matthias Hein Team",
        "pdf_url": "http://arxiv.org/abs/2602.15927",
        "code_url": null,
        "date": "2026-02-17",
        "primary_category": "cs.CV",
        "chinese_summary": "鉴于大型生成式视觉-语言模型（LVLM）在长上下文多轮对话中安全性的研究不足，本论文提出了一种名为视觉记忆注入（VMI）的隐蔽攻击。该攻击通过操纵图像，使得LVLM在正常提示下行为正常，但在触发提示下输出预设的目标信息以操纵用户。实验证明，VMI攻击在多轮对话中依然有效，并在多个开源LVLM上得到验证，揭示了通过扰动图像进行大规模用户操纵的可行性，强调了提高LVLM对抗此类攻击的鲁棒性需求。"
      },
      {
        "paper_id": "2602.15650",
        "title": "Concept-Enhanced Multimodal RAG: Towards Interpretable and Accurate Radiology Report Generation",
        "abstract": "Radiology Report Generation (RRG) through Vision-Language Models (VLMs) promises to reduce documentation burden, improve reporting consistency, and accelerate clinical workflows. However, their clinical adoption remains limited by the lack of interpretability and the tendency to hallucinate findings misaligned with imaging evidence. Existing research typically treats interpretability and accuracy as separate objectives, with concept-based explainability techniques focusing primarily on transparency, while Retrieval-Augmented Generation (RAG) methods targeting factual grounding through external retrieval. We present Concept-Enhanced Multimodal RAG (CEMRAG), a unified framework that decomposes visual representations into interpretable clinical concepts and integrates them with multimodal RAG. This approach exploits enriched contextual prompts for RRG, improving both interpretability and factual accuracy. Experiments on MIMIC-CXR and IU X-Ray across multiple VLM architectures, training regimes, and retrieval configurations demonstrate consistent improvements over both conventional RAG and concept-only baselines on clinical accuracy metrics and standard NLP measures. These results challenge the assumed trade-off between interpretability and performance, showing that transparent visual concepts can enhance rather than compromise diagnostic accuracy in medical VLMs. Our modular design decomposes interpretability into visual transparency and structured language model conditioning, providing a principled pathway toward clinically trustworthy AI-assisted radiology.",
        "authors_display": "Valerio Guarrasi Team",
        "pdf_url": "http://arxiv.org/abs/2602.15650",
        "code_url": null,
        "date": "2026-02-17",
        "primary_category": "cs.CV",
        "chinese_summary": "针对放射学报告生成（RRG）中视觉-语言模型（VLM）存在的解释性差和易产生“幻觉”的问题，本研究提出了概念增强多模态检索增强生成（CEMRAG）框架。该框架将视觉表示分解为可解释的临床概念，并将其与多模态RAG结合，通过丰富的上下文提示提升RRG的解释性和事实准确性。实验结果表明，CEMRAG在多个数据集和VLM架构下，在临床准确性和NLP指标上均优于现有基线，挑战了可解释性与性能之间的权衡假设，为医疗VLM提供了可信赖的AI辅助放射学途径。"
      },
      {
        "paper_id": "2602.15645",
        "title": "CARE Drive A Framework for Evaluating Reason-Responsiveness of Vision Language Models in Automated Driving",
        "abstract": "Foundation models, including vision language models, are increasingly used in automated driving to interpret scenes, recommend actions, and generate natural language explanations. However, existing evaluation methods primarily assess outcome based performance, such as safety and trajectory accuracy, without determining whether model decisions reflect human relevant considerations. As a result, it remains unclear whether explanations produced by such models correspond to genuine reason responsive decision making or merely post hoc rationalizations. This limitation is especially significant in safety critical domains because it can create false confidence. To address this gap, we propose CARE Drive, Context Aware Reasons Evaluation for Driving, a model agnostic framework for evaluating reason responsiveness in vision language models applied to automated driving. CARE Drive compares baseline and reason augmented model decisions under controlled contextual variation to assess whether human reasons causally influence decision behavior. The framework employs a two stage evaluation process. Prompt calibration ensures stable outputs. Systematic contextual perturbation then measures decision sensitivity to human reasons such as safety margins, social pressure, and efficiency constraints. We demonstrate CARE Drive in a cyclist overtaking scenario involving competing normative considerations. Results show that explicit human reasons significantly influence model decisions, improving alignment with expert recommended behavior. However, responsiveness varies across contextual factors, indicating uneven sensitivity to different types of reasons. These findings provide empirical evidence that reason responsiveness in foundation models can be systematically evaluated without modifying model parameters.",
        "authors_display": "Arkady Zgonnikov Team",
        "pdf_url": "http://arxiv.org/abs/2602.15645",
        "code_url": null,
        "date": "2026-02-17",
        "primary_category": "cs.AI",
        "chinese_summary": "针对自动驾驶中基础模型评估主要关注结果性能而忽视决策是否反映人类相关考量的问题，本研究提出了CARE Drive框架。该框架独立于模型，通过在受控上下文变化下比较基线和原因增强模型决策，评估人类原因对决策行为的因果影响。在一个骑车人超车场景中，结果显示明确的人类原因显著影响模型决策，提高了与专家推荐行为的一致性，证明了可以在不修改模型参数的情况下系统评估基础模型的原因响应性。"
      },
      {
        "paper_id": "2602.15591",
        "title": "Req2Road: A GenAI Pipeline for SDV Test Artifact Generation and On-Vehicle Execution",
        "abstract": "Testing functionality in Software-Defined Vehicles is challenging because requirements are written in natural language, specifications combine text, tables, and diagrams, while test assets are scattered across heterogeneous toolchains. Large Language Models and Vision-Language Models are used to extract signals and behavioral logic to automatically generate Gherkin scenarios, which are then converted into runnable test scripts. The Vehicle Signal Specification (VSS) integration standardizes signal references, supporting portability across subsystems and test benches. The pipeline uses retrieval-augmented generation to preselect candidate VSS signals before mapping. We evaluate the approach on the safety-relevant Child Presence Detection System, executing the generated tests in a virtual environment and on an actual vehicle. Our evaluation covers Gherkin validity, VSS mapping quality, and end-to-end executability. Results show that 32 of 36 requirements (89\\%) can be transformed into executable scenarios in our setting, while human review and targeted substitutions remain necessary. This paper is a feasibility and architectural demonstration of an end-to-end requirements-to-test pipeline for SDV subsystems, evaluated on a CPDS case in simulation and Vehicle-in-the-Loop settings.",
        "authors_display": "Alois Knoll Team",
        "pdf_url": "http://arxiv.org/abs/2602.15591",
        "code_url": null,
        "date": "2026-02-17",
        "primary_category": "cs.SE",
        "chinese_summary": "为解决软件定义汽车（SDV）功能测试中需求分散、规范复杂以及测试资产异构的挑战，本研究提出了一种自动化管道。该管道利用大型语言模型和视觉-语言模型提取信号和行为逻辑，自动生成Gherkin场景并转换为可运行的测试脚本，并通过VSS集成和检索增强生成（RAG）实现标准化。在儿童存在检测系统（CPDS）上的评估显示，89%的需求可转化为可执行场景，验证了该端到端需求-测试管道在仿真和在环测试环境中的可行性，但仍需人工审查。"
      },
      {
        "paper_id": "2602.15549",
        "title": "VLM-DEWM: Dynamic External World Model for Verifiable and Resilient Vision-Language Planning in Manufacturing",
        "abstract": "Vision-language model (VLM) shows promise for high-level planning in smart manufacturing, yet their deployment in dynamic workcells faces two critical challenges: (1) stateless operation, they cannot persistently track out-of-view states, causing world-state drift; and (2) opaque reasoning, failures are difficult to diagnose, leading to costly blind retries. This paper presents VLM-DEWM, a cognitive architecture that decouples VLM reasoning from world-state management through a persistent, queryable Dynamic External World Model (DEWM). Each VLM decision is structured into an Externalizable Reasoning Trace (ERT), comprising action proposal, world belief, and causal assumption, which is validated against DEWM before execution. When failures occur, discrepancy analysis between predicted and observed states enables targeted recovery instead of global replanning. We evaluate VLM-DEWM on multi-station assembly, large-scale facility exploration, and real-robot recovery under induced failures. Compared to baseline memory-augmented VLM systems, VLM DEWM improves state-tracking accuracy from 56% to 93%, increases recovery success rate from below 5% to 95%, and significantly reduces computational overhead through structured memory. These results establish VLM-DEWM as a verifiable and resilient solution for long-horizon robotic operations in dynamic manufacturing environments.",
        "authors_display": "Ning Ji Team",
        "pdf_url": "http://arxiv.org/abs/2602.15549",
        "code_url": null,
        "date": "2026-02-17",
        "primary_category": "cs.RO",
        "chinese_summary": "针对视觉-语言模型（VLM）在智能制造动态工作单元中面临的无状态操作和不透明推理挑战，本研究提出了VLM-DEWM认知架构。该架构通过一个持久且可查询的动态外部世界模型（DEWM），将VLM推理与世界状态管理解耦，并结构化每个VLM决策为可外部化的推理轨迹。实验结果表明，VLM-DEWM显著提高了状态跟踪准确性（从56%提升至93%）和恢复成功率（从低于5%提升至95%），并通过结构化内存减少了计算开销，为动态制造环境中的长周期机器人操作提供了可靠且弹性的解决方案。"
      },
      {
        "paper_id": "2602.15543",
        "title": "Selective Perception for Robot: Task-Aware Attention in Multimodal VLA",
        "abstract": "In robotics, Vision-Language-Action (VLA) models that integrate diverse multimodal signals from multi-view inputs have emerged as an effective approach. However, most prior work adopts static fusion that processes all visual inputs uniformly, which incurs unnecessary computational overhead and allows task-irrelevant background information to act as noise. Inspired by the principles of human active perception, we propose a dynamic information fusion framework designed to maximize the efficiency and robustness of VLA models. Our approach introduces a lightweight adaptive routing architecture that analyzes the current text prompt and observations from a wrist-mounted camera in real-time to predict the task-relevance of multiple camera views. By conditionally attenuating computations for views with low informational utility and selectively providing only essential visual features to the policy network, Our framework achieves computation efficiency proportional to task relevance. Furthermore, to efficiently secure large-scale annotation data for router training, we established an automated labeling pipeline utilizing Vision-Language Models (VLMs) to minimize data collection and annotation costs. Experimental results in real-world robotic manipulation scenarios demonstrate that the proposed approach achieves significant improvements in both inference efficiency and control performance compared to existing VLA models, validating the effectiveness and practicality of dynamic information fusion in resource-constrained, real-time robot control environments.",
        "authors_display": "Soo-Chul Lim Team",
        "pdf_url": "http://arxiv.org/abs/2602.15543",
        "code_url": null,
        "date": "2026-02-17",
        "primary_category": "cs.RO",
        "chinese_summary": "针对机器人领域中视觉-语言-动作（VLA）模型静态融合多视角输入导致的计算冗余和噪声问题，本研究提出了一种动态信息融合框架。该框架引入轻量级自适应路由架构，实时分析文本提示和腕部摄像头观测，预测多摄像头视图的任务相关性，并有选择地向策略网络提供必要视觉特征。为路由器训练，本研究还开发了利用VLM的自动化标注管道。实验结果表明，该方法在真实世界机器人操纵场景中显著提高了推理效率和控制性能，验证了动态信息融合在资源受限实时机器人控制环境中的有效性。"
      },
      {
        "paper_id": "2602.15516",
        "title": "Semantic-Guided 3D Gaussian Splatting for Transient Object Removal",
        "abstract": "Transient objects in casual multi-view captures cause ghosting artifacts in 3D Gaussian Splatting (3DGS) reconstruction. Existing solutions relied on scene decomposition at significant memory cost or on motion-based heuristics that were vulnerable to parallax ambiguity. A semantic filtering framework was proposed for category-aware transient removal using vision-language models. CLIP similarity scores between rendered views and distractor text prompts were accumulated per-Gaussian across training iterations. Gaussians exceeding a calibrated threshold underwent opacity regularization and periodic pruning. Unlike motion-based approaches, semantic classification resolved parallax ambiguity by identifying object categories independently of motion patterns. Experiments on the RobustNeRF benchmark demonstrated consistent improvement in reconstruction quality over vanilla 3DGS across four sequences, while maintaining minimal memory overhead and real-time rendering performance. Threshold calibration and comparisons with baselines validated semantic guidance as a practical strategy for transient removal in scenarios with predictable distractor categories.",
        "authors_display": "Priyesh Shukla Team",
        "pdf_url": "http://arxiv.org/abs/2602.15516",
        "code_url": null,
        "date": "2026-02-17",
        "primary_category": "cs.CV",
        "chinese_summary": "为解决3D Gaussian Splatting (3DGS) 重建中瞬态物体导致的重影伪影问题，本研究提出了一种基于视觉-语言模型（VLM）的语义过滤框架。该方法通过计算渲染视图与干扰文本提示之间的CLIP相似度得分，并累积到每个高斯函数上，对超出校准阈值的高斯函数进行不透明度正则化和周期性修剪。实验结果表明，该方法在RobustNeRF基准测试中持续改善了重建质量，同时保持了最小内存开销和实时渲染性能，通过语义分类有效解决了视差模糊问题。"
      },
      {
        "paper_id": "2602.15460",
        "title": "On the Out-of-Distribution Generalization of Reasoning in Multimodal LLMs for Simple Visual Planning Tasks",
        "abstract": "Integrating reasoning in large language models and large vision-language models has recently led to significant improvement of their capabilities. However, the generalization of reasoning models is still vaguely defined and poorly understood. In this work, we present an evaluation framework to rigorously examine how well chain-of-thought (CoT) approaches generalize on a simple planning task. Specifically, we consider a grid-based navigation task in which a model is provided with a map and must output a sequence of moves that guides a player from a start position to a goal while avoiding obstacles. The versatility of the task and its data allows us to fine-tune model variants using different input representations (visual and textual) and CoT reasoning strategies, and systematically evaluate them under both in-distribution (ID) and out-of-distribution (OOD) test conditions. Our experiments show that, while CoT reasoning improves in-distribution generalization across all representations, out-of-distribution generalization (e.g., to larger maps) remains very limited in most cases when controlling for trivial matches with the ID data. Surprisingly, we find that reasoning traces which combine multiple text formats yield the best (and non-trivial) OOD generalization. Finally, purely text-based models consistently outperform those utilizing image-based inputs, including a recently proposed approach relying on latent space reasoning.",
        "authors_display": "Francesco Croce Team",
        "pdf_url": "http://arxiv.org/abs/2602.15460",
        "code_url": null,
        "date": "2026-02-17",
        "primary_category": "cs.LG",
        "chinese_summary": "本研究旨在深入探究大型语言模型和视觉-语言模型中思维链（CoT）推理方法的泛化能力，尤其是在简单规划任务上的表现。研究者提出了一个评估框架，对基于网格的导航任务中不同输入表示和CoT策略的模型进行了微调和系统评估。实验结果表明，CoT推理能提高分布内泛化能力，但对分布外（如更大地图）的泛化能力多数情况下仍非常有限；值得注意的是，结合多种文本格式的推理轨迹产生了最佳的分布外泛化效果，且纯文本模型表现优于基于图像输入的模型。"
      },
      {
        "paper_id": "2602.15010",
        "title": "BPP: Long-Context Robot Imitation Learning by Focusing on Key History Frames",
        "abstract": "Many robot tasks require attending to the history of past observations. For example, finding an item in a room requires remembering which places have already been searched. However, the best-performing robot policies typically condition only on the current observation, limiting their applicability to such tasks. Naively conditioning on past observations often fails due to spurious correlations: policies latch onto incidental features of training histories that do not generalize to out-of-distribution trajectories upon deployment. We analyze why policies latch onto these spurious correlations and find that this problem stems from limited coverage over the space of possible histories during training, which grows exponentially with horizon. Existing regularization techniques provide inconsistent benefits across tasks, as they do not fundamentally address this coverage problem. Motivated by these findings, we propose Big Picture Policies (BPP), an approach that conditions on a minimal set of meaningful keyframes detected by a vision-language model. By projecting diverse rollouts onto a compact set of task-relevant events, BPP substantially reduces distribution shift between training and deployment, without sacrificing expressivity. We evaluate BPP on four challenging real-world manipulation tasks and three simulation tasks, all requiring history conditioning. BPP achieves 70% higher success rates than the best comparison on real-world evaluations.",
        "authors_display": "Aviral Kumar Team",
        "pdf_url": "http://arxiv.org/abs/2602.15010",
        "code_url": null,
        "date": "2026-02-16",
        "primary_category": "cs.RO",
        "chinese_summary": "在机器人任务中，传统策略因仅依赖当前观测而无法有效利用历史信息，导致在需要记忆的任务中泛化性差，尤其是在部署时容易受训练中虚假关联的影响。为解决此问题，本研究提出大图策略（BPP），通过视觉-语言模型识别并利用一组最小的关键帧作为历史观测条件。实验结果表明，BPP显著减少了训练与部署间的分布偏移，并在四项真实世界操作任务和三项仿真任务中，成功率比现有最佳方法提高了70%。"
      },
      {
        "paper_id": "2602.14989",
        "title": "ThermEval: A Structured Benchmark for Evaluation of Vision-Language Models on Thermal Imagery",
        "abstract": "Vision language models (VLMs) achieve strong performance on RGB imagery, but they do not generalize to thermal images. Thermal sensing plays a critical role in settings where visible light fails, including nighttime surveillance, search and rescue, autonomous driving, and medical screening. Unlike RGB imagery, thermal images encode physical temperature rather than color or texture, requiring perceptual and reasoning capabilities that existing RGB-centric benchmarks do not evaluate. We introduce ThermEval-B, a structured benchmark of approximately 55,000 thermal visual question answering pairs designed to assess the foundational primitives required for thermal vision language understanding. ThermEval-B integrates public datasets with our newly collected ThermEval-D, the first dataset to provide dense per-pixel temperature maps with semantic body-part annotations across diverse indoor and outdoor environments. Evaluating 25 open-source and closed-source VLMs, we find that models consistently fail at temperature-grounded reasoning, degrade under colormap transformations, and default to language priors or fixed responses, with only marginal gains from prompting or supervised fine-tuning. These results demonstrate that thermal understanding requires dedicated evaluation beyond RGB-centric assumptions, positioning ThermEval as a benchmark to drive progress in thermal vision language modeling.",
        "authors_display": "Nipun Batra Team",
        "pdf_url": "http://arxiv.org/abs/2602.14989",
        "code_url": null,
        "date": "2026-02-16",
        "primary_category": "cs.CV",
        "chinese_summary": "当前视觉语言模型（VLMs）在RGB图像上表现优异，但无法泛化至夜间监控、搜救等关键场景的热成像图像，且现有基准无法评估其对温度感知和推理能力。本研究引入ThermEval-B，一个包含5.5万个热视觉问答对的结构化基准，并整合了ThermEval-D数据集，首次提供带有语义身体部位标注的密集逐像素温度图。实验评估25个VLM后发现，模型在温度推理上普遍失败，在色图变换下性能下降，且容易依赖语言先验，提示或微调仅带来微弱提升，证实了热图像理解需专门评估。 "
      },
      {
        "paper_id": "2602.14974",
        "title": "DM0: An Embodied-Native Vision-Language-Action Model towards Physical AI",
        "abstract": "Moving beyond the traditional paradigm of adapting internet-pretrained models to physical tasks, we present DM0, an Embodied-Native Vision-Language-Action (VLA) framework designed for Physical AI. Unlike approaches that treat physical grounding as a fine-tuning afterthought, DM0 unifies embodied manipulation and navigation by learning from heterogeneous data sources from the onset. Our methodology follows a comprehensive three-stage pipeline: Pretraining, Mid-Training, and Post-Training. First, we conduct large-scale unified pretraining on the Vision-Language Model (VLM) using diverse corpora--seamlessly integrating web text, autonomous driving scenarios, and embodied interaction logs-to jointly acquire semantic knowledge and physical priors. Subsequently, we build a flow-matching action expert atop the VLM. To reconcile high-level reasoning with low-level control, DM0 employs a hybrid training strategy: for embodied data, gradients from the action expert are not backpropagated to the VLM to preserve generalized representations, while the VLM remains trainable on non-embodied data. Furthermore, we introduce an Embodied Spatial Scaffolding strategy to construct spatial Chain-of-Thought (CoT) reasoning, effectively constraining the action solution space. Experiments on the RoboChallenge benchmark demonstrate that DM0 achieves state-of-the-art performance in both Specialist and Generalist settings on Table30.",
        "authors_display": "Tiancai Wang Team",
        "pdf_url": "http://arxiv.org/abs/2602.14974",
        "code_url": "https://github.com/Dexmal/dexbotic",
        "date": "2026-02-16",
        "primary_category": "cs.RO",
        "chinese_summary": "针对传统方法将互联网预训练模型适配到物理任务的局限性，本研究提出了DM0，一个具身原生（Embodied-Native）的视觉-语言-动作（VLA）框架，旨在为物理AI提供统一的具身操作和导航能力。该框架采用三阶段训练流水线：首先对VLM进行大规模统一预训练，整合网络文本、自动驾驶和具身交互日志数据，随后构建流匹配动作专家，并通过混合训练策略及具身空间支架策略实现高层推理与低层控制的协调。DM0在RoboChallenge基准测试中，于专业和通用设置下均取得了最先进的性能。"
      },
      {
        "paper_id": "2602.14589",
        "title": "MATEO: A Multimodal Benchmark for Temporal Reasoning and Planning in LVLMs",
        "abstract": "AI agents need to plan to achieve complex goals that involve orchestrating perception, sub-goal decomposition, and execution. These plans consist of ordered steps structured according to a Temporal Execution Order (TEO, a directed acyclic graph that ensures each step executes only after its preconditions are satisfied. Existing research on foundational models' understanding of temporal execution is limited to automatically derived annotations, approximations of the TEO as a linear chain, or text-only inputs. To address this gap, we introduce MATEO (MultimodAl Temporal Execution Order), a benchmark designed to assess and improve the temporal reasoning abilities of Large Vision Language Models (LVLMs) required for real-world planning. We acquire a high-quality professional multimodal recipe corpus, authored through a standardized editorial process that decomposes instructions into discrete steps, each paired with corresponding images. We collect TEO annotations as graphs by designing and using a scalable crowdsourcing pipeline. Using MATEO, we evaluate six state-of-the-art LVLMs across model scales, varying language context, multimodal input structure, and fine-tuning strategies.",
        "authors_display": "Giuseppe Riccardi Team",
        "pdf_url": "http://arxiv.org/abs/2602.14589",
        "code_url": null,
        "date": "2026-02-16",
        "primary_category": "cs.AI",
        "chinese_summary": "AI智能体实现复杂目标需要精细规划，其中时间执行顺序（TEO）至关重要，但现有基础模型对TEO的理解研究不足，多限于线性近似或纯文本输入。为弥补这一空白，本研究引入MATEO基准，旨在评估和提升大型视觉语言模型（LVLMs）的时间推理能力。通过收集高质量多模态菜谱语料并利用众包构建TEO图谱，MATEO提供了丰富的标注。对六个主流LVLMs的评估揭示了语言上下文、多模态输入结构和微调策略对时间推理能力的关键影响，突显了当前LVLMs在该领域的局限性。"
      },
      {
        "paper_id": "2602.14551",
        "title": "Replanning Human-Robot Collaborative Tasks with Vision-Language Models via Semantic and Physical Dual-Correction",
        "abstract": "Human-Robot Collaboration (HRC) plays an important role in assembly tasks by enabling robots to plan and adjust their motions based on interactive, real-time human instructions. However, such instructions are often linguistically ambiguous and underspecified, making it difficult to generate physically feasible and cooperative robot behaviors. To address this challenge, many studies have applied Vision-Language Models (VLMs) to interpret high-level instructions and generate corresponding actions. Nevertheless, VLM-based approaches still suffer from hallucinated reasoning and an inability to anticipate physical execution failures. To address these challenges, we propose an HRC framework that augments a VLM-based reasoning with a dual-correction mechanism: an internal correction model that verifies logical consistency and task feasibility prior to action execution, and an external correction model that detects and rectifies physical failures through post-execution feedback. Simulation ablation studies demonstrate that the proposed method improves the success rate compared to baselines without correction models. Our real-world experiments in collaborative assembly tasks supported by object fixation or tool preparation by an upper body humanoid robot further confirm the framewor's effectiveness in enabling interactive replanning across different collaborative tasks in response to human instructions, validating its practical feasibility.",
        "authors_display": "Kensuke Harada Team",
        "pdf_url": "http://arxiv.org/abs/2602.14551",
        "code_url": null,
        "date": "2026-02-16",
        "primary_category": "cs.RO",
        "chinese_summary": "人机协作（HRC）在装配任务中面临人类指令模糊导致机器人行为不可靠的问题，现有基于VLM的方法虽能解释指令，但易产生幻觉推理和物理执行失败。为此，本研究提出了一个HRC框架，通过引入双重校正机制增强VLM的推理能力。该机制包含一个内部校正模型在执行前验证逻辑和可行性，以及一个外部校正模型通过事后反馈纠正物理失败。仿真和真实世界实验均表明，该框架显著提高了任务成功率，并能有效支持机器人根据人类指令进行交互式重规划。"
      },
      {
        "paper_id": "2602.14524",
        "title": "Error Patterns in Historical OCR: A Comparative Analysis of TrOCR and a Vision-Language Model",
        "abstract": "Optical Character Recognition (OCR) of eighteenth-century printed texts remains challenging due to degraded print quality, archaic glyphs, and non-standardized orthography. Although transformer-based OCR systems and Vision-Language Models (VLMs) achieve strong aggregate accuracy, metrics such as Character Error Rate (CER) and Word Error Rate (WER) provide limited insight into their reliability for scholarly use. We compare a dedicated OCR transformer (TrOCR) and a general-purpose Vision-Language Model (Qwen) on line-level historical English texts using length-weighted accuracy metrics and hypothesis driven error analysis.   While Qwen achieves lower CER/WER and greater robustness to degraded input, it exhibits selective linguistic regularization and orthographic normalization that may silently alter historically meaningful forms. TrOCR preserves orthographic fidelity more consistently but is more prone to cascading error propagation. Our findings show that architectural inductive biases shape OCR error structure in systematic ways. Models with similar aggregate accuracy can differ substantially in error locality, detectability, and downstream scholarly risk, underscoring the need for architecture-aware evaluation in historical digitization workflows.",
        "authors_display": "Mikko Tolonen Team",
        "pdf_url": "http://arxiv.org/abs/2602.14524",
        "code_url": null,
        "date": "2026-02-16",
        "primary_category": "cs.CV",
        "chinese_summary": "18世纪印刷文本的OCR因降级打印、古体字和非标准化拼写而充满挑战，现有基于Transformer和VLM的OCR系统虽总体准确率高，但CER和WER等指标对学术使用可靠性洞察有限。本研究通过比较专用OCR Transformer（TrOCR）和通用VLM（Qwen）在历史英文文本线级识别上的表现，发现Qwen虽在总体准确率和鲁棒性方面占优，但存在选择性语言正则化和拼写规范化，可能改变历史原貌。TrOCR则能更好地保持拼写忠实度，但易出现级联错误。研究强调了架构归纳偏差对OCR错误结构的影响，以及在历史文献数字化中进行架构感知评估的必要性。"
      },
      {
        "paper_id": "2602.14432",
        "title": "S2D: Selective Spectral Decay for Quantization-Friendly Conditioning of Neural Activations",
        "abstract": "Activation outliers in large-scale transformer models pose a fundamental challenge to model quantization, creating excessively large ranges that cause severe accuracy drops during quantization. We empirically observe that outlier severity intensifies with pre-training scale (e.g., progressing from CLIP to the more extensively trained SigLIP and SigLIP2). Through theoretical analysis as well as empirical correlation studies, we establish the direct link between these activation outliers and dominant singular values of the weights. Building on this insight, we propose Selective Spectral Decay ($S^2D$), a geometrically-principled conditioning method that surgically regularizes only the weight components corresponding to the largest singular values during fine-tuning. Through extensive experiments, we demonstrate that $S^2D$ significantly reduces activation outliers and produces well-conditioned representations that are inherently quantization-friendly. Models trained with $S^2D$ achieve up to 7% improved PTQ accuracy on ImageNet under W4A4 quantization and 4% gains when combined with QAT. These improvements also generalize across downstream tasks and vision-language models, enabling the scaling of increasingly large and rigorously trained models without sacrificing deployment efficiency.",
        "authors_display": "Deepak Gupta Team",
        "pdf_url": "http://arxiv.org/abs/2602.14432",
        "code_url": null,
        "date": "2026-02-16",
        "primary_category": "cs.LG",
        "chinese_summary": "大规模Transformer模型中的激活异常值对模型量化构成严重挑战，导致量化精度显著下降，且随预训练规模的增加而加剧。本研究通过理论分析和实证观察，揭示了激活异常值与权重的主奇异值之间的直接联系。在此基础上，提出选择性谱衰减（S²D）方法，在微调阶段仅对最大奇异值对应的权重分量进行正则化。实验证明S²D显著减少了激活异常值，使得模型在W4A4量化下PTQ精度提升高达7%，结合QAT时提升4%，且泛化至下游任务和视觉-语言模型，提高了大规模模型的部署效率。"
      },
      {
        "paper_id": "2602.14399",
        "title": "Multi-Turn Adaptive Prompting Attack on Large Vision-Language Models",
        "abstract": "Multi-turn jailbreak attacks are effective against text-only large language models (LLMs) by gradually introducing malicious content across turns. When extended to large vision-language models (LVLMs), we find that naively adding visual inputs can cause existing multi-turn jailbreaks to be easily defended. For example, overly malicious visual input will easily trigger the defense mechanism of safety-aligned LVLMs, making the response more conservative. To address this, we propose MAPA: a multi-turn adaptive prompting attack that 1) at each turn, alternates text-vision attack actions to elicit the most malicious response; and 2) across turns, adjusts the attack trajectory through iterative back-and-forth refinement to gradually amplify response maliciousness. This two-level design enables MAPA to consistently outperform state-of-the-art methods, improving attack success rates by 11-35% on recent benchmarks against LLaVA-V1.6-Mistral-7B, Qwen2.5-VL-7B-Instruct, Llama-3.2-Vision-11B-Instruct and GPT-4o-mini.",
        "authors_display": "Yiliao Song Team",
        "pdf_url": "http://arxiv.org/abs/2602.14399",
        "code_url": null,
        "date": "2026-02-16",
        "primary_category": "cs.CV",
        "chinese_summary": "针对多轮越狱攻击在大型视觉-语言模型（LVLMs）中因视觉输入过于恶意而易被防御的问题，本研究提出了MAPA（Multi-turn Adaptive Prompting Attack）方法。MAPA在每个回合中交替使用文本和视觉攻击动作以引发最恶意响应，并在跨回合中通过迭代式来回调整攻击轨迹，逐步放大响应的恶意性。这种双层设计使MAPA持续优于现有最先进方法，在对抗Llava-V1.6-Mistral-7B、Qwen2.5-VL-7B-Instruct、Llama-3.2-Vision-11B-Instruct和GPT-4o-mini等模型的最新基准测试中，攻击成功率提高了11-35%。"
      },
      {
        "paper_id": "2602.14276",
        "title": "Moving Beyond Sparse Grounding with Complete Screen Parsing Supervision",
        "abstract": "Modern computer-use agents (CUA) must perceive a screen as a structured state, what elements are visible, where they are, and what text they contain, before they can reliably ground instructions and act. Yet, most available grounding datasets provide sparse supervision, with insufficient and low-diversity labels that annotate only a small subset of task-relevant elements per screen, which limits both coverage and generalization; moreover, practical deployment requires efficiency to enable low-latency, on-device use. We introduce ScreenParse, a large-scale dataset for complete screen parsing, with dense annotations of all visible UI elements (boxes, 55-class types, and text) across 771K web screenshots (21M elements). ScreenParse is generated by Webshot, an automated, scalable pipeline that renders diverse urls, extracts annotations and applies VLM-based relabeling and quality filtering. Using ScreenParse, we train ScreenVLM, a compact, 316M-parameter vision language model (VLM) that decodes a compact ScreenTag markup representation with a structure-aware loss that upweights structure-critical tokens. ScreenVLM substantially outperforms much larger foundation VLMs on dense parsing (e.g., 0.592 vs. 0.294 PageIoU on ScreenParse) and shows strong transfer to public benchmarks. Moreover, finetuning foundation VLMs on ScreenParse consistently improves their grounding performance, suggesting that dense screen supervision provides transferable structural priors for UI understanding. Project page: https://saidgurbuz.github.io/screenparse/.",
        "authors_display": "Peter Staar Team",
        "pdf_url": "http://arxiv.org/abs/2602.14276",
        "code_url": null,
        "date": "2026-02-15",
        "primary_category": "cs.CV",
        "chinese_summary": "现代计算机使用智能体（CUA）需要对屏幕进行结构化感知才能可靠地理解指令和执行操作，但现有标注数据集稀疏且多样性不足，限制了其泛化能力，且实际部署需要高效率。本研究引入ScreenParse，一个用于完整屏幕解析的大规模数据集，包含771K个网页截图的密集标注。通过Webshot自动化流程及VLM辅助，ScreenParse提供了详尽的UI元素信息。基于此数据集，我们训练了紧凑型ScreenVLM，其在密集解析上显著优于更大规模的基础VLM，并在公共基准上展现了强大的迁移能力，证明了密集屏幕监督能为UI理解提供可迁移的结构先验。"
      },
      {
        "paper_id": "2602.14236",
        "title": "Dual-Signal Adaptive KV-Cache Optimization for Long-Form Video Understanding in Vision-Language Models",
        "abstract": "Vision-Language Models (VLMs) face a critical memory bottleneck when processing long-form video content due to the linear growth of the Key-Value (KV) cache with sequence length. Existing solutions predominantly employ reactive eviction strategies that compute full attention matrices before discarding tokens, resulting in substantial computational waste. We propose Sali-Cache, a novel a priori optimization framework that implements dual-signal adaptive caching through proactive memory management. By integrating a temporal filter based on optical flow analysis for detecting inter-frame redundancy and a spatial filter leveraging saliency detection for identifying visually significant regions, Sali-Cache intelligently manages memory allocation before entering computationally expensive attention operations. Experimental evaluation on the LLaVA 1.6 architecture demonstrates that our method achieves a 2.20x compression ratio in effective memory usage while maintaining 100% accuracy across BLEU, ROUGE-L, and Exact Match metrics. Furthermore, under identical memory budget constraints, Sali-Cache preserves context-rich features over extended temporal durations without degrading model performance, enabling efficient processing of long-form video content on consumer-grade hardware.",
        "authors_display": "Priyesh Shukla Team",
        "pdf_url": "http://arxiv.org/abs/2602.14236",
        "code_url": null,
        "date": "2026-02-15",
        "primary_category": "cs.CV",
        "chinese_summary": "视觉-语言模型（VLMs）在处理长视频时面临内存瓶颈，因KV缓存随序列长度线性增长，而现有驱逐策略先计算完整注意力矩阵再丢弃token，导致计算浪费。本研究提出Sali-Cache，一个先验优化框架，通过主动内存管理实现双信号自适应缓存。该方法结合光流分析的时间滤波器和显著性检测的空间滤波器，在注意力操作前智能管理内存。实验表明，Sali-Cache在LLaVA 1.6架构上实现了2.20倍的内存压缩比，同时保持100%的准确率，并在相同内存预算下能更长时间保留上下文特征，实现了在消费级硬件上高效处理长视频内容。"
      },
      {
        "paper_id": "2602.14162",
        "title": "Index Light, Reason Deep: Deferred Visual Ingestion for Visual-Dense Document Question Answering",
        "abstract": "Existing multimodal document question answering methods universally adopt a supply-side ingestion strategy: running a Vision-Language Model (VLM) on every page during indexing to generate comprehensive descriptions, then answering questions through text retrieval. However, this \"pre-ingestion\" approach is costly (a 113-page engineering drawing package requires approximately 80,000 VLM tokens), end-to-end unreliable (VLM outputs may fail to be correctly retrieved due to format mismatches in the retrieval infrastructure), and irrecoverable once it fails. This paper proposes the Deferred Visual Ingestion (DVI) framework, adopting a demand-side ingestion strategy: the indexing phase performs only lightweight metadata extraction, deferring visual understanding to the moment users pose specific questions. DVI's core principle is \"Index for locating, not understanding\"--achieving page localization through structured metadata indexes and BM25 full-text search, then sending original images along with specific questions to a VLM for targeted analysis. Experiments on two real industrial engineering drawings (113 pages + 7 pages) demonstrate that DVI achieves comparable overall accuracy at zero ingestion VLM cost (46.7% vs. 48.9%), an effectiveness rate of 50% on visually necessary queries (vs. 0% for pre-ingestion), and 100% page localization (98% search space compression). DVI also supports interactive refinement and progressive caching, transforming the \"QA accuracy\" problem into a \"page localization\" problem--once the correct drawing page is found, obtaining the answer becomes a matter of interaction rounds.",
        "authors_display": "Tao Xu Team",
        "pdf_url": "http://arxiv.org/abs/2602.14162",
        "code_url": null,
        "date": "2026-02-15",
        "primary_category": "cs.CL",
        "chinese_summary": "当前多模态文档问答系统采用“供给侧摄取”策略，即在索引阶段全面生成视觉描述，导致成本高昂且不可靠。本文提出了“延迟视觉摄取（DVI）”框架，该框架采用“需求侧摄取”策略，仅在索引阶段进行轻量级元数据提取以实现页面定位，将视觉理解延迟到用户提问时，再将原始图像与特定问题发送给视觉语言模型进行分析。实验结果表明，DVI在零摄取视觉语言模型成本下，取得了与现有方法相当的整体准确率（46.7% vs 48.9%），在视觉必要查询上的有效率达50%，并实现了100%的页面定位率，有效将问答准确率问题转化为页面定位问题。"
      },
      {
        "paper_id": "2602.14073",
        "title": "Annotation-Efficient Vision-Language Model Adaptation to the Polish Language Using the LLaVA Framework",
        "abstract": "Most vision-language models (VLMs) are trained on English-centric data, limiting their performance in other languages and cultural contexts. This restricts their usability for non-English-speaking users and hinders the development of multimodal systems that reflect diverse linguistic and cultural realities. In this work, we reproduce and adapt the LLaVA-Next methodology to create a set of Polish VLMs. We rely on a fully automated pipeline for translating and filtering existing multimodal datasets, and complement this with synthetic Polish data for OCR and culturally specific tasks. Despite relying almost entirely on automatic translation and minimal manual intervention to the training data, our approach yields strong results: we observe a +9.5% improvement over LLaVA-1.6-Vicuna-13B on a Polish-adapted MMBench, along with higher-quality captions in generative evaluations, as measured by human annotators in terms of linguistic correctness. These findings highlight that large-scale automated translation, combined with lightweight filtering, can effectively bootstrap high-quality multimodal models for low-resource languages. Some challenges remain, particularly in cultural coverage and evaluation. To facilitate further research, we make our models and evaluation dataset publicly available.",
        "authors_display": "Wojciech Kusa Team",
        "pdf_url": "http://arxiv.org/abs/2602.14073",
        "code_url": null,
        "date": "2026-02-15",
        "primary_category": "cs.CL",
        "chinese_summary": "现有视觉-语言模型（VLMs）多基于英语数据训练，限制了其在其他语言和文化背景下的应用。为解决此问题，本研究复现并调整了LLaVA-Next方法，通过全自动化流程翻译、过滤现有数据集并补充合成数据，构建了一套波兰语VLM。实验结果显示，该方法在波兰语改编的MMBench上相较于LLaVA-1.6-Vicuna-13B实现了9.5%的性能提升，并在生成性评估中，其生成的标题在语言正确性方面获得了更高评价，证明大规模自动化翻译结合轻量级过滤能有效为低资源语言引导高质量多模态模型。"
      },
      {
        "paper_id": "2602.13961",
        "title": "MarsRetrieval: Benchmarking Vision-Language Models for Planetary-Scale Geospatial Retrieval on Mars",
        "abstract": "Data-driven approaches like deep learning are rapidly advancing planetary science, particularly in Mars exploration. Despite recent progress, most existing benchmarks remain confined to closed-set supervised visual tasks and do not support text-guided retrieval for geospatial discovery. We introduce MarsRetrieval, a retrieval benchmark for evaluating vision-language models for Martian geospatial discovery. MarsRetrieval includes three tasks: (1) paired image-text retrieval, (2) landform retrieval, and (3) global geo-localization, covering multiple spatial scales and diverse geomorphic origins. We propose a unified retrieval-centric protocol to benchmark multimodal embedding architectures, including contrastive dual-tower encoders and generative vision-language models. Our evaluation shows MarsRetrieval is challenging: even strong foundation models often fail to capture domain-specific geomorphic distinctions. We further show that domain-specific fine-tuning is critical for generalizable geospatial discovery in planetary settings. Our code is available at https://github.com/ml-stat-Sustech/MarsRetrieval",
        "authors_display": "Hongxin Wei Team",
        "pdf_url": "http://arxiv.org/abs/2602.13961",
        "code_url": null,
        "date": "2026-02-15",
        "primary_category": "cs.CV",
        "chinese_summary": "行星科学中，现有深度学习基准多局限于监督视觉任务，不支持文本引导的地理空间发现。为此，本研究引入了MarsRetrieval，一个用于评估视觉-语言模型在火星地理空间发现能力的检索基准，包含图像-文本检索、地貌检索和全球地理定位等任务。研究提出统一的检索协议以评估多模态嵌入架构。实验结果表明MarsRetrieval极具挑战性，即使是强大的基础模型也难以捕捉领域特定的地貌区别，且领域特定微调对于行星环境中的可泛化地理空间发现至关重要。"
      },
      {
        "paper_id": "2602.13748",
        "title": "RMPL: Relation-aware Multi-task Progressive Learning with Stage-wise Training for Multimedia Event Extraction",
        "abstract": "Multimedia Event Extraction (MEE) aims to identify events and their arguments from documents that contain both text and images. It requires grounding event semantics across different modalities. Progress in MEE is limited by the lack of annotated training data. M2E2 is the only established benchmark, but it provides annotations only for evaluation. This makes direct supervised training impractical. Existing methods mainly rely on cross-modal alignment or inference-time prompting with Vision--Language Models (VLMs). These approaches do not explicitly learn structured event representations and often produce weak argument grounding in multimodal settings. To address these limitations, we propose RMPL, a Relation-aware Multi-task Progressive Learning framework for MEE under low-resource conditions. RMPL incorporates heterogeneous supervision from unimodal event extraction and multimedia relation extraction with stage-wise training. The model is first trained with a unified schema to learn shared event-centric representations across modalities. It is then fine-tuned for event mention identification and argument role extraction using mixed textual and visual data. Experiments on the M2E2 benchmark with multiple VLMs show consistent improvements across different modality settings.",
        "authors_display": "Yu Hong Team",
        "pdf_url": "http://arxiv.org/abs/2602.13748",
        "code_url": null,
        "date": "2026-02-14",
        "primary_category": "cs.CL",
        "chinese_summary": "多媒体事件抽取（MEE）受标注数据缺乏限制，现有基准M2E2仅提供评估标注，导致直接监督训练困难，现有方法未能有效学习结构化事件表示。为解决这些局限，本文提出了RMPL（Relation-aware Multi-task Progressive Learning）框架，用于低资源条件下的MEE。RMPL通过阶段式训练，整合了来自单模态事件抽取和多媒体关系抽取的异构监督，先学习事件中心表示，再进行微调。实验结果表明，在M2E2基准上，结合多个视觉语言模型的RMPL在不同模态设置下均显示出持续的性能改进。"
      },
      {
        "paper_id": "2602.13712",
        "title": "Fine-tuned Vision Language Model for Localization of Parasitic Eggs in Microscopic Images",
        "abstract": "Soil-transmitted helminth (STH) infections continuously affect a large proportion of the global population, particularly in tropical and sub-tropical regions, where access to specialized diagnostic expertise is limited. Although manual microscopic diagnosis of parasitic eggs remains the diagnostic gold standard, the approach can be labour-intensive, time-consuming, and prone to human error. This paper aims to utilize a vision language model (VLM) such as Microsoft Florence that was fine-tuned to localize all parasitic eggs within microscopic images. The preliminary results show that our localization VLM performs comparatively better than the other object detection methods, such as EfficientDet, with an mIOU of 0.94. This finding demonstrates the potential of the proposed VLM to serve as a core component of an automated framework, offering a scalable engineering solution for intelligent parasitological diagnosis.",
        "authors_display": "Nouar AlDahoul Team",
        "pdf_url": "http://arxiv.org/abs/2602.13712",
        "code_url": null,
        "date": "2026-02-14",
        "primary_category": "cs.CV",
        "chinese_summary": "土壤传播性蠕虫（STH）感染在全球范围广泛，但诊断专业知识有限，手动显微镜诊断耗时且易错。为实现自动化诊断，本文旨在利用经过微调的视觉语言模型（VLM），例如Microsoft Florence，来定位显微图像中的所有寄生虫卵。初步实验结果显示，该定位VLM的mIOU达到了0.94，优于其他目标检测方法，表明其有望成为自动化框架的核心组件，为智能寄生虫诊断提供可扩展的工程解决方案。"
      },
      {
        "paper_id": "2602.13662",
        "title": "LeafNet: A Large-Scale Dataset and Comprehensive Benchmark for Foundational Vision-Language Understanding of Plant Diseases",
        "abstract": "Foundation models and vision-language pre-training have significantly advanced Vision-Language Models (VLMs), enabling multimodal processing of visual and linguistic data. However, their application in domain-specific agricultural tasks, such as plant pathology, remains limited due to the lack of large-scale, comprehensive multimodal image--text datasets and benchmarks. To address this gap, we introduce LeafNet, a comprehensive multimodal dataset, and LeafBench, a visual question-answering benchmark developed to systematically evaluate the capabilities of VLMs in understanding plant diseases. The dataset comprises 186,000 leaf digital images spanning 97 disease classes, paired with metadata, generating 13,950 question-answer pairs spanning six critical agricultural tasks. The questions assess various aspects of plant pathology understanding, including visual symptom recognition, taxonomic relationships, and diagnostic reasoning. Benchmarking 12 state-of-the-art VLMs on our LeafBench dataset, we reveal substantial disparity in their disease understanding capabilities. Our study shows performance varies markedly across tasks: binary healthy--diseased classification exceeds 90\\% accuracy, while fine-grained pathogen and species identification remains below 65\\%. Direct comparison between vision-only models and VLMs demonstrates the critical advantage of multimodal architectures: fine-tuned VLMs outperform traditional vision models, confirming that integrating linguistic representations significantly enhances diagnostic precision. These findings highlight critical gaps in current VLMs for plant pathology applications and underscore the need for LeafBench as a rigorous framework for methodological advancement and progress evaluation toward reliable AI-assisted plant disease diagnosis. Code is available at https://github.com/EnalisUs/LeafBench.",
        "authors_display": "Luyl-Da Quach Team",
        "pdf_url": "http://arxiv.org/abs/2602.13662",
        "code_url": null,
        "date": "2026-02-14",
        "primary_category": "cs.CV",
        "chinese_summary": "基础模型和视觉-语言预训练在VLM领域取得显著进展，但在植物病理学等农业特定领域的应用受限于缺乏大规模、全面的多模态数据集和基准。为弥补此空白，本研究引入了LeafNet数据集和LeafBench视觉问答基准，涵盖97种病害的18.6万张图像和13,950个问答对。对12个先进VLM的基准测试揭示了其疾病理解能力的显著差异，二元分类准确率超90%，但细粒度识别低于65%。研究证实，多模态架构整合语言表示显著增强了诊断精度，凸显了LeafBench对VLM在植物病理学应用中方法学进展和评估的重要性。"
      },
      {
        "paper_id": "2602.13650",
        "title": "KorMedMCQA-V: A Multimodal Benchmark for Evaluating Vision-Language Models on the Korean Medical Licensing Examination",
        "abstract": "We introduce KorMedMCQA-V, a Korean medical licensing-exam-style multimodal multiple-choice question answering benchmark for evaluating vision-language models (VLMs). The dataset consists of 1,534 questions with 2,043 associated images from Korean Medical Licensing Examinations (2012-2023), with about 30% containing multiple images requiring cross-image evidence integration. Images cover clinical modalities including X-ray, computed tomography (CT), electrocardiography (ECG), ultrasound, endoscopy, and other medical visuals. We benchmark over 50 VLMs across proprietary and open-source categories-spanning general-purpose, medical-specialized, and Korean-specialized families-under a unified zero-shot evaluation protocol. The best proprietary model (Gemini-3.0-Pro) achieves 96.9% accuracy, the best open-source model (Qwen3-VL-32B-Thinking) 83.7%, and the best Korean-specialized model (VARCO-VISION-2.0-14B) only 43.2%. We further find that reasoning-oriented model variants gain up to +20 percentage points over instruction-tuned counterparts, medical domain specialization yields inconsistent gains over strong general-purpose baselines, all models degrade on multi-image questions, and performance varies notably across imaging modalities. By complementing the text-only KorMedMCQA benchmark, KorMedMCQA-V forms a unified evaluation suite for Korean medical reasoning across text-only and multimodal conditions. The dataset is available via Hugging Face Datasets: https://huggingface.co/datasets/seongsubae/KorMedMCQA-V.",
        "authors_display": "Edward Choi Team",
        "pdf_url": "http://arxiv.org/abs/2602.13650",
        "code_url": null,
        "date": "2026-02-14",
        "primary_category": "cs.CV",
        "chinese_summary": "当前视觉-语言模型（VLM）评估基准多为英语或通用领域，缺乏针对韩语医疗领域的多模态问答基准。为此，本研究引入了KorMedMCQA-V，一个韩语医学执照考试风格的多模态多项选择问答基准，包含1534个问题和2043张图像，涵盖多种临床模态，且约30%的问题需整合多图像证据。在统一零样本评估协议下，对50多个VLM的基准测试显示，最佳专有模型准确率达96.9%，最佳开源模型为83.7%，而最佳韩语专用模型仅为43.2%。研究还发现推理导向模型性能显著提升，医学领域专业化收益不一，所有模型在多图像问题上表现下降，且性能因成像模态而异。"
      },
      {
        "paper_id": "2602.13602",
        "title": "Towards Sparse Video Understanding and Reasoning",
        "abstract": "We present \\revise (\\underline{Re}asoning with \\underline{Vi}deo \\underline{S}parsity), a multi-round agent for video question answering (VQA). Instead of uniformly sampling frames, \\revise selects a small set of informative frames, maintains a summary-as-state across rounds, and stops early when confident. It supports proprietary vision-language models (VLMs) in a ``plug-and-play'' setting and enables reinforcement fine-tuning for open-source models. For fine-tuning, we introduce EAGER (Evidence-Adjusted Gain for Efficient Reasoning), an annotation-free reward with three terms: (1) Confidence gain: after new frames are added, we reward the increase in the log-odds gap between the correct option and the strongest alternative; (2) Summary sufficiency: at answer time we re-ask using only the last committed summary and reward success; (3) Correct-and-early stop: answering correctly within a small turn budget is rewarded. Across multiple VQA benchmarks, \\revise improves accuracy while reducing frames, rounds, and prompt tokens, demonstrating practical sparse video reasoning.",
        "authors_display": "Han Liu Team",
        "pdf_url": "http://arxiv.org/abs/2602.13602",
        "code_url": null,
        "date": "2026-02-14",
        "primary_category": "cs.CV",
        "chinese_summary": "现有视频问答（VQA）方法通常均匀采样视频帧，效率低下且未能有效捕捉关键信息，在多轮VQA中存在挑战。本研究提出了REVISE（Reasoning with Video Sparsity），一个多轮视频问答智能体，它选择少量信息帧，在多轮中维护摘要状态，并在有信心时提前停止。为微调开源模型，引入了EAGER（Evidence-Adjusted Gain for Efficient Reasoning）这一无标注奖励机制，包含置信度增益、摘要充分性和正确且提前停止三项。在多个VQA基准测试中，REVISE在提高准确率的同时，显著减少了帧数、轮次和提示token，展现了实用的稀疏视频推理能力。"
      },
      {
        "paper_id": "2602.13600",
        "title": "AdaVBoost: Mitigating Hallucinations in LVLMs via Token-Level Adaptive Visual Attention Boosting",
        "abstract": "Visual attention boosting has emerged as a promising direction for mitigating hallucinations in Large Vision-Language Models (LVLMs), where existing methods primarily focus on where to boost by applying a predefined scaling to the attention of method-specific visual tokens during autoregressive generation. In this paper, we identify a fundamental trade-off in these methods: a predefined scaling factor can be too weak at some generation steps, leaving hallucinations unresolved, yet too strong at others, leading to new hallucinations. Motivated by this finding, we propose AdaVBoost, a token-level visual attention boosting framework that adaptively determines how much attention to boost at each generation step. Specifically, we introduce Visual Grounding Entropy (VGE) to estimate hallucination risk, which leverages visual grounding as a complementary signal to capture evidence mismatches beyond entropy. Guided by VGE, AdaVBoost applies stronger visual attention boosting to high-risk tokens and weaker boosting to low-risk tokens, enabling token-level adaptive intervention at each generation step. Extensive experiments show that AdaVBoost significantly outperforms baseline methods across multiple LVLMs and hallucination benchmarks.",
        "authors_display": "Tianyu Pang Team",
        "pdf_url": "http://arxiv.org/abs/2602.13600",
        "code_url": null,
        "date": "2026-02-14",
        "primary_category": "cs.CV",
        "chinese_summary": "大规模视觉-语言模型（LVLMs）存在幻觉问题，现有视觉注意力增强方法通过预定义缩放缓解，但固定缩放因子可能在不同生成步骤中表现出过弱或过强的局限性。为解决此问题，本文提出了AdaVBoost，一个token级别的自适应视觉注意力增强框架，旨在每个生成步骤动态确定注意力增强的程度。该框架引入视觉接地熵（VGE）来估计幻觉风险，并根据VGE对高风险token施加强视觉注意力增强，对低风险token施加弱增强。实验结果表明，AdaVBoost在多个LVLMs和幻觉基准测试中显著优于基线方法。"
      },
      {
        "paper_id": "2602.13559",
        "title": "OpAgent: Operator Agent for Web Navigation",
        "abstract": "To fulfill user instructions, autonomous web agents must contend with the inherent complexity and volatile nature of real-world websites. Conventional paradigms predominantly rely on Supervised Fine-Tuning (SFT) or Offline Reinforcement Learning (RL) using static datasets. However, these methods suffer from severe distributional shifts, as offline trajectories fail to capture the stochastic state transitions and real-time feedback of unconstrained wide web environments. In this paper, we propose a robust Online Reinforcement Learning WebAgent, designed to optimize its policy through direct, iterative interactions with unconstrained wide websites. Our approach comprises three core innovations: 1) Hierarchical Multi-Task Fine-tuning: We curate a comprehensive mixture of datasets categorized by functional primitives -- Planning, Acting, and Grounding -- establishing a Vision-Language Model (VLM) with strong instruction-following capabilities for Web GUI tasks. 2) Online Agentic RL in the Wild: We develop an online interaction environment and fine-tune the VLM using a specialized RL pipeline. We introduce a Hybrid Reward Mechanism that combines a ground-truth-agnostic WebJudge for holistic outcome assessment with a Rule-based Decision Tree (RDT) for progress reward. This system effectively mitigates the credit assignment challenge in long-horizon navigation. Notably, our RL-enhanced model achieves a 38.1\\% success rate (pass@5) on WebArena, outperforming all existing monolithic baselines. 3) Operator Agent: We introduce a modular agentic framework, namely \\textbf{OpAgent}, orchestrating a Planner, Grounder, Reflector, and Summarizer. This synergy enables robust error recovery and self-correction, elevating the agent's performance to a new State-of-the-Art (SOTA) success rate of \\textbf{71.6\\%}.",
        "authors_display": "Peng Di Team",
        "pdf_url": "http://arxiv.org/abs/2602.13559",
        "code_url": null,
        "date": "2026-02-14",
        "primary_category": "cs.AI",
        "chinese_summary": "自主网络智能体在复杂且不稳定的真实网站环境中，面临现有SFT或离线RL方法因分布漂移而导致的性能局限。本文提出了一个强大的在线强化学习WebAgent，通过与非受限广域网站的直接迭代交互来优化策略。该方法包含分层多任务微调，建立了强大的VLM；开发了在线交互环境和RL管道，引入混合奖励机制缓解长期导航中的信用分配挑战；并提出了OpAgent模块化框架，整合规划器、接地器、反射器和总结器以实现错误恢复和自校正。实验结果显示，RL增强模型在WebArena上成功率达38.1%，而OpAgent框架进一步提升至71.6%，达到新的SOTA水平。"
      },
      {
        "paper_id": "2602.13193",
        "title": "Steerable Vision-Language-Action Policies for Embodied Reasoning and Hierarchical Control",
        "abstract": "Pretrained vision-language models (VLMs) can make semantic and visual inferences across diverse settings, providing valuable common-sense priors for robotic control. However, effectively grounding this knowledge in robot behaviors remains an open challenge. Prior methods often employ a hierarchical approach where VLMs reason over high-level commands to be executed by separate low-level policies, e.g., vision-language-action models (VLAs). The interface between VLMs and VLAs is usually natural language task instructions, which fundamentally limits how much VLM reasoning can steer low-level behavior. We thus introduce Steerable Policies: VLAs trained on rich synthetic commands at various levels of abstraction, like subtasks, motions, and grounded pixel coordinates. By improving low-level controllability, Steerable Policies can unlock pretrained knowledge in VLMs, enabling improved task generalization. We demonstrate this benefit by controlling our Steerable Policies with both a learned high-level embodied reasoner and an off-the-shelf VLM prompted to reason over command abstractions via in-context learning. Across extensive real-world manipulation experiments, these two novel methods outperform prior embodied reasoning VLAs and VLM-based hierarchical baselines, including on challenging generalization and long-horizon tasks.   Website: steerable-policies.github.io",
        "authors_display": "Sergey Levine Team",
        "pdf_url": "http://arxiv.org/abs/2602.13193",
        "code_url": null,
        "date": "2026-02-13",
        "primary_category": "cs.RO",
        "chinese_summary": "预训练视觉语言模型(VLMs)拥有丰富的常识先验知识，但在机器人控制中有效落地仍面临挑战，现有分层方法中VLM对低层行为的引导受限于自然语言接口。为此，研究者提出了“可操控策略”(Steerable Policies)，通过在不同抽象级别（如子任务、动作、像素坐标）的丰富合成指令上训练视觉语言动作模型(VLA)，以提升低层可控性并释放VLM的预训练知识。实验结果表明，无论是通过学习型高层具身推理器还是即插即用VLM控制，Steerable Policies在真实的机器人操作实验中均优于现有基线，尤其在泛化和长任务方面表现出色。"
      },
      {
        "paper_id": "2602.13041",
        "title": "Implicit-Scale 3D Reconstruction for Multi-Food Volume Estimation from Monocular Images",
        "abstract": "We present Implicit-Scale 3D Reconstruction from Monocular Multi-Food Images, a benchmark dataset designed to advance geometry-based food portion estimation in realistic dining scenarios. Existing dietary assessment methods largely rely on single-image analysis or appearance-based inference, including recent vision-language models, which lack explicit geometric reasoning and are sensitive to scale ambiguity. This benchmark reframes food portion estimation as an implicit-scale 3D reconstruction problem under monocular observations. To reflect real-world conditions, explicit physical references and metric annotations are removed; instead, contextual objects such as plates and utensils are provided, requiring algorithms to infer scale from implicit cues and prior knowledge. The dataset emphasizes multi-food scenes with diverse object geometries, frequent occlusions, and complex spatial arrangements. The benchmark was adopted as a challenge at the MetaFood 2025 Workshop, where multiple teams proposed reconstruction-based solutions. Experimental results show that while strong vision--language baselines achieve competitive performance, geometry-based reconstruction methods provide both improved accuracy and greater robustness, with the top-performing approach achieving 0.21 MAPE in volume estimation and 5.7 L1 Chamfer Distance in geometric accuracy.",
        "authors_display": "Jiangpeng He Team",
        "pdf_url": "http://arxiv.org/abs/2602.13041",
        "code_url": "https://www.kaggle.com/competitions/3d-reconstruction-from-monocular-multi-food-images/data",
        "date": "2026-02-13",
        "primary_category": "cs.CV",
        "chinese_summary": "现有膳食评估方法多依赖单图像分析或基于外观的推断，缺乏明确几何推理且对尺度模糊敏感，难以在真实用餐场景中准确估计食物份量。本研究提出了Implicit-Scale 3D Reconstruction from Monocular Multi-Food Images基准数据集，将食物份量估计重构为单目观测下的隐式尺度3D重建问题，通过餐盘、餐具等上下文线索而非显式度量来推断尺度，并着重于复杂的多食物场景。实验结果显示，几何重建方法相比强大的视觉语言基线，在准确性和鲁棒性上均有提升，最佳方法在体积估计上达到了0.21 MAPE，几何精度为5.7 L1 Chamfer Distance。"
      },
      {
        "paper_id": "2602.12957",
        "title": "Training-Free Acceleration for Document Parsing Vision-Language Model with Hierarchical Speculative Decoding",
        "abstract": "Document parsing is a fundamental task in multimodal understanding, supporting a wide range of downstream applications such as information extraction and intelligent document analysis. Benefiting from strong semantic modeling and robust generalization, VLM-based end-to-end approaches have emerged as the mainstream paradigm in recent years. However, these models often suffer from substantial inference latency, as they must auto-regressively generate long token sequences when processing long-form documents. In this work, motivated by the extremely long outputs and complex layout structures commonly found in document parsing, we propose a training-free and highly efficient acceleration method. Inspired by speculative decoding, we employ a lightweight document parsing pipeline as a draft model to predict batches of future tokens, while the more accurate VLM verifies these draft predictions in parallel. Moreover, we further exploit the layout-structured nature of documents by partitioning each page into independent regions, enabling parallel decoding of each region using the same draft-verify strategy. The final predictions are then assembled according to the natural reading order. Experimental results demonstrate the effectiveness of our approach: on the general-purpose OmniDocBench, our method provides a 2.42x lossless acceleration for the dots.ocr model, and achieves up to 4.89x acceleration on long-document parsing tasks. We will release our code to facilitate reproducibility and future research.",
        "authors_display": "Lianwen Jin Team",
        "pdf_url": "http://arxiv.org/abs/2602.12957",
        "code_url": null,
        "date": "2026-02-13",
        "primary_category": "cs.CV",
        "chinese_summary": "文档解析是多模态理解中的核心任务，但基于视觉语言模型(VLM)的端到端方法在处理长文档时，常因自回归生成长序列而导致显著的推理延迟。针对这一问题，本研究提出了一种免训练且高效的加速方法，借鉴推测解码的思想，使用轻量级文档解析流水线作为草稿模型预测未来批次token，并由更精确的VLM并行验证。此外，该方法还利用文档的布局结构将页面划分为独立区域进行并行解码。实验结果表明，该方法在通用OmniDocBench上为dots.ocr模型提供了2.42倍的无损加速，在长文档解析任务上加速高达4.89倍。"
      },
      {
        "paper_id": "2602.12942",
        "title": "HoRAMA: Holistic Reconstruction with Automated Material Assignment for Ray Tracing using NYURay",
        "abstract": "Next-generation wireless networks at upper mid-band and millimeter-wave frequencies require accurate site-specific deterministic channel propagation prediction. Wireless ray tracing (RT) provides site-specific predictions but demands high-fidelity three-dimensional (3D) environment models with material properties. Manual 3D model reconstruction achieves high accuracy but requires weeks of expert effort, creating scalability bottlenecks for large environment reconstruction. Traditional vision-based 3D reconstruction methods lack RT compatibility due to geometrically defective meshes and missing material properties. This paper presents Holistic Reconstruction with Automated Material Assignment (HoRAMA) for wireless propagation prediction using NYURay. HoRAMA generates RT-compatible 3D models from RGB video readily captured using a smartphone or low-cost portable camera, by integrating MASt3R-SLAM dense point cloud generation with vision language model-assisted material assignment. The HoRAMA 3D reconstruction method is verified by comparing NYURay RT predictions, using both manually created and HoRAMA-generated 3D models, against field measurements at 6.75 GHz and 16.95 GHz across 12 TX-RX locations in a 700 square meter factory. HoRAMA ray tracing predictions achieve a 2.28 dB RMSE for matched multipath component (MPC) power predictions, comparable to the manually created 3D model baseline (2.18 dB), while reducing 3D reconstruction time from two months to 16 hours. HoRAMA enables scalable wireless digital twin creation for RT network planning, infrastructure deployment, and beam management in 5G/6G systems, as well as eventual real-time implementation at the edge.",
        "authors_display": "Theodore S. Rappaport Team",
        "pdf_url": "http://arxiv.org/abs/2602.12942",
        "code_url": null,
        "date": "2026-02-13",
        "primary_category": "eess.SP",
        "chinese_summary": "下一代无线网络需要精确的特定站点确定性信道传播预测，而无线射线追踪(RT)依赖高精度3D环境模型和材料属性，手动建模耗时且传统视觉3D重建方法缺乏RT兼容性。为此，本研究提出了HoRAMA（Holistic Reconstruction with Automated Material Assignment）系统，该系统能利用智能手机捕获的RGB视频，结合MASt3R-SLAM的密集点云生成和视觉语言模型辅助的材料分配，自动生成RT兼容的3D模型。实验结果表明，HoRAMA的射线追踪预测在匹配多径分量功率预测方面与手动创建的3D模型基线表现相当（2.28 dB RMSE vs 2.18 dB），同时将3D重建时间从两个月缩短到16小时，显著提高了效率。"
      },
      {
        "paper_id": "2602.12877",
        "title": "RoadscapesQA: A Multitask, Multimodal Dataset for Visual Question Answering on Indian Roads",
        "abstract": "Understanding road scenes is essential for autonomous driving, as it enables systems to interpret visual surroundings to aid in effective decision-making. We present Roadscapes, a multitask multimodal dataset consisting of upto 9,000 images captured in diverse Indian driving environments, accompanied by manually verified bounding boxes. To facilitate scalable scene understanding, we employ rule-based heuristics to infer various scene attributes, which are subsequently used to generate question-answer (QA) pairs for tasks such as object grounding, reasoning, and scene understanding. The dataset includes a variety of scenes from urban and rural India, encompassing highways, service roads, village paths, and congested city streets, captured in both daytime and nighttime settings. Roadscapes has been curated to advance research on visual scene understanding in unstructured environments. In this paper, we describe the data collection and annotation process, present key dataset statistics, and provide initial baselines for image QA tasks using vision-language models.",
        "authors_display": "Jyothikamalesh S Team",
        "pdf_url": "http://arxiv.org/abs/2602.12877",
        "code_url": null,
        "date": "2026-02-13",
        "primary_category": "cs.CV",
        "chinese_summary": "理解道路场景对自动驾驶至关重要，但现有数据集可能无法充分覆盖印度复杂多样的驾驶环境。本研究推出了Roadscapes，一个多任务多模态数据集，包含多达9,000张在印度不同驾驶环境中拍摄的图像，并附带手动验证的边界框。该数据集利用基于规则的启发式方法推断场景属性，并生成用于对象定位、推理和场景理解的问答对，涵盖了印度城市和乡村的多种昼夜场景。Roadscapes旨在推动非结构化环境中视觉场景理解的研究，并提供了使用视觉语言模型进行图像问答任务的初步基线。"
      },
      {
        "paper_id": "2602.12843",
        "title": "Thinking Like a Radiologist: A Dataset for Anatomy-Guided Interleaved Vision Language Reasoning in Chest X-ray Interpretation",
        "abstract": "Radiological diagnosis is a perceptual process in which careful visual inspection and language reasoning are repeatedly interleaved. Most medical large vision language models (LVLMs) perform visual inspection only once and then rely on text-only chain-of-thought (CoT) reasoning, which operates purely in the linguistic space and is prone to hallucination. Recent methods attempt to mitigate this issue by introducing visually related coordinates, such as bounding boxes. However, these remain a pseudo-visual solution: coordinates are still text and fail to preserve rich visual details like texture and density. Motivated by the interleaved nature of radiological diagnosis, we introduce MMRad-IVL-22K, the first large-scale dataset designed for natively interleaved visual language reasoning in chest X-ray interpretation. MMRad-IVL-22K reflects a repeated cycle of reasoning and visual inspection workflow of radiologists, in which visual rationales complement textual descriptions and ground each step of the reasoning process. MMRad-IVL-22K comprises 21,994 diagnostic traces, enabling systematic scanning across 35 anatomical regions. Experimental results on advanced closed-source LVLMs demonstrate that report generation guided by multimodal CoT significantly outperforms that guided by text-only CoT in clinical accuracy and report quality (e.g., 6\\% increase in the RadGraph metric), confirming that high-fidelity interleaved vision language evidence is a non-substitutable component of reliable medical AI. Furthermore, benchmarking across seven state-of-the-art open-source LVLMs demonstrates that models fine-tuned on MMRad-IVL-22K achieve superior reasoning consistency and report quality compared with both general-purpose and medical-specific LVLMs. The project page is available at https://github.com/qiuzyc/thinking_like_a_radiologist.",
        "authors_display": "Wei Shen Team",
        "pdf_url": "http://arxiv.org/abs/2602.12843",
        "code_url": null,
        "date": "2026-02-13",
        "primary_category": "cs.CV",
        "chinese_summary": "放射学诊断涉及视觉检查与语言推理的反复交织，但现有医学大型视觉语言模型(LVLMs)多依赖纯文本思维链推理，易产生幻觉，且现有伪视觉解决方案仍缺乏丰富的视觉细节。为此，本研究提出了MMRad-IVL-22K，这是首个专为胸部X射线解读中原生交织的视觉语言推理设计的大规模数据集，反映了放射科医生反复推理和视觉检查的工作流程。实验结果表明，多模态思维链引导的报告生成在临床准确性和报告质量方面显著优于纯文本思维链（RadGraph指标提高6%），证实高保真交织视觉语言证据是可靠医疗AI不可替代的组成部分。在MMRad-IVL-22K上微调的模型在推理一致性和报告质量方面也优于通用和医学专用LVLMs。"
      },
      {
        "paper_id": "2602.12748",
        "title": "X-SYS: A Reference Architecture for Interactive Explanation Systems",
        "abstract": "The explainable AI (XAI) research community has proposed numerous technical methods, yet deploying explainability as systems remains challenging: Interactive explanation systems require both suitable algorithms and system capabilities that maintain explanation usability across repeated queries, evolving models and data, and governance constraints. We argue that operationalizing XAI requires treating explainability as an information systems problem where user interaction demands induce specific system requirements. We introduce X-SYS, a reference architecture for interactive explanation systems, that guides (X)AI researchers, developers and practitioners in connecting interactive explanation user interfaces (XUI) with system capabilities. X-SYS organizes around four quality attributes named STAR (scalability, traceability, responsiveness, and adaptability), and specifies a five-component decomposition (XUI Services, Explanation Services, Model Services, Data Services, Orchestration and Governance). It maps interaction patterns to system capabilities to decouple user interface evolution from backend computation. We implement X-SYS through SemanticLens, a system for semantic search and activation steering in vision-language models. SemanticLens demonstrates how contract-based service boundaries enable independent evolution, offline/online separation ensures responsiveness, and persistent state management supports traceability. Together, this work provides a reusable blueprint and concrete instantiation for interactive explanation systems supporting end-to-end design under operational constraints.",
        "authors_display": "Sebastian Lapuschkin Team",
        "pdf_url": "http://arxiv.org/abs/2602.12748",
        "code_url": null,
        "date": "2026-02-13",
        "primary_category": "cs.AI",
        "chinese_summary": "可解释AI (XAI) 方法虽多，但将其部署为交互式系统仍面临挑战，因其需要兼顾算法与系统能力以维持解释可用性。本研究将可解释性视为一个信息系统问题，提出了X-SYS，一个交互式解释系统的参考架构。X-SYS围绕STAR（可伸缩性、可追溯性、响应性、适应性）四个质量属性，并指定了包含XUI服务、解释服务等五个组件的分解结构，将交互模式映射到系统能力以解耦用户界面和后端计算。通过SemanticLens系统实现X-SYS，展示了其如何通过契约化服务边界实现独立演进、通过离线/在线分离确保响应性以及通过持久状态管理支持可追溯性，为在操作约束下设计交互式解释系统提供了可重用蓝图和具体实例。"
      },
      {
        "paper_id": "2602.12686",
        "title": "SignScene: Visual Sign Grounding for Mapless Navigation",
        "abstract": "Navigational signs enable humans to navigate unfamiliar environments without maps. This work studies how robots can similarly exploit signs for mapless navigation in the open world. A central challenge lies in interpreting signs: real-world signs are diverse and complex, and their abstract semantic contents need to be grounded in the local 3D scene. We formalize this as sign grounding, the problem of mapping semantic instructions on signs to corresponding scene elements and navigational actions. Recent Vision-Language Models (VLMs) offer the semantic common-sense and reasoning capabilities required for this task, but are sensitive to how spatial information is represented. We propose SignScene, a sign-centric spatial-semantic representation that captures navigation-relevant scene elements and sign information, and presents them to VLMs in a form conducive to effective reasoning. We evaluate our grounding approach on a dataset of 114 queries collected across nine diverse environment types, achieving 88% grounding accuracy and significantly outperforming baselines. Finally, we demonstrate that it enables real-world mapless navigation on a Spot robot using only signs.",
        "authors_display": "David Hsu Team",
        "pdf_url": "http://arxiv.org/abs/2602.12686",
        "code_url": null,
        "date": "2026-02-13",
        "primary_category": "cs.RO",
        "chinese_summary": "导航标志能帮助人类在陌生环境中无地图导航，但机器人如何利用标志进行无地图导航是一个挑战，核心在于如何解释复杂多样的标志及其抽象语义内容，并将其与局部3D场景匹配。本研究将此形式化为“标志接地”(sign grounding)问题，即把标志上的语义指令映射到对应的场景元素和导航动作。研究者利用视觉语言模型(VLMs)的语义常识和推理能力，并提出了SignScene，一种以标志为中心的空-语义表示，旨在以利于VLM有效推理的形式呈现导航相关场景元素和标志信息。在包含九种环境类型、114个查询的数据集上，该方法实现了88%的接地准确率，显著优于基线，并成功使Spot机器人在真实世界中仅依赖标志进行无地图导航。"
      },
      {
        "paper_id": "2602.12659",
        "title": "IndicFairFace: Balanced Indian Face Dataset for Auditing and Mitigating Geographical Bias in Vision-Language Models",
        "abstract": "Vision-Language Models (VLMs) are known to inherit and amplify societal biases from their web-scale training data with Indian being particularly misrepresented. Existing fairness-aware datasets have significantly improved demographic balance across global race and gender groups, yet they continue to treat Indian as a single monolithic category. The oversimplification ignores the vast intra-national diversity across 28 states and 8 Union Territories of India and leads to representational and geographical bias. To address the limitation, we present IndicFairFace, a novel and balanced face dataset comprising 14,400 images representing geographical diversity of India. Images were sourced ethically from Wikimedia Commons and open-license web repositories and uniformly balanced across states and gender. Using IndicFairFace, we quantify intra-national geographical bias in prominent CLIP-based VLMs and reduce it using post-hoc Iterative Nullspace Projection debiasing approach. We also show that the adopted debiasing approach does not adversely impact the existing embedding space as the average drop in retrieval accuracy on benchmark datasets is less than 1.5 percent. Our work establishes IndicFairFace as the first benchmark to study geographical bias in VLMs for the Indian context.",
        "authors_display": "Jiechao Gao Team",
        "pdf_url": "http://arxiv.org/abs/2602.12659",
        "code_url": null,
        "date": "2026-02-13",
        "primary_category": "cs.CV",
        "chinese_summary": "视觉语言模型(VLMs)常从训练数据中继承并放大社会偏见，印度群体尤其被错误代表，现有公平性数据集将印度视为单一类别，忽视了其内部地理多样性。为解决这一局限，本研究提出了IndicFairFace，一个包含14,400张图像的新颖且平衡的人脸数据集，旨在代表印度的地理多样性，图像伦理获取并在各邦和性别间均匀平衡。通过IndicFairFace，研究量化了基于CLIP的VLM中存在的印度国内地理偏见，并利用后验迭代零空间投影去偏方法成功减少了这种偏见。实验证明，该去偏方法对现有嵌入空间的影响很小，基准数据集上的检索准确率平均下降不到1.5%，确立了IndicFairFace作为研究印度背景下VLM地理偏见的第一个基准。"
      },
      {
        "paper_id": "2602.12597",
        "title": "PISHYAR: A Socially Intelligent Smart Cane for Indoor Social Navigation and Multimodal Human-Robot Interaction for Visually Impaired People",
        "abstract": "This paper presents PISHYAR, a socially intelligent smart cane designed by our group to combine socially aware navigation with multimodal human-AI interaction to support both physical mobility and interactive assistance. The system consists of two components: (1) a social navigation framework implemented on a Raspberry Pi 5 that integrates real-time RGB-D perception using an OAK-D Lite camera, YOLOv8-based object detection, COMPOSER-based collective activity recognition, D* Lite dynamic path planning, and haptic feedback via vibration motors for tasks such as locating a vacant seat; and (2) an agentic multimodal LLM-VLM interaction framework that integrates speech recognition, vision language models, large language models, and text-to-speech, with dynamic routing between voice-only and vision-only modes to enable natural voice-based communication, scene description, and object localization from visual input. The system is evaluated through a combination of simulation-based tests, real-world field experiments, and user-centered studies. Results from simulated and real indoor environments demonstrate reliable obstacle avoidance and socially compliant navigation, achieving an overall system accuracy of approximately 80% under different social conditions. Group activity recognition further shows robust performance across diverse crowd scenarios. In addition, a preliminary exploratory user study with eight visually impaired and low-vision participants evaluates the agentic interaction framework through structured tasks and a UTAUT-based questionnaire reveals high acceptance and positive perceptions of usability, trust, and perceived sociability during our experiments. The results highlight the potential of PISHYAR as a multimodal assistive mobility aid that extends beyond navigation to provide socially interactive support for such users.",
        "authors_display": "Alireza Taheri Team",
        "pdf_url": "http://arxiv.org/abs/2602.12597",
        "code_url": null,
        "date": "2026-02-13",
        "primary_category": "cs.RO",
        "chinese_summary": "现有智能助行设备多侧重物理导航，但缺乏社交智能和多模态人机交互能力。本研究提出了PISHYAR，一款结合社交感知导航和多模态人机交互的智能拐杖。该系统包含社交导航框架（集成RGB-D感知、对象检测、活动识别、路径规划和触觉反馈）和代理式多模态LLM-VLM交互框架（集成语音识别、VLM、LLM和文本转语音，并支持动态模式路由）。通过仿真、真实世界实验和用户研究，PISHYAR在避障和社会顺从性导航中表现可靠，整体准确率约80%，集体活动识别稳健。初步用户研究显示，视障用户对其交互框架的可用性、信任度和感知社交性给予高度评价，突显了PISHYAR作为多模态辅助移动设备在提供社交互动支持方面的潜力。"
      },
      {
        "paper_id": "2602.12506",
        "title": "On Robustness and Chain-of-Thought Consistency of RL-Finetuned VLMs",
        "abstract": "Reinforcement learning (RL) fine-tuning has become a key technique for enhancing large language models (LLMs) on reasoning-intensive tasks, motivating its extension to vision language models (VLMs). While RL-tuned VLMs improve on visual reasoning benchmarks, they remain vulnerable to weak visual grounding, hallucinations, and over-reliance on textual cues. We show that simple, controlled textual perturbations--misleading captions or incorrect chain-of-thought (CoT) traces--cause substantial drops in robustness and confidence, and that these effects are more pronounced when CoT consistency is taken into account across open-source multimodal reasoning models. Entropy-based metrics further show that these perturbations reshape model uncertainty and probability mass on the correct option, exposing model-specific trends in miscalibration. To better understand these vulnerabilities, we further analyze RL fine-tuning dynamics and uncover an accuracy-faithfulness trade-off: fine-tuning raises benchmark accuracy, but can simultaneously erode the reliability of the accompanying CoT and its robustness to contextual shifts. Although adversarial augmentation improves robustness, it does not by itself prevent faithfulness drift. Incorporating a faithfulness-aware reward can restore alignment between answers and reasoning, but when paired with augmentation, training risks collapsing onto shortcut strategies and robustness remains elusive. Together, these findings highlight the limitations of accuracy-only evaluations and motivate training and assessment protocols that jointly emphasize correctness, robustness, and the faithfulness of visually grounded reasoning.",
        "authors_display": "Arnab Mondal Team",
        "pdf_url": "http://arxiv.org/abs/2602.12506",
        "code_url": null,
        "date": "2026-02-13",
        "primary_category": "cs.LG",
        "chinese_summary": "在视觉语言模型（VLM）中，尽管强化学习（RL）微调能提升推理任务性能，但仍面临视觉基础薄弱、幻觉和过度依赖文本线索的问题。研究发现，简单的文本扰动（如误导性描述）会显著降低模型的鲁棒性和置信度，并揭示了RL微调中存在的准确性与忠实性之间的权衡。具体来说，微调提高了基准准确性，却可能损害推理链的可靠性和模型对上下文变化的鲁棒性。这些结果强调了仅凭准确性评估的局限性，并呼吁在训练和评估中同时关注正确性、鲁棒性以及视觉基础推理的忠实性。"
      },
      {
        "paper_id": "2602.12498",
        "title": "Layer-Specific Fine-Tuning for Improved Negation Handling in Medical Vision-Language Models",
        "abstract": "Negation is a fundamental linguistic operation in clinical reporting, yet vision-language models (VLMs) frequently fail to distinguish affirmative from negated medical statements. To systematically characterize this limitation, we introduce a radiology-specific diagnostic benchmark that evaluates polarity sensitivity under controlled clinical conditions, revealing that common medical VLMs consistently confuse negated and non-negated findings. To enable learning beyond simple condition absence, we further construct a contextual clinical negation dataset that encodes structured claims and supports attribute-level negations involving location and severity. Building on these resources, we propose Negation-Aware Selective Training (NAST), an interpretability-guided adaptation method that uses causal tracing effects (CTEs) to modulate layer-wise gradient updates during fine-tuning. Rather than applying uniform learning rates, NAST scales each layer's update according to its causal contribution to negation processing, transforming mechanistic interpretability signals into a principled optimization rule. Experiments demonstrate improved discrimination of affirmative and negated clinical statements without degrading general vision-language alignment, highlighting the value of causal interpretability for targeted model adaptation in safety-critical medical settings. Code and resources are available at https://github.com/healthylaife/NAST.",
        "authors_display": "Rahmatollah Beheshti Team",
        "pdf_url": "http://arxiv.org/abs/2602.12498",
        "code_url": null,
        "date": "2026-02-13",
        "primary_category": "cs.CV",
        "chinese_summary": "由于视觉语言模型（VLM）在区分肯定和否定医疗陈述方面存在局限性，本研究引入了一个放射学诊断基准来系统评估VLM对极性的敏感性。为解决此问题，我们构建了一个包含结构化声明和属性级否定上下文临床否定数据集，并提出了一种名为否定感知选择性训练（NAST）的自适应方法。NAST利用因果追踪效应（CTE）根据各层对否定处理的因果贡献来调整梯度更新。实验结果表明，NAST在不损害通用视觉-语言对齐的情况下，显著提高了VLM对肯定和否定临床陈述的辨别能力，凸显了因果可解释性在安全关键医疗领域中进行有针对性模型适应的价值。"
      },
      {
        "paper_id": "2602.12281",
        "title": "Scaling Verification Can Be More Effective than Scaling Policy Learning for Vision-Language-Action Alignment",
        "abstract": "The long-standing vision of general-purpose robots hinges on their ability to understand and act upon natural language instructions. Vision-Language-Action (VLA) models have made remarkable progress toward this goal, yet their generated actions can still misalign with the given instructions. In this paper, we investigate test-time verification as a means to shrink the \"intention-action gap.'' We first characterize the test-time scaling law for embodied instruction following and demonstrate that jointly scaling the number of rephrased instructions and generated actions greatly increases test-time sample diversity, often recovering correct actions more efficiently than scaling each dimension independently. To capitalize on these scaling laws, we present CoVer, a contrastive verifier for vision-language-action alignment, and show that our architecture scales gracefully with additional computational resources and data. We then introduce \"boot-time compute\" and a hierarchical verification inference pipeline for VLAs. At deployment, our framework precomputes a diverse set of rephrased instructions from a Vision-Language-Model (VLM), repeatedly generates action candidates for each instruction, and then uses a verifier to select the optimal high-level prompt and low-level action chunks. Compared to scaling policy pre-training on the same data, our verification approach yields 22% gains in-distribution and 13% out-of-distribution on the SIMPLER benchmark, with a further 45% improvement in real-world experiments. On the PolaRiS benchmark, CoVer achieves 14% gains in task progress and 9% in success rate.",
        "authors_display": "Marco Pavone Team",
        "pdf_url": "http://arxiv.org/abs/2602.12281",
        "code_url": null,
        "date": "2026-02-12",
        "primary_category": "cs.RO",
        "chinese_summary": "针对视觉-语言-动作（VLA）模型在执行自然语言指令时存在的\"意图-动作差距\"问题，本研究探索了测试时验证方法。我们首先揭示了具身指令遵循的测试时标度定律，发现联合扩展复述指令和生成动作数量能更高效地恢复正确动作。为利用这些规律，我们提出了CoVer，一个对比验证器，并引入了“启动时计算”和分层验证推理管道。在部署时，该框架预先计算VLM生成的复述指令，生成动作候选项，然后使用验证器选择最优提示和动作块。实验结果表明，CoVer在SIMPLER基准上取得了显著提升（分布内22%，分布外13%），并在真实世界实验中进一步提升45%，在PolaRiS基准上任务进展提升14%，成功率提升9%。"
      },
      {
        "paper_id": "2602.12203",
        "title": "ExStrucTiny: A Benchmark for Schema-Variable Structured Information Extraction from Document Images",
        "abstract": "Enterprise documents, such as forms and reports, embed critical information for downstream applications like data archiving, automated workflows, and analytics. Although generalist Vision Language Models (VLMs) perform well on established document understanding benchmarks, their ability to conduct holistic, fine-grained structured extraction across diverse document types and flexible schemas is not well studied. Existing Key Entity Extraction (KEE), Relation Extraction (RE), and Visual Question Answering (VQA) datasets are limited by narrow entity ontologies, simple queries, or homogeneous document types, often overlooking the need for adaptable and structured extraction. To address these gaps, we introduce ExStrucTiny, a new benchmark dataset for structured Information Extraction (IE) from document images, unifying aspects of KEE, RE, and VQA. Built through a novel pipeline combining manual and synthetic human-validated samples, ExStrucTiny covers more varied document types and extraction scenarios. We analyze open and closed VLMs on this benchmark, highlighting challenges such as schema adaptation, query under-specification, and answer localization. We hope our work provides a bedrock for improving generalist models for structured IE in documents.",
        "authors_display": "Manuela Veloso Team",
        "pdf_url": "http://arxiv.org/abs/2602.12203",
        "code_url": null,
        "date": "2026-02-12",
        "primary_category": "cs.CL",
        "chinese_summary": "尽管通用视觉语言模型（VLM）在传统文档理解基准上表现良好，但它们在多样化文档类型和灵活模式下进行整体、细粒度结构化信息提取的能力仍未充分研究。现有数据集在实体本体、查询复杂度或文档类型上存在局限。为解决这些不足，本研究引入了ExStrucTiny，一个新的文档图像结构化信息提取（IE）基准数据集，它统一了关键实体提取、关系提取和视觉问答的特性。该数据集通过结合人工和合成并经过人工验证的样本构建，涵盖了更广泛的文档类型和提取场景。对开放和封闭式VLM在该基准上的分析揭示了模式适应、查询欠规范和答案定位等挑战，为未来改进通用模型在文档结构化信息提取方面的能力奠定了基础。"
      },
      {
        "paper_id": "2602.12159",
        "title": "3DGSNav: Enhancing Vision-Language Model Reasoning for Object Navigation via Active 3D Gaussian Splatting",
        "abstract": "Object navigation is a core capability of embodied intelligence, enabling an agent to locate target objects in unknown environments. Recent advances in vision-language models (VLMs) have facilitated zero-shot object navigation (ZSON). However, existing methods often rely on scene abstractions that convert environments into semantic maps or textual representations, causing high-level decision making to be constrained by the accuracy of low-level perception. In this work, we present 3DGSNav, a novel ZSON framework that embeds 3D Gaussian Splatting (3DGS) as persistent memory for VLMs to enhance spatial reasoning. Through active perception, 3DGSNav incrementally constructs a 3DGS representation of the environment, enabling trajectory-guided free-viewpoint rendering of frontier-aware first-person views. Moreover, we design structured visual prompts and integrate them with Chain-of-Thought (CoT) prompting to further improve VLM reasoning. During navigation, a real-time object detector filters potential targets, while VLM-driven active viewpoint switching performs target re-verification, ensuring efficient and reliable recognition. Extensive evaluations across multiple benchmarks and real-world experiments on a quadruped robot demonstrate that our method achieves robust and competitive performance against state-of-the-art approaches.The Project Page:https://aczheng-cai.github.io/3dgsnav.github.io/",
        "authors_display": "Xinyi Yu Team",
        "pdf_url": "http://arxiv.org/abs/2602.12159",
        "code_url": null,
        "date": "2026-02-12",
        "primary_category": "cs.RO",
        "chinese_summary": "针对零样本对象导航（ZSON）中视觉语言模型（VLM）决策受低级感知准确性限制的问题，本研究提出了3DGSNav框架。3DGSNav利用3D高斯泼溅（3DGS）作为VLM的持久记忆来增强空间推理能力。通过主动感知，该框架逐步构建环境的3DGS表示，从而实现轨迹引导的、边界感知的第一人称视角的自由视点渲染。此外，研究设计了结构化视觉提示并结合思维链（CoT）提示来进一步提升VLM的推理能力。在导航过程中，实时对象检测器用于过滤潜在目标，而VLM驱动的主动视点切换则进行目标再验证，确保高效可靠的识别。在多个基准测试和真实世界四足机器人实验中，3DGSNav展现出鲁棒且具竞争力的性能。"
      },
      {
        "paper_id": "2602.12065",
        "title": "Affordance-Graphed Task Worlds: Self-Evolving Task Generation for Scalable Embodied Learning",
        "abstract": "Training robotic policies directly in the real world is expensive and unscalable. Although generative simulation enables large-scale data synthesis, current approaches often fail to generate logically coherent long-horizon tasks and struggle with dynamic physical uncertainties due to open-loop execution. To address these challenges, we propose Affordance-Graphed Task Worlds (AGT-World), a unified framework that autonomously constructs interactive simulated environments and corresponding robot task policies based on real-world observations. Unlike methods relying on random proposals or static replication, AGT-World formalizes the task space as a structured graph, enabling the precise, hierarchical decomposition of complex goals into theoretically grounded atomic primitives. Furthermore, we introduce a Self-Evolution mechanism with hybrid feedback to autonomously refine policies, combining Vision-Language Model reasoning and geometric verification. Extensive experiments demonstrate that our method significantly outperforms in success rates and generalization, achieving a self-improving cycle of proposal, execution, and correction for scalable robot learning.",
        "authors_display": "Changshui Zhang Team",
        "pdf_url": "http://arxiv.org/abs/2602.12065",
        "code_url": null,
        "date": "2026-02-12",
        "primary_category": "cs.RO",
        "chinese_summary": "鉴于在真实世界中训练机器人策略成本高昂且现有生成模拟难以生成逻辑连贯的长周期任务并处理动态物理不确定性，本研究提出了Affordance-Graphed Task Worlds (AGT-World) 统一框架。该框架基于真实世界观测自主构建交互式模拟环境和机器人任务策略，通过将任务空间形式化为结构化图，实现复杂目标的精确分层分解，并引入了结合VLM推理和几何验证的自演化机制来优化策略。实验结果表明，该方法在成功率和泛化能力上显著优于现有方法，实现了提议、执行和修正的自我改进循环，以支持可扩展的机器人学习。"
      },
      {
        "paper_id": "2602.12002",
        "title": "Can Local Vision-Language Models improve Activity Recognition over Vision Transformers? -- Case Study on Newborn Resuscitation",
        "abstract": "Accurate documentation of newborn resuscitation is essential for quality improvement and adherence to clinical guidelines, yet remains underutilized in practice. Previous work using 3D-CNNs and Vision Transformers (ViT) has shown promising results in detecting key activities from newborn resuscitation videos, but also highlighted the challenges in recognizing such fine-grained activities. This work investigates the potential of generative AI (GenAI) methods to improve activity recognition from such videos. Specifically, we explore the use of local vision-language models (VLMs), combined with large language models (LLMs), and compare them to a supervised TimeSFormer baseline. Using a simulated dataset comprising 13.26 hours of newborn resuscitation videos, we evaluate several zero-shot VLM-based strategies and fine-tuned VLMs with classification heads, including Low-Rank Adaptation (LoRA). Our results suggest that small (local) VLMs struggle with hallucinations, but when fine-tuned with LoRA, the results reach F1 score at 0.91, surpassing the TimeSformer results of 0.70.",
        "authors_display": "Øyvind Meinich-Bache Team",
        "pdf_url": "http://arxiv.org/abs/2602.12002",
        "code_url": null,
        "date": "2026-02-12",
        "primary_category": "cs.CV",
        "chinese_summary": "鉴于新生儿复苏活动准确记录的重要性及其在实践中的不足，以及现有方法在识别细粒度活动上的挑战，本研究探索了生成式AI（GenAI）方法，特别是结合大型语言模型（LLM）的局部视觉-语言模型（VLM），以改进新生儿复苏视频中的活动识别。研究将几种零样本VLM策略和微调VLM（包括LoRA）与分类头进行评估，并与监督式TimeSFormer基线进行比较。结果表明，小型VLM虽然容易产生幻觉，但经过LoRA微调后，F1分数可达0.91，显著超越TimeSFormer的0.70。"
      },
      {
        "paper_id": "2602.11960",
        "title": "Benchmarking Vision-Language Models for French PDF-to-Markdown Conversion",
        "abstract": "This report evaluates PDF-to-Markdown conversion using recent Vision-Language Models (VLMs) on challenging French documents. Document parsing is a critical step for Retrieval-Augmented Generation (RAG) pipelines, where transcription and layout errors propagate to downstream retrieval and grounding. Existing benchmarks often emphasize English or Chinese and can over-penalize benign formatting and linearization choices (e.g., line breaks, list segmentation, alternative table renderings) that are largely irrelevant for downstream use.   We introduce a French-focused benchmark of difficult pages selected via model-disagreement sampling from a corpus of 60{,}000 documents, covering handwritten forms, complex layouts, dense tables, and graphics-rich pages. Evaluation is performed with unit-test-style checks that target concrete failure modes (text presence, reading order, and local table constraints) combined with category-specific normalization designed to discount presentation-only variance. Across 15 models, we observe substantially higher robustness for the strongest proprietary models on handwriting and forms, while several open-weights systems remain competitive on standard printed layouts.",
        "authors_display": "Nicolas Mery Team",
        "pdf_url": "http://arxiv.org/abs/2602.11960",
        "code_url": null,
        "date": "2026-02-12",
        "primary_category": "cs.CV",
        "chinese_summary": "针对PDF到Markdown转换作为RAG管道关键步骤，以及现有基准多侧重英文或中文并可能过度惩罚无关格式差异的问题，本研究引入了一个专门针对法语文档、通过模型不一致采样挑选的挑战性新基准。该基准涵盖手写表格、复杂布局、密集表格和富含图形的页面，并采用单元测试式检查和类别特定归一化进行评估，以减少无关的呈现差异。实验结果显示，在15个模型中，最强的专有模型在手写和表格处理上表现出更高的鲁棒性，同时一些开源系统在标准打印布局上仍具竞争力。"
      },
      {
        "paper_id": "2602.11957",
        "title": "Are Two LLMs Better Than One? A Student-Teacher Dual-Head LLMs Architecture for Pharmaceutical Content Optimization",
        "abstract": "Large language models (LLMs) are increasingly used to create content in regulated domains such as pharmaceuticals, where outputs must be scientifically accurate and legally compliant. Manual quality control (QC) is slow, error prone, and can become a publication bottleneck. We introduce LRBTC, a modular LLM and vision language model (VLM) driven QC architecture covering Language, Regulatory, Brand, Technical, and Content Structure checks. LRBTC combines a Student-Teacher dual model architecture, human in the loop (HITL) workflow with waterfall rule filtering to enable scalable, verifiable content validation and optimization. On AIReg-Bench, our approach achieves 83.0% F1 and 97.5% recall, reducing missed violations by 5x compared with Gemini 2.5 Pro. On CSpelling, it improves mean accuracy by 26.7%. Error analysis further reveals that while current models are strong at detecting misspellings (92.5 recall), they fail to identify complex medical grammatical (25.0 recall) and punctuation (41.7 recall) errors, highlighting a key area for future work. This work provides a practical, plug and play solution for reliable, transparent quality control of content in high stakes, compliance critical industries. We also provide access to our Demo under MIT Licenses.",
        "authors_display": "Anubhav Girdhar Team",
        "pdf_url": "http://arxiv.org/abs/2602.11957",
        "code_url": null,
        "date": "2026-02-12",
        "primary_category": "cs.LG",
        "chinese_summary": "针对大型语言模型（LLM）在制药等受监管领域内容创作中面临的科学准确性和合规性挑战，以及手动质量控制（QC）低效易错的问题，本研究引入了LRBTC，一个模块化的LLM和VLM驱动的QC架构。该架构涵盖语言、法规、品牌、技术和内容结构检查，并结合了师生双模型架构、人工干预（HITL）工作流和瀑布式规则过滤机制。实验结果表明，LRBTC在AIReg-Bench上取得了83.0%的F1和97.5%的召回率，相比Gemini 2.5 Pro，遗漏违规减少5倍；在CSpelling上平均准确率提高26.7%。误差分析揭示当前模型擅长检测拼写错误，但在复杂医学语法和标点错误识别方面仍有提升空间。"
      },
      {
        "paper_id": "2602.11862",
        "title": "LAMP: Implicit Language Map for Robot Navigation",
        "abstract": "Recent advances in vision-language models have made zero-shot navigation feasible, enabling robots to follow natural language instructions without requiring labeling. However, existing methods that explicitly store language vectors in grid or node-based maps struggle to scale to large environments due to excessive memory requirements and limited resolution for fine-grained planning. We introduce LAMP (Language Map), a novel neural language field-based navigation framework that learns a continuous, language-driven map and directly leverages it for fine-grained path generation. Unlike prior approaches, our method encodes language features as an implicit neural field rather than storing them explicitly at every location. By combining this implicit representation with a sparse graph, LAMP supports efficient coarse path planning and then performs gradient-based optimization in the learned field to refine poses near the goal. This coarse-to-fine pipeline, language-driven, gradient-guided optimization is the first application of an implicit language map for precise path generation. This refinement is particularly effective at selecting goal regions not directly observed by leveraging semantic similarities in the learned feature space. To further enhance robustness, we adopt a Bayesian framework that models embedding uncertainty via the von Mises-Fisher distribution, thereby improving generalization to unobserved regions. To scale to large environments, LAMP employs a graph sampling strategy that prioritizes spatial coverage and embedding confidence, retaining only the most informative nodes and substantially reducing computational overhead. Our experimental results, both in NVIDIA Isaac Sim and on a real multi-floor building, demonstrate that LAMP outperforms existing explicit methods in both memory efficiency and fine-grained goal-reaching accuracy.",
        "authors_display": "Sunwook Choi Team",
        "pdf_url": "http://arxiv.org/abs/2602.11862",
        "code_url": "https://lab-of-ai-and-robotics.github.io/LAMP/",
        "date": "2026-02-12",
        "primary_category": "cs.RO",
        "chinese_summary": "为解决现有视觉-语言模型（VLM）零样本导航方法在大环境中因显式存储语言向量而导致的内存需求过高和细粒度规划分辨率有限的问题，本研究提出了LAMP（Language Map）框架。该框架通过学习连续的、语言驱动的隐式神经语言场来编码语言特征，并结合稀疏图实现高效的粗略路径规划，进而通过梯度优化在学习场中细化目标姿态。该方法还采用贝叶斯框架建模嵌入不确定性以增强泛化能力，并通过图采样策略扩展到大环境。实验证明，LAMP在内存效率和细粒度目标到达精度方面均优于现有显式方法。"
      },
      {
        "paper_id": "2602.11832",
        "title": "JEPA-VLA: Video Predictive Embedding is Needed for VLA Models",
        "abstract": "Recent vision-language-action (VLA) models built upon pretrained vision-language models (VLMs) have achieved significant improvements in robotic manipulation. However, current VLAs still suffer from low sample efficiency and limited generalization. This paper argues that these limitations are closely tied to an overlooked component, pretrained visual representation, which offers insufficient knowledge on both aspects of environment understanding and policy prior. Through an in-depth analysis, we find that commonly used visual representations in VLAs, whether pretrained via language-image contrastive learning or image-based self-supervised learning, remain inadequate at capturing crucial, task-relevant environment information and at inducing effective policy priors, i.e., anticipatory knowledge of how the environment evolves under successful task execution. In contrast, we discover that predictive embeddings pretrained on videos, in particular V-JEPA 2, are adept at flexibly discarding unpredictable environment factors and encoding task-relevant temporal dynamics, thereby effectively compensating for key shortcomings of existing visual representations in VLAs. Building on these observations, we introduce JEPA-VLA, a simple yet effective approach that adaptively integrates predictive embeddings into existing VLAs. Our experiments demonstrate that JEPA-VLA yields substantial performance gains across a range of benchmarks, including LIBERO, LIBERO-plus, RoboTwin2.0, and real-robot tasks.",
        "authors_display": "Mingsheng Long Team",
        "pdf_url": "http://arxiv.org/abs/2602.11832",
        "code_url": null,
        "date": "2026-02-12",
        "primary_category": "cs.CV",
        "chinese_summary": "针对现有视觉-语言-动作（VLA）模型在机器人操作中存在的样本效率低和泛化能力有限的问题，本研究认为这与被忽视的预训练视觉表征在环境理解和策略先验方面知识不足有关。研究发现，视频上预训练的预测嵌入（特别是V-JEPA 2）能有效捕捉任务相关的时态动态并过滤不可预测的环境因素，从而弥补现有视觉表征的不足。基于此，论文提出了JEPA-VLA，一种将预测嵌入自适应集成到现有VLA中的简单而有效的方法。实验结果表明，JEPA-VLA在一系列基准和真实机器人任务中均取得了显著的性能提升。"
      },
      {
        "paper_id": "2602.11824",
        "title": "Revis: Sparse Latent Steering to Mitigate Object Hallucination in Large Vision-Language Models",
        "abstract": "Despite the advanced capabilities of Large Vision-Language Models (LVLMs), they frequently suffer from object hallucination. One reason is that visual features and pretrained textual representations often become intertwined in the deeper network layers. To address this, we propose REVIS, a training-free framework designed to explicitly re-activate this suppressed visual information. Rooted in latent space geometry, REVIS extracts the pure visual information vector via orthogonal projection and employs a calibrated strategy to perform sparse intervention only at the precise depth where suppression occurs. This surgical approach effectively restores visual information with minimal computational cost. Empirical evaluations on standard benchmarks demonstrate that REVIS reduces object hallucination rates by approximately 19% compared to state-of-the-art baselines, while preserving general reasoning capabilities.",
        "authors_display": "Zhou Yang Team",
        "pdf_url": "http://arxiv.org/abs/2602.11824",
        "code_url": null,
        "date": "2026-02-12",
        "primary_category": "cs.AI",
        "chinese_summary": "为解决大型视觉-语言模型（LVLMs）普遍存在的物体幻觉问题，该问题源于视觉特征和预训练文本表示在网络深层中的纠缠，本研究提出了REVIS，一个无需训练的框架。REVIS基于潜在空间几何原理，通过正交投影精确提取纯视觉信息向量，并采用校准策略，仅在视觉信息被抑制的精确深度进行稀疏干预，从而有效恢复视觉信息。经验评估结果显示，REVIS与最先进的基线相比，将目标幻觉率降低了约19%，同时保持了模型原有的通用推理能力。"
      },
      {
        "paper_id": "2602.11743",
        "title": "Adaptive Debiasing Tsallis Entropy for Test-Time Adaptation",
        "abstract": "Mainstream Test-Time Adaptation (TTA) methods for adapting vision-language models, e.g., CLIP, typically rely on Shannon Entropy (SE) at test time to measure prediction uncertainty and inconsistency. However, since CLIP has a built-in bias from pretraining on highly imbalanced web-crawled data, SE inevitably results in producing biased estimates of uncertainty entropy. To address this issue, we notably find and demonstrate that Tsallis Entropy (TE), a generalized form of SE, is naturally suited for characterizing biased distributions by introducing a non-extensive parameter q, with the performance of SE serving as a lower bound for TE. Building upon this, we generalize TE into Adaptive Debiasing Tsallis Entropy (ADTE) for TTA, customizing a class-specific parameter q^l derived by normalizing the estimated label bias from continuously incoming test instances, for each category. This adaptive approach allows ADTE to accurately select high-confidence views and seamlessly integrate with a label adjustment strategy to enhance adaptation, without introducing distribution-specific hyperparameter tuning. Besides, our investigation reveals that both TE and ADTE can serve as direct, advanced alternatives to SE in TTA, without any other modifications. Experimental results show that ADTE outperforms state-of-the-art methods on ImageNet and its five variants, and achieves the highest average performance on 10 cross-domain benchmarks, regardless of the model architecture or text prompts used. Our code is available at https://github.com/Jinx630/ADTE.",
        "authors_display": "Jianfeng Lu Team",
        "pdf_url": "http://arxiv.org/abs/2602.11743",
        "code_url": null,
        "date": "2026-02-12",
        "primary_category": "cs.CV",
        "chinese_summary": "现有主流的视觉-语言模型测试时自适应（TTA）方法依赖香农熵（SE）衡量预测不确定性，但由于预训练数据偏差，SE会产生有偏的不确定性估计。为解决此问题，研究发现并证明Tsallis熵（TE）通过引入非广延参数q，天然适合表征有偏分布。在此基础上，本文提出了自适应去偏Tsallis熵（ADTE），为每个类别定制类特定的q^l参数，该参数通过对持续传入的测试实例估计标签偏差进行归一化得到。实验结果表明，ADTE在ImageNet及其五种变体上超越了最先进的方法，并在10个跨域基准测试中取得了最高的平均性能，证实了其有效性。"
      },
      {
        "paper_id": "2602.11733",
        "title": "Adapting Vision-Language Models for E-commerce Understanding at Scale",
        "abstract": "E-commerce product understanding demands by nature, strong multimodal comprehension from text, images, and structured attributes. General-purpose Vision-Language Models (VLMs) enable generalizable multimodal latent modelling, yet there is no documented, well-known strategy for adapting them to the attribute-centric, multi-image, and noisy nature of e-commerce data, without sacrificing general performance. In this work, we show through a large-scale experimental study, how targeted adaptation of general VLMs can substantially improve e-commerce performance while preserving broad multimodal capabilities. Furthermore, we propose a novel extensive evaluation suite covering deep product understanding, strict instruction following, and dynamic attribute extraction.",
        "authors_display": "Shahram Khadivi Team",
        "pdf_url": "http://arxiv.org/abs/2602.11733",
        "code_url": null,
        "date": "2026-02-12",
        "primary_category": "cs.CV",
        "chinese_summary": "电商产品理解需要文本、图像和结构化属性等多模态的强大理解能力。通用视觉-语言模型（VLMs）虽能提供泛化能力，但如何将其有效地适应电商数据中以属性为中心、多图像和噪声多的特性，同时不牺牲通用性能，是一个未被充分探索的问题。本文通过大规模实验研究，展示了对通用VLM进行有针对性的适配，可以在大幅提升电商任务性能的同时，保持其广泛的多模态能力。此外，本文还提出了一个新颖且全面的评估套件，涵盖了深度产品理解、严格指令遵循和动态属性提取等任务。"
      },
      {
        "paper_id": "2602.11730",
        "title": "STVG-R1: Incentivizing Instance-Level Reasoning and Grounding in Videos via Reinforcement Learning",
        "abstract": "In vision-language models (VLMs), misalignment between textual descriptions and visual coordinates often induces hallucinations. This issue becomes particularly severe in dense prediction tasks such as spatial-temporal video grounding (STVG). Prior approaches typically focus on enhancing visual-textual alignment or attaching auxiliary decoders. However, these strategies inevitably introduce additional trainable modules, leading to significant annotation costs and computational overhead. In this work, we propose a novel visual prompting paradigm that avoids the difficult problem of aligning coordinates across modalities. Specifically, we reformulate per-frame coordinate prediction as a compact instance-level identification problem by assigning each object a unique, temporally consistent ID. These IDs are embedded into the video as visual prompts, providing explicit and interpretable inputs to the VLMs. Furthermore, we introduce STVG-R1, the first reinforcement learning framework for STVG, which employs a task-driven reward to jointly optimize temporal accuracy, spatial consistency, and structural format regularization. Extensive experiments on six benchmarks demonstrate the effectiveness of our approach. STVG-R1 surpasses the baseline Qwen2.5-VL-7B by a remarkable margin of 20.9% on m_IoU on the HCSTVG-v2 benchmark, establishing a new state of the art (SOTA). Surprisingly, STVG-R1 also exhibits strong zero-shot generalization to multi-object referring video object segmentation tasks, achieving a SOTA 47.3% J&F on MeViS.",
        "authors_display": "Qing Li Team",
        "pdf_url": "http://arxiv.org/abs/2602.11730",
        "code_url": null,
        "date": "2026-02-12",
        "primary_category": "cs.CV",
        "chinese_summary": "在视觉-语言模型（VLMs）中，文本描述与视觉坐标之间的不匹配常导致幻觉问题，在时空视频定位（STVG）等密集预测任务中尤为突出。现有方法通常增强视觉-文本对齐或添加辅助解码器，但这会引入额外的可训练模块，导致高昂的标注和计算成本。本文提出了一种新颖的视觉提示范式，通过将每帧坐标预测重新表述为实例级识别问题，为每个对象分配唯一且时间一致的ID，并将其嵌入视频作为视觉提示。此外，研究引入了STVG-R1，这是首个用于STVG的强化学习框架，通过任务驱动奖励联合优化时间精度、空间一致性和结构化格式正则化。实验结果表明，STVG-R1在六个基准测试中表现出色，在HCSTVG-v2上m_IoU比基线Qwen2.5-VL-7B高出20.9%，达到SOTA，并展现出强大的零样本泛化能力。"
      },
      {
        "paper_id": "2602.11636",
        "title": "ScalSelect: Scalable Training-Free Multimodal Data Selection for Efficient Visual Instruction Tuning",
        "abstract": "Large-scale Visual Instruction Tuning (VIT) has become a key paradigm for advancing the performance of vision-language models (VLMs) across various multimodal tasks. However, training on the large-scale datasets is computationally expensive and inefficient due to redundancy in the data, which motivates the need for multimodal data selection to improve training efficiency. Existing data selection methods for VIT either require costly training or gradient computation. Training-free alternatives often depend on proxy models or datasets, instruction-agnostic representations, and pairwise similarity with quadratic complexity, limiting scalability and representation fidelity. In this work, we propose ScalSelect, a scalable training-free multimodal data selection method with linear-time complexity with respect to the number of samples, eliminating the need for external models or auxiliary datasets. ScalSelect first constructs sample representations by extracting visual features most attended by instruction tokens in the target VLM, capturing instruction-relevant information. It then identifies samples whose representations best approximate the dominant subspace of the full dataset representations, enabling scalable importance scoring without pairwise comparisons. Extensive experiments across multiple VLMs, datasets, and selection budgets demonstrate that ScalSelect achieves over 97.5% of the performance of training on the full dataset using only 16% of the data, and even outperforms full-data training in some settings. The code is available at \\href{https://github.com/ChangtiWu/ScalSelect}{ScalSelect}.",
        "authors_display": "Kai Chen Team",
        "pdf_url": "http://arxiv.org/abs/2602.11636",
        "code_url": "https://github.com/ChangtiWu/ScalSelect}{ScalSelect}",
        "date": "2026-02-12",
        "primary_category": "cs.CV",
        "chinese_summary": "大规模视觉指令微调（VIT）是提升视觉-语言模型（VLMs）在多模态任务中性能的关键范式，但其在大型数据集上的训练因数据冗余而计算昂贵且低效。现有的数据选择方法要么需要昂贵的训练或梯度计算，要么依赖代理模型或指令无关表示且复杂度高，限制了可扩展性。本文提出了ScalSelect，一种可扩展的无训练多模态数据选择方法，其复杂度与样本数量呈线性关系，无需外部模型或辅助数据集。ScalSelect通过提取指令令牌在目标VLM中最受关注的视觉特征来构建样本表示，然后识别其表示最能近似整个数据集主导子空间的样本，从而实现无需成对比较的可扩展重要性评分。广泛的实验表明，ScalSelect仅使用16%的数据即可达到全数据集训练97.5%以上的性能，在某些设置下甚至超越了全数据训练。"
      },
      {
        "paper_id": "2602.11615",
        "title": "SkillRater: Untangling Capabilities in Multimodal Data",
        "abstract": "Data curation methods typically assign samples a single quality score. We argue this scalar framing is fundamentally limited: when training requires multiple distinct capabilities, a monolithic scorer cannot maximize useful signals for all of them simultaneously. Quality is better understood as multidimensional, with each dimension corresponding to a capability the model must acquire. We introduce SkillRater, a framework that decomposes data filtering into specialized raters - one per capability, each trained via meta-learning on a disjoint validation objective - and composes their scores through a progressive selection rule: at each training stage, a sample is retained if any rater ranks it above a threshold that tightens over time, preserving diversity early while concentrating on high-value samples late. We validate this approach on vision language models, decomposing quality into three capability dimensions: visual understanding, OCR, and STEM reasoning. At 2B parameters, SkillRater improves over unfiltered baselines by 5.63% on visual understanding, 2.00% on OCR, and 3.53% on STEM on held out benchmarks. The learned rater signals are near orthogonal, confirming that the decomposition captures genuinely independent quality dimensions and explaining why it outperforms both unfiltered training and monolithic learned filtering.",
        "authors_display": "Akshat Shrivastava Team",
        "pdf_url": "http://arxiv.org/abs/2602.11615",
        "code_url": null,
        "date": "2026-02-12",
        "primary_category": "cs.LG",
        "chinese_summary": "传统数据筛选方法通常只给样本分配一个单一的质量分数，但这在模型需要学习多种不同能力时显得局限。本文认为质量应是多维的，每个维度对应模型需掌握的一种能力。为此，我们提出了SkillRater框架，将数据过滤分解为多个专业的评估器，每个评估器负责一种能力，并通过元学习在独立验证目标上进行训练。这些评估器的分数通过渐进式选择规则组合：在每个训练阶段，只要任一评估器将样本排名高于随时间收紧的阈值，该样本即被保留，以在早期保持多样性，后期聚焦高价值样本。在视觉语言模型上的验证表明，SkillRater在视觉理解、OCR和STEM推理能力上均显著优于未过滤基线，且学习到的评估器信号几乎正交，证实了多维度分解的有效性。"
      },
      {
        "paper_id": "2602.11073",
        "title": "Chatting with Images for Introspective Visual Thinking",
        "abstract": "Current large vision-language models (LVLMs) typically rely on text-only reasoning based on a single-pass visual encoding, which often leads to loss of fine-grained visual information. Recently the proposal of ''thinking with images'' attempts to alleviate this limitation by manipulating images via external tools or code; however, the resulting visual states are often insufficiently grounded in linguistic semantics, impairing effective cross-modal alignment - particularly when visual semantics or geometric relationships must be reasoned over across distant regions or multiple images. To address these challenges, we propose ''chatting with images'', a new framework that reframes visual manipulation as language-guided feature modulation. Under the guidance of expressive language prompts, the model dynamically performs joint re-encoding over multiple image regions, enabling tighter coupling between linguistic reasoning and visual state updates. We instantiate this paradigm in ViLaVT, a novel LVLM equipped with a dynamic vision encoder explicitly designed for such interactive visual reasoning, and trained it with a two-stage curriculum combining supervised fine-tuning and reinforcement learning to promote effective reasoning behaviors. Extensive experiments across eight benchmarks demonstrate that ViLaVT achieves strong and consistent improvements, with particularly pronounced gains on complex multi-image and video-based spatial reasoning tasks.",
        "authors_display": "Tieniu Tan Team",
        "pdf_url": "http://arxiv.org/abs/2602.11073",
        "code_url": null,
        "date": "2026-02-12",
        "primary_category": "cs.CV",
        "chinese_summary": "当前大型视觉-语言模型（LVLMs）通常依赖基于单次视觉编码的纯文本推理，这常导致细粒度视觉信息丢失，尤其是在处理跨区域或多图像的视觉语义或几何关系推理时。为解决这些挑战，本文提出了“与图像对话”这一新框架，将视觉操作重新定义为语言引导的特征调制。在该框架下，模型在富有表现力的语言提示指导下，动态地对多个图像区域进行联合再编码，从而实现语言推理与视觉状态更新之间更紧密的耦合。我们通过ViLaVT实例化了这一范式，它是一个配备动态视觉编码器的新型LVLM，并采用两阶段课程（监督微调和强化学习）进行训练。广泛的实验表明，ViLaVT在八个基准测试中取得了显著且一致的改进，尤其在复杂的多图像和基于视频的空间推理任务上表现突出。"
      },
      {
        "paper_id": "2602.12405",
        "title": "Self-Refining Vision Language Model for Robotic Failure Detection and Reasoning",
        "abstract": "Reasoning about failures is crucial for building reliable and trustworthy robotic systems. Prior approaches either treat failure reasoning as a closed-set classification problem or assume access to ample human annotations. Failures in the real world are typically subtle, combinatorial, and difficult to enumerate, whereas rich reasoning labels are expensive to acquire. We address this problem by introducing ARMOR: Adaptive Round-based Multi-task mOdel for Robotic failure detection and reasoning. We formulate detection and reasoning as a multi-task self-refinement process, where the model iteratively predicts detection outcomes and natural language reasoning conditioned on past outputs. During training, ARMOR learns from heterogeneous supervision - large-scale sparse binary labels and small-scale rich reasoning annotations - optimized via a combination of offline and online imitation learning. At inference time, ARMOR generates multiple refinement trajectories and selects the most confident prediction via a self-certainty metric. Experiments across diverse environments show that ARMOR achieves state-of-the-art performance by improving over the previous approaches by up to 30% on failure detection rate and up to 100% in reasoning measured through LLM fuzzy match score, demonstrating robustness to heterogeneous supervision and open-ended reasoning beyond predefined failure modes. We provide dditional visualizations on our website: https://sites.google.com/utexas.edu/armor",
        "authors_display": "Yesh Dattatreya Team",
        "pdf_url": "http://arxiv.org/abs/2602.12405",
        "code_url": null,
        "date": "2026-02-12",
        "primary_category": "cs.RO",
        "chinese_summary": "针对机器人系统故障推理面临的挑战，即真实世界故障的复杂性及丰富推理标签获取成本高昂，本研究提出了ARMOR模型。ARMOR将故障检测与自然语言推理视为一个多任务自完善过程，通过迭代预测和基于历史输出的条件推理进行训练。它利用大规模稀疏二元标签和少量丰富推理标注的异构监督，并通过离线和在线模仿学习进行优化。在推理阶段，ARMOR生成多条完善轨迹并利用自确定性指标选择最可靠的预测。实验结果显示，ARMOR在故障检测率上比现有方法提升了30%，在LLM模糊匹配分数衡量的推理能力上提升了100%，证明了其在异构监督下的鲁棒性及超越预定义故障模式的开放式推理能力。"
      },
      {
        "paper_id": "2602.12395",
        "title": "What does RL improve for Visual Reasoning? A Frankenstein-Style Analysis",
        "abstract": "Reinforcement learning (RL) with verifiable rewards has become a standard post-training stage for boosting visual reasoning in vision-language models, yet it remains unclear what capabilities RL actually improves compared with supervised fine-tuning as cold-start initialization (IN). End-to-end benchmark gains conflate multiple factors, making it difficult to attribute improvements to specific skills. To bridge the gap, we propose a Frankenstein-style analysis framework including: (i) functional localization via causal probing; (ii) update characterization via parameter comparison; and (iii) transferability test via model merging. Instead, RL induces a consistent inference-time shift primarily in mid-to-late layers, and these mid-to-late refinements are both transferable (via merging) and necessary (via freezing) for RL gains. Overall, our results suggest that RL's reliable contribution in visual reasoning is not a uniform enhancement of visual perception, but a systematic refinement of mid-to-late transformer computation that improves vision-to-reasoning alignment and reasoning performance, highlighting the limitations of benchmark-only evaluation for understanding multimodal reasoning improvements.",
        "authors_display": "Tianyi Zhou Team",
        "pdf_url": "http://arxiv.org/abs/2602.12395",
        "code_url": null,
        "date": "2026-02-12",
        "primary_category": "cs.CV",
        "chinese_summary": "为了理解强化学习（RL）在视觉语言模型（VLM）视觉推理中相比监督微调（IN）的具体贡献，本研究提出了一种“弗兰肯斯坦式”分析框架。该框架通过因果探测定位功能、参数比较表征更新以及模型合并测试可迁移性。研究发现，RL主要通过在模型的中间到后期层诱导一致的推理时段转移来提升性能，并且这些中间到后期的优化对于RL的性能增益是可迁移和必要的。这些结果表明，RL对视觉推理的可靠贡献并非统一增强视觉感知，而是系统地细化了Transformer中间到后期层的计算，从而改善了视觉到推理的对齐和推理性能，揭示了仅通过基准评估来理解多模态推理改进的局限性。"
      },
      {
        "paper_id": "2602.12381",
        "title": "Synthetic Image Detection with CLIP: Understanding and Assessing Predictive Cues",
        "abstract": "Recent generative models produce near-photorealistic images, challenging the trustworthiness of photographs. Synthetic image detection (SID) has thus become an important area of research. Prior work has highlighted how synthetic images differ from real photographs--unfortunately, SID methods often struggle to generalize to novel generative models and often perform poorly in practical settings. CLIP, a foundational vision-language model which yields semantically rich image-text embeddings, shows strong accuracy and generalization for SID. Yet, the underlying relevant cues embedded in CLIP-features remain unknown. It is unclear, whether CLIP-based detectors simply detect strong visual artifacts or exploit subtle semantic biases, both of which would render them useless in practical settings or on generative models of high quality. We introduce SynthCLIC, a paired dataset of real photographs and high-quality synthetic counterparts from recent diffusion models, designed to reduce semantic bias in SID. Using an interpretable linear head with de-correlated activations and a text-grounded concept-model, we analyze what CLIP-based detectors learn. CLIP-based linear detectors reach 0.96 mAP on a GAN-based benchmark but only 0.92 on our high-quality diffusion dataset SynthCLIC, and generalization across generator families drops to as low as 0.37 mAP. We find that the detectors primarily rely on high-level photographic attributes (e.g., minimalist style, lens flare, or depth layering), rather than overt generator-specific artifacts. CLIP-based detectors perform well overall but generalize unevenly across diverse generative architectures. This highlights the need for continual model updates and broader training exposure, while reinforcing CLIP-based approaches as a strong foundation for more universal, robust SID.",
        "authors_display": "Michael Graber Team",
        "pdf_url": "http://arxiv.org/abs/2602.12381",
        "code_url": null,
        "date": "2026-02-12",
        "primary_category": "cs.CV",
        "chinese_summary": "鉴于现代生成模型能产生近乎真实的照片，合成图像检测（SID）的泛化能力及其在实际应用中的表现面临挑战，特别是在新生成模型面前。本研究旨在探究CLIP作为SID基础模型的有效性及其内在线索。为此，我们构建了SynthCLIC数据集以减少语义偏差，并利用可解释的线性分类器和文本概念模型分析CLIP特征。结果显示，CLIP检测器在GAN基准上表现优异，但在高质量扩散数据集SynthCLIC上性能略有下降，且跨不同生成器家族的泛化能力显著受限。研究发现，检测器主要依赖高层摄影属性而非明显的生成器伪影。这些发现强调了持续模型更新和更广泛训练暴露的必要性，并肯定了CLIP作为更通用、鲁棒SID方法的强大基础。"
      },
      {
        "paper_id": "2602.12322",
        "title": "ForeAct: Steering Your VLA with Efficient Visual Foresight Planning",
        "abstract": "Vision-Language-Action (VLA) models convert high-level language instructions into concrete, executable actions, a task that is especially challenging in open-world environments. We present Visual Foresight Planning (ForeAct), a general and efficient planner that guides a VLA step-by-step using imagined future observations and subtask descriptions. With an imagined future observation, the VLA can focus on visuo-motor inference rather than high-level semantic reasoning, leading to improved accuracy and generalization. Our planner comprises a highly efficient foresight image generation module that predicts a high-quality 640$\\times$480 future observation from the current visual input and language instruction within only 0.33s on an H100 GPU, together with a vision-language model that reasons over the task and produces subtask descriptions for both the generator and the VLA. Importantly, state-of-the-art VLAs can integrate our planner seamlessly by simply augmenting their visual inputs, without any architectural modification. The foresight generator is pretrained on over 1 million multi-task, cross-embodiment episodes, enabling it to learn robust embodied dynamics. We evaluate our framework on a benchmark that consists of 11 diverse, multi-step real-world tasks. It achieves an average success rate of 87.4%, demonstrating a +40.9% absolute improvement over the $π_0$ baseline (46.5%) and a +30.3% absolute improvement over $π_0$ augmented with textual subtask guidance (57.1%).",
        "authors_display": "Song Han Team",
        "pdf_url": "http://arxiv.org/abs/2602.12322",
        "code_url": null,
        "date": "2026-02-12",
        "primary_category": "cs.RO",
        "chinese_summary": "在开放世界环境中，视觉-语言-动作（VLA）模型将高级语言指令转换为具体可执行动作面临挑战。本研究提出了视觉预见规划器（ForeAct），一个通用高效的规划器，它通过想象未来观察和子任务描述来逐步指导VLA。ForeAct包含一个高效的预见图像生成模块，能从当前视觉输入和语言指令在0.33秒内预测高质量的未来观察，并结合一个视觉-语言模型生成子任务描述。最先进的VLA模型无需修改架构，只需通过增强视觉输入即可无缝集成ForeAct。预见生成器在超过100万个多任务、跨具身情景中进行预训练，学习了鲁棒的具身动力学。在包含11个真实世界任务的基准测试中，ForeAct取得了87.4%的平均成功率，比基线模型有显著提升。"
      },
      {
        "paper_id": "2602.12314",
        "title": "LatentAM: Real-Time, Large-Scale Latent Gaussian Attention Mapping via Online Dictionary Learning",
        "abstract": "We present LatentAM, an online 3D Gaussian Splatting (3DGS) mapping framework that builds scalable latent feature maps from streaming RGB-D observations for open-vocabulary robotic perception. Instead of distilling high-dimensional Vision-Language Model (VLM) embeddings using model-specific decoders, LatentAM proposes an online dictionary learning approach that is both model-agnostic and pretraining-free, enabling plug-and-play integration with different VLMs at test time. Specifically, our approach associates each Gaussian primitive with a compact query vector that can be converted into approximate VLM embeddings using an attention mechanism with a learnable dictionary. The dictionary is initialized efficiently from streaming observations and optimized online to adapt to evolving scene semantics under trust-region regularization. To scale to long trajectories and large environments, we further propose an efficient map management strategy based on voxel hashing, where optimization is restricted to an active local map on the GPU, while the global map is stored and indexed on the CPU to maintain bounded GPU memory usage. Experiments on public benchmarks and a large-scale custom dataset demonstrate that LatentAM attains significantly better feature reconstruction fidelity compared to state-of-the-art methods, while achieving near-real-time speed (12-35 FPS) on the evaluated datasets. Our project page is at: https://junwoonlee.github.io/projects/LatentAM",
        "authors_display": "Yulun Tian Team",
        "pdf_url": "http://arxiv.org/abs/2602.12314",
        "code_url": null,
        "date": "2026-02-12",
        "primary_category": "cs.RO",
        "chinese_summary": "为解决开放词汇机器人感知中从流式RGB-D观测构建可扩展潜在特征地图的挑战，并克服传统VLM嵌入方法缺乏通用性和依赖预训练的问题，本研究提出了LatentAM框架。LatentAM是一种在线3D高斯泼溅（3DGS）映射框架，它采用模型无关且无需预训练的在线字典学习方法，实现了与不同VLM的即插即用集成。该方法将高斯基元与紧凑查询向量关联，通过带有可学习字典的注意力机制转换为近似VLM嵌入。字典在流式观测中高效初始化并在线优化，同时结合基于体素哈希的地图管理策略以实现大规模环境下的GPU内存有界使用。实验结果表明，LatentAM在特征重建保真度上显著优于现有方法，并在评估数据集上实现了接近实时的速度（12-35 FPS）。"
      },
      {
        "paper_id": "2602.11448",
        "title": "Hierarchical Concept Embedding & Pursuit for Interpretable Image Classification",
        "abstract": "Interpretable-by-design models are gaining traction in computer vision because they provide faithful explanations for their predictions. In image classification, these models typically recover human-interpretable concepts from an image and use them for classification. Sparse concept recovery methods leverage the latent space of vision-language models to represent image embeddings as a sparse combination of concept embeddings. However, because such methods ignore the hierarchical structure of concepts, they can produce correct predictions with explanations that are inconsistent with the hierarchy. In this work, we propose Hierarchical Concept Embedding \\& Pursuit (HCEP), a framework that induces a hierarchy of concept embeddings in the latent space and uses hierarchical sparse coding to recover the concepts present in an image. Given a hierarchy of semantic concepts, we construct a corresponding hierarchy of concept embeddings and, assuming the correct concepts for an image form a rooted path in the hierarchy, derive desirable conditions for identifying them in the embedded space. We show that hierarchical sparse coding reliably recovers hierarchical concept embeddings, whereas vanilla sparse coding fails. Our experiments on real-world datasets demonstrate that HCEP outperforms baselines in concept precision and recall while maintaining competitive classification accuracy. Moreover, when the number of samples is limited, HCEP achieves superior classification accuracy and concept recovery. These results show that incorporating hierarchical structures into sparse coding yields more reliable and interpretable image classification models.",
        "authors_display": "René Vidal Team",
        "pdf_url": "http://arxiv.org/abs/2602.11448",
        "code_url": null,
        "date": "2026-02-11",
        "primary_category": "cs.LG",
        "chinese_summary": "可解释设计模型在计算机视觉中日益受到关注，因为它们能为其预测提供忠实的解释。在图像分类中，这类模型通常从图像中提取人类可解释的概念并用于分类。稀疏概念恢复方法利用视觉-语言模型的潜在空间将图像嵌入表示为概念嵌入的稀疏组合，但这类方法忽视了概念的层次结构，可能导致预测正确但解释与层次结构不一致。为解决此问题，本文提出了分层概念嵌入与追踪（HCEP）框架，它在潜在空间中诱导概念嵌入的层次结构，并利用分层稀疏编码来恢复图像中存在的概念。实验结果表明，HCEP在概念精度和召回率上均优于基线，同时保持了有竞争力的分类准确率，并且在样本数量有限时展现出卓越的分类和概念恢复性能，证明了将层次结构融入稀疏编码能产生更可靠和可解释的图像分类模型。"
      },
      {
        "paper_id": "2602.11146",
        "title": "Beyond VLM-Based Rewards: Diffusion-Native Latent Reward Modeling",
        "abstract": "Preference optimization for diffusion and flow-matching models relies on reward functions that are both discriminatively robust and computationally efficient. Vision-Language Models (VLMs) have emerged as the primary reward provider, leveraging their rich multimodal priors to guide alignment. However, their computation and memory cost can be substantial, and optimizing a latent diffusion generator through a pixel-space reward introduces a domain mismatch that complicates alignment. In this paper, we propose DiNa-LRM, a diffusion-native latent reward model that formulates preference learning directly on noisy diffusion states. Our method introduces a noise-calibrated Thurstone likelihood with diffusion-noise-dependent uncertainty. DiNa-LRM leverages a pretrained latent diffusion backbone with a timestep-conditioned reward head, and supports inference-time noise ensembling, providing a diffusion-native mechanism for test-time scaling and robust rewarding. Across image alignment benchmarks, DiNa-LRM substantially outperforms existing diffusion-based reward baselines and achieves performance competitive with state-of-the-art VLMs at a fraction of the computational cost. In preference optimization, we demonstrate that DiNa-LRM improves preference optimization dynamics, enabling faster and more resource-efficient model alignment.",
        "authors_display": "Wenhan Luo Team",
        "pdf_url": "http://arxiv.org/abs/2602.11146",
        "code_url": "https://github.com/HKUST-C4G/diffusion-rm",
        "date": "2026-02-11",
        "primary_category": "cs.CV",
        "chinese_summary": "扩散模型和流匹配模型的偏好优化需要既具有判别鲁棒性又计算高效的奖励函数。视觉-语言模型（VLMs）已成为主要的奖励提供者，利用其丰富的多模态先验来指导对齐。然而，它们的计算和内存成本很高，并且通过像素空间奖励优化潜在扩散生成器会引入域不匹配，使对齐复杂化。本文提出了DiNa-LRM，一种扩散原生的潜在奖励模型，它直接在噪声扩散状态上构建偏好学习。DiNa-LRM引入了一种噪声校准的Thurstone似然，其不确定性依赖于扩散噪声，并利用预训练的潜在扩散骨干网络与时间步条件奖励头。实验结果显示，DiNa-LRM在图像对齐基准测试中显著优于现有基于扩散的奖励基线，并以极低的计算成本实现了与最先进VLM相当的性能，显著改善了偏好优化动态，实现了更快、更资源高效的模型对齐。"
      },
      {
        "paper_id": "2602.11241",
        "title": "Active Zero: Self-Evolving Vision-Language Models through Active Environment Exploration",
        "abstract": "Self-play has enabled large language models to autonomously improve through self-generated challenges. However, existing self-play methods for vision-language models rely on passive interaction with static image collections, resulting in strong dependence on initial datasets and inefficient learning. Without the ability to actively seek visual data tailored to their evolving capabilities, agents waste computational effort on samples that are either trivial or beyond their current skill level. To address these limitations, we propose Active-Zero, a framework that shifts from passive interaction to active exploration of visual environments. Active-Zero employs three co-evolving agents: a Searcher that retrieves images from open-world repositories based on the model's capability frontier, a Questioner that synthesizes calibrated reasoning tasks, and a Solver refined through accuracy rewards. This closed loop enables self-scaffolding auto-curricula where the model autonomously constructs its learning trajectory. On Qwen2.5-VL-7B-Instruct across 12 benchmarks, Active-Zero achieves 53.97 average accuracy on reasoning tasks (5.7% improvement) and 59.77 on general understanding (3.9% improvement), consistently outperforming existing self-play baselines. These results highlight active exploration as a key ingredient for scalable and adaptive self-evolving vision-language systems.",
        "authors_display": "Tat-Seng Chua Team",
        "pdf_url": "http://arxiv.org/abs/2602.11241",
        "code_url": null,
        "date": "2026-02-11",
        "primary_category": "cs.CV",
        "chinese_summary": "自博弈已使大型语言模型（LLMs）通过自生成挑战实现自主提升。然而，现有视觉-语言模型（VLMs）的自博弈方法依赖与静态图像集的被动交互，导致对初始数据集的强依赖和学习效率低下。为解决这些局限性，本文提出了Active-Zero框架，将模型学习从被动交互转向主动探索视觉环境。Active-Zero采用三个共同进化的智能体：一个Searcher根据模型能力前沿从开放世界存储库检索图像；一个Questioner合成校准的推理任务；一个Solver通过准确性奖励进行优化。这种闭环机制实现了自我脚手架的自动课程学习。在Qwen2.5-VL-7B-Instruct的12个基准测试中，Active-Zero在推理任务上平均准确率达到53.97%（提升5.7%），在通用理解上达到59.77%（提升3.9%），持续优于现有自博弈基线，表明主动探索是可扩展和自适应自进化视觉-语言系统的关键要素。"
      },
      {
        "paper_id": "2602.10910",
        "title": "Safe mobility support system using crowd mapping and avoidance route planning using VLM",
        "abstract": "Autonomous mobile robots offer promising solutions for labor shortages and increased operational efficiency. However, navigating safely and effectively in dynamic environments, particularly crowded areas, remains challenging. This paper proposes a novel framework that integrates Vision-Language Models (VLM) and Gaussian Process Regression (GPR) to generate dynamic crowd-density maps (``Abstraction Maps'') for autonomous robot navigation. Our approach utilizes VLM's capability to recognize abstract environmental concepts, such as crowd densities, and represents them probabilistically via GPR. Experimental results from real-world trials on a university campus demonstrated that robots successfully generated routes avoiding both static obstacles and dynamic crowds, enhancing navigation safety and adaptability.",
        "authors_display": "Koichi Ozaki Team",
        "pdf_url": "http://arxiv.org/abs/2602.10910",
        "code_url": null,
        "date": "2026-02-11",
        "primary_category": "cs.RO",
        "chinese_summary": "自主移动机器人在动态、尤其是拥挤环境中安全有效导航仍面临挑战。本文提出了一种新颖的框架，该框架集成了视觉-语言模型（VLM）和高斯过程回归（GPR），用于生成动态人群密度图（“抽象图”），以实现自主机器人导航。我们的方法利用VLM识别抽象环境概念（如人群密度）的能力，并通过GPR以概率方式表示这些概念。在大学校园进行的真实世界试验结果表明，机器人成功地生成了避开静态障碍物和动态人群的路径，从而增强了导航的安全性和适应性。"
      }
    ],
    "VLA": [
      {
        "paper_id": "2602.22988",
        "title": "Residual Koopman Spectral Profiling for Predicting and Preventing Transformer Training Instability",
        "abstract": "Training divergence in transformers wastes compute, yet practitioners discover instability only after expensive runs begin. They therefore need an expected probability of failure for a transformer before training starts. Our study of Residual Koopman Spectral Profiling (RKSP) provides such an estimate. From a single forward pass at initialization, RKSP extracts Koopman spectral features by applying whitened dynamic mode decomposition to layer-wise residual snapshots. Our central diagnostic, the near-unit spectral mass, quantifies the fraction of modes concentrated near the unit circle, which captures instability risk. For predicting divergence across extensive configurations, this estimator achieves an AUROC of 0.995, outperforming the best gradient baseline. We further make this diagnostic actionable through Koopman Spectral Shaping (KSS), which reshapes spectra during training. We empirically validate that our method works in practice: RKSP predicts divergence at initialization, and when RKSP flags high risk, turning on KSS successfully prevents divergence. In the challenging high learning rate regime without normalization layers, KSS reduces the divergence rate from 66.7% to 12.5% and enables learning rates that are 50% to 150% higher. These findings generalize to WikiText-103 language modeling, vision transformers on CIFAR-10, and pretrained language models, including GPT-2 and LLaMA-2 up to 7B, as well as emerging architectures such as MoE, Mamba-style SSMs, and KAN.",
        "authors_display": "Yutaka Matsuo Team",
        "pdf_url": "http://arxiv.org/abs/2602.22988",
        "code_url": null,
        "date": "2026-02-26",
        "primary_category": "cs.LG",
        "chinese_summary": "针对Transformer训练中难以提前预测发散的问题，研究提出残差库普曼谱分析（RKSP）方法。该方法通过初始化时的单次前向传播，运用白化动态模态分解提取层级残差快照的库普曼谱特征，并以“近单位谱质量”作为核心诊断指标来量化不稳定性风险。实验表明，RKSP在预测发散方面实现了0.995的AUROC，并通过库普曼谱整形（KSS）技术，在高学习率无归一化层的情况下将发散率从66.7%降低至12.5%，并支持更高学习率，在多种模型和任务上均显示出有效性。"
      },
      {
        "paper_id": "2602.22896",
        "title": "DySL-VLA: Efficient Vision-Language-Action Model Inference via Dynamic-Static Layer-Skipping for Robot Manipulation",
        "abstract": "Vision-Language-Action (VLA) models have shown remarkable success in robotic tasks like manipulation by fusing a language model's reasoning with a vision model's 3D understanding. However, their high computational cost remains a major obstacle for real-world applications that require real-time performance. We observe that the actions within a task have varying levels of importance: critical steps demand high precision, while less important ones can tolerate more variance. Leveraging this insight, we propose DySL-VLA, a novel framework that addresses computational cost by dynamically skipping VLA layers based on each action's importance. DySL-VLA categorizes its layers into two types: informative layers, which are consistently executed, and incremental layers, which can be selectively skipped. To intelligently skip layers without sacrificing accuracy, we invent a prior-post skipping guidance mechanism to determine when to initiate layer-skipping. We also propose a skip-aware two-stage knowledge distillation algorithm to efficiently train a standard VLA into a DySL-VLA. Our experiments indicate that DySL-VLA achieves 2.1% improvement in success length over Deer-VLA on the Calvin dataset, while simultaneously reducing trainable parameters by a factor of 85.7 and providing a 3.75x speedup relative to the RoboFlamingo baseline at iso-accuracy. Our code is available on https://github.com/PKU-SEC-Lab/DYSL_VLA.",
        "authors_display": "Meng Li Team",
        "pdf_url": "http://arxiv.org/abs/2602.22896",
        "code_url": null,
        "date": "2026-02-26",
        "primary_category": "cs.RO",
        "chinese_summary": "鉴于视觉-语言-动作（VLA）模型在机器人任务中计算成本高昂，阻碍了其实时应用，研究提出DySL-VLA框架。该框架基于任务中动作重要性差异的洞察，动态跳过VLA层以降低计算量，并设计了先验-后验跳过指导机制与跳过感知两阶段知识蒸馏算法。实验结果显示，DySL-VLA在Calvin数据集上相较于基线模型成功长度提升2.1%，可训练参数减少85.7倍，并在同等精度下实现3.75倍加速。"
      },
      {
        "paper_id": "2602.22663",
        "title": "Rethinking the Practicality of Vision-language-action Model: A Comprehensive Benchmark and An Improved Baseline",
        "abstract": "Vision-Language-Action (VLA) models have emerged as a generalist robotic agent. However, existing VLAs are hindered by excessive parameter scales, prohibitive pre-training requirements, and limited applicability to diverse embodiments. To improve the practicality of VLAs, we propose a comprehensive benchmark and an improved baseline. First, we propose CEBench, a new benchmark spanning diverse embodiments in both simulation and the real world with consideration of domain randomization. We collect 14.4k simulated trajectories and 1.6k real-world expert-curated trajectories to support training on CEBench. Second, using CEBench as our testbed, we study three critical aspects of VLAs' practicality and offer several key findings. Informed by these findings, we introduce LLaVA-VLA, a lightweight yet powerful VLA designed for practical deployment on consumer-grade GPUs. Architecturally, it integrates a compact VLM backbone with multi-view perception, proprioceptive tokenization, and action chunking. To eliminate reliance on costly pre-training, LLaVA-VLA adopts a two-stage training paradigm including post-training and fine-tuning. Furthermore, LLaVA-VLA extends the action space to unify navigation and manipulation. Experiments across embodiments demonstrate the capabilities of generalization and versatility of LLaVA-VLA , while real-world mobile manipulation experiments establish it as the first end-to-end VLA model for mobile manipulation. We will open-source all datasets, codes, and checkpoints upon acceptance to foster reproducibility and future research.",
        "authors_display": "Haoang Li Team",
        "pdf_url": "http://arxiv.org/abs/2602.22663",
        "code_url": null,
        "date": "2026-02-26",
        "primary_category": "cs.RO",
        "chinese_summary": "针对现有视觉-语言-动作（VLA）模型参数规模大、预训练要求高、对多样机器人本体适用性有限等实用性挑战，研究提出了CEBench基准和LLaVA-VLA模型。CEBench涵盖仿真与真实世界多样本体数据，LLaVA-VLA则是一个轻量级VLA，集成紧凑型VLM骨干、多视角感知、本体感知词元化和动作分块，采用两阶段训练消除昂贵预训练依赖，并统一导航与操作。实验证明LLaVA-VLA具有强大的泛化性和多功能性，并首次实现了端到端移动操作。"
      },
      {
        "paper_id": "2602.22579",
        "title": "Metamorphic Testing of Vision-Language Action-Enabled Robots",
        "abstract": "Vision-Language-Action (VLA) models are multimodal robotic task controllers that, given an instruction and visual inputs, produce a sequence of low-level control actions (or motor commands) enabling a robot to execute the requested task in the physical environment. These systems face the test oracle problem from multiple perspectives. On the one hand, a test oracle must be defined for each instruction prompt, which is a complex and non-generalizable approach. On the other hand, current state-of-the-art oracles typically capture symbolic representations of the world (e.g., robot and object states), enabling the correctness evaluation of a task, but fail to assess other critical aspects, such as the quality with which VLA-enabled robots perform a task. In this paper, we explore whether Metamorphic Testing (MT) can alleviate the test oracle problem in this context. To do so, we propose two metamorphic relation patterns and five metamorphic relations to assess whether changes to the test inputs impact the original trajectory of the VLA-enabled robots. An empirical study involving five VLA models, two simulated robots, and four robotic tasks shows that MT can effectively alleviate the test oracle problem by automatically detecting diverse types of failures, including, but not limited to, uncompleted tasks. More importantly, the proposed MRs are generalizable, making the proposed approach applicable across different VLA models, robots, and tasks, even in the absence of test oracles.",
        "authors_display": "Aitor Arrieta Team",
        "pdf_url": "http://arxiv.org/abs/2602.22579",
        "code_url": null,
        "date": "2026-02-26",
        "primary_category": "cs.RO",
        "chinese_summary": "鉴于视觉-语言-动作（VLA）模型在测试中面临复杂的预言机问题，且现有方法难以评估任务执行质量，研究探索了变质测试（MT）的应用。通过提出两种变质关系模式和五种变质关系，用于评估输入变化对VLA机器人轨迹的影响。实证研究表明，MT能有效自动检测多种故障，包括任务未完成，且提出的变质关系具有普适性，适用于不同VLA模型、机器人和任务，即使在缺少测试预言机的情况下也适用。"
      },
      {
        "paper_id": "2602.22514",
        "title": "SignVLA: A Gloss-Free Vision-Language-Action Framework for Real-Time Sign Language-Guided Robotic Manipulation",
        "abstract": "We present, to our knowledge, the first sign language-driven Vision-Language-Action (VLA) framework for intuitive and inclusive human-robot interaction. Unlike conventional approaches that rely on gloss annotations as intermediate supervision, the proposed system adopts a gloss-free paradigm and directly maps visual sign gestures to semantic instructions. This design reduces annotation cost and avoids the information loss introduced by gloss representations, enabling more natural and scalable multimodal interaction.   In this work, we focus on a real-time alphabet-level finger-spelling interface that provides a robust and low-latency communication channel for robotic control. Compared with large-scale continuous sign language recognition, alphabet-level interaction offers improved reliability, interpretability, and deployment feasibility in safety-critical embodied environments. The proposed pipeline transforms continuous gesture streams into coherent language commands through geometric normalization, temporal smoothing, and lexical refinement, ensuring stable and consistent interaction.   Furthermore, the framework is designed to support future integration of transformer-based gloss-free sign language models, enabling scalable word-level and sentence-level semantic understanding. Experimental results demonstrate the effectiveness of the proposed system in grounding sign-derived instructions into precise robotic actions under diverse interaction scenarios. These results highlight the potential of the framework to advance accessible, scalable, and multimodal embodied intelligence.",
        "authors_display": "Zezhi Tang Team",
        "pdf_url": "http://arxiv.org/abs/2602.22514",
        "code_url": null,
        "date": "2026-02-26",
        "primary_category": "cs.RO",
        "chinese_summary": "为解决现有机器人人机交互缺乏直观性与包容性，以及手语驱动VLA系统高注释成本和信息损失的问题，本研究提出了首个手语驱动的视觉-语言-动作（VLA）框架。该系统采用无词汇注释范式，将视觉手势直接映射至语义指令，并通过几何归一化、时间平滑和词汇细化处理连续手势流。实验证明，该系统能有效将手语指令转化为精确的机器人动作，在多样交互场景中展现出卓越性能，为无障碍、可扩展的多模态具身智能奠定了基础。"
      },
      {
        "paper_id": "2602.20659",
        "title": "Recursive Belief Vision Language Action Models",
        "abstract": "Vision-language-action models must enable agents to execute long-horizon tasks under partial observability. However, most existing approaches remain observation-driven, relying on short context windows or repeated queries to vision-language models (VLMs). This leads to loss of task progress, action repetition under perceptual aliasing, and high inference latency. While semantic grounding is important, long-horizon manipulation fundamentally requires persistent, action-conditioned state representations. Current VLAs lack such representations and exhibit limited temporal and physical reasoning, making them ill-suited for multi-stage control. This paper introduces RB-VLA, a belief-centric architecture trained with self-supervised world-model objectives that maintains a compact latent state encoding task-relevant history, dynamics, and object interactions. Queried once per task, the VLM provides high-level intent, while the belief tracks task progress and enables phase-aware, causally grounded control under partial observability without storing raw observations or scaling memory with time. The belief and intent jointly condition a diffusion policy for robust closed-loop execution. RB-VLA outperforms prior VLAs on long-horizon benchmarks, achieving 52.5 percent and 37.5 percent higher success rates on multi-stage pick-and-place and stacking tasks, respectively, compared to pi_0. It also reduces inference latency by up to five times relative to baselines and eliminates memory growth across timesteps observed in existing VLAs. Ablations show the belief module is the primary driver of performance, increasing success rates from 32.5 percent without belief to 77.5 percent with belief.",
        "authors_display": "Nirav Kumar Patel Team",
        "pdf_url": "http://arxiv.org/abs/2602.20659",
        "code_url": null,
        "date": "2026-02-25",
        "primary_category": "cs.AI",
        "chinese_summary": "针对现有视觉-语言-动作（VLA）模型在部分可观测性下执行长周期任务时，因观察驱动和缺乏持久状态表示而导致的进度丢失、动作重复及高推理延迟问题，本文提出了RB-VLA架构。RB-VLA采用自监督世界模型目标训练，维护一个紧凑的潜在信念状态，编码任务相关历史和动态，并在每个任务仅查询VLM一次以获取高层意图。实验结果显示，RB-VLA在长周期基准测试中表现优于现有VLA，在多阶段任务中成功率分别提高52.5%和37.5%，并显著降低推理延迟和内存增长，证明信念模块是性能提升的关键驱动因素。"
      },
      {
        "paper_id": "2602.22010",
        "title": "World Guidance: World Modeling in Condition Space for Action Generation",
        "abstract": "Leveraging future observation modeling to facilitate action generation presents a promising avenue for enhancing the capabilities of Vision-Language-Action (VLA) models. However, existing approaches struggle to strike a balance between maintaining efficient, predictable future representations and preserving sufficient fine-grained information to guide precise action generation. To address this limitation, we propose WoG (World Guidance), a framework that maps future observations into compact conditions by injecting them into the action inference pipeline. The VLA is then trained to simultaneously predict these compressed conditions alongside future actions, thereby achieving effective world modeling within the condition space for action inference. We demonstrate that modeling and predicting this condition space not only facilitates fine-grained action generation but also exhibits superior generalization capabilities. Moreover, it learns effectively from substantial human manipulation videos. Extensive experiments across both simulation and real-world environments validate that our method significantly outperforms existing methods based on future prediction. Project page is available at: https://selen-suyue.github.io/WoGNet/",
        "authors_display": "Xihui Liu Team",
        "pdf_url": "http://arxiv.org/abs/2602.22010",
        "code_url": "https://selen-suyue.github.io/WoGNet/",
        "date": "2026-02-25",
        "primary_category": "cs.RO",
        "chinese_summary": "针对视觉-语言-动作（VLA）模型中未来观察建模难以平衡高效表示与精细动作指导的问题，本研究提出了WoG（World Guidance）框架。该框架将未来观察映射为紧凑条件并注入动作推理管道，VLA模型则同时预测这些压缩条件和未来动作，从而在条件空间内实现高效的世界建模。实验证明，该方法不仅促进了精细动作生成，还展现出卓越的泛化能力和从人类操作视频中学习的有效性，显著优于现有基于未来预测的方法。"
      },
      {
        "paper_id": "2602.21877",
        "title": "How to Take a Memorable Picture? Empowering Users with Actionable Feedback",
        "abstract": "Image memorability, i.e., how likely an image is to be remembered, has traditionally been studied in computer vision either as a passive prediction task, with models regressing a scalar score, or with generative methods altering the visual input to boost the image likelihood of being remembered. Yet, none of these paradigms supports users at capture time, when the crucial question is how to improve a photo memorability. We introduce the task of Memorability Feedback (MemFeed), where an automated model should provide actionable, human-interpretable guidance to users with the goal to enhance an image future recall. We also present MemCoach, the first approach designed to provide concrete suggestions in natural language for memorability improvement (e.g., \"emphasize facial expression,\" \"bring the subject forward\"). Our method, based on Multimodal Large Language Models (MLLMs), is training-free and employs a teacher-student steering strategy, aligning the model internal activations toward more memorable patterns learned from a teacher model progressing along least-to-most memorable samples. To enable systematic evaluation on this novel task, we further introduce MemBench, a new benchmark featuring sequence-aligned photoshoots with annotated memorability scores. Our experiments, considering multiple MLLMs, demonstrate the effectiveness of MemCoach, showing consistently improved performance over several zero-shot models. The results indicate that memorability can not only be predicted but also taught and instructed, shifting the focus from mere prediction to actionable feedback for human creators.",
        "authors_display": "Elisa Ricci Team",
        "pdf_url": "http://arxiv.org/abs/2602.21877",
        "code_url": "https://laitifranz.github.io/MemCoach/",
        "date": "2026-02-25",
        "primary_category": "cs.CV",
        "chinese_summary": "传统图像记忆度研究仅限于预测或生成性修改，无法在拍摄时提供可操作的指导。为此，研究引入了记忆度反馈（MemFeed）任务，并提出了MemCoach方法。MemCoach基于多模态大语言模型，通过教师-学生引导策略，以自然语言提供具体记忆度改进建议。为系统评估该新任务，还构建了MemBench基准。实验结果表明，MemCoach在多个MLLM上持续提升性能，证明记忆度不仅可预测，更可通过可操作的反馈进行指导。"
      },
      {
        "paper_id": "2602.21736",
        "title": "Joint-Aligned Latent Action: Towards Scalable VLA Pretraining in the Wild",
        "abstract": "Despite progress, Vision-Language-Action models (VLAs) are limited by a scarcity of large-scale, diverse robot data. While human manipulation videos offer a rich alternative, existing methods are forced to choose between small, precisely-labeled datasets and vast in-the-wild footage with unreliable hand tracking labels. We present JALA, a pretraining framework that learns Jointly-Aligned Latent Actions. JALA bypasses full visual dynamic reconstruction, instead learns a predictive action embedding aligned with both inverse dynamics and real actions. This yields a transition-aware, behavior-centric latent space for learning from heterogeneous human data. We scale this approach with UniHand-Mix, a 7.5M video corpus (>2,000 hours) blending laboratory and in-the-wild footage. Experiments demonstrate that JALA generates more realistic hand motions in both controlled and unconstrained scenarios, significantly improving downstream robot manipulation performance in both simulation and real-world tasks. These results indicate that jointly-aligned latent actions offer a scalable pathway for VLA pretraining from human data.",
        "authors_display": "Zongqing Lu Team",
        "pdf_url": "http://arxiv.org/abs/2602.21736",
        "code_url": null,
        "date": "2026-02-25",
        "primary_category": "cs.RO",
        "chinese_summary": "针对视觉-语言-动作（VLA）模型受限于大规模、多样化机器人数据稀缺的问题，现有方法难以有效利用人类操作视频。研究提出了JALA（Jointly-Aligned Latent Actions）预训练框架，其通过学习与逆动力学和真实动作对齐的预测性动作嵌入，绕过完整的视觉动态重建，构建了转换感知、以行为为中心的潜在空间。该方法结合750万视频的UniHand-Mix语料库，实验证明JALA能生成更逼真的手部动作，显著提升了下游机器人操作性能，为VLA模型从人类数据进行预训练提供了可扩展途径。"
      },
      {
        "paper_id": "2602.21706",
        "title": "SurGo-R1: Benchmarking and Modeling Contextual Reasoning for Operative Zone in Surgical Video",
        "abstract": "Minimally invasive surgery has dramatically improved patient operative outcomes, yet identifying safe operative zones remains challenging in critical phases, requiring surgeons to integrate visual cues, procedural phase, and anatomical context under high cognitive load. Existing AI systems offer binary safety verification or static detection, ignoring the phase-dependent nature of intraoperative reasoning. We introduce ResGo, a benchmark of laparoscopic frames annotated with Go Zone bounding boxes and clinician-authored rationales covering phase, exposure quality reasoning, next action and risk reminder. We introduce evaluation metrics that treat correct grounding under incorrect phase as failures, revealing that most vision-language models cannot handle such tasks and perform poorly. We then present SurGo-R1, a model optimized via RLHF with a multi-turn phase-then-go architecture where the model first identifies the surgical phase, then generates reasoning and Go Zone coordinates conditioned on that context. On unseen procedures, SurGo-R1 achieves 76.6% phase accuracy, 32.7 mIoU, and 54.8% hardcore accuracy, a 6.6$\\times$ improvement over the mainstream generalist VLMs. Code, model and benchmark will be available at https://github.com/jinlab-imvr/SurGo-R1",
        "authors_display": "Yueming Jin Team",
        "pdf_url": "http://arxiv.org/abs/2602.21706",
        "code_url": null,
        "date": "2026-02-25",
        "primary_category": "cs.CV",
        "chinese_summary": "为解决微创手术中安全手术区识别的挑战性，现有AI系统忽略了术中推理的阶段依赖性，研究引入了ResGo基准，其包含带Go Zone边界框标注及临床医生理由的腹腔镜图像。在此基础上，提出了SurGo-R1模型，该模型通过RLHF优化，采用多轮“先识别手术阶段，再生成推理和Go Zone坐标”的架构。实验证明，SurGo-R1在未见过手术中实现了76.6%的阶段准确率、32.7 mIoU和54.8%的硬核准确率，相较于主流通用VLM提升了6.6倍，显著提高了手术安全区识别能力。"
      },
      {
        "paper_id": "2602.21633",
        "title": "Self-Correcting VLA: Online Action Refinement via Sparse World Imagination",
        "abstract": "Standard vision-language-action (VLA) models rely on fitting statistical data priors, limiting their robust understanding of underlying physical dynamics. Reinforcement learning enhances physical grounding through exploration yet typically relies on external reward signals that remain isolated from the agent's internal states. World action models have emerged as a promising paradigm that integrates imagination and control to enable predictive planning. However, they rely on implicit context modeling, lacking explicit mechanisms for self-improvement. To solve these problems, we propose Self-Correcting VLA (SC-VLA), which achieve self-improvement by intrinsically guiding action refinement through sparse imagination. We first design sparse world imagination by integrating auxiliary predictive heads to forecast current task progress and future trajectory trends, thereby constraining the policy to encode short-term physical evolution. Then we introduce the online action refinement module to reshape progress-dependent dense rewards, adjusting trajectory orientation based on the predicted sparse future states. Evaluations on challenging robot manipulation tasks from simulation benchmarks and real-world settings demonstrate that SC-VLA achieve state-of-the-art performance, yielding the highest task throughput with 16% fewer steps and a 9% higher success rate than the best-performing baselines, alongside a 14% gain in real-world experiments. Code is available at https://github.com/Kisaragi0/SC-VLA.",
        "authors_display": "Heng Tao Shen Team",
        "pdf_url": "http://arxiv.org/abs/2602.21633",
        "code_url": null,
        "date": "2026-02-25",
        "primary_category": "cs.RO",
        "chinese_summary": "针对标准VLA模型缺乏物理动力学理解和世界动作模型缺乏自我改进机制的问题，本文提出了自纠正VLA（SC-VLA）。该方法通过稀疏世界想象，整合辅助预测头部来预测任务进展和未来轨迹，并引入在线动作细化模块，根据预测的稀疏未来状态调整轨迹方向，从而实现内在引导的动作细化和自我改进。实验结果表明，SC-VLA在模拟和真实机器人操作任务中均达到了最先进的性能，任务吞吐量更高，完成步骤减少16%，成功率比最佳基线高9%，真实世界实验中性能提升14%。"
      },
      {
        "paper_id": "2602.21531",
        "title": "LiLo-VLA: Compositional Long-Horizon Manipulation via Linked Object-Centric Policies",
        "abstract": "General-purpose robots must master long-horizon manipulation, defined as tasks involving multiple kinematic structure changes (e.g., attaching or detaching objects) in unstructured environments. While Vision-Language-Action (VLA) models offer the potential to master diverse atomic skills, they struggle with the combinatorial complexity of sequencing them and are prone to cascading failures due to environmental sensitivity. To address these challenges, we propose LiLo-VLA (Linked Local VLA), a modular framework capable of zero-shot generalization to novel long-horizon tasks without ever being trained on them. Our approach decouples transport from interaction: a Reaching Module handles global motion, while an Interaction Module employs an object-centric VLA to process isolated objects of interest, ensuring robustness against irrelevant visual features and invariance to spatial configurations. Crucially, this modularity facilitates robust failure recovery through dynamic replanning and skill reuse, effectively mitigating the cascading errors common in end-to-end approaches. We introduce a 21-task simulation benchmark consisting of two challenging suites: LIBERO-Long++ and Ultra-Long. In these simulations, LiLo-VLA achieves a 69% average success rate, outperforming Pi0.5 by 41% and OpenVLA-OFT by 67%. Furthermore, real-world evaluations across 8 long-horizon tasks demonstrate an average success rate of 85%. Project page: https://yy-gx.github.io/LiLo-VLA/.",
        "authors_display": "Daniel Szafir Team",
        "pdf_url": "http://arxiv.org/abs/2602.21531",
        "code_url": null,
        "date": "2026-02-25",
        "primary_category": "cs.RO",
        "chinese_summary": "针对通用机器人在长程操作中VLA模型组合复杂性和级联失败的问题，本文提出了LiLo-VLA（Linked Local VLA）模块化框架，以实现零样本泛化。该方法将运输与交互解耦，通过Reaching模块处理全局运动，Interaction模块利用以对象为中心的VLA处理感兴趣对象，确保对无关特征的鲁棒性。其模块化设计还通过动态重规划和技能重用实现鲁棒的故障恢复。在21个任务的模拟基准测试中，LiLo-VLA平均成功率达69%，显著优于现有基线，并在8个真实世界长程任务中实现了85%的平均成功率。"
      },
      {
        "paper_id": "2602.21172",
        "title": "NoRD: A Data-Efficient Vision-Language-Action Model that Drives without Reasoning",
        "abstract": "Vision-Language-Action (VLA) models are advancing autonomous driving by replacing modular pipelines with unified end-to-end architectures. However, current VLAs face two expensive requirements: (1) massive dataset collection, and (2) dense reasoning annotations. In this work, we address both challenges with NORD (No Reasoning for Driving). Compared to existing VLAs, NORD achieves competitive performance while being fine-tuned on <60% of the data and no reasoning annotations, resulting in 3x fewer tokens. We identify that standard Group Relative Policy Optimization (GRPO) fails to yield significant improvements when applied to policies trained on such small, reasoning-free datasets. We show that this limitation stems from difficulty bias, which disproportionately penalizes reward signals from scenarios that produce high-variance rollouts within GRPO. NORD overcomes this by incorporating Dr. GRPO, a recent algorithm designed to mitigate difficulty bias in LLMs. As a result, NORD achieves competitive performance on Waymo and NAVSIM with a fraction of the training data and no reasoning overhead, enabling more efficient autonomous systems. Website: https://nord-vla-ai.github.io/",
        "authors_display": "Wei Zhan Team",
        "pdf_url": "http://arxiv.org/abs/2602.21172",
        "code_url": null,
        "date": "2026-02-25",
        "primary_category": "cs.AI",
        "chinese_summary": "针对VLA模型在自动驾驶中对大规模数据集和密集推理标注的昂贵需求，本文提出了NORD (No Reasoning for Driving) 方法。NORD在不需要推理标注且仅用少于60%的数据集进行微调的情况下，实现了与现有VLA模型相当的性能，并减少了3倍的token数量。研究发现标准GRPO在小规模、无推理数据集上训练的策略中改进不显著，源于难度偏差，NORD通过引入Dr. GRPO算法缓解此问题。实验证明，NORD在Waymo和NAVSIM上以更少的数据和无推理开销实现了具有竞争力的性能，提高了自动驾驶系统的效率。"
      },
      {
        "paper_id": "2602.22474",
        "title": "When to Act, Ask, or Learn: Uncertainty-Aware Policy Steering",
        "abstract": "Policy steering is an emerging way to adapt robot behaviors at deployment-time: a learned verifier analyzes low-level action samples proposed by a pre-trained policy (e.g., diffusion policy) and selects only those aligned with the task. While Vision-Language Models (VLMs) are promising general-purpose verifiers due to their reasoning capabilities, existing frameworks often assume these models are well-calibrated. In practice, the overconfident judgment from VLM can degrade the steering performance under both high-level semantic uncertainty in task specifications and low-level action uncertainty or incapability of the pre-trained policy. We propose uncertainty-aware policy steering (UPS), a framework that jointly reasons about semantic task uncertainty and low-level action feasibility, and selects an uncertainty resolution strategy: execute a high-confidence action, clarify task ambiguity via natural language queries, or ask for action interventions to correct the low-level policy when it is deemed incapable at the task. We leverage conformal prediction to calibrate the composition of the VLM and the pre-trained base policy, providing statistical assurances that the verifier selects the correct strategy. After collecting interventions during deployment, we employ residual learning to improve the capability of the pre-trained policy, enabling the system to learn continually but with minimal expensive human feedback. We demonstrate our framework through experiments in simulation and on hardware, showing that UPS can disentangle confident, ambiguous, and incapable scenarios and minimizes expensive user interventions compared to uncalibrated baselines and prior human- or robot-gated continual learning approaches. Videos can be found at https://jessie-yuan.github.io/ups/",
        "authors_display": "Andrea Bajcsy Team",
        "pdf_url": "http://arxiv.org/abs/2602.22474",
        "code_url": null,
        "date": "2026-02-25",
        "primary_category": "cs.RO",
        "chinese_summary": "针对策略引导（Policy steering）中视觉-语言模型（VLM）作为验证器可能因过度自信判断，在高层语义和低层动作不确定性下降低性能的问题，研究提出了不确定性感知策略引导（UPS）框架。该框架联合推理语义任务不确定性和低级动作可行性，并通过共形预测校准VLM与基础策略，选择高置信度动作执行、澄清任务模糊性或请求动作干预的策略。仿真和硬件实验表明，UPS能有效区分不同场景，并最大限度减少用户干预。"
      },
      {
        "paper_id": "2602.21161",
        "title": "ActionReasoning: Robot Action Reasoning in 3D Space with LLM for Robotic Brick Stacking",
        "abstract": "Classical robotic systems typically rely on custom planners designed for constrained environments. While effective in restricted settings, these systems lack generalization capabilities, limiting the scalability of embodied AI and general-purpose robots. Recent data-driven Vision-Language-Action (VLA) approaches aim to learn policies from large-scale simulation and real-world data. However, the continuous action space of the physical world significantly exceeds the representational capacity of linguistic tokens, making it unclear if scaling data alone can yield general robotic intelligence. To address this gap, we propose ActionReasoning, an LLM-driven framework that performs explicit action reasoning to produce physics-consistent, prior-guided decisions for robotic manipulation. ActionReasoning leverages the physical priors and real-world knowledge already encoded in Large Language Models (LLMs) and structures them within a multi-agent architecture. We instantiate this framework on a tractable case study of brick stacking, where the environment states are assumed to be already accurately measured. The environmental states are then serialized and passed to a multi-agent LLM framework that generates physics-aware action plans. The experiments demonstrate that the proposed multi-agent LLM framework enables stable brick placement while shifting effort from low-level domain-specific coding to high-level tool invocation and prompting, highlighting its potential for broader generalization. This work introduces a promising approach to bridging perception and execution in robotic manipulation by integrating physical reasoning with LLMs.",
        "authors_display": "Brian Sheil Team",
        "pdf_url": "http://arxiv.org/abs/2602.21161",
        "code_url": null,
        "date": "2026-02-24",
        "primary_category": "cs.RO",
        "chinese_summary": "针对传统机器人系统泛化能力差和数据驱动VLA方法在连续动作空间表示能力不足的问题，本文提出了ActionReasoning框架。该框架是一个LLM驱动的显式动作推理系统，能够为机器人操作生成符合物理原理并受先验引导的决策。ActionReasoning利用LLM中编码的物理先验和真实世界知识，并将其构建在多智能体架构中。通过堆砖任务的案例研究表明，该框架实现了稳定的砖块放置，并将工作重心从低级编码转向高级工具调用和提示，展示了其在机器人操作中弥合感知与执行鸿沟的潜力。"
      },
      {
        "paper_id": "2602.21157",
        "title": "HALO: A Unified Vision-Language-Action Model for Embodied Multimodal Chain-of-Thought Reasoning",
        "abstract": "Vision-Language-Action (VLA) models have shown strong performance in robotic manipulation, but often struggle in long-horizon or out-of-distribution scenarios due to the lack of explicit mechanisms for multimodal reasoning and anticipating how the world will evolve under action. Recent works introduce textual chain-of-thought or visual subgoal prediction within VLA models to reason, but still fail to offer a unified human-like reasoning framework for joint textual reasoning, visual foresight, and action prediction. To this end, we propose HALO, a unified VLA model that enables embodied multimodal chain-of-thought (EM-CoT) reasoning through a sequential process of textual task reasoning, visual subgoal prediction for fine-grained guidance, and EM-CoT-augmented action prediction. We instantiate HALO with a Mixture-of-Transformers (MoT) architecture that decouples semantic reasoning, visual foresight, and action prediction into specialized experts while allowing seamless cross-expert collaboration. To enable HALO learning at scale, we introduce an automated pipeline to synthesize EM-CoT training data along with a carefully crafted training recipe. Extensive experiments demonstrate that: (1) HALO achieves superior performance in both simulated and real-world environments, surpassing baseline policy pi_0 by 34.1% on RoboTwin benchmark; (2) all proposed components of the training recipe and EM-CoT design help improve task success rate; and (3) HALO exhibits strong generalization capabilities under aggressive unseen environmental randomization with our proposed EM-CoT reasoning.",
        "authors_display": "Song Guo Team",
        "pdf_url": "http://arxiv.org/abs/2602.21157",
        "code_url": null,
        "date": "2026-02-24",
        "primary_category": "cs.RO",
        "chinese_summary": "针对VLA模型在长程或分布外场景中缺乏多模态推理和预测世界演变能力的挑战，本文提出了HALO，一个统一的VLA模型，通过具身多模态思维链（EM-CoT）推理实现。HALO通过文本任务推理、视觉子目标预测和EM-CoT增强的动作预测进行顺序处理，并采用Transformer混合架构将语义推理、视觉预见和动作预测解耦为专业专家。为实现大规模学习，研究引入了自动化流水线合成EM-CoT训练数据。实验表明，HALO在模拟和真实世界环境中性能卓越，在RoboTwin基准测试中超越基线34.1%，且在未见环境随机化下展现出强大的泛化能力。"
      },
      {
        "paper_id": "2602.21015",
        "title": "From Perception to Action: An Interactive Benchmark for Vision Reasoning",
        "abstract": "Understanding the physical structure is essential for real-world applications such as embodied agents, interactive design, and long-horizon manipulation. Yet, prevailing Vision-Language Model (VLM) evaluations still center on structure-agnostic, single-turn setups (e.g., VQA), which fail to assess agents' ability to reason about how geometry, contact, and support relations jointly constrain what actions are possible in a dynamic environment. To address this gap, we introduce the Causal Hierarchy of Actions and Interactions (CHAIN) benchmark, an interactive 3D, physics-driven testbed designed to evaluate whether models can understand, plan, and execute structured action sequences grounded in physical constraints. CHAIN shifts evaluation from passive perception to active problem solving, spanning tasks such as interlocking mechanical puzzles and 3D stacking and packing. We conduct a comprehensive study of state-of-the-art VLMs and diffusion-based models under unified interactive settings. Our results show that top-performing models still struggle to internalize physical structure and causal constraints, often failing to produce reliable long-horizon plans and cannot robustly translate perceived structure into effective actions. The project is available at https://social-ai-studio.github.io/CHAIN/.",
        "authors_display": "Roy Ka-Wei Lee Team",
        "pdf_url": "http://arxiv.org/abs/2602.21015",
        "code_url": "https://social-ai-studio.github.io/CHAIN/",
        "date": "2026-02-24",
        "primary_category": "cs.CV",
        "chinese_summary": "针对现有VLM评估未能有效衡量模型对物理结构和因果约束的理解能力的问题，本文引入了Causal Hierarchy of Actions and Interactions (CHAIN) 基准。这是一个交互式3D、物理驱动的测试平台，旨在评估模型理解、规划和执行基于物理约束的结构化动作序列的能力，涵盖机械拼图和3D堆叠等任务。通过对最先进的VLM和扩散模型进行全面研究，结果显示，即使是表现最佳的模型也难以内化物理结构和因果约束，在生成可靠的长程计划和将感知结构转化为有效动作方面仍面临挑战。"
      },
      {
        "paper_id": "2602.21013",
        "title": "Notes-to-Self: Scratchpad Augmented VLAs for Memory Dependent Manipulation Tasks",
        "abstract": "Many dexterous manipulation tasks are non-markovian in nature, yet little attention has been paid to this fact in the recent upsurge of the vision-language-action (VLA) paradigm. Although they are successful in bringing internet-scale semantic understanding to robotics, existing VLAs are primarily \"stateless\" and struggle with memory-dependent long horizon tasks. In this work, we explore a way to impart both spatial and temporal memory to a VLA by incorporating a language scratchpad. The scratchpad makes it possible to memorize task-specific information, such as object positions, and it allows the model to keep track of a plan and progress towards subgoals within that plan. We evaluate this approach on a split of memory-dependent tasks from the ClevrSkills environment, on MemoryBench, as well as on a challenging real-world pick-and-place task. We show that incorporating a language scratchpad significantly improves generalization on these tasks for both non-recurrent and recurrent models.",
        "authors_display": "Roland Memisevic Team",
        "pdf_url": "http://arxiv.org/abs/2602.21013",
        "code_url": null,
        "date": "2026-02-24",
        "primary_category": "cs.RO",
        "chinese_summary": "针对当前VLA模型“无状态”且难以处理记忆依赖的非马尔可夫长程任务的问题，本文提出通过整合语言暂存器（language scratchpad）为VLA模型赋予空间和时间记忆。该暂存器能够记忆特定任务信息，如对象位置，并允许模型跟踪计划及其子目标的进展。在ClevrSkills环境中的记忆依赖任务、MemoryBench以及真实的抓取放置任务上的评估表明，引入语言暂存器显著提高了非循环和循环模型在这些任务上的泛化能力。"
      },
      {
        "paper_id": "2602.20979",
        "title": "Toward an Agentic Infused Software Ecosystem",
        "abstract": "Fully leveraging the capabilities of AI agents in software development requires a rethinking of the software ecosystem itself. To this end, this paper outlines the creation of an Agentic Infused Software Ecosystem (AISE), that rests on three pillars. The first, of course, is the AI agents themselves, which in the past 5 years have moved from simple code completion and toward sophisticated independent development tasks, a trend which will only continue. The second pillar is the programming language and APIs (or tools) that these agents use to accomplish tasks, and increasingly, serve as the communication substrate that humans and AI agents interact and collaborate through. The final pillar is the runtime environment and ecosystem that agents operate within, and which provide the capabilities that programmatic agents use to interface with (and effect actions in) the external world. To realize the vision of AISE, all three pillars must be advanced in a holistic manner, and critically, in a manner that is synergistic for AI agents as they exist today, those that will exist in the future, and for the human developers that work alongside them.",
        "authors_display": "Mark Marron Team",
        "pdf_url": "http://arxiv.org/abs/2602.20979",
        "code_url": null,
        "date": "2026-02-24",
        "primary_category": "cs.SE",
        "chinese_summary": "为了充分发挥AI智能体在软件开发中的潜力，本文提出了Agentic Infused Software Ecosystem (AISE) 的概念，该生态系统建立在三个核心支柱之上。这包括日益独立和复杂的AI智能体本身、作为人类与AI智能体之间沟通基础的编程语言与API，以及为智能体提供与外部世界交互能力的运行时环境。为实现AISE的愿景，本文强调必须以整体协同的方式推进这三个支柱，以实现对现有和未来AI智能体以及人类开发者的共同增效。"
      },
      {
        "paper_id": "2602.20715",
        "title": "IG-RFT: An Interaction-Guided RL Framework for VLA Models in Long-Horizon Robotic Manipulation",
        "abstract": "Vision-Language-Action (VLA) models have demonstrated significant potential for generalist robotic policies; however, they struggle to generalize to long-horizon complex tasks in novel real-world domains due to distribution shifts and the scarcity of high-quality demonstrations. Although reinforcement learning (RL) offers a promising avenue for policy improvement, applying it to real-world VLA fine-tuning faces challenges regarding exploration efficiency, training stability, and sample cost. To address these issues, we propose IG-RFT, a novel Interaction-Guided Reinforced Fine-Tuning system designed for flow-based VLA models. Firstly, to facilitate effective policy optimization, we introduce Interaction-Guided Advantage Weighted Regression (IG-AWR), an RL algorithm that dynamically modulates exploration intensity based on the robot's interaction status. Furthermore, to address the limitations of sparse or task-specific rewards, we design a novel hybrid dense reward function that integrates the trajectory-level reward and the subtask-level reward. Finally, we construct a three-stage RL system comprising SFT, Offline RL, and Human-in-the-Loop RL for fine-tuning VLA models. Extensive real-world experiments on four challenging long-horizon tasks demonstrate that IG-RFT achieves an average success rate of 85.0%, significantly outperforming SFT (18.8%) and standard Offline RL baselines (40.0%). Ablation studies confirm the critical contributions of IG-AWR and hybrid reward shaping. In summary, our work establishes and validates a novel reinforced fine-tuning system for VLA models in real-world robotic manipulation.",
        "authors_display": "Huixu Dong Team",
        "pdf_url": "http://arxiv.org/abs/2602.20715",
        "code_url": null,
        "date": "2026-02-24",
        "primary_category": "cs.RO",
        "chinese_summary": "针对视觉-语言-动作（VLA）模型在真实世界复杂任务中泛化能力差以及强化学习（RL）微调面临的探索效率、训练稳定性和样本成本挑战，本文提出了IG-RFT系统。该系统引入交互引导优势加权回归（IG-AWR）算法动态调节探索强度，设计了整合轨迹级和子任务级奖励的混合密集奖励函数，并构建了包含监督微调、离线RL和人机协作RL的三阶段RL系统。实验表明，IG-RFT在四项长周期真实世界任务中平均成功率达85.0%，显著优于基线，消融研究证实了IG-AWR和混合奖励塑形的有效性。"
      },
      {
        "paper_id": "2602.20687",
        "title": "How Foundational Skills Influence VLM-based Embodied Agents:A Native Perspective",
        "abstract": "Recent advances in vision-language models (VLMs) have shown promise for human-level embodied intelligence. However, existing benchmarks for VLM-driven embodied agents often rely on high-level commands or discretized action spaces, which are non-native settings that differ markedly from real-world control. In addition, current benchmarks focus primarily on high-level tasks and lack joint evaluation and analysis at both low and high levels. To address these limitations, we present NativeEmbodied, a challenging benchmark for VLM-driven embodied agents that uses a unified, native low-level action space. Built on diverse simulated scenes, NativeEmbodied includes three representative high-level tasks in complex scenarios to evaluate overall performance. For more detailed analysis, we further decouple the skills required by complex tasks and construct four types of low-level tasks, each targeting a fundamental embodied skill. This joint evaluation across task and skill granularities enables fine-grained assessment of embodied agents. Experiments with state-of-the-art VLMs reveal clear deficiencies in several fundamental embodied skills, and further analysis shows that these bottlenecks significantly limit performance on high-level tasks. NativeEmbodied highlights key challenges for current VLM-driven embodied agents and provides insights to guide future research.",
        "authors_display": "Tong Xu Team",
        "pdf_url": "http://arxiv.org/abs/2602.20687",
        "code_url": null,
        "date": "2026-02-24",
        "primary_category": "cs.AI",
        "chinese_summary": "鉴于现有VLM驱动具身智能体基准依赖非原生的高层指令或离散动作空间，且缺乏低层和高层任务的联合评估，本文提出了NativeEmbodied基准。该基准采用统一的原生低层动作空间，在多样化模拟场景中包含三个复杂高层任务以评估整体性能，并解耦了任务技能以构建四种类型的低层任务进行细粒度评估。对最先进VLM的实验揭示了模型在多项基本具身技能上的明显缺陷，这些瓶颈显著限制了其在高层任务中的表现，为未来研究提供了关键洞察。"
      },
      {
        "paper_id": "2602.20577",
        "title": "Efficient and Explainable End-to-End Autonomous Driving via Masked Vision-Language-Action Diffusion",
        "abstract": "Large Language Models (LLMs) and Vision-Language Models (VLMs) have emerged as promising candidates for end-to-end autonomous driving. However, these models typically face challenges in inference latency, action precision, and explainability. Existing autoregressive approaches struggle with slow token-by-token generation, while prior diffusion-based planners often rely on verbose, general-purpose language tokens that lack explicit geometric structure. In this work, we propose Masked Vision-Language-Action Diffusion for Autonomous Driving (MVLAD-AD), a novel framework designed to bridge the gap between efficient planning and semantic explainability via a masked vision-language-action diffusion model. Unlike methods that force actions into the language space, we introduce a discrete action tokenization strategy that constructs a compact codebook of kinematically feasible waypoints from real-world driving distributions. Moreover, we propose geometry-aware embedding learning to ensure that embeddings in the latent space approximate physical geometric metrics. Finally, an action-priority decoding strategy is introduced to prioritize trajectory generation. Extensive experiments on nuScenes and derived benchmarks demonstrate that MVLAD-AD achieves superior efficiency and outperforms state-of-the-art autoregressive and diffusion baselines in planning precision, while providing high-fidelity and explainable reasoning.",
        "authors_display": "Ziran Wang Team",
        "pdf_url": "http://arxiv.org/abs/2602.20577",
        "code_url": null,
        "date": "2026-02-24",
        "primary_category": "cs.CV",
        "chinese_summary": "针对大型语言模型和视觉-语言模型在自动驾驶中面临的推理延迟、动作精度和可解释性挑战，本文提出了MVLAD-AD框架。该框架通过一个掩码视觉-语言-动作扩散模型，弥合了高效规划与语义可解释性之间的鸿沟。方法上，引入了离散动作token化策略构建运动可行路径点码本，并设计了几何感知嵌入学习以近似物理几何指标，最后通过动作优先级解码策略优化轨迹生成。在nuScenes数据集上的实验表明，MVLAD-AD在规划精度上优于现有自回归和扩散基线，并提供了高保真和可解释的推理。"
      },
      {
        "paper_id": "2602.20575",
        "title": "An interactive enhanced driving dataset for autonomous driving",
        "abstract": "The evolution of autonomous driving towards full automation demands robust interactive capabilities; however, the development of Vision-Language-Action (VLA) models is constrained by the sparsity of interactive scenarios and inadequate multimodal alignment in existing data. To this end, this paper proposes the Interactive Enhanced Driving Dataset (IEDD). We develop a scalable pipeline to mine million-level interactive segments from naturalistic driving data based on interactive trajectories, and design metrics to quantify the interaction processes. Furthermore, the IEDD-VQA dataset is constructed by generating synthetic Bird's Eye View (BEV) videos where semantic actions are strictly aligned with structured language. Benchmark results evaluating ten mainstream Vision Language Models (VLMs) are provided to demonstrate the dataset's reuse value in assessing and fine-tuning the reasoning capabilities of autonomous driving models.",
        "authors_display": "Lu Xiong Team",
        "pdf_url": "http://arxiv.org/abs/2602.20575",
        "code_url": null,
        "date": "2026-02-24",
        "primary_category": "cs.CV",
        "chinese_summary": "鉴于自动驾驶对鲁棒交互能力的需求以及现有VLA模型在交互场景数据稀疏和多模态对齐不足的限制，本文提出了交互增强驾驶数据集（IEDD）。该数据集通过可扩展的流水线从自然驾驶数据中挖掘百万级交互片段并量化交互过程，同时构建了语义动作与结构化语言严格对齐的合成鸟瞰图（BEV）视频数据集IEDD-VQA。基准测试结果评估了十个主流VLM，证明了IEDD在评估和微调自动驾驶模型推理能力方面的价值。"
      },
      {
        "paper_id": "2602.20566",
        "title": "BFA++: Hierarchical Best-Feature-Aware Token Prune for Multi-View Vision Language Action Model",
        "abstract": "Vision-Language-Action (VLA) models have achieved significant breakthroughs by leveraging Large Vision Language Models (VLMs) to jointly interpret instructions and visual inputs. However, the substantial increase in visual tokens, particularly from multi-view inputs, poses serious challenges to real-time robotic manipulation. Existing acceleration techniques for VLMs, such as token pruning, often result in degraded performance when directly applied to VLA models, as they overlook the relationships between different views and fail to account for the dynamic and task-specific characteristics of robotic operation. To address this, we propose BFA++, a dynamic token pruning framework designed specifically for VLA models. BFA++ introduces a hierarchical pruning strategy guided by two-level importance predictors: an intra-view predictor highlights task-relevant regions within each image to suppress spatial noise, while an inter-view predictor identifies critical camera views throughout different manipulation phases to reduce cross-view redundancy. This design enables efficient token selection while preserving essential visual cues, resulting in improved computational efficiency and higher manipulation success rates. Evaluations on the RoboTwin benchmark and real-world robotic tasks demonstrate that BFA++ consistently outperforms existing methods. BFA++ improves the success rate by about 10% on both the π0 and RDT models, achieving speedup of 1.8X and 1.5X, respectively. Our results highlight that context-sensitive and task-aware token pruning serves as a more effective strategy than full visual processing, enabling faster inference and improved manipulation accuracy in real-world robotic systems.",
        "authors_display": "Hua Chen Team",
        "pdf_url": "http://arxiv.org/abs/2602.20566",
        "code_url": null,
        "date": "2026-02-24",
        "primary_category": "cs.RO",
        "chinese_summary": "针对视觉-语言-动作（VLA）模型在处理多视图输入时视觉tokens急剧增加导致实时机器人操作面临的挑战，本文提出了BFA++，一个专为VLA模型设计的动态token剪枝框架。BFA++引入了两级重要性预测器引导的分层剪枝策略：视图内预测器抑制图像空间噪声，视图间预测器识别跨操作阶段的关键相机视图，以减少冗余。实验结果表明，BFA++在RoboTwin基准和真实世界任务中始终优于现有方法，成功率在π0和RDT模型上提高约10%，并分别实现1.8倍和1.5倍的速度提升，验证了上下文敏感和任务感知的token剪枝能有效提升推理速度和操作精度。"
      },
      {
        "paper_id": "2602.20517",
        "title": "Inner Speech as Behavior Guides: Steerable Imitation of Diverse Behaviors for Human-AI coordination",
        "abstract": "Effective human-AI coordination requires artificial agents capable of exhibiting and responding to human-like behaviors while adapting to changing contexts. Imitation learning has emerged as one of the prominent approaches to build such agents by training them to mimic human-demonstrated behaviors. However, current methods struggle to capture the inherent diversity and non-Markovian nature of human behavior and lack the ability to steer behavior at inference time. Drawing inspiration from the theory of human cognitive processes, where inner speech guides action selection before execution, we propose MIMIC (Modeling Inner Motivations for Imitation and Control), a framework that uses language as an internal representation of behavioral intent. MIMIC employs the novel use of vision-language models as linguistic scaffolding to train a conditional variational autoencoder capable of generating inner speech from observations. A diffusion-based behavior cloning policy then selects actions conditioned on current observations and the generated inner speech. MIMIC enables fine-grained steering of behavior at inference time by conditioning the agent on behavior-specific speech. Experiments across robotic manipulation tasks and human-AI collaboration games demonstrate that MIMIC significantly enhances both behavior diversity and fidelity to human demonstrations while enabling nuanced behavioral steering without training on additional demonstrations. We open source our code and provide pre-trained MIMIC agents and qualitative demos at: https://mimic-research.github.io.",
        "authors_display": "David C Parkes Team",
        "pdf_url": "http://arxiv.org/abs/2602.20517",
        "code_url": null,
        "date": "2026-02-24",
        "primary_category": "cs.AI",
        "chinese_summary": "针对现有模仿学习方法难以捕捉人类行为多样性和非马尔可夫性，且缺乏推理时行为引导能力的问题，本文提出了MIMIC框架，旨在通过将语言作为行为意图的内部表征，实现更有效的人机协作。MIMIC创新性地利用视觉语言模型作为语言支架，训练一个条件变分自编码器以从观察中生成“内在言语”，随后扩散行为克隆策略结合观察和内在言语选择行动。实验证明，MIMIC显著提升了行为多样性和对人类演示的忠实度，并实现了无需额外演示训练的精细行为引导。"
      },
      {
        "paper_id": "2602.20502",
        "title": "ActionEngine: From Reactive to Programmatic GUI Agents via State Machine Memory",
        "abstract": "Existing Graphical User Interface (GUI) agents operate through step-by-step calls to vision language models--taking a screenshot, reasoning about the next action, executing it, then repeating on the new page--resulting in high costs and latency that scale with the number of reasoning steps, and limited accuracy due to no persistent memory of previously visited pages.   We propose ActionEngine, a training-free framework that transitions from reactive execution to programmatic planning through a novel two-agent architecture: a Crawling Agent that constructs an updatable state-machine memory of the GUIs through offline exploration, and an Execution Agent that leverages this memory to synthesize complete, executable Python programs for online task execution.   To ensure robustness against evolving interfaces, execution failures trigger a vision-based re-grounding fallback that repairs the failed action and updates the memory.   This design drastically improves both efficiency and accuracy: on Reddit tasks from the WebArena benchmark, our agent achieves 95% task success with on average a single LLM call, compared to 66% for the strongest vision-only baseline, while reducing cost by 11.8x and end-to-end latency by 2x.   Together, these components yield scalable and reliable GUI interaction by combining global programmatic planning, crawler-validated action templates, and node-level execution with localized validation and repair.",
        "authors_display": "Suman Nath Team",
        "pdf_url": "http://arxiv.org/abs/2602.20502",
        "code_url": null,
        "date": "2026-02-24",
        "primary_category": "cs.AI",
        "chinese_summary": "针对现有GUI代理因逐步调用VLM而导致高成本、高延迟和低准确性的问题，本文提出了ActionEngine，一个无训练框架。该框架采用双代理架构：爬行代理通过离线探索构建GUI状态机记忆，执行代理利用此记忆合成可执行Python程序进行在线任务执行。为增强鲁棒性，执行失败会触发视觉重定位回退机制进行修复。实验结果显示，ActionEngine在WebArena基准测试中实现了95%的任务成功率，平均仅需一次LLM调用，同时成本降低11.8倍，端到端延迟降低2倍，显著提升了效率和准确性。"
      },
      {
        "paper_id": "2602.21445",
        "title": "VLA Knows Its Limits",
        "abstract": "Action chunking has recently emerged as a standard practice in flow-based Vision-Language-Action (VLA) models. However, the effect and choice of the execution horizon - the number of actions to be executed from each predicted chunk - remains underexplored. In this work, we first show that varying the execution horizon leads to substantial performance deviations, with performance initially improving and then declining as the horizon increases. To uncover the reasons, we analyze the cross- and self-attention weights in flow-based VLAs and reveal two key phenomena: (i) intra-chunk actions attend invariantly to vision-language tokens, limiting adaptability to environmental changes; and (ii) the initial and terminal action tokens serve as stable anchors, forming latent centers around which intermediate actions are organized. Motivated by these insights, we interpret action self-attention weights as a proxy for the model's predictive limit and propose AutoHorizon, the first test-time method that dynamically estimates the execution horizon for each predicted action chunk to adapt to changing perceptual conditions. Across simulated and real-world robotic manipulation tasks, AutoHorizon is performant, incurs negligible computational overhead, and generalizes across diverse tasks and flow-based models.",
        "authors_display": "Gaowen Liu Team",
        "pdf_url": "http://arxiv.org/abs/2602.21445",
        "code_url": "https://hatchetproject.github.io/autohorizon/",
        "date": "2026-02-24",
        "primary_category": "cs.RO",
        "chinese_summary": "针对流式VLA模型中动作分块执行步长的影响和选择问题，本文研究发现执行步长变化会导致显著的性能波动。通过分析VLA的注意力权重，揭示了块内动作对视觉-语言token的注意力不变性和初始/终端动作token作为稳定锚点组织中间动作的现象。受此启发，本文提出AutoHorizon，一个在测试时动态估计每个动作块执行步长的方法，以适应不断变化的感知条件。实验证明，AutoHorizon在模拟和真实机器人操作任务中表现出色，计算开销可忽略不计，并能泛化到各种任务和模型。"
      },
      {
        "paper_id": "2602.21406",
        "title": "Exploring Vision-Language Models for Open-Vocabulary Zero-Shot Action Segmentation",
        "abstract": "Temporal Action Segmentation (TAS) requires dividing videos into action segments, yet the vast space of activities and alternative breakdowns makes collecting comprehensive datasets infeasible. Existing methods remain limited to closed vocabularies and fixed label sets. In this work, we explore the largely unexplored problem of Open-Vocabulary Zero-Shot Temporal Action Segmentation (OVTAS) by leveraging the strong zero-shot capabilities of Vision-Language Models (VLMs). We introduce a training-free pipeline that follows a segmentation-by-classification design: Frame-Action Embedding Similarity (FAES) matches video frames to candidate action labels, and Similarity-Matrix Temporal Segmentation (SMTS) enforces temporal consistency. Beyond proposing OVTAS, we present a systematic study across 14 diverse VLMs, providing the first broad analysis of their suitability for open-vocabulary action segmentation. Experiments on standard benchmarks show that OVTAS achieves strong results without task-specific supervision, underscoring the potential of VLMs for structured temporal understanding.",
        "authors_display": "Karthik Ramani Team",
        "pdf_url": "http://arxiv.org/abs/2602.21406",
        "code_url": null,
        "date": "2026-02-24",
        "primary_category": "cs.CV",
        "chinese_summary": "针对时间动作分割（TAS）中数据集收集困难和现有方法受限于封闭词汇的问题，本文探索了开放词汇零样本时间动作分割（OVTAS）。该研究利用视觉-语言模型（VLMs）的零样本能力，提出了一个免训练的流水线，包括帧-动作嵌入相似性（FAES）匹配和相似度矩阵时间分割（SMTS）强制时间一致性。本文还对14个不同的VLM进行了系统性研究，首次广泛分析了它们在开放词汇动作分割中的适用性。实验结果表明，OVTAS在没有特定任务监督的情况下，在标准基准测试上取得了良好结果，彰显了VLM在结构化时间理解方面的潜力。"
      },
      {
        "paper_id": "2602.20119",
        "title": "NovaPlan: Zero-Shot Long-Horizon Manipulation via Closed-Loop Video Language Planning",
        "abstract": "Solving long-horizon tasks requires robots to integrate high-level semantic reasoning with low-level physical interaction. While vision-language models (VLMs) and video generation models can decompose tasks and imagine outcomes, they often lack the physical grounding necessary for real-world execution. We introduce NovaPlan, a hierarchical framework that unifies closed-loop VLM and video planning with geometrically grounded robot execution for zero-shot long-horizon manipulation. At the high level, a VLM planner decomposes tasks into sub-goals and monitors robot execution in a closed loop, enabling the system to recover from single-step failures through autonomous re-planning. To compute low-level robot actions, we extract and utilize both task-relevant object keypoints and human hand poses as kinematic priors from the generated videos, and employ a switching mechanism to choose the better one as a reference for robot actions, maintaining stable execution even under heavy occlusion or depth inaccuracy. We demonstrate the effectiveness of NovaPlan on three long-horizon tasks and the Functional Manipulation Benchmark (FMB). Our results show that NovaPlan can perform complex assembly tasks and exhibit dexterous error recovery behaviors without any prior demonstrations or training. Project page: https://nova-plan.github.io/",
        "authors_display": "George Konidaris Team",
        "pdf_url": "http://arxiv.org/abs/2602.20119",
        "code_url": "https://nova-plan.github.io/",
        "date": "2026-02-23",
        "primary_category": "cs.RO",
        "chinese_summary": "为解决机器人执行长周期任务时VLM和视频生成模型缺乏物理基础的问题，本文提出了NovaPlan，一个分层框架，将闭环VLM和视频规划与几何接地的机器人执行相结合，以实现零样本长周期操作。在高层，VLM规划器分解子目标并监控执行；在低层，系统从生成视频中提取任务相关的对象关键点和人手姿态作为运动学先验，并动态选择最优参考以稳定执行。实验表明，NovaPlan无需预先演示或训练，即可成功执行复杂组装任务并展现灵活的错误恢复能力。"
      },
      {
        "paper_id": "2602.19710",
        "title": "Universal Pose Pretraining for Generalizable Vision-Language-Action Policies",
        "abstract": "Existing Vision-Language-Action (VLA) models often suffer from feature collapse and low training efficiency because they entangle high-level perception with sparse, embodiment-specific action supervision. Since these models typically rely on VLM backbones optimized for Visual Question Answering (VQA), they excel at semantic identification but often overlook subtle 3D state variations that dictate distinct action patterns.   To resolve these misalignments, we propose Pose-VLA, a decoupled paradigm that separates VLA training into a pre-training phase for extracting universal 3D spatial priors in a unified camera-centric space, and a post-training phase for efficient embodiment alignment within robot-specific action space. By introducing discrete pose tokens as a universal representation, Pose-VLA seamlessly integrates spatial grounding from diverse 3D datasets with geometry-level trajectories from robotic demonstrations. Our framework follows a two-stage pre-training pipeline, establishing fundamental spatial grounding via poses followed by motion alignment through trajectory supervision.   Extensive evaluations demonstrate that Pose-VLA achieves state-of-the-art results on RoboTwin 2.0 with a 79.5% average success rate and competitive performance on LIBERO at 96.0%. Real-world experiments further showcase robust generalization across diverse objects using only 100 demonstrations per task, validating the efficiency of our pre-training paradigm.",
        "authors_display": "Yanwei Fu Team",
        "pdf_url": "http://arxiv.org/abs/2602.19710",
        "code_url": null,
        "date": "2026-02-23",
        "primary_category": "cs.CV",
        "chinese_summary": "针对现有VLA模型因高层感知与稀疏、具身特定动作监督的纠缠导致的特征塌陷和训练效率低下问题，本文提出了Pose-VLA，一种解耦范式。该方法将VLA训练分为预训练和后训练两阶段：预训练阶段在统一相机中心空间中提取通用3D空间先验，后训练阶段高效对齐机器人特定动作空间。通过引入离散姿态令牌作为通用表示，Pose-VLA无缝整合3D空间基础与几何级轨迹。实验结果表明，Pose-VLA在RoboTwin 2.0和LIBERO上取得了最先进的成功率，并在真实世界实验中验证了其在仅少量演示下对不同对象的鲁棒泛化能力。"
      },
      {
        "paper_id": "2602.20309",
        "title": "QuantVLA: Scale-Calibrated Post-Training Quantization for Vision-Language-Action Models",
        "abstract": "Vision-language-action (VLA) models unify perception, language, and control for embodied agents but face significant challenges in practical deployment due to rapidly increasing compute and memory demands, especially as models scale to longer horizons and larger backbones. To address these bottlenecks, we introduce QuantVLA, a training-free post-training quantization (PTQ) framework that, to our knowledge, is the first PTQ approach for VLA systems and the first to successfully quantize a diffusion transformer (DiT) action head. QuantVLA incorporates three scale-calibrated components: (1) a selective quantization layout that integerizes all linear layers in both the language backbone and the DiT while keeping attention projections in floating point to preserve the original operator schedule; (2) attention temperature matching, a lightweight per-head scaling mechanism that stabilizes attention logits and is folded into the dequantization scales at inference; and (3) output head balancing, a per-layer residual interface calibration that mitigates post-projection energy drift. The framework requires no additional training, uses only a small unlabeled calibration buffer, and supports integer kernels for low-bit weights and activations while leaving the architecture unchanged. Across representative VLA models on LIBERO, QuantVLA exceeds the task success rates of full-precision baselines, achieves about 70% relative memory savings on the quantized components, and delivers a 1.22x speedup in end-to-end inference latency, providing a practical pathway toward scalable low-bit embodied intelligence under strict compute, memory, and power constraints.",
        "authors_display": "Mi Zhang Team",
        "pdf_url": "http://arxiv.org/abs/2602.20309",
        "code_url": null,
        "date": "2026-02-23",
        "primary_category": "cs.LG",
        "chinese_summary": "为应对VLA模型在实际部署中日益增长的计算和内存需求，特别是模型规模扩大时的挑战，本文提出了QuantVLA，一个无训练的后训练量化（PTQ）框架。QuantVLA首次将PTQ应用于VLA系统并成功量化了扩散Transformer动作头，它整合了选择性量化布局、注意力温度匹配和输出头平衡等三个尺度校准组件。实验结果表明，QuantVLA在LIBERO基准上超越了全精度基线，实现了约70%的内存节省和1.22倍的端到端推理延迟加速，为低位具身智能提供了实际可行的解决方案。"
      },
      {
        "paper_id": "2602.20291",
        "title": "De-rendering, Reasoning, and Repairing Charts with Vision-Language Models",
        "abstract": "Data visualizations are central to scientific communication, journalism, and everyday decision-making, yet they are frequently prone to errors that can distort interpretation or mislead audiences. Rule-based visualization linters can flag violations, but they miss context and do not suggest meaningful design changes. Directly querying general-purpose LLMs about visualization quality is unreliable: lacking training to follow visualization design principles, they often produce inconsistent or incorrect feedback. In this work, we introduce a framework that combines chart de-rendering, automated analysis, and iterative improvement to deliver actionable, interpretable feedback on visualization design. Our system reconstructs the structure of a chart from an image, identifies design flaws using vision-language reasoning, and proposes concrete modifications supported by established principles in visualization research. Users can selectively apply these improvements and re-render updated figures, creating a feedback loop that promotes both higher-quality visualizations and the development of visualization literacy. In our evaluation on 1,000 charts from the Chart2Code benchmark, the system generated 10,452 design recommendations, which clustered into 10 coherent categories (e.g., axis formatting, color accessibility, legend consistency). These results highlight the promise of LLM-driven recommendation systems for delivering structured, principle-based feedback on visualization design, opening the door to more intelligent and accessible authoring tools.",
        "authors_display": "Emmanuel Iarussi Team",
        "pdf_url": "http://arxiv.org/abs/2602.20291",
        "code_url": null,
        "date": "2026-02-23",
        "primary_category": "cs.CV",
        "chinese_summary": "针对数据可视化中存在的错误及其对受众的误导问题，以及现有规则校验器和通用LLM在提供有意义设计反馈上的不足，本文提出了一个结合图表逆渲染、自动化分析和迭代改进的框架。该系统能够从图像重建图表结构，利用视觉语言推理识别设计缺陷，并基于可视化研究原则提供具体的修改建议。在Chart2Code基准的1000张图表上进行评估，系统生成了10,452条设计建议，分类为10个连贯类别，证明了LLM驱动的推荐系统在提供结构化、基于原则的可视化设计反馈方面的巨大潜力。"
      },
      {
        "paper_id": "2602.20231",
        "title": "UniLACT: Depth-Aware RGB Latent Action Learning for Vision-Language-Action Models",
        "abstract": "Latent action representations learned from unlabeled videos have recently emerged as a promising paradigm for pretraining vision-language-action (VLA) models without explicit robot action supervision. However, latent actions derived solely from RGB observations primarily encode appearance-driven dynamics and lack explicit 3D geometric structure, which is essential for precise and contact-rich manipulation. To address this limitation, we introduce UniLACT, a transformer-based VLA model that incorporates geometric structure through depth-aware latent pretraining, enabling downstream policies to inherit stronger spatial priors. To facilitate this process, we propose UniLARN, a unified latent action learning framework based on inverse and forward dynamics objectives that learns a shared embedding space for RGB and depth while explicitly modeling their cross-modal interactions. This formulation produces modality-specific and unified latent action representations that serve as pseudo-labels for the depth-aware pretraining of UniLACT. Extensive experiments in both simulation and real-world settings demonstrate the effectiveness of depth-aware unified latent action representations. UniLACT consistently outperforms RGB-based latent action baselines under in-domain and out-of-domain pretraining regimes, as well as on both seen and unseen manipulation tasks.",
        "authors_display": "Srijan Das Team",
        "pdf_url": "http://arxiv.org/abs/2602.20231",
        "code_url": "https://manishgovind.github.io/unilact-vla/",
        "date": "2026-02-23",
        "primary_category": "cs.RO",
        "chinese_summary": "针对现有VLA模型中基于RGB的潜在动作表示缺乏精确操作所需的3D几何结构问题，本文引入了UniLACT，一个基于Transformer的VLA模型，通过深度感知潜在预训练融入几何结构。为实现此目的，本文提出了UniLARN，一个统一的潜在动作学习框架，利用逆动力学和正向动力学目标学习RGB和深度的共享嵌入空间，并显式建模它们的跨模态交互。实验证明，UniLACT在模拟和真实世界环境中，无论是域内还是域外预训练，以及在已见或未见操作任务上，都持续优于基于RGB的潜在动作基线，验证了深度感知统一潜在动作表示的有效性。"
      },
      {
        "paper_id": "2602.20219",
        "title": "An Approach to Combining Video and Speech with Large Language Models in Human-Robot Interaction",
        "abstract": "Interpreting human intent accurately is a central challenge in human-robot interaction (HRI) and a key requirement for achieving more natural and intuitive collaboration between humans and machines. This work presents a novel multimodal HRI framework that combines advanced vision-language models, speech processing, and fuzzy logic to enable precise and adaptive control of a Dobot Magician robotic arm. The proposed system integrates Florence-2 for object detection, Llama 3.1 for natural language understanding, and Whisper for speech recognition, providing users with a seamless and intuitive interface for object manipulation through spoken commands. By jointly addressing scene perception and action planning, the approach enhances the reliability of command interpretation and execution. Experimental evaluations conducted on consumer-grade hardware demonstrate a command execution accuracy of 75\\%, highlighting both the robustness and adaptability of the system. Beyond its current performance, the proposed architecture serves as a flexible and extensible foundation for future HRI research, offering a practical pathway toward more sophisticated and natural human-robot collaboration through tightly coupled speech and vision-language processing.",
        "authors_display": "Zi Tian Team",
        "pdf_url": "http://arxiv.org/abs/2602.20219",
        "code_url": null,
        "date": "2026-02-23",
        "primary_category": "cs.RO",
        "chinese_summary": "为解决人机交互中准确理解人类意图的核心挑战，本文提出了一个新颖的多模态HRI框架，结合了先进的视觉语言模型、语音处理和模糊逻辑，以实现对Dobot Magician机械臂的精确自适应控制。该系统集成Florence-2进行物体检测、Llama 3.1进行自然语言理解和Whisper进行语音识别，为用户提供了通过语音命令操纵物体的无缝直观界面，显著提高了命令解释和执行的可靠性。在消费级硬件上的实验评估显示，该系统命令执行准确率为75%，为未来更复杂、自然的人机协作提供了灵活可扩展的基础。"
      },
      {
        "paper_id": "2602.19372",
        "title": "Seeing Farther and Smarter: Value-Guided Multi-Path Reflection for VLM Policy Optimization",
        "abstract": "Solving complex, long-horizon robotic manipulation tasks requires a deep understanding of physical interactions, reasoning about their long-term consequences, and precise high-level planning. Vision-Language Models (VLMs) offer a general perceive-reason-act framework for this goal. However, previous approaches using reflective planning to guide VLMs in correcting actions encounter significant limitations. These methods rely on inefficient and often inaccurate implicit learning of state-values from noisy foresight predictions, evaluate only a single greedy future, and suffer from substantial inference latency. To address these limitations, we propose a novel test-time computation framework that decouples state evaluation from action generation. This provides a more direct and fine-grained supervisory signal for robust decision-making. Our method explicitly models the advantage of an action plan, quantified by its reduction in distance to the goal, and uses a scalable critic to estimate. To address the stochastic nature of single-trajectory evaluation, we employ beam search to explore multiple future paths and aggregate them during decoding to model their expected long-term returns, leading to more robust action generation. Additionally, we introduce a lightweight, confidence-based trigger that allows for early exit when direct predictions are reliable, invoking reflection only when necessary. Extensive experiments on diverse, unseen multi-stage robotic manipulation tasks demonstrate a 24.6% improvement in success rate over state-of-the-art baselines, while significantly reducing inference time by 56.5%.",
        "authors_display": "Dimitris N. Metaxas Team",
        "pdf_url": "http://arxiv.org/abs/2602.19372",
        "code_url": null,
        "date": "2026-02-22",
        "primary_category": "cs.RO",
        "chinese_summary": "为解决复杂长周期机器人操作中VLM反射规划效率低、精度差、延迟大等问题，本文提出一种新型测试时计算框架，将状态评估与动作生成解耦，以提供更直接、细粒度的监督信号。该方法显式建模动作计划优势，通过可扩展的Critic进行估计，并采用束搜索探索多条未来路径以聚合预期长期回报。此外，引入基于置信度的触发器，实现必要时才启动反射。实验结果显示，该方法在多阶段机器人操作任务中成功率提高24.6%，同时推理时间减少56.5%。"
      },
      {
        "paper_id": "2602.19357",
        "title": "MentalBlackboard: Evaluating Spatial Visualization via Mathematical Transformations",
        "abstract": "Spatial visualization is the mental ability to imagine, transform, and manipulate the spatial characteristics of objects and actions. This intelligence is a part of human cognition where actions and perception are connected on a mental level. To explore whether state-of-the-art Vision-Language Models (VLMs) exhibit this ability, we develop MentalBlackboard, an open-ended spatial visualization benchmark for Paper Folding and Hole Punching tests within two core tasks: prediction and planning. Our prediction experiments reveal that models struggle with applying symmetrical transformations, even when they predict the sequence of unfolding steps correctly. Also, rotations introduce a significant challenge to the physical situational awareness for models. The planning task reveals limitations of models in analyzing symmetrical relationships and in implementing the multi-stage symmetry process, with Claude Opus 4.1 achieving the highest planning score at an accuracy of 10\\%. The top-performing model, o3, attains a peak performance of 71.6\\% on the generalization task, which does not require spatial visualization but transfers spatial data; however, it achieves only 25\\% accuracy on text-based prediction tasks.",
        "authors_display": "Yezhou Yang Team",
        "pdf_url": "http://arxiv.org/abs/2602.19357",
        "code_url": null,
        "date": "2026-02-22",
        "primary_category": "cs.CV",
        "chinese_summary": "为探究当前视觉-语言模型（VLMs）是否具备人类的空间可视化能力，本文开发了MentalBlackboard基准，涵盖纸张折叠和打孔的预测与规划任务。研究发现，模型在应用对称变换和处理旋转方面存在挑战，即便能正确预测展开步骤序列。规划任务进一步揭示了模型在分析对称关系和执行多阶段对称过程上的局限性，其中Claude Opus 4.1的规划准确率最高仅为10%。在不需要空间可视化的泛化任务中，最佳模型o3表现优异（71.6%），但在文本预测任务上准确率仅为25%。"
      },
      {
        "paper_id": "2602.19313",
        "title": "TOPReward: Token Probabilities as Hidden Zero-Shot Rewards for Robotics",
        "abstract": "While Vision-Language-Action (VLA) models have seen rapid progress in pretraining, their advancement in Reinforcement Learning (RL) remains hampered by low sample efficiency and sparse rewards in real-world settings. Developing generalizable process reward models is essential for providing the fine-grained feedback necessary to bridge this gap, yet existing temporal value functions often fail to generalize beyond their training domains. We introduce TOPReward, a novel, probabilistically grounded temporal value function that leverages the latent world knowledge of pretrained video Vision-Language Models (VLMs) to estimate robotic task progress. Unlike prior methods that prompt VLMs to directly output progress values, which are prone to numerical misrepresentation, TOPReward extracts task progress directly from the VLM's internal token logits. In zero-shot evaluations across 130+ distinct real-world tasks and multiple robot platforms (e.g., Franka, YAM, SO-100/101), TOPReward achieves 0.947 mean Value-Order Correlation (VOC) on Qwen3-VL, dramatically outperforming the state-of-the-art GVL baseline which achieves near-zero correlation on the same open-source model. We further demonstrate that TOPReward serves as a versatile tool for downstream applications, including success detection and reward-aligned behavior cloning.",
        "authors_display": "Ranjay Krishna Team",
        "pdf_url": "http://arxiv.org/abs/2602.19313",
        "code_url": null,
        "date": "2026-02-22",
        "primary_category": "cs.RO",
        "chinese_summary": "现有VLA模型在预训练方面取得进展，但在强化学习中受限于低样本效率和稀疏奖励，同时现有时间价值函数泛化性差。针对此问题，本文提出TOPReward，一种利用预训练视频VLM的潜在世界知识来估计机器人任务进度的新型概率时间价值函数。与直接提示VLM输出进度值易产生数值误解不同，TOPReward直接从VLM的内部token logits中提取任务进度。零样本评估结果显示，TOPReward在130多个真实世界任务和多个机器人平台上取得0.947的平均价值顺序相关性，显著优于现有基线，并能作为成功检测和奖励对齐行为克隆的通用工具。"
      },
      {
        "paper_id": "2602.19260",
        "title": "The Price Is Not Right: Neuro-Symbolic Methods Outperform VLAs on Structured Long-Horizon Manipulation Tasks with Significantly Lower Energy Consumption",
        "abstract": "Vision-Language-Action (VLA) models have recently been proposed as a pathway toward generalist robotic policies capable of interpreting natural language and visual inputs to generate manipulation actions. However, their effectiveness and efficiency on structured, long-horizon manipulation tasks remain unclear. In this work, we present a head-to-head empirical comparison between a fine-tuned open-weight VLA model π0 and a neuro-symbolic architecture that combines PDDL-based symbolic planning with learned low-level control. We evaluate both approaches on structured variants of the Towers of Hanoi manipulation task in simulation while measuring both task performance and energy consumption during training and execution. On the 3-block task, the neuro-symbolic model achieves 95% success compared to 34% for the best-performing VLA. The neuro-symbolic model also generalizes to an unseen 4-block variant (78% success), whereas both VLAs fail to complete the task. During training, VLA fine-tuning consumes nearly two orders of magnitude more energy than the neuro-symbolic approach. These results highlight important trade-offs between end-to-end foundation-model approaches and structured reasoning architectures for long-horizon robotic manipulation, emphasizing the role of explicit symbolic structure in improving reliability, data efficiency, and energy efficiency. Code and models are available at https://price-is-not-right.github.io",
        "authors_display": "Matthias Scheutz Team",
        "pdf_url": "http://arxiv.org/abs/2602.19260",
        "code_url": null,
        "date": "2026-02-22",
        "primary_category": "cs.RO",
        "chinese_summary": "为评估VLA模型在结构化、长周期操作任务中的有效性和效率，本研究对比了一个微调的开源VLA模型（π0）与一个结合PDDL符号规划和低层控制的神经符号架构。实验在模拟汉诺塔任务中进行，同时测量任务性能和能耗。结果表明，在3块汉诺塔任务中，神经符号模型成功率达95%，远超VLA模型的34%，并能泛化至4块变体（78%成功），而VLA模型失败。此外，VLA模型训练能耗比神经符号方法高出近两个数量级，凸显了端到端基础模型与结构化推理架构在可靠性、数据效率和能效方面的权衡。"
      },
      {
        "paper_id": "2602.19184",
        "title": "Human-to-Robot Interaction: Learning from Video Demonstration for Robot Imitation",
        "abstract": "Learning from Demonstration (LfD) offers a promising paradigm for robot skill acquisition. Recent approaches attempt to extract manipulation commands directly from video demonstrations, yet face two critical challenges: (1) general video captioning models prioritize global scene features over task-relevant objects, producing descriptions unsuitable for precise robotic execution, and (2) end-to-end architectures coupling visual understanding with policy learning require extensive paired datasets and struggle to generalize across objects and scenarios. To address these limitations, we propose a novel ``Human-to-Robot'' imitation learning pipeline that enables robots to acquire manipulation skills directly from unstructured video demonstrations, inspired by the human ability to learn by watching and imitating. Our key innovation is a modular framework that decouples the learning process into two distinct stages: (1) Video Understanding, which combines Temporal Shift Modules (TSM) with Vision-Language Models (VLMs) to extract actions and identify interacted objects, and (2) Robot Imitation, which employs TD3-based deep reinforcement learning to execute the demonstrated manipulations. We validated our approach in PyBullet simulation environments with a UR5e manipulator and in a real-world experiment with a UF850 manipulator across four fundamental actions: reach, pick, move, and put. For video understanding, our method achieves 89.97% action classification accuracy and BLEU-4 scores of 0.351 on standard objects and 0.265 on novel objects, representing improvements of 76.4% and 128.4% over the best baseline, respectively. For robot manipulation, our framework achieves an average success rate of 87.5% across all actions, with 100% success on reaching tasks and up to 90% on complex pick-and-place operations. The project website is available at https://thanhnguyencanh.github.io/LfD4hri.",
        "authors_display": "Xiem HoangVan Team",
        "pdf_url": "http://arxiv.org/abs/2602.19184",
        "code_url": null,
        "date": "2026-02-22",
        "primary_category": "cs.RO",
        "chinese_summary": "针对示教学习（LfD）中视频字幕模型不聚焦任务相关对象、端到端架构泛化性差且需大量数据的挑战，本文提出一种“人到机器人”模仿学习流程。该流程解耦为两阶段：视频理解阶段，结合TSM与VLM提取动作并识别交互对象；机器人模仿阶段，采用TD3深度强化学习执行操作。实验在PyBullet仿真和真实世界机器人上进行，视频理解部分动作分类准确率达89.97%，BLEU-4分数显著超越基线；机器人操作部分，所有动作平均成功率87.5%，其中抓取任务100%成功，抓取放置操作成功率高达90%。"
      },
      {
        "paper_id": "2602.19146",
        "title": "VIGiA: Instructional Video Guidance via Dialogue Reasoning and Retrieval",
        "abstract": "We introduce VIGiA, a novel multimodal dialogue model designed to understand and reason over complex, multi-step instructional video action plans. Unlike prior work which focuses mainly on text-only guidance, or treats vision and language in isolation, VIGiA supports grounded, plan-aware dialogue that requires reasoning over visual inputs, instructional plans, and interleaved user interactions. To this end, VIGiA incorporates two key capabilities: (1) multimodal plan reasoning, enabling the model to align uni- and multimodal queries with the current task plan and respond accurately; and (2) plan-based retrieval, allowing it to retrieve relevant plan steps in either textual or visual representations. Experiments were done on a novel dataset with rich Instructional Video Dialogues aligned with Cooking and DIY plans. Our evaluation shows that VIGiA outperforms existing state-of-the-art models on all tasks in a conversational plan guidance setting, reaching over 90\\% accuracy on plan-aware VQA.",
        "authors_display": "João Maglhães Team",
        "pdf_url": "http://arxiv.org/abs/2602.19146",
        "code_url": null,
        "date": "2026-02-22",
        "primary_category": "cs.CV",
        "chinese_summary": "针对现有模型难以理解和推理复杂、多步骤指导性视频动作计划的局限性，本文提出VIGiA，一个新颖的多模态对话模型。VIGiA旨在支持基于地面的、计划感知的对话，需推理视觉输入、指导计划和交错的用户交互。为此，它整合了多模态计划推理能力，能将查询与当前任务计划对齐并准确响应；以及基于计划的检索能力，能以文本或视觉表示检索相关计划步骤。在包含烹饪和DIY计划的指导性视频对话数据集上的实验表明，VIGiA在会话计划指导设置中的所有任务上均优于现有SOTA模型，在计划感知的视觉问答（VQA）上准确率超过90%。"
      },
      {
        "paper_id": "2602.18941",
        "title": "Global Commander and Local Operative: A Dual-Agent Framework for Scene Navigation",
        "abstract": "Vision-and-Language Scene navigation is a fundamental capability for embodied human-AI collaboration, requiring agents to follow natural language instructions to execute coherent action sequences in complex environments. Existing approaches either rely on multiple agents, incurring high coordination and resource costs, or adopt a single-agent paradigm, which overloads the agent with both global planning and local perception, often leading to degraded reasoning and instruction drift in long-horizon settings. To address these issues, we introduce DACo, a planning-grounding decoupled architecture that disentangles global deliberation from local grounding. Concretely, it employs a Global Commander for high-level strategic planning and a Local Operative for egocentric observing and fine-grained execution. By disentangling global reasoning from local action, DACo alleviates cognitive overload and improves long-horizon stability. The framework further integrates dynamic subgoal planning and adaptive replanning to enable structured and resilient navigation. Extensive evaluations on R2R, REVERIE, and R4R demonstrate that DACo achieves 4.9%, 6.5%, 5.4% absolute improvements over the best-performing baselines in zero-shot settings, and generalizes effectively across both closed-source (e.g., GPT-4o) and open-source (e.g., Qwen-VL Series) backbones. DACo provides a principled and extensible paradigm for robust long-horizon navigation. Project page: https://github.com/ChocoWu/DACo",
        "authors_display": "Tat-Seng Chua Team",
        "pdf_url": "http://arxiv.org/abs/2602.18941",
        "code_url": null,
        "date": "2026-02-21",
        "primary_category": "cs.CV",
        "chinese_summary": "为解决视觉与语言场景导航中，多智能体成本高昂或单智能体全局规划与局部感知过载导致推理退化和指令漂移的问题，本文引入DACo。DACo是一种规划-落地解耦架构，它通过全局指挥官负责高层战略规划，局部操作员负责自我中心观察和精细执行，从而减轻认知过载并提高长周期稳定性。该框架还整合了动态子目标规划和自适应重规划，以实现结构化和弹性的导航。DACo在R2R、REVERIE和R4R等基准上的零样本评估中，成功率比最佳基线分别提高4.9%、6.5%和5.4%，并能有效泛化到多种骨干模型。"
      },
      {
        "paper_id": "2602.18813",
        "title": "Habilis-$β$: A Fast-Motion and Long-Lasting On-Device Vision-Language-Action Model",
        "abstract": "We introduce Habilis-$β$, a fast-motion and long-lasting on-device vision-language-action (VLA) model designed for real-world deployment. Current VLA evaluation remains largely confined to single-trial success rates under curated resets, which fails to capture the fast-motion and long-lasting capabilities essential for practical operation. To address this, we introduce the Productivity-Reliability Plane (PRP), which evaluates performance through Tasks per Hour (TPH) and Mean Time Between Intervention (MTBI) under a continuous-run protocol that demands both high-speed execution and sustained robustness. Habilis-$β$ achieves high performance by integrating language-free pre-training on large-scale play data for robust interaction priors with post-training on cyclic task demonstrations that capture state drift across consecutive task iterations. The system further employs ESPADA for phase-adaptive motion shaping to accelerate free-space transit, utilizes rectified-flow distillation to enable high-frequency control on edge devices, and incorporates classifier-free guidance (CFG) as a deployment-time knob to dynamically balance instruction adherence and learned interaction priors. In 1-hour continuous-run evaluations, Habilis-$β$ achieves strong performance under the PRP metrics, compared to $π_{0.5}$ in both simulation and real-world environments. In simulation, Habilis-$β$ achieves 572.6 TPH and 39.2 s MTBI (vs. 120.5 TPH and 30.5 s for $π_{0.5}$), while in a real-world humanoid logistics workflow it achieves 124 TPH and 137.4 s MTBI (vs. 19 TPH and 46.1 s for $π_{0.5}$). Finally, Habilis-$β$ achieves the highest reported performance on the standard RoboTwin 2.0 leaderboard across representative tasks, validating its effectiveness in complex manipulation scenarios.",
        "authors_display": "Theo Taeyeong Kim Team",
        "pdf_url": "http://arxiv.org/abs/2602.18813",
        "code_url": null,
        "date": "2026-02-21",
        "primary_category": "cs.RO",
        "chinese_summary": "为解决现有VLA评估无法捕捉实际操作所需的高速和持久能力问题，本文提出Habilis-β，一个用于实际部署的快速、持久的设备端VLA模型，并引入生产力-可靠性平面（PRP）指标来衡量其性能。Habilis-β通过大规模游戏数据上的无语言预训练获得鲁棒交互先验，通过循环任务演示上的后训练捕捉状态漂移，并结合相位自适应运动塑形、整流流蒸馏实现高频控制和无分类器指导来动态平衡指令遵循与交互先验。连续运行评估显示，Habilis-β在模拟和真实世界环境中均在PRP指标下表现出色，并在RoboTwin 2.0排行榜上取得了最高性能。"
      },
      {
        "paper_id": "2602.18763",
        "title": "TAG: Thinking with Action Unit Grounding for Facial Expression Recognition",
        "abstract": "Facial Expression Recognition (FER) is a fine-grained visual understanding task where reliable predictions require reasoning over localized and meaningful facial cues. Recent vision--language models (VLMs) enable natural language explanations for FER, but their reasoning is often ungrounded, producing fluent yet unverifiable rationales that are weakly tied to visual evidence and prone to hallucination, leading to poor robustness across different datasets. We propose TAG (Thinking with Action Unit Grounding), a vision--language framework that explicitly constrains multimodal reasoning to be supported by facial Action Units (AUs). TAG requires intermediate reasoning steps to be grounded in AU-related facial regions, yielding predictions accompanied by verifiable visual evidence. The model is trained via supervised fine-tuning on AU-grounded reasoning traces followed by reinforcement learning with an AU-aware reward that aligns predicted regions with external AU detectors. Evaluated on RAF-DB, FERPlus, and AffectNet, TAG consistently outperforms strong open-source and closed-source VLM baselines while simultaneously improving visual faithfulness. Ablation and preference studies further show that AU-grounded rewards stabilize reasoning and mitigate hallucination, demonstrating the importance of structured grounded intermediate representations for trustworthy multimodal reasoning in FER. The code will be available at https://github.com/would1920/FER_TAG .",
        "authors_display": "Wentao Zhang Team",
        "pdf_url": "http://arxiv.org/abs/2602.18763",
        "code_url": null,
        "date": "2026-02-21",
        "primary_category": "cs.CV",
        "chinese_summary": "面部表情识别(FER)作为一项细粒度视觉理解任务，现有视觉-语言模型(VLMs)的推理常缺乏视觉依据，易产生幻觉且鲁棒性差。为此，本研究提出了TAG (Thinking with Action Unit Grounding) 框架，通过显式地将多模态推理约束于面部动作单元(AUs)支持，并通过AU-grounded推理轨迹的监督微调和AU感知奖励的强化学习进行训练。实验结果表明，TAG在RAF-DB、FERPlus和AffectNet数据集上持续超越现有VLM基线，并显著提高了视觉忠实度，证明了结构化、有根据的中间表示对于FER中可信多模态推理的重要性。"
      },
      {
        "paper_id": "2602.18746",
        "title": "MIRROR: Multimodal Iterative Reasoning via Reflection on Visual Regions",
        "abstract": "In the era of Vision-Language Models (VLMs), enhancing multimodal reasoning capabilities remains a critical challenge, particularly in handling ambiguous or complex visual inputs, where initial inferences often lead to hallucinations or logic errors. Existing VLMs often produce plausible yet ungrounded answers, and even when prompted to \"reflect\", their corrections may remain detached from the image evidence. To address this, we propose the MIRROR framework for Multimodal Iterative Reasoning via Reflection On visual Regions. By embedding visual reflection as a core mechanism, MIRROR is formulated as a closed-loop process comprising draft, critique, region-based verification, and revision, which are repeated until the output is visually grounded. To facilitate training of this model, we construct **ReflectV**, a visual reflective dataset for multi-turn supervision that explicitly contains reflection triggers, region-based verification actions, and answer revision grounded in visual evidence. Experiments on both general vision-language benchmarks and representative vision-language reasoning benchmarks show that MIRROR improves correctness and reduces visual hallucinations, demonstrating the value of training reflection as an evidence-seeking, region-aware verification process rather than a purely textual revision step.",
        "authors_display": "Yunde Jia Team",
        "pdf_url": "http://arxiv.org/abs/2602.18746",
        "code_url": null,
        "date": "2026-02-21",
        "primary_category": "cs.CV",
        "chinese_summary": "在视觉-语言模型（VLMs）时代，处理模糊或复杂视觉输入时，现有模型常产生幻觉或逻辑错误，即使反思也可能脱离图像证据。为此，本研究提出了MIRROR框架，将视觉反思嵌入为核心机制，形成一个包括草稿、评论、基于区域的验证和修正的闭环过程，直至输出具备视觉依据。为训练该模型，构建了ReflectV数据集。实验结果表明，MIRROR框架提高了正确性并减少了视觉幻觉，强调了将反思训练为证据寻求和区域感知验证过程的价值。"
      },
      {
        "paper_id": "2602.18742",
        "title": "RoboCurate: Harnessing Diversity with Action-Verified Neural Trajectory for Robot Learning",
        "abstract": "Synthetic data generated by video generative models has shown promise for robot learning as a scalable pipeline, but it often suffers from inconsistent action quality due to imperfectly generated videos. Recently, vision-language models (VLMs) have been leveraged to validate video quality, but they have limitations in distinguishing physically accurate videos and, even then, cannot directly evaluate the generated actions themselves. To tackle this issue, we introduce RoboCurate, a novel synthetic robot data generation framework that evaluates and filters the quality of annotated actions by comparing them with simulation replay. Specifically, RoboCurate replays the predicted actions in a simulator and assesses action quality by measuring the consistency of motion between the simulator rollout and the generated video. In addition, we unlock observation diversity beyond the available dataset via image-to-image editing and apply action-preserving video-to-video transfer to further augment appearance. We observe RoboCurate's generated data yield substantial relative improvements in success rates compared to using real data only, achieving +70.1% on GR-1 Tabletop (300 demos), +16.1% on DexMimicGen in the pre-training setup, and +179.9% in the challenging real-world ALLEX humanoid dexterous manipulation setting.",
        "authors_display": "Jinwoo Shin Team",
        "pdf_url": "http://arxiv.org/abs/2602.18742",
        "code_url": "https://seungkukim.github.io/robocurate/",
        "date": "2026-02-21",
        "primary_category": "cs.RO",
        "chinese_summary": "视频生成模型产生的合成数据虽具潜力，但常因生成不完善导致动作质量不一致，且现有视觉-语言模型难以精确评估动作。本研究引入RoboCurate框架，通过在模拟器中回放预测动作并测量运动一致性，来评估和过滤合成机器人数据的动作质量。此外，该框架利用图像编辑和视频转换增强观测多样性。实验结果显示，RoboCurate生成的合成数据显著提高了机器人任务的成功率，在多个基准测试中均优于仅使用真实数据的表现。"
      },
      {
        "paper_id": "2602.16898",
        "title": "MALLVI: A Multi-Agent Framework for Integrated Generalized Robotics Manipulation",
        "abstract": "Task planning for robotic manipulation with large language models (LLMs) is an emerging area. Prior approaches rely on specialized models, fine tuning, or prompt tuning, and often operate in an open loop manner without robust environmental feedback, making them fragile in dynamic settings.MALLVi present a Multi Agent Large Language and Vision framework that enables closed loop feedback driven robotic manipulation. Given a natural language instruction and an image of the environment, MALLVi generates executable atomic actions for a robot manipulator. After action execution, a Vision Language Model (VLM) evaluates environmental feedback and decides whether to repeat the process or proceed to the next step Rather than using a single model, MALLVi coordinates specialized agents, Decomposer, Localizer, Thinker, and Reflector, to manage perception, localization, reasoning, and high level planning. An optional Descriptor agent provides visual memory of the initial state. The Reflector supports targeted error detection and recovery by reactivating only relevant agents, avoiding full replanning.Experiments in simulation and real world settings show that iterative closed loop multi agent coordination improves generalization and increases success rates in zero shot manipulation tasks.Code available at https://github.com/iman1234ahmadi/MALLVI.",
        "authors_display": "Babak Khalaj Team",
        "pdf_url": "http://arxiv.org/abs/2602.16898",
        "code_url": null,
        "date": "2026-02-20",
        "primary_category": "cs.RO",
        "chinese_summary": "现有利用大语言模型（LLM）进行机器人操作任务规划的方法常依赖特定模型、微调或提示调优，且多以开环方式运行，缺乏鲁棒的环境反馈，导致在动态环境中脆弱。为解决此问题，本文提出了MALLVi，一个多智能体大语言与视觉框架，实现了闭环反馈驱动的机器人操作。MALLVi协调专门的智能体（如Decomposer、Localizer、Thinker、Reflector）来管理感知、定位、推理和高级规划，其中Reflector智能体支持有针对性的错误检测和恢复。模拟和真实世界实验结果表明，迭代闭环多智能体协调显著提高了零样本操作任务的泛化能力和成功率。"
      },
      {
        "paper_id": "2602.18397",
        "title": "How Fast Can I Run My VLA? Demystifying VLA Inference Performance with VLA-Perf",
        "abstract": "Vision-Language-Action (VLA) models have recently demonstrated impressive capabilities across various embodied AI tasks. While deploying VLA models on real-world robots imposes strict real-time inference constraints, the inference performance landscape of VLA remains poorly understood due to the large combinatorial space of model architectures and inference systems. In this paper, we ask a fundamental research question: How should we design future VLA models and systems to support real-time inference? To address this question, we first introduce VLA-Perf, an analytical performance model that can analyze inference performance for arbitrary combinations of VLA models and inference systems. Using VLA-Perf, we conduct the first systematic study of the VLA inference performance landscape. From a model-design perspective, we examine how inference performance is affected by model scaling, model architectural choices, long-context video inputs, asynchronous inference, and dual-system model pipelines. From the deployment perspective, we analyze where VLA inference should be executed -- on-device, on edge servers, or in the cloud -- and how hardware capability and network performance jointly determine end-to-end latency. By distilling 15 key takeaways from our comprehensive evaluation, we hope this work can provide practical guidance for the design of future VLA models and inference systems.",
        "authors_display": "Christos Kozyrakis Team",
        "pdf_url": "http://arxiv.org/abs/2602.18397",
        "code_url": null,
        "date": "2026-02-20",
        "primary_category": "cs.RO",
        "chinese_summary": "视觉-语言-动作（VLA）模型在具身AI任务中展现出强大能力，但其在真实机器人上的实时推理性能受模型架构和推理系统复杂性限制。本研究提出了VLA-Perf分析性能模型，首次系统地研究了VLA推理性能，从模型设计和部署角度分析了模型扩展、架构选择、长上下文视频输入、异步推理、双系统管道、执行位置、硬件能力和网络性能等因素的影响。研究提炼出15个关键发现，为未来VLA模型和推理系统的设计提供了实用指导。"
      },
      {
        "paper_id": "2602.18374",
        "title": "Zero-shot Interactive Perception",
        "abstract": "Interactive perception (IP) enables robots to extract hidden information in their workspace and execute manipulation plans by physically interacting with objects and altering the state of the environment -- crucial for resolving occlusions and ambiguity in complex, partially observable scenarios. We present Zero-Shot IP (ZS-IP), a novel framework that couples multi-strategy manipulation (pushing and grasping) with a memory-driven Vision Language Model (VLM) to guide robotic interactions and resolve semantic queries. ZS-IP integrates three key components: (1) an Enhanced Observation (EO) module that augments the VLM's visual perception with both conventional keypoints and our proposed pushlines -- a novel 2D visual augmentation tailored to pushing actions, (2) a memory-guided action module that reinforces semantic reasoning through context lookup, and (3) a robotic controller that executes pushing, pulling, or grasping based on VLM output. Unlike grid-based augmentations optimized for pick-and-place, pushlines capture affordances for contact-rich actions, substantially improving pushing performance. We evaluate ZS-IP on a 7-DOF Franka Panda arm across diverse scenes with varying occlusions and task complexities. Our experiments demonstrate that ZS-IP outperforms passive and viewpoint-based perception techniques such as Mark-Based Visual Prompting (MOKA), particularly in pushing tasks, while preserving the integrity of non-target elements.",
        "authors_display": "Amir Ghalamzan Team",
        "pdf_url": "http://arxiv.org/abs/2602.18374",
        "code_url": "https://openreview.net/forum?id=7MhpFcr5Nx",
        "date": "2026-02-20",
        "primary_category": "cs.RO",
        "chinese_summary": "交互式感知（IP）对机器人解决复杂场景中的遮挡和歧义至关重要。本研究提出了零样本交互式感知（ZS-IP）框架，将多策略操作（推和抓取）与记忆驱动的视觉语言模型相结合，以指导机器人交互并解决语义查询。ZS-IP整合了增强观察模块（引入推线）、记忆引导动作模块和机器人控制器。实验结果表明，ZS-IP在多样场景中表现优异，尤其在推任务中显著超越了被动和基于视角的感知技术，并保持了非目标元素的完整性，证明了其在接触式动作中的有效性。"
      },
      {
        "paper_id": "2602.18224",
        "title": "SimVLA: A Simple VLA Baseline for Robotic Manipulation",
        "abstract": "Vision-Language-Action (VLA) models have emerged as a promising paradigm for general-purpose robotic manipulation, leveraging large-scale pre-training to achieve strong performance. The field has rapidly evolved with additional spatial priors and diverse architectural innovations. However, these advancements are often accompanied by varying training recipes and implementation details, which can make it challenging to disentangle the precise source of empirical gains. In this work, we introduce SimVLA, a streamlined baseline designed to establish a transparent reference point for VLA research. By strictly decoupling perception from control, using a standard vision-language backbone and a lightweight action head, and standardizing critical training dynamics, we demonstrate that a minimal design can achieve state-of-the-art performance. Despite having only 0.5B parameters, SimVLA outperforms multi-billion-parameter models on standard simulation benchmarks without robot pretraining. SimVLA also reaches on-par real-robot performance compared to pi0.5. Our results establish SimVLA as a robust, reproducible baseline that enables clear attribution of empirical gains to future architectural innovations. Website: https://frontierrobo.github.io/SimVLA",
        "authors_display": "Zhenguo Li Team",
        "pdf_url": "http://arxiv.org/abs/2602.18224",
        "code_url": null,
        "date": "2026-02-20",
        "primary_category": "cs.RO",
        "chinese_summary": "视觉-语言-动作（VLA）模型在机器人操作中表现出色，但由于训练方案多样，难以厘清经验增益来源。本研究引入SimVLA，一个精简的VLA基线模型，通过严格解耦感知与控制、使用标准视觉-语言骨干和轻量级动作头，并标准化训练动态。实验结果表明，尽管SimVLA参数量仅为0.5B，但在标准模拟基准上无需机器人预训练即可超越数十亿参数模型，并在真实机器人任务中达到与pi0.5相当的性能，为VLA研究提供了稳健可复现的参考点。"
      },
      {
        "paper_id": "2602.18020",
        "title": "UAOR: Uncertainty-aware Observation Reinjection for Vision-Language-Action Models",
        "abstract": "Vision-Language-Action (VLA) models leverage pretrained Vision-Language Models (VLMs) as backbones to map images and instructions to actions, demonstrating remarkable potential for generalizable robotic manipulation. To enhance performance, existing methods often incorporate extra observation cues (e.g., depth maps, point clouds) or auxiliary modules (e.g., object detectors, encoders) to enable more precise and reliable task execution, yet these typically require costly data collection and additional training. Inspired by the finding that Feed-Forward Network (FFN) in language models can act as \"key-value memory\", we propose Uncertainty-aware Observation Reinjection (UAOR), an effective, training-free and plug-and-play module for VLA models. Specifically, when the current language model layer exhibits high uncertainty, measured by Action Entropy, it reinjects key observation information into the next layer's Feed-Forward Network (FFN) through attention retrieval. This mechanism helps VLAs better attend to observations during inference, enabling more confident and faithful action generation. Comprehensive experiments show that our method consistently improves diverse VLA models across simulation and real-world tasks with minimal overhead. Notably, UAOR eliminates the need for additional observation cues or modules, making it a versatile and practical plug-in for existing VLA pipelines. The project page is at https://uaor.jiabingyang.cn.",
        "authors_display": "Liang Wang Team",
        "pdf_url": "http://arxiv.org/abs/2602.18020",
        "code_url": null,
        "date": "2026-02-20",
        "primary_category": "cs.CV",
        "chinese_summary": "视觉-语言-动作（VLA）模型在机器人操作中具有巨大潜力，但现有方法常依赖额外观测线索或辅助模块，导致高成本和额外训练。受语言模型中前馈网络（FFN）作为“键值记忆”的启发，本研究提出了不确定性感知观测重注（UAOR），一个无需训练且即插即用的模块。该模块在语言模型层表现出高不确定性时，通过注意力检索将关键观测信息重新注入到下一层FFN中，以提高动作生成信心。实验结果表明，UAOR以最小开销持续改进了多种VLA模型在模拟和真实世界任务中的表现。"
      },
      {
        "paper_id": "2602.17951",
        "title": "ROCKET: Residual-Oriented Multi-Layer Alignment for Spatially-Aware Vision-Language-Action Models",
        "abstract": "Vision-Language-Action (VLA) models enable instruction-following robotic manipulation, but they are typically pretrained on 2D data and lack 3D spatial understanding. An effective approach is representation alignment, where a strong vision foundation model is used to guide a 2D VLA model. However, existing methods usually apply supervision at only a single layer, failing to fully exploit the rich information distributed across depth; meanwhile, naïve multi-layer alignment can cause gradient interference. We introduce ROCKET, a residual-oriented multi-layer representation alignment framework that formulates multi-layer alignment as aligning one residual stream to another. Concretely, ROCKET employs a shared projector to align multiple layers of the VLA backbone with multiple layers of a powerful 3D vision foundation model via a layer-invariant mapping, which reduces gradient conflicts. We provide both theoretical justification and empirical analyses showing that a shared projector is sufficient and outperforms prior designs, and further propose a Matryoshka-style sparse activation scheme for the shared projector to balance multiple alignment losses. Our experiments show that, combined with a training-free layer selection strategy, ROCKET requires only about 4% of the compute budget while achieving 98.5% state-of-the-art success rate on LIBERO. We further demonstrate the superior performance of ROCKET across LIBERO-Plus and RoboTwin, as well as multiple VLA models. The code and model weights can be found at https://github.com/CASE-Lab-UMD/ROCKET-VLA.",
        "authors_display": "Ang Li Team",
        "pdf_url": "http://arxiv.org/abs/2602.17951",
        "code_url": null,
        "date": "2026-02-20",
        "primary_category": "cs.CV",
        "chinese_summary": "视觉-语言-动作（VLA）模型在机器人操作中缺乏3D空间理解能力，且现有表示对齐方法常仅在单层施加监督或导致梯度冲突。本研究提出了ROCKET，一个残差导向的多层表示对齐框架，通过共享投影仪将VLA骨干的多个层与强大的3D视觉基础模型的多个层进行层不变映射对齐，从而减少梯度冲突。实验结果表明，结合无需训练的层选择策略，ROCKET仅需约4%的计算预算即可在LIBERO上达到98.5%的SOTA成功率，并在多个基准和VLA模型上表现卓越。"
      },
      {
        "paper_id": "2602.18532",
        "title": "VLANeXt: Recipes for Building Strong VLA Models",
        "abstract": "Following the rise of large foundation models, Vision-Language-Action models (VLAs) emerged, leveraging strong visual and language understanding for general-purpose policy learning. Yet, the current VLA landscape remains fragmented and exploratory. Although many groups have proposed their own VLA models, inconsistencies in training protocols and evaluation settings make it difficult to identify which design choices truly matter. To bring structure to this evolving space, we reexamine the VLA design space under a unified framework and evaluation setup. Starting from a simple VLA baseline similar to RT-2 and OpenVLA, we systematically dissect design choices along three dimensions: foundational components, perception essentials, and action modelling perspectives. From this study, we distill 12 key findings that together form a practical recipe for building strong VLA models. The outcome of this exploration is a simple yet effective model, VLANeXt. VLANeXt outperforms prior state-of-the-art methods on the LIBERO and LIBERO-plus benchmarks and demonstrates strong generalization in real-world experiments. We will release a unified, easy-to-use codebase that serves as a common platform for the community to reproduce our findings, explore the design space, and build new VLA variants on top of a shared foundation.",
        "authors_display": "Chen Change Loy Team",
        "pdf_url": "http://arxiv.org/abs/2602.18532",
        "code_url": "https://dravenalg.github.io/VLANeXt/",
        "date": "2026-02-20",
        "primary_category": "cs.CV",
        "chinese_summary": "视觉-语言-动作（VLA）模型利用强大的视觉和语言理解能力实现通用策略学习，但当前领域碎片化，难以确定关键设计选择。本研究在统一框架下重新审视VLA设计空间，系统剖析了基础组件、感知要素和动作建模维度的设计选择，并提炼出12个构建强大VLA模型的关键发现。研究成果是VLANeXt模型，在LIBERO和LIBERO-plus基准测试中超越了现有先进方法，并在真实世界实验中展现出强大的泛化能力，为社区提供了统一的代码库和共享基础。"
      },
      {
        "paper_id": "2602.17659",
        "title": "When Vision Overrides Language: Evaluating and Mitigating Counterfactual Failures in VLAs",
        "abstract": "Vision-Language-Action models (VLAs) promise to ground language instructions in robot control, yet in practice often fail to faithfully follow language. When presented with instructions that lack strong scene-specific supervision, VLAs suffer from counterfactual failures: they act based on vision shortcuts induced by dataset biases, repeatedly executing well-learned behaviors and selecting objects frequently seen during training regardless of language intent. To systematically study it, we introduce LIBERO-CF, the first counterfactual benchmark for VLAs that evaluates language following capability by assigning alternative instructions under visually plausible LIBERO layouts. Our evaluation reveals that counterfactual failures are prevalent yet underexplored across state-of-the-art VLAs. We propose Counterfactual Action Guidance (CAG), a simple yet effective dual-branch inference scheme that explicitly regularizes language conditioning in VLAs. CAG combines a standard VLA policy with a language-unconditioned Vision-Action (VA) module, enabling counterfactual comparison during action selection. This design reduces reliance on visual shortcuts, improves robustness on under-observed tasks, and requires neither additional demonstrations nor modifications to existing architectures or pretrained models. Extensive experiments demonstrate its plug-and-play integration across diverse VLAs and consistent improvements. For example, on LIBERO-CF, CAG improves $π_{0.5}$ by 9.7% in language following accuracy and 3.6% in task success on under-observed tasks using a training-free strategy, with further gains of 15.5% and 8.5%, respectively, when paired with a VA model. In real-world evaluations, CAG reduces counterfactual failures of 9.4% and improves task success by 17.2% on average.",
        "authors_display": "Mingyu Ding Team",
        "pdf_url": "http://arxiv.org/abs/2602.17659",
        "code_url": "https://vla-va.github.io/",
        "date": "2026-02-19",
        "primary_category": "cs.CV",
        "chinese_summary": "视觉-语言-动作（VLA）模型在实际应用中常因数据集偏差导致视觉捷径，未能忠实遵循语言指令，即出现“反事实失败”。为系统研究此问题，本文引入了LIBERO-CF，首个评估VLA语言遵循能力的反事实基准。为解决反事实失败，提出了一种简单有效的双分支推理方案：反事实动作指导（CAG）。CAG结合标准VLA策略与语言非条件的视觉-动作（VA）模块，通过反事实比较减少对视觉捷径的依赖。实验证明，CAG无需额外演示或模型修改，即可在多种VLA模型上实现即插即用和性能提升，例如在LIBERO-CF上显著提高了语言遵循准确率和任务成功率，并在真实世界中减少了反事实失败并提升了任务成功率。"
      },
      {
        "paper_id": "2602.17245",
        "title": "Web Verbs: Typed Abstractions for Reliable Task Composition on the Agentic Web",
        "abstract": "The Web is evolving from a medium that humans browse to an environment where software agents act on behalf of users. Advances in large language models (LLMs) make natural language a practical interface for goal-directed tasks, yet most current web agents operate on low-level primitives such as clicks and keystrokes. These operations are brittle, inefficient, and difficult to verify. Complementing content-oriented efforts such as NLWeb's semantic layer for retrieval, we argue that the agentic web also requires a semantic layer for web actions. We propose \\textbf{Web Verbs}, a web-scale set of typed, semantically documented functions that expose site capabilities through a uniform interface, whether implemented through APIs or robust client-side workflows. These verbs serve as stable and composable units that agents can discover, select, and synthesize into concise programs. This abstraction unifies API-based and browser-based paradigms, enabling LLMs to synthesize reliable and auditable workflows with explicit control and data flow. Verbs can carry preconditions, postconditions, policy tags, and logging support, which improves \\textbf{reliability} by providing stable interfaces, \\textbf{efficiency} by reducing dozens of steps into a few function calls, and \\textbf{verifiability} through typed contracts and checkable traces. We present our vision, a proof-of-concept implementation, and representative case studies that demonstrate concise and robust execution compared to existing agents. Finally, we outline a roadmap for standardization to make verbs deployable and trustworthy at web scale.",
        "authors_display": "Suman Nath Team",
        "pdf_url": "http://arxiv.org/abs/2602.17245",
        "code_url": null,
        "date": "2026-02-19",
        "primary_category": "cs.AI",
        "chinese_summary": "随着网络向软件代理操作环境演进，利用大语言模型（LLM）进行目标导向任务变得可行，但当前多数网络代理依赖低级、脆弱且低效的操作原语。为解决这一问题，本文提出了“Web Verbs”，一套网络规模的、类型化、语义文档化的函数，通过统一接口暴露网站功能。这些Verbs作为稳定且可组合的单元，代理可以发现、选择并合成简洁的程序，从而统一了基于API和基于浏览器的范式，使LLM能够合成具有显式控制和数据流的可靠且可审计的工作流。概念验证和案例研究表明，Web Verbs实现了相比现有代理更简洁和鲁棒的执行，并展望了其标准化以实现网络规模的部署。"
      },
      {
        "paper_id": "2602.17770",
        "title": "CLUTCH: Contextualized Language model for Unlocking Text-Conditioned Hand motion modelling in the wild",
        "abstract": "Hands play a central role in daily life, yet modeling natural hand motions remains underexplored. Existing methods that tackle text-to-hand-motion generation or hand animation captioning rely on studio-captured datasets with limited actions and contexts, making them costly to scale to \"in-the-wild\" settings. Further, contemporary models and their training schemes struggle to capture animation fidelity with text-motion alignment. To address this, we (1) introduce '3D Hands in the Wild' (3D-HIW), a dataset of 32K 3D hand-motion sequences and aligned text, and (2) propose CLUTCH, an LLM-based hand animation system with two critical innovations: (a) SHIFT, a novel VQ-VAE architecture to tokenize hand motion, and (b) a geometric refinement stage to finetune the LLM. To build 3D-HIW, we propose a data annotation pipeline that combines vision-language models (VLMs) and state-of-the-art 3D hand trackers, and apply it to a large corpus of egocentric action videos covering a wide range of scenarios. To fully capture motion in-the-wild, CLUTCH employs SHIFT, a part-modality decomposed VQ-VAE, which improves generalization and reconstruction fidelity. Finally, to improve animation quality, we introduce a geometric refinement stage, where CLUTCH is co-supervised with a reconstruction loss applied directly to decoded hand motion parameters. Experiments demonstrate state-of-the-art performance on text-to-motion and motion-to-text tasks, establishing the first benchmark for scalable in-the-wild hand motion modelling. Code, data and models will be released.",
        "authors_display": "Justus Thies Team",
        "pdf_url": "http://arxiv.org/abs/2602.17770",
        "code_url": "https://balamuruganthambiraja.github.io/CLUTCH/",
        "date": "2026-02-19",
        "primary_category": "cs.CV",
        "chinese_summary": "现有手部动作建模方法依赖于受限的录音棚数据集，难以捕捉“野外”复杂场景下的动作保真度和文本-动作对齐。为解决此问题，本研究首先构建了“3D Hands in the Wild”(3D-HIW) 数据集，包含32K 3D手部动作序列及对齐文本，通过结合VLM和SOTA 3D手部追踪器进行数据标注。在此基础上，提出了CLUTCH，一个基于LLM的手部动画系统，其核心创新包括SHIFT（一种部分模态分解的VQ-VAE架构，用于手部动作标记化）和几何精修阶段（通过重建损失微调LLM）。实验证明，CLUTCH在文本到动作和动作到文本任务上均达到了最先进的性能，并建立了野外手部动作建模的首个可扩展基准。"
      },
      {
        "paper_id": "2602.17768",
        "title": "KPM-Bench: A Kinematic Parsing Motion Benchmark for Fine-grained Motion-centric Video Understanding",
        "abstract": "Despite recent advancements, video captioning models still face significant limitations in accurately describing fine-grained motion details and suffer from severe hallucination issues. These challenges become particularly prominent when generating captions for motion-centric videos, where precise depiction of intricate movements and limb dynamics is crucial yet often neglected. To alleviate this gap, we introduce an automated annotation pipeline that integrates kinematic-based motion computation with linguistic parsing, enabling detailed decomposition and description of complex human motions. Based on this pipeline, we construct and release the Kinematic Parsing Motion Benchmark (KPM-Bench), a novel open-source dataset designed to facilitate fine-grained motion understanding. KPM-Bench consists of (i) fine-grained video-caption pairs that comprehensively illustrate limb-level dynamics in complex actions, (ii) diverse and challenging question-answer pairs focusing specifically on motion understanding, and (iii) a meticulously curated evaluation set specifically designed to assess hallucination phenomena associated with motion descriptions. Furthermore, to address hallucination issues systematically, we propose the linguistically grounded Motion Parsing and Extraction (MoPE) algorithm, capable of accurately extracting motion-specific attributes directly from textual captions. Leveraging MoPE, we introduce a precise hallucination evaluation metric that functions independently of large-scale vision-language or language-only models. By integrating MoPE into the GRPO post-training framework, we effectively mitigate hallucination problems, significantly improving the reliability of motion-centric video captioning models.",
        "authors_display": "Meng Wang Team",
        "pdf_url": "http://arxiv.org/abs/2602.17768",
        "code_url": null,
        "date": "2026-02-19",
        "primary_category": "cs.CV",
        "chinese_summary": "视频字幕模型在描述精细动作细节和避免幻觉方面存在显著局限性，尤其是在以动作为主的视频中，精确描绘复杂动作和肢体动态是关键挑战。为弥补这一不足，本研究引入了一个自动化标注流程，通过结合基于运动学的计算与语言解析，实现了复杂人体运动的详细分解和描述，并基于此构建并发布了Kinematic Parsing Motion Benchmark (KPM-Bench) 数据集。此外，为系统解决幻觉问题，本文提出了语言基础的运动解析与提取（MoPE）算法，并引入了独立的精确幻觉评估指标。实验表明，将MoPE整合到GRPO后训练框架中能有效缓解幻觉问题，显著提高了以运动为中心的视频字幕模型的可靠性。"
      },
      {
        "paper_id": "2602.18520",
        "title": "Sketch2Feedback: Grammar-in-the-Loop Framework for Rubric-Aligned Feedback on Student STEM Diagrams",
        "abstract": "Providing timely, rubric-aligned feedback on student-drawn diagrams is a persistent challenge in STEM education. While large multimodal models (LMMs) can jointly parse images and generate explanations, their tendency to hallucinate undermines trust in classroom deployments. We present Sketch2Feedback, a grammar-in-the-loop framework that decomposes the problem into four stages -- hybrid perception, symbolic graph construction, constraint checking, and constrained VLM feedback -- so that the language model verbalizes only violations verified by an upstream rule engine. We evaluate on two synthetic micro-benchmarks, FBD-10 (free-body diagrams) and Circuit-10 (circuit schematics), each with 500 images spanning standard and hard noise augmentation tiers, comparing our pipeline against end-to-end LMMs (LLaVA-1.5-7B, Qwen2-VL-7B), a vision-only detector, a YOLOv8-nano learned detector, and an ensemble oracle. On n=100 test samples per benchmark with 95% bootstrap CIs, results are mixed and instructive: Qwen2-VL-7B achieves the highest micro-F1 on both FBDs (0.570) and circuits (0.528), but with extreme hallucination rates (0.78, 0.98). An ensemble oracle that selects the best prediction per sample reaches F1=0.556 with hallucination 0.320 on FBDs, demonstrating exploitable complementarity between grammar and end-to-end approaches. Confidence thresholding at tau=0.7 reduces circuit hallucination from 0.970 to 0.880 with no F1 loss. Hard noise augmentation reveals domain-dependent robustness: FBD detection is resilient while circuit detection degrades sharply. An LLM-as-judge evaluation confirms that the grammar pipeline produces more actionable circuit feedback (4.85/5) than the end-to-end LMM (3.11/5). We release all code, datasets, and evaluation scripts.",
        "authors_display": "Aayam Bansal Team",
        "pdf_url": "http://arxiv.org/abs/2602.18520",
        "code_url": null,
        "date": "2026-02-19",
        "primary_category": "cs.CV",
        "chinese_summary": "在STEM教育中，为学生绘制的图表提供及时、准确的反馈是一个挑战，而大型多模态模型（LMMs）的幻觉倾向损害了其可信度。本研究提出了Sketch2Feedback，一个基于语法循环的框架，将问题分解为混合感知、符号图构建、约束检查和受约束的VLM反馈四个阶段，确保语言模型只报告经规则引擎验证的违规。实验结果显示，端到端LMMs虽然F1分数高但幻觉率极高，而语法管道能产生更具可操作性的反馈，证明了语法与端到端方法之间存在可利用的互补性。"
      },
      {
        "paper_id": "2602.16710",
        "title": "EgoScale: Scaling Dexterous Manipulation with Diverse Egocentric Human Data",
        "abstract": "Human behavior is among the most scalable sources of data for learning physical intelligence, yet how to effectively leverage it for dexterous manipulation remains unclear. While prior work demonstrates human to robot transfer in constrained settings, it is unclear whether large scale human data can support fine grained, high degree of freedom dexterous manipulation. We present EgoScale, a human to dexterous manipulation transfer framework built on large scale egocentric human data. We train a Vision Language Action (VLA) model on over 20,854 hours of action labeled egocentric human video, more than 20 times larger than prior efforts, and uncover a log linear scaling law between human data scale and validation loss. This validation loss strongly correlates with downstream real robot performance, establishing large scale human data as a predictable supervision source. Beyond scale, we introduce a simple two stage transfer recipe: large scale human pretraining followed by lightweight aligned human robot mid training. This enables strong long horizon dexterous manipulation and one shot task adaptation with minimal robot supervision. Our final policy improves average success rate by 54% over a no pretraining baseline using a 22 DoF dexterous robotic hand, and transfers effectively to robots with lower DoF hands, indicating that large scale human motion provides a reusable, embodiment agnostic motor prior.",
        "authors_display": "Linxi Fan Team",
        "pdf_url": "http://arxiv.org/abs/2602.16710",
        "code_url": null,
        "date": "2026-02-18",
        "primary_category": "cs.RO",
        "chinese_summary": "在灵巧操作领域，如何有效利用大规模人类行为数据仍是挑战。本研究提出了EgoScale框架，通过在一个比现有数据集大20倍的20,854小时以自我为中心的人类视频数据上训练Vision Language Action (VLA) 模型。研究发现人类数据规模与验证损失之间存在对数线性缩放定律，并提出两阶段迁移策略：大规模人类预训练后进行轻量级人机对齐中期训练。实验结果表明，该策略使22自由度机械手的平均成功率比无预训练基线提高了54%，并能有效迁移到其他自由度机器人，证实大规模人类运动数据为机器人提供了可复用、与形态无关的运动先验。"
      },
      {
        "paper_id": "2602.15724",
        "title": "Learning to Retrieve Navigable Candidates for Efficient Vision-and-Language Navigation",
        "abstract": "Vision-and-Language Navigation (VLN) requires an agent to follow natural-language instructions and navigate through previously unseen environments. Recent approaches increasingly employ large language models (LLMs) as high-level navigators due to their flexibility and reasoning capability. However, prompt-based LLM navigation often suffers from inefficient decision-making, as the model must repeatedly interpret instructions from scratch and reason over noisy and verbose navigable candidates at each step. In this paper, we propose a retrieval-augmented framework to improve the efficiency and stability of LLM-based VLN without modifying or fine-tuning the underlying language model. Our approach introduces retrieval at two complementary levels. At the episode level, an instruction-level embedding retriever selects semantically similar successful navigation trajectories as in-context exemplars, providing task-specific priors for instruction grounding. At the step level, an imitation-learned candidate retriever prunes irrelevant navigable directions before LLM inference, reducing action ambiguity and prompt complexity. Both retrieval modules are lightweight, modular, and trained independently of the LLM. We evaluate our method on the Room-to-Room (R2R) benchmark. Experimental results demonstrate consistent improvements in Success Rate, Oracle Success Rate, and SPL on both seen and unseen environments. Ablation studies further show that instruction-level exemplar retrieval and candidate pruning contribute complementary benefits to global guidance and step-wise decision efficiency. These results indicate that retrieval-augmented decision support is an effective and scalable strategy for enhancing LLM-based vision-and-language navigation.",
        "authors_display": "Lina Yao Team",
        "pdf_url": "http://arxiv.org/abs/2602.15724",
        "code_url": null,
        "date": "2026-02-17",
        "primary_category": "cs.CV",
        "chinese_summary": "针对视觉与语言导航（VLN）中LLM导航效率低下的问题，即LLM需重复解释指令并处理冗余导航候选。本研究提出了一种检索增强框架，在不修改LLM的前提下提升效率和稳定性。该框架在任务层面通过指令级嵌入检索器提供任务先验，在步骤层面通过模仿学习的候选检索器剪枝不相关的导航方向。在R2R基准上的评估显示，该方法在成功率、Oracle成功率和SPL方面均有持续改进，证明检索增强决策支持能有效提升LLM基VLN的效率和稳定性。"
      },
      {
        "paper_id": "2602.15682",
        "title": "The Next Paradigm Is User-Centric Agent, Not Platform-Centric Service",
        "abstract": "Modern digital services have evolved into indispensable tools, driving the present large-scale information systems. Yet, the prevailing platform-centric model, where services are optimized for platform-driven metrics such as engagement and conversion, often fails to align with users' true needs. While platform technologies have advanced significantly-especially with the integration of large language models (LLMs)-we argue that improvements in platform service quality do not necessarily translate to genuine user benefit. Instead, platform-centric services prioritize provider objectives over user welfare, resulting in conflicts against user interests. This paper argues that the future of digital services should shift from a platform-centric to a user-centric agent. These user-centric agents prioritize privacy, align with user-defined goals, and grant users control over their preferences and actions. With advancements in LLMs and on-device intelligence, the realization of this vision is now feasible. This paper explores the opportunities and challenges in transitioning to user-centric intelligence, presents a practical device-cloud pipeline for its implementation, and discusses the necessary governance and ecosystem structures for its adoption.",
        "authors_display": "Enhong Chen Team",
        "pdf_url": "http://arxiv.org/abs/2602.15682",
        "code_url": null,
        "date": "2026-02-17",
        "primary_category": "cs.IR",
        "chinese_summary": "现代数字服务的平台中心化模式往往与用户真实需求不符，尽管LLMs等技术进步，但平台服务质量提升未必然转化为用户利益，反而优先考虑提供商目标。本文提出数字服务的未来应转向用户中心化代理，这类代理优先保护隐私、符合用户目标并赋予用户控制权。文章探讨了实现用户中心化智能的机遇与挑战，提出了一个实用的设备-云协同管道，并讨论了必要的治理和生态系统结构。"
      },
      {
        "paper_id": "2602.15645",
        "title": "CARE Drive A Framework for Evaluating Reason-Responsiveness of Vision Language Models in Automated Driving",
        "abstract": "Foundation models, including vision language models, are increasingly used in automated driving to interpret scenes, recommend actions, and generate natural language explanations. However, existing evaluation methods primarily assess outcome based performance, such as safety and trajectory accuracy, without determining whether model decisions reflect human relevant considerations. As a result, it remains unclear whether explanations produced by such models correspond to genuine reason responsive decision making or merely post hoc rationalizations. This limitation is especially significant in safety critical domains because it can create false confidence. To address this gap, we propose CARE Drive, Context Aware Reasons Evaluation for Driving, a model agnostic framework for evaluating reason responsiveness in vision language models applied to automated driving. CARE Drive compares baseline and reason augmented model decisions under controlled contextual variation to assess whether human reasons causally influence decision behavior. The framework employs a two stage evaluation process. Prompt calibration ensures stable outputs. Systematic contextual perturbation then measures decision sensitivity to human reasons such as safety margins, social pressure, and efficiency constraints. We demonstrate CARE Drive in a cyclist overtaking scenario involving competing normative considerations. Results show that explicit human reasons significantly influence model decisions, improving alignment with expert recommended behavior. However, responsiveness varies across contextual factors, indicating uneven sensitivity to different types of reasons. These findings provide empirical evidence that reason responsiveness in foundation models can be systematically evaluated without modifying model parameters.",
        "authors_display": "Arkady Zgonnikov Team",
        "pdf_url": "http://arxiv.org/abs/2602.15645",
        "code_url": null,
        "date": "2026-02-17",
        "primary_category": "cs.AI",
        "chinese_summary": "基础模型在自动驾驶中的解释性评估多关注结果，而非决策是否反映了人类考量，可能导致安全关键领域的虚假信心。本研究提出了CARE Drive框架，一个模型无关的自动驾驶视觉语言模型原因响应性评估方法。该框架通过控制上下文变化，比较基线与原因增强模型的决策，以评估人类原因（如安全裕度、社会压力）对决策的因果影响，并采用提示校准和系统性上下文扰动两阶段评估。实验证明，明确的人类原因能显著影响模型决策，提高与专家行为的一致性，但不同上下文因素的响应性存在差异，表明基础模型的原因响应性可系统评估。"
      },
      {
        "paper_id": "2602.15922",
        "title": "World Action Models are Zero-shot Policies",
        "abstract": "State-of-the-art Vision-Language-Action (VLA) models excel at semantic generalization but struggle to generalize to unseen physical motions in novel environments. We introduce DreamZero, a World Action Model (WAM) built upon a pretrained video diffusion backbone. Unlike VLAs, WAMs learn physical dynamics by predicting future world states and actions, using video as a dense representation of how the world evolves. By jointly modeling video and action, DreamZero learns diverse skills effectively from heterogeneous robot data without relying on repetitive demonstrations. This results in over 2x improvement in generalization to new tasks and environments compared to state-of-the-art VLAs in real robot experiments. Crucially, through model and system optimizations, we enable a 14B autoregressive video diffusion model to perform real-time closed-loop control at 7Hz. Finally, we demonstrate two forms of cross-embodiment transfer: video-only demonstrations from other robots or humans yield a relative improvement of over 42% on unseen task performance with just 10-20 minutes of data. More surprisingly, DreamZero enables few-shot embodiment adaptation, transferring to a new embodiment with only 30 minutes of play data while retaining zero-shot generalization.",
        "authors_display": "Joel Jang Team",
        "pdf_url": "http://arxiv.org/abs/2602.15922",
        "code_url": "https://dreamzero0.github.io/",
        "date": "2026-02-17",
        "primary_category": "cs.RO",
        "chinese_summary": "现有Vision-Language-Action (VLA) 模型在语义泛化方面表现出色，但在新环境中泛化到未见过的物理动作时仍面临挑战。本研究提出了DreamZero，一个基于预训练视频扩散骨干的“世界动作模型 (WAM)”，它通过预测未来世界状态和动作来学习物理动力学，将视频作为密集表示。DreamZero通过联合建模视频和动作，能从异构机器人数据中有效学习技能，无需重复演示。实验结果显示，在真实机器人任务中，DreamZero在新任务和环境上的泛化能力比最先进的VLA模型提升超过2倍，并实现了14B视频扩散模型的实时闭环控制。此外，它还展示了仅需少量数据即可进行跨实体迁移和少样本实体适应的能力。"
      },
      {
        "paper_id": "2602.15549",
        "title": "VLM-DEWM: Dynamic External World Model for Verifiable and Resilient Vision-Language Planning in Manufacturing",
        "abstract": "Vision-language model (VLM) shows promise for high-level planning in smart manufacturing, yet their deployment in dynamic workcells faces two critical challenges: (1) stateless operation, they cannot persistently track out-of-view states, causing world-state drift; and (2) opaque reasoning, failures are difficult to diagnose, leading to costly blind retries. This paper presents VLM-DEWM, a cognitive architecture that decouples VLM reasoning from world-state management through a persistent, queryable Dynamic External World Model (DEWM). Each VLM decision is structured into an Externalizable Reasoning Trace (ERT), comprising action proposal, world belief, and causal assumption, which is validated against DEWM before execution. When failures occur, discrepancy analysis between predicted and observed states enables targeted recovery instead of global replanning. We evaluate VLM-DEWM on multi-station assembly, large-scale facility exploration, and real-robot recovery under induced failures. Compared to baseline memory-augmented VLM systems, VLM DEWM improves state-tracking accuracy from 56% to 93%, increases recovery success rate from below 5% to 95%, and significantly reduces computational overhead through structured memory. These results establish VLM-DEWM as a verifiable and resilient solution for long-horizon robotic operations in dynamic manufacturing environments.",
        "authors_display": "Ning Ji Team",
        "pdf_url": "http://arxiv.org/abs/2602.15549",
        "code_url": null,
        "date": "2026-02-17",
        "primary_category": "cs.RO",
        "chinese_summary": "视觉语言模型（VLM）在智能制造高层规划中应用时面临无状态操作导致世界状态漂移和推理不透明导致故障难以诊断的问题。本文提出了VLM-DEWM认知架构，通过一个持久、可查询的动态外部世界模型（DEWM）将VLM推理与世界状态管理解耦。VLM决策被结构化为包含动作提议、世界信念和因果假设的可外部化推理轨迹，并在执行前与DEWM验证，故障发生时能通过状态差异分析实现有针对性恢复。实验证明，VLM-DEWM将状态跟踪准确率从56%提升至93%，恢复成功率从不足5%提升至95%，并显著降低了计算开销，展现了其在动态制造环境中实现长周期机器人操作的可靠性和弹性。"
      },
      {
        "paper_id": "2602.15543",
        "title": "Selective Perception for Robot: Task-Aware Attention in Multimodal VLA",
        "abstract": "In robotics, Vision-Language-Action (VLA) models that integrate diverse multimodal signals from multi-view inputs have emerged as an effective approach. However, most prior work adopts static fusion that processes all visual inputs uniformly, which incurs unnecessary computational overhead and allows task-irrelevant background information to act as noise. Inspired by the principles of human active perception, we propose a dynamic information fusion framework designed to maximize the efficiency and robustness of VLA models. Our approach introduces a lightweight adaptive routing architecture that analyzes the current text prompt and observations from a wrist-mounted camera in real-time to predict the task-relevance of multiple camera views. By conditionally attenuating computations for views with low informational utility and selectively providing only essential visual features to the policy network, Our framework achieves computation efficiency proportional to task relevance. Furthermore, to efficiently secure large-scale annotation data for router training, we established an automated labeling pipeline utilizing Vision-Language Models (VLMs) to minimize data collection and annotation costs. Experimental results in real-world robotic manipulation scenarios demonstrate that the proposed approach achieves significant improvements in both inference efficiency and control performance compared to existing VLA models, validating the effectiveness and practicality of dynamic information fusion in resource-constrained, real-time robot control environments.",
        "authors_display": "Soo-Chul Lim Team",
        "pdf_url": "http://arxiv.org/abs/2602.15543",
        "code_url": null,
        "date": "2026-02-17",
        "primary_category": "cs.RO",
        "chinese_summary": "机器人领域中的Vision-Language-Action (VLA) 模型通常采用静态融合处理所有视觉输入，导致计算开销大且易受任务无关噪声干扰。本研究受人类主动感知启发，提出了一个动态信息融合框架，旨在提高VLA模型的效率和鲁棒性。该框架引入了轻量级自适应路由架构，实时分析文本提示和腕部摄像头观测，预测多摄像头视图的任务相关性，有条件地衰减低信息效用视图的计算，并选择性地提供关键视觉特征。为高效训练路由，还建立了自动化标注流程。实验结果表明，在真实机器人操作场景中，该方法显著提升了推理效率和控制性能，验证了动态信息融合在资源受限实时机器人控制环境中的有效性。"
      },
      {
        "paper_id": "2602.15400",
        "title": "One Agent to Guide Them All: Empowering MLLMs for Vision-and-Language Navigation via Explicit World Representation",
        "abstract": "A navigable agent needs to understand both high-level semantic instructions and precise spatial perceptions. Building navigation agents centered on Multimodal Large Language Models (MLLMs) demonstrates a promising solution due to their powerful generalization ability. However, the current tightly coupled design dramatically limits system performance. In this work, we propose a decoupled design that separates low-level spatial state estimation from high-level semantic planning. Unlike previous methods that rely on predefined, oversimplified textual maps, we introduce an interactive metric world representation that maintains rich and consistent information, allowing MLLMs to interact with and reason on it for decision-making. Furthermore, counterfactual reasoning is introduced to further elicit MLLMs' capacity, while the metric world representation ensures the physical validity of the produced actions. We conduct comprehensive experiments in both simulated and real-world environments. Our method establishes a new zero-shot state-of-the-art, achieving 48.8\\% Success Rate (SR) in R2R-CE and 42.2\\% in RxR-CE benchmarks. Furthermore, to validate the versatility of our metric representation, we demonstrate zero-shot sim-to-real transfer across diverse embodiments, including a wheeled TurtleBot 4 and a custom-built aerial drone. These real-world deployments verify that our decoupled framework serves as a robust, domain-invariant interface for embodied Vision-and-Language navigation.",
        "authors_display": "Qi Wu Team",
        "pdf_url": "http://arxiv.org/abs/2602.15400",
        "code_url": null,
        "date": "2026-02-17",
        "primary_category": "cs.RO",
        "chinese_summary": "可导航智能体需同时理解高层语义指令和精确空间感知，现有以MLLMs为核心的导航智能体虽具泛化能力，但紧耦合设计限制了性能。本研究提出解耦设计，分离低层空间状态估计与高层语义规划，并引入交互式度量世界表示，维护丰富一致的信息供MLLMs推理决策。此外，通过反事实推理进一步激发MLLMs能力，并确保生成动作的物理有效性。模拟和真实环境实验表明，该方法在R2R-CE和RxR-CE基准上达到了零样本SOTA，并实现了对不同实体（如轮式机器人、无人机）的零样本模拟到真实迁移，验证了该解耦框架作为具身视觉与语言导航的鲁棒通用接口的有效性。"
      },
      {
        "paper_id": "2602.15397",
        "title": "ActionCodec: What Makes for Good Action Tokenizers",
        "abstract": "Vision-Language-Action (VLA) models leveraging the native autoregressive paradigm of Vision-Language Models (VLMs) have demonstrated superior instruction-following and training efficiency. Central to this paradigm is action tokenization, yet its design has primarily focused on reconstruction fidelity, failing to address its direct impact on VLA optimization. Consequently, the fundamental question of \\textit{what makes for good action tokenizers} remains unanswered. In this paper, we bridge this gap by establishing design principles specifically from the perspective of VLA optimization. We identify a set of best practices based on information-theoretic insights, including maximized temporal token overlap, minimized vocabulary redundancy, enhanced multimodal mutual information, and token independence. Guided by these principles, we introduce \\textbf{ActionCodec}, a high-performance action tokenizer that significantly enhances both training efficiency and VLA performance across diverse simulation and real-world benchmarks. Notably, on LIBERO, a SmolVLM2-2.2B fine-tuned with ActionCodec achieves a 95.5\\% success rate without any robotics pre-training. With advanced architectural enhancements, this reaches 97.4\\%, representing a new SOTA for VLA models without robotics pre-training. We believe our established design principles, alongside the released model, will provide a clear roadmap for the community to develop more effective action tokenizers.",
        "authors_display": "Jianye Hao Team",
        "pdf_url": "http://arxiv.org/abs/2602.15397",
        "code_url": null,
        "date": "2026-02-17",
        "primary_category": "cs.RO",
        "chinese_summary": "VLA模型利用VLM的自回归范式展现出色的指令遵循和训练效率，但动作令牌化设计主要侧重重建保真度，忽视其对VLA优化的直接影响，导致缺乏对“优秀动作令牌化器”的理解。本文从VLA优化角度建立了动作令牌化的设计原则，基于信息论洞察，提出最大化时间令牌重叠、最小化词汇冗余、增强多模态互信息和令牌独立性等最佳实践。在此指导下，引入ActionCodec高性能动作令牌化器。实验结果显示，ActionCodec显著提升了训练效率和VLA性能，在LIBERO基准上，未经机器人预训练的SmolVLM2-2.2B结合ActionCodec成功率达95.5%，进一步增强后达到97.4%，刷新了无机器人预训练VLA模型的SOTA。"
      },
      {
        "paper_id": "2602.15294",
        "title": "EAA: Automating materials characterization with vision language model agents",
        "abstract": "We present Experiment Automation Agents (EAA), a vision-language-model-driven agentic system designed to automate complex experimental microscopy workflows. EAA integrates multimodal reasoning, tool-augmented action, and optional long-term memory to support both autonomous procedures and interactive user-guided measurements. Built on a flexible task-manager architecture, the system enables workflows ranging from fully agent-driven automation to logic-defined routines that embed localized LLM queries. EAA further provides a modern tool ecosystem with two-way compatibility for Model Context Protocol (MCP), allowing instrument-control tools to be consumed or served across applications. We demonstrate EAA at an imaging beamline at the Advanced Photon Source, including automated zone plate focusing, natural language-described feature search, and interactive data acquisition. These results illustrate how vision-capable agents can enhance beamline efficiency, reduce operational burden, and lower the expertise barrier for users.",
        "authors_display": "Mathew J. Cherukara Team",
        "pdf_url": "http://arxiv.org/abs/2602.15294",
        "code_url": null,
        "date": "2026-02-17",
        "primary_category": "cs.AI",
        "chinese_summary": "针对复杂实验显微镜工作流程的自动化需求，本文提出了实验自动化代理（EAA），一个由视觉-语言模型驱动的代理系统。EAA集成了多模态推理、工具增强动作和长期记忆，支持自主程序和交互式用户引导测量。系统基于灵活的任务管理器架构，可实现从完全代理驱动到局部LLM查询的工作流程，并提供与模型上下文协议（MCP）双向兼容的工具生态系统。在高级光子源的成像光束线演示中，EAA成功实现了自动化区域板聚焦、自然语言描述的特征搜索和交互式数据采集，验证了其在提高光束线效率、减轻操作负担和降低用户专业知识门槛方面的潜力。"
      },
      {
        "paper_id": "2602.12628",
        "title": "Beyond Imitation: Reinforcement Learning-Based Sim-Real Co-Training for VLA Models",
        "abstract": "Simulation offers a scalable and low-cost way to enrich vision-language-action (VLA) training, reducing reliance on expensive real-robot demonstrations. However, most sim-real co-training methods rely on supervised fine-tuning (SFT), which treats simulation as a static source of demonstrations and does not exploit large-scale closed-loop interaction. Consequently, real-world gains and generalization are often limited. In this paper, we propose an \\underline{\\textit{RL}}-based sim-real \\underline{\\textit{Co}}-training \\modify{(RL-Co)} framework that leverages interactive simulation while preserving real-world capabilities. Our method follows a generic two-stage design: we first warm-start the policy with SFT on a mixture of real and simulated demonstrations, then fine-tune it with reinforcement learning in simulation while adding an auxiliary supervised loss on real-world data to anchor the policy and mitigate catastrophic forgetting. We evaluate our framework on four real-world tabletop manipulation tasks using two representative VLA architectures, OpenVLA and $π_{0.5}$, and observe consistent improvements over real-only fine-tuning and SFT-based co-training, including +24% real-world success on OpenVLA and +20% on $π_{0.5}$. Beyond higher success rates, RL co-training yields stronger generalization to unseen task variations and substantially improved real-world data efficiency, providing a practical and scalable pathway for leveraging simulation to enhance real-robot deployment.",
        "authors_display": "Yu Wang Team",
        "pdf_url": "http://arxiv.org/abs/2602.12628",
        "code_url": null,
        "date": "2026-02-16",
        "primary_category": "cs.RO",
        "chinese_summary": "模拟提供了可扩展且低成本的方式来丰富视觉-语言-动作（VLA）训练，减少对昂贵真实机器人演示的依赖，但多数模拟-现实协同训练方法依赖于监督微调（SFT），将模拟视为静态演示源，未充分利用大规模闭环交互，从而限制了真实世界收益和泛化。针对此问题，本文提出了RL-based sim-real Co-training (RL-Co) 框架，该框架利用交互式模拟同时保留真实世界能力。其设计分为两阶段：首先，通过对真实和模拟演示的混合数据进行SFT来预热策略，然后通过在模拟中进行强化学习来微调策略，并添加辅助的真实世界数据监督损失以锚定策略并缓解灾难性遗忘。在四项真实世界桌面操作任务上，RL-Co在两种VLA架构（OpenVLA和π₀.₅）上均持续优于仅真实世界微调和基于SFT的协同训练，例如OpenVLA的真实世界成功率提升24%，π₀.₅提升20%，同时在未见任务变体上表现出更强的泛化能力和显著提升的真实世界数据效率。"
      },
      {
        "paper_id": "2602.14974",
        "title": "DM0: An Embodied-Native Vision-Language-Action Model towards Physical AI",
        "abstract": "Moving beyond the traditional paradigm of adapting internet-pretrained models to physical tasks, we present DM0, an Embodied-Native Vision-Language-Action (VLA) framework designed for Physical AI. Unlike approaches that treat physical grounding as a fine-tuning afterthought, DM0 unifies embodied manipulation and navigation by learning from heterogeneous data sources from the onset. Our methodology follows a comprehensive three-stage pipeline: Pretraining, Mid-Training, and Post-Training. First, we conduct large-scale unified pretraining on the Vision-Language Model (VLM) using diverse corpora--seamlessly integrating web text, autonomous driving scenarios, and embodied interaction logs-to jointly acquire semantic knowledge and physical priors. Subsequently, we build a flow-matching action expert atop the VLM. To reconcile high-level reasoning with low-level control, DM0 employs a hybrid training strategy: for embodied data, gradients from the action expert are not backpropagated to the VLM to preserve generalized representations, while the VLM remains trainable on non-embodied data. Furthermore, we introduce an Embodied Spatial Scaffolding strategy to construct spatial Chain-of-Thought (CoT) reasoning, effectively constraining the action solution space. Experiments on the RoboChallenge benchmark demonstrate that DM0 achieves state-of-the-art performance in both Specialist and Generalist settings on Table30.",
        "authors_display": "Tiancai Wang Team",
        "pdf_url": "http://arxiv.org/abs/2602.14974",
        "code_url": "https://github.com/Dexmal/dexbotic",
        "date": "2026-02-16",
        "primary_category": "cs.RO",
        "chinese_summary": "针对传统方法将物理接地视为微调后期的限制，本研究提出了DM0，一个为物理AI设计的具身原生视觉-语言-动作（VLA）框架，旨在通过学习异构数据源从一开始就统一具身操作和导航。该方法采用三阶段流程：预训练阶段对VLM进行大规模统一预训练，整合网页文本、自动驾驶场景和具身交互日志；中训练阶段构建流匹配动作专家并采用混合训练策略；后训练阶段引入具身空间支架策略构建空间思维链（CoT）推理。实验结果表明，DM0在RoboChallenge基准测试的Table30上，无论在专业还是通用设置中均达到了最先进的性能。"
      },
      {
        "paper_id": "2602.14577",
        "title": "DriveFine: Refining-Augmented Masked Diffusion VLA for Precise and Robust Driving",
        "abstract": "Vision-Language-Action (VLA) models for autonomous driving increasingly adopt generative planners trained with imitation learning followed by reinforcement learning. Diffusion-based planners suffer from modality alignment difficulties, low training efficiency, and limited generalization. Token-based planners are plagued by cumulative causal errors and irreversible decoding. In summary, the two dominant paradigms exhibit complementary strengths and weaknesses. In this paper, we propose DriveFine, a masked diffusion VLA model that combines flexible decoding with self-correction capabilities. In particular, we design a novel plug-and-play block-MoE, which seamlessly injects a refinement expert on top of the generation expert. By enabling explicit expert selection during inference and gradient blocking during training, the two experts are fully decoupled, preserving the foundational capabilities and generic patterns of the pretrained weights, which highlights the flexibility and extensibility of the block-MoE design. Furthermore, we design a hybrid reinforcement learning strategy that encourages effective exploration of refinement expert while maintaining training stability. Extensive experiments on NAVSIM v1, v2, and Navhard benchmarks demonstrate that DriveFine exhibits strong efficacy and robustness. The code will be released at https://github.com/MSunDYY/DriveFine.",
        "authors_display": "Yan Wang Team",
        "pdf_url": "http://arxiv.org/abs/2602.14577",
        "code_url": null,
        "date": "2026-02-16",
        "primary_category": "cs.CV",
        "chinese_summary": "针对自动驾驶中扩散式和基于Token的VLA规划器各自存在的模态对齐困难、训练效率低或累积因果错误等问题，本研究提出了DriveFine，一种结合了灵活解码与自我修正能力的掩码扩散VLA模型。该模型设计了一个即插即用的block-MoE，将细化专家无缝注入生成专家，通过明确的专家选择和梯度阻断实现专家解耦。此外，引入混合强化学习策略以鼓励细化专家探索并保持训练稳定性。广泛实验表明，DriveFine在NAVSIM v1、v2和Navhard基准测试中表现出强大的效能和鲁棒性。"
      },
      {
        "paper_id": "2602.14551",
        "title": "Replanning Human-Robot Collaborative Tasks with Vision-Language Models via Semantic and Physical Dual-Correction",
        "abstract": "Human-Robot Collaboration (HRC) plays an important role in assembly tasks by enabling robots to plan and adjust their motions based on interactive, real-time human instructions. However, such instructions are often linguistically ambiguous and underspecified, making it difficult to generate physically feasible and cooperative robot behaviors. To address this challenge, many studies have applied Vision-Language Models (VLMs) to interpret high-level instructions and generate corresponding actions. Nevertheless, VLM-based approaches still suffer from hallucinated reasoning and an inability to anticipate physical execution failures. To address these challenges, we propose an HRC framework that augments a VLM-based reasoning with a dual-correction mechanism: an internal correction model that verifies logical consistency and task feasibility prior to action execution, and an external correction model that detects and rectifies physical failures through post-execution feedback. Simulation ablation studies demonstrate that the proposed method improves the success rate compared to baselines without correction models. Our real-world experiments in collaborative assembly tasks supported by object fixation or tool preparation by an upper body humanoid robot further confirm the framewor's effectiveness in enabling interactive replanning across different collaborative tasks in response to human instructions, validating its practical feasibility.",
        "authors_display": "Kensuke Harada Team",
        "pdf_url": "http://arxiv.org/abs/2602.14551",
        "code_url": null,
        "date": "2026-02-16",
        "primary_category": "cs.RO",
        "chinese_summary": "针对人机协作（HRC）中人类指令模糊性及VLM在物理可行性判断上的局限，本研究提出了一个增强VLM推理的双重校正HRC框架。该框架包含一个内部校正模型在动作执行前验证逻辑一致性和任务可行性，以及一个外部校正模型通过执行后反馈检测并纠正物理失败。模拟研究表明该方法显著提高了成功率，真实世界的人形机器人协作装配实验进一步验证了其在根据人类指令进行交互式重新规划方面的有效性。"
      },
      {
        "paper_id": "2602.14482",
        "title": "TikArt: Aperture-Guided Observation for Fine-Grained Visual Reasoning via Reinforcement Learning",
        "abstract": "We address fine-grained visual reasoning in multimodal large language models (MLLMs), where key evidence may reside in tiny objects, cluttered regions, or subtle markings that are lost under a single global image encoding. We introduce TikArt (Thinking Aperture), an aperture-guided agent that casts multi-step vision-language reasoning as a decision process over regions of interest. TikArt follows a Think-Aperture-Observe loop, alternating between language generation and two aperture actions: Zoom extracts rectangular crops, while Segment invokes SAM2 to obtain mask-based crops for irregular targets. After every action, the model must produce an explicit observation, turning local visual cues into persistent linguistic memory. Built on Qwen3-VL-8B, TikArt optimizes its reasoning policy with AGRPO, a GRPO-style reinforcement learning algorithm with a two-stage curriculum: it warms up segmentation actions and then jointly optimizes visual math, fine-grained VQA, and segmentation, using rewards that couple task success with purposeful aperture use. Experiments on V*, HR-Bench-4K/8K, MME-RealWorld-Lite, MMStar, RefCOCO, and ReasonSeg show consistent gains over the backbone and yield interpretable aperture trajectories for high-resolution reasoning.",
        "authors_display": "Lei Zhao Team",
        "pdf_url": "http://arxiv.org/abs/2602.14482",
        "code_url": null,
        "date": "2026-02-16",
        "primary_category": "cs.CV",
        "chinese_summary": "针对多模态大语言模型（MLLMs）在细粒度视觉推理中关键信息易丢失的问题，本研究引入了TikArt（Thinking Aperture），一种光圈引导的智能体。TikArt通过“思考-光圈-观察”循环，交替进行语言生成和两种光圈动作（Zoom和Segment），将局部视觉线索转化为持久的语言记忆。该方法基于Qwen3-VL-8B构建，并使用AGRPO强化学习算法优化推理策略。实验结果显示，TikArt在多项基准测试中均超越基线模型，并在高分辨率推理中展现出可解释的光圈轨迹。"
      },
      {
        "paper_id": "2602.14425",
        "title": "Hierarchical Vision-Language Interaction for Facial Action Unit Detection",
        "abstract": "Facial Action Unit (AU) detection seeks to recognize subtle facial muscle activations as defined by the Facial Action Coding System (FACS). A primary challenge w.r.t AU detection is the effective learning of discriminative and generalizable AU representations under conditions of limited annotated data. To address this, we propose a Hierarchical Vision-language Interaction for AU Understanding (HiVA) method, which leverages textual AU descriptions as semantic priors to guide and enhance AU detection. Specifically, HiVA employs a large language model to generate diverse and contextually rich AU descriptions to strengthen language-based representation learning. To capture both fine-grained and holistic vision-language associations, HiVA introduces an AU-aware dynamic graph module that facilitates the learning of AU-specific visual representations. These features are further integrated within a hierarchical cross-modal attention architecture comprising two complementary mechanisms: Disentangled Dual Cross-Attention (DDCA), which establishes fine-grained, AU-specific interactions between visual and textual features, and Contextual Dual Cross-Attention (CDCA), which models global inter-AU dependencies. This collaborative, cross-modal learning paradigm enables HiVA to leverage multi-grained vision-based AU features in conjunction with refined language-based AU details, culminating in robust and semantically enriched AU detection capabilities. Extensive experiments show that HiVA consistently surpasses state-of-the-art approaches. Besides, qualitative analyses reveal that HiVA produces semantically meaningful activation patterns, highlighting its efficacy in learning robust and interpretable cross-modal correspondences for comprehensive facial behavior analysis.",
        "authors_display": "Cuntai Guan Team",
        "pdf_url": "http://arxiv.org/abs/2602.14425",
        "code_url": null,
        "date": "2026-02-16",
        "primary_category": "cs.CV",
        "chinese_summary": "为解决面部动作单元（AU）检测在有限标注数据下学习判别性和可泛化表示的挑战，本研究提出了分层视觉-语言交互AU理解（HiVA）方法。HiVA利用大型语言模型生成丰富的AU文本描述作为语义先验，并通过AU感知动态图模块捕获细粒度和整体视觉-语言关联。此外，它采用分层跨模态注意力架构，包含解耦双重交叉注意力（DDCA）和上下文双重交叉注意力（CDCA），以建立细粒度AU特定交互和建模全局AU间依赖。广泛实验表明HiVA持续超越SOTA方法，并能产生语义上有意义的激活模式。"
      },
      {
        "paper_id": "2602.14399",
        "title": "Multi-Turn Adaptive Prompting Attack on Large Vision-Language Models",
        "abstract": "Multi-turn jailbreak attacks are effective against text-only large language models (LLMs) by gradually introducing malicious content across turns. When extended to large vision-language models (LVLMs), we find that naively adding visual inputs can cause existing multi-turn jailbreaks to be easily defended. For example, overly malicious visual input will easily trigger the defense mechanism of safety-aligned LVLMs, making the response more conservative. To address this, we propose MAPA: a multi-turn adaptive prompting attack that 1) at each turn, alternates text-vision attack actions to elicit the most malicious response; and 2) across turns, adjusts the attack trajectory through iterative back-and-forth refinement to gradually amplify response maliciousness. This two-level design enables MAPA to consistently outperform state-of-the-art methods, improving attack success rates by 11-35% on recent benchmarks against LLaVA-V1.6-Mistral-7B, Qwen2.5-VL-7B-Instruct, Llama-3.2-Vision-11B-Instruct and GPT-4o-mini.",
        "authors_display": "Yiliao Song Team",
        "pdf_url": "http://arxiv.org/abs/2602.14399",
        "code_url": null,
        "date": "2026-02-16",
        "primary_category": "cs.CV",
        "chinese_summary": "针对多轮越狱攻击在视觉-语言模型（LVLMs）上因视觉输入过于恶意而易被防御的问题，本研究提出了MAPA，一种多轮自适应提示攻击方法。MAPA在每轮中交替进行文本-视觉攻击动作以引发最恶意响应，并在跨轮次通过迭代细化逐步放大响应的恶意程度。这种两级设计使MAPA持续优于现有SOTA方法，在Llama-3.2-Vision-11B-Instruct、GPT-4o-mini等主流LVLMs上，攻击成功率提高了11-35%。"
      },
      {
        "paper_id": "2602.13977",
        "title": "WoVR: World Models as Reliable Simulators for Post-Training VLA Policies with RL",
        "abstract": "Reinforcement learning (RL) promises to unlock capabilities beyond imitation learning for Vision-Language-Action (VLA) models, but its requirement for massive real-world interaction prevents direct deployment on physical robots. Recent work attempts to use learned world models as simulators for policy optimization, yet closed-loop imagined rollouts inevitably suffer from hallucination and long-horizon error accumulation. Such errors do not merely degrade visual fidelity; they corrupt the optimization signal, encouraging policies to exploit model inaccuracies rather than genuine task progress. We propose WoVR, a reliable world-model-based reinforcement learning framework for post-training VLA policies. Instead of assuming a faithful world model, WoVR explicitly regulates how RL interacts with imperfect imagined dynamics. It improves rollout stability through a controllable action-conditioned video world model, reshapes imagined interaction to reduce effective error depth via Keyframe-Initialized Rollouts, and maintains policy-simulator alignment through World Model-Policy co-evolution. Extensive experiments on LIBERO benchmarks and real-world robotic manipulation demonstrate that WoVR enables stable long-horizon imagined rollouts and effective policy optimization, improving average LIBERO success from 39.95% to 69.2% (+29.3 points) and real-robot success from 61.7% to 91.7% (+30.0 points). These results show that learned world models can serve as practical simulators for reinforcement learning when hallucination is explicitly controlled.",
        "authors_display": "Dongbin Zhao Team",
        "pdf_url": "http://arxiv.org/abs/2602.13977",
        "code_url": null,
        "date": "2026-02-15",
        "primary_category": "cs.RO",
        "chinese_summary": "为解决强化学习（RL）在视觉-语言-动作（VLA）模型中因真实世界交互需求巨大而难以直接部署的问题，并克服现有世界模型在想象轨迹中存在的幻觉和长期误差积累，本研究提出了WoVR框架。WoVR通过可控的动作条件视频世界模型提高轨迹稳定性，通过关键帧初始化轨迹（Keyframe-Initialized Rollouts）减少有效误差深度，并通过世界模型-策略协同演化保持策略与模拟器对齐。实验结果表明，WoVR在LIBERO基准和真实机器人操作中显著提升了平均成功率，验证了在明确控制幻觉的情况下，学习到的世界模型可作为实用的RL模拟器。"
      },
      {
        "paper_id": "2602.13833",
        "title": "Semantic-Contact Fields for Category-Level Generalizable Tactile Tool Manipulation",
        "abstract": "Generalizing tool manipulation requires both semantic planning and precise physical control. Modern generalist robot policies, such as Vision-Language-Action (VLA) models, often lack the high-fidelity physical grounding required for contact-rich tool manipulation. Conversely, existing contact-aware policies that leverage tactile or haptic sensing are typically instance-specific and fail to generalize across diverse tool geometries. Bridging this gap requires learning unified contact representations from diverse data, yet a fundamental barrier remains: diverse real-world tactile data are prohibitive at scale, while direct zero-shot sim-to-real transfer is challenging due to the complex dynamics of nonlinear deformation of soft sensors.   To address this, we propose Semantic-Contact Fields (SCFields), a unified 3D representation fusing visual semantics with dense contact estimates. We enable this via a two-stage Sim-to-Real Contact Learning Pipeline: first, we pre-train on a large simulation data set to learn general contact physics; second, we fine-tune on a small set of real data, pseudo-labeled via geometric heuristics and force optimization, to align sensor characteristics. This allows physical generalization to unseen tools. We leverage SCFields as the dense observation input for a diffusion policy to enable robust execution of contact-rich tool manipulation tasks. Experiments on scraping, crayon drawing, and peeling demonstrate robust category-level generalization, significantly outperforming vision-only and raw-tactile baselines.",
        "authors_display": "Yan Wu Team",
        "pdf_url": "http://arxiv.org/abs/2602.13833",
        "code_url": null,
        "date": "2026-02-14",
        "primary_category": "cs.RO",
        "chinese_summary": "通用工具操作需要语义规划和精确物理控制，但现有通用机器人策略缺乏高保真物理基础，而接触感知策略又往往实例特定，难以泛化。大规模真实世界触觉数据难以获取，且软传感器复杂动力学使零样本仿真到真实迁移充满挑战。针对此，本文提出了语义-接触场（SCFields），一种融合视觉语义和密集接触估计的统一3D表示。通过两阶段Sim-to-Real接触学习管道实现：首先在大规模模拟数据上预训练以学习通用接触物理，再通过少量真实数据和伪标签进行微调以对齐传感器特性，从而实现对未见工具的物理泛化。SCFields作为扩散策略的密集观测输入，在刮擦、蜡笔画和剥皮任务中表现出鲁棒的类别级泛化能力，显著优于纯视觉和原始触觉基线。"
      },
      {
        "paper_id": "2602.13764",
        "title": "MOTIF: Learning Action Motifs for Few-shot Cross-Embodiment Transfer",
        "abstract": "While vision-language-action (VLA) models have advanced generalist robotic learning, cross-embodiment transfer remains challenging due to kinematic heterogeneity and the high cost of collecting sufficient real-world demonstrations to support fine-tuning. Existing cross-embodiment policies typically rely on shared-private architectures, which suffer from limited capacity of private parameters and lack explicit adaptation mechanisms. To address these limitations, we introduce MOTIF for efficient few-shot cross-embodiment transfer that decouples embodiment-agnostic spatiotemporal patterns, termed action motifs, from heterogeneous action data. Specifically, MOTIF first learns unified motifs via vector quantization with progress-aware alignment and embodiment adversarial constraints to ensure temporal and cross-embodiment consistency. We then design a lightweight predictor that predicts these motifs from real-time inputs to guide a flow-matching policy, fusing them with robot-specific states to enable action generation on new embodiments. Evaluations across both simulation and real-world environments validate the superiority of MOTIF, which significantly outperforms strong baselines in few-shot transfer scenarios by 6.5% in simulation and 43.7% in real-world settings. Code is available at https://github.com/buduz/MOTIF.",
        "authors_display": "Heng Tao Shen Team",
        "pdf_url": "http://arxiv.org/abs/2602.13764",
        "code_url": null,
        "date": "2026-02-14",
        "primary_category": "cs.RO",
        "chinese_summary": "视觉-语言-动作（VLA）模型在通用机器人学习方面取得进展，但由于运动学异质性和高昂的数据收集成本，跨具身（cross-embodiment）迁移仍具挑战。现有跨具身策略多依赖共享-私有架构，存在私有参数容量有限和缺乏明确适应机制的问题。为解决这些限制，本文提出了MOTIF框架，旨在通过解耦具身无关的时空模式（称为动作基序）实现高效的少样本跨具身迁移。MOTIF首先通过带有进度感知对齐和具身对抗约束的向量量化学习统一基序，确保时空和跨具身一致性；随后，设计轻量级预测器从实时输入中预测基序，并将其与机器人特定状态融合，指导流匹配策略生成新具身动作。在仿真和真实世界环境中的评估均验证了MOTIF的优越性，少样本迁移成功率显著提升。"
      },
      {
        "paper_id": "2602.13710",
        "title": "HBVLA: Pushing 1-Bit Post-Training Quantization for Vision-Language-Action Models",
        "abstract": "Vision-Language-Action (VLA) models enable instruction-following embodied control, but their large compute and memory footprints hinder deployment on resource-constrained robots and edge platforms. While reducing weights to 1-bit precision through binarization can greatly improve efficiency, existing methods fail to narrow the distribution gap between binarized and full-precision weights, causing quantization errors to accumulate under long-horizon closed-loop execution and severely degrade actions. To fill this gap, we propose HBVLA, a VLA-tailored binarization framework. First, we use a policy-aware enhanced Hessian to identify weights that are truly critical for action generation. Then, we employ a sparse orthogonal transform for non-salient weights to induce a low-entropy intermediate state. Finally, we quantize both salient and non-salient weights in the Harr domain with group-wise 1-bit quantization. We have evaluated our approach on different VLAs: on LIBERO, quantized OpenVLA-OFT retains 92.2% of full-precision performance; on SimplerEnv, quantized CogAct retains 93.6%, significantly outperforming state-of-the-art binarization methods. We further validate our method on real-world evaluation suite and the results show that HBVLA incurs only marginal success-rate degradation compared to the full-precision model, demonstrating robust deployability under tight hardware constraints. Our work provides a practical foundation for ultra-low-bit quantization of VLAs, enabling more reliable deployment on hardware-limited robotic platforms.",
        "authors_display": "Ivor Tsang Team",
        "pdf_url": "http://arxiv.org/abs/2602.13710",
        "code_url": null,
        "date": "2026-02-14",
        "primary_category": "cs.LG",
        "chinese_summary": "视觉-语言-动作（VLA）模型因其巨大的计算和内存开销，难以部署在资源受限的机器人和边缘平台。尽管权重二值化可提高效率，但现有方法无法弥合二值化与全精度权重之间的分布差距，导致长期闭环执行中量化误差累积并严重降低动作质量。针对此，本文提出了HBVLA，一个VLA定制的二值化框架。HBVLA首先利用策略感知的增强Hessian识别对动作生成关键的权重，然后对非显著权重进行稀疏正交变换以引入低熵中间状态，最后在Harr域中对所有权重进行组式1比特量化。实验结果表明，HBVLA在LIBERO和SimplerEnv上，量化模型分别保留了92.2%和93.6%的全精度性能，并显著优于现有最先进的二值化方法，在真实世界评估中也仅造成微小的成功率下降，展示了在严格硬件约束下的鲁棒部署能力。"
      },
      {
        "paper_id": "2602.13193",
        "title": "Steerable Vision-Language-Action Policies for Embodied Reasoning and Hierarchical Control",
        "abstract": "Pretrained vision-language models (VLMs) can make semantic and visual inferences across diverse settings, providing valuable common-sense priors for robotic control. However, effectively grounding this knowledge in robot behaviors remains an open challenge. Prior methods often employ a hierarchical approach where VLMs reason over high-level commands to be executed by separate low-level policies, e.g., vision-language-action models (VLAs). The interface between VLMs and VLAs is usually natural language task instructions, which fundamentally limits how much VLM reasoning can steer low-level behavior. We thus introduce Steerable Policies: VLAs trained on rich synthetic commands at various levels of abstraction, like subtasks, motions, and grounded pixel coordinates. By improving low-level controllability, Steerable Policies can unlock pretrained knowledge in VLMs, enabling improved task generalization. We demonstrate this benefit by controlling our Steerable Policies with both a learned high-level embodied reasoner and an off-the-shelf VLM prompted to reason over command abstractions via in-context learning. Across extensive real-world manipulation experiments, these two novel methods outperform prior embodied reasoning VLAs and VLM-based hierarchical baselines, including on challenging generalization and long-horizon tasks.   Website: steerable-policies.github.io",
        "authors_display": "Sergey Levine Team",
        "pdf_url": "http://arxiv.org/abs/2602.13193",
        "code_url": null,
        "date": "2026-02-13",
        "primary_category": "cs.RO",
        "chinese_summary": "预训练的视觉-语言模型（VLMs）虽能提供丰富的常识先验，但将其有效落地到机器人行为仍具挑战，现有分层方法通过自然语言指令连接VLM与VLA，限制了VLM对低层行为的引导。为增强VLM对低层行为的控制，本文引入“可控策略”（Steerable Policies），即在多抽象层次（如子任务、运动、像素坐标）的丰富合成指令上训练VLA模型。这使得VLM能够通过上下文学习引导这些策略，从而解锁其预训练知识并提升任务泛化能力。广泛的真实世界操作实验表明，与现有基于VLM的VLA和分层基线相比，新方法在泛化和长周期任务中表现更优。"
      },
      {
        "paper_id": "2602.13086",
        "title": "UniManip: General-Purpose Zero-Shot Robotic Manipulation with Agentic Operational Graph",
        "abstract": "Achieving general-purpose robotic manipulation requires robots to seamlessly bridge high-level semantic intent with low-level physical interaction in unstructured environments. However, existing approaches falter in zero-shot generalization: end-to-end Vision-Language-Action (VLA) models often lack the precision required for long-horizon tasks, while traditional hierarchical planners suffer from semantic rigidity when facing open-world variations. To address this, we present UniManip, a framework grounded in a Bi-level Agentic Operational Graph (AOG) that unifies semantic reasoning and physical grounding. By coupling a high-level Agentic Layer for task orchestration with a low-level Scene Layer for dynamic state representation, the system continuously aligns abstract planning with geometric constraints, enabling robust zero-shot execution. Unlike static pipelines, UniManip operates as a dynamic agentic loop: it actively instantiates object-centric scene graphs from unstructured perception, parameterizes these representations into collision-free trajectories via a safety-aware local planner, and exploits structured memory to autonomously diagnose and recover from execution failures. Extensive experiments validate the system's robust zero-shot capability on unseen objects and tasks, demonstrating a 22.5% and 25.0% higher success rate compared to state-of-the-art VLA and hierarchical baselines, respectively. Notably, the system enables direct zero-shot transfer from fixed-base setups to mobile manipulation without fine-tuning or reconfiguration. Our open-source project page can be found at https://henryhcliu.github.io/unimanip.",
        "authors_display": "Ziwei Wang Team",
        "pdf_url": "http://arxiv.org/abs/2602.13086",
        "code_url": "https://henryhcliu.github.io/unimanip",
        "date": "2026-02-13",
        "primary_category": "cs.RO",
        "chinese_summary": "通用机器人操作需要机器人无缝连接高层语义意图与低层物理交互，但现有方法在零样本泛化方面存在不足。针对此问题，本文提出了UniManip框架，其核心是双层代理操作图（AOG），旨在统一语义推理与物理接地。该框架通过高层代理层进行任务编排，低层场景层表示动态状态，实现抽象规划与几何约束的持续对齐，从而支持鲁棒的零样本执行。作为一个动态代理循环，UniManip能从非结构化感知中实例化以物体为中心的场景图，通过安全感知局部规划器参数化无碰撞轨迹，并利用结构化记忆自主诊断和从执行失败中恢复。广泛实验证明，该系统在未见过物体和任务上的零样本成功率分别比VLA和分层基线高22.5%和25.0%，并能直接零样本迁移到移动操作任务。"
      },
      {
        "paper_id": "2602.12978",
        "title": "Learning Native Continuation for Action Chunking Flow Policies",
        "abstract": "Action chunking enables Vision Language Action (VLA) models to run in real time, but naive chunked execution often exhibits discontinuities at chunk boundaries. Real-Time Chunking (RTC) alleviates this issue but is external to the policy, leading to spurious multimodal switching and trajectories that are not intrinsically smooth. We propose Legato, a training-time continuation method for action-chunked flow-based VLA policies. Specifically, Legato initializes denoising from a schedule-shaped mixture of known actions and noise, exposing the model to partial action information. Moreover, Legato reshapes the learned flow dynamics to ensure that the denoising process remains consistent between training and inference under per-step guidance. Legato further uses randomized schedule condition during training to support varying inference delays and achieve controllable smoothness. Empirically, Legato produces smoother trajectories and reduces spurious multimodal switching during execution, leading to less hesitation and shorter task completion time. Extensive real-world experiments show that Legato consistently outperforms RTC across five manipulation tasks, achieving approximately 10% improvements in both trajectory smoothness and task completion time.",
        "authors_display": "Yang Gao Team",
        "pdf_url": "http://arxiv.org/abs/2602.12978",
        "code_url": "https://lyfeng001.github.io/Legato/",
        "date": "2026-02-13",
        "primary_category": "cs.RO",
        "chinese_summary": "动作分块能使视觉-语言-动作（VLA）模型实时运行，但简单的分块执行常导致块边界处的不连续性。现有的实时分块（RTC）方法虽能缓解此问题，但由于其在策略外部，会引起虚假的多模态切换和非内在平滑的轨迹。为此，本文提出了Legato，一种针对基于动作分块流的VLA策略的训练时序延续方法。Legato通过将去噪初始化为已知动作和噪声的调度形混合物，使模型接触部分动作信息，并重塑学习到的流动力学以确保训练和推理在每步引导下保持一致性，同时使用随机调度条件训练以支持不同推理延迟和实现可控平滑性。实验证明，Legato能生成更平滑的轨迹，减少执行时的虚假多模态切换，从而减少犹豫和缩短任务完成时间，在五项操作任务中均比RTC表现更优，轨迹平滑度和任务完成时间均提升约10%。"
      },
      {
        "paper_id": "2602.12691",
        "title": "ALOE: Action-Level Off-Policy Evaluation for Vision-Language-Action Model Post-Training",
        "abstract": "We study how to improve large foundation vision-language-action (VLA) systems through online reinforcement learning (RL) in real-world settings. Central to this process is the value function, which provides learning signals to guide VLA learning from experience. In practice, the value function is estimated from trajectory fragments collected from different data sources, including historical policies and intermittent human interventions. Estimating the value function of current behavior quality from the mixture data is inherently an off-policy evaluation problem. However, prior work often adopts conservative on-policy estimation for stability, which avoids direct evaluation of the current high-capacity policy and limits learning effectiveness. In this paper, we propose ALOE, an action-level off-policy evaluation framework for VLA post-training. ALOE applies chunking-based temporal-difference bootstrapping to evaluate individual action sequences instead of predicting final task outcomes. This design improves effective credit assignment to critical action chunks under sparse rewards and supports stable policy improvement. We evaluate our method on three real-world manipulation tasks, including smartphone packing as a high-precision task, laundry folding as a long-horizon deformable-object task, and bimanual pick-and-place involving multi-object perception. Across all tasks, ALOE improves learning efficiency without compromising execution speed, showing that off-policy RL can be reintroduced in a reliable manner for real-world VLA post-training. Videos and additional materials are available at our project website.",
        "authors_display": "Maoqing Yao Team",
        "pdf_url": "http://arxiv.org/abs/2602.12691",
        "code_url": null,
        "date": "2026-02-13",
        "primary_category": "cs.RO",
        "chinese_summary": "在真实世界中通过在线强化学习（RL）改进大型视觉-语言-动作（VLA）系统时，价值函数估计是关键，但其通常从混合数据源中收集的轨迹片段进行，这本质上是一个离策略评估问题，而现有工作常采用保守的同策略估计，限制了学习效果。为解决这一问题，本文提出了ALOE（Action-Level Off-Policy Evaluation）框架，用于VLA的后期训练。ALOE采用基于分块的时间差分自举法来评估单个动作序列而非预测最终任务结果，这在稀疏奖励下能更好地将信用归因于关键动作分块，并支持稳定的策略改进。在三项真实世界操作任务上的评估表明，ALOE在不影响执行速度的前提下提高了学习效率，验证了离策略RL在VLA后期训练中可被可靠地重新引入。"
      },
      {
        "paper_id": "2602.12686",
        "title": "SignScene: Visual Sign Grounding for Mapless Navigation",
        "abstract": "Navigational signs enable humans to navigate unfamiliar environments without maps. This work studies how robots can similarly exploit signs for mapless navigation in the open world. A central challenge lies in interpreting signs: real-world signs are diverse and complex, and their abstract semantic contents need to be grounded in the local 3D scene. We formalize this as sign grounding, the problem of mapping semantic instructions on signs to corresponding scene elements and navigational actions. Recent Vision-Language Models (VLMs) offer the semantic common-sense and reasoning capabilities required for this task, but are sensitive to how spatial information is represented. We propose SignScene, a sign-centric spatial-semantic representation that captures navigation-relevant scene elements and sign information, and presents them to VLMs in a form conducive to effective reasoning. We evaluate our grounding approach on a dataset of 114 queries collected across nine diverse environment types, achieving 88% grounding accuracy and significantly outperforming baselines. Finally, we demonstrate that it enables real-world mapless navigation on a Spot robot using only signs.",
        "authors_display": "David Hsu Team",
        "pdf_url": "http://arxiv.org/abs/2602.12686",
        "code_url": null,
        "date": "2026-02-13",
        "primary_category": "cs.RO",
        "chinese_summary": "人类可利用导航标识在陌生环境中无需地图进行导航，本研究旨在使机器人也能利用标识实现开放世界中的无图导航。核心挑战在于解释标识：真实世界标识多样复杂，其抽象语义内容需与局部3D场景进行接地。本文将此形式化为标识接地问题，即把标识上的语义指令映射到对应的场景元素和导航动作。考虑到视觉-语言模型（VLMs）具备所需的语义常识和推理能力但对空间表示敏感，我们提出了SignScene，一种以标识为中心的空间-语义表示方法，它捕获导航相关的场景元素和标识信息，并以有利于VLM有效推理的形式呈现。在包含9种不同环境类型、114个查询的数据集上评估，SignScene达到了88%的接地准确率，显著优于基线方法，并能驱动Spot机器人在真实世界中仅依赖标识进行无图导航。"
      },
      {
        "paper_id": "2602.12684",
        "title": "Xiaomi-Robotics-0: An Open-Sourced Vision-Language-Action Model with Real-Time Execution",
        "abstract": "In this report, we introduce Xiaomi-Robotics-0, an advanced vision-language-action (VLA) model optimized for high performance and fast and smooth real-time execution. The key to our method lies in a carefully designed training recipe and deployment strategy. Xiaomi-Robotics-0 is first pre-trained on large-scale cross-embodiment robot trajectories and vision-language data, endowing it with broad and generalizable action-generation capabilities while avoiding catastrophic forgetting of the visual-semantic knowledge of the underlying pre-trained VLM. During post-training, we propose several techniques for training the VLA model for asynchronous execution to address the inference latency during real-robot rollouts. During deployment, we carefully align the timesteps of consecutive predicted action chunks to ensure continuous and seamless real-time rollouts. We evaluate Xiaomi-Robotics-0 extensively in simulation benchmarks and on two challenging real-robot tasks that require precise and dexterous bimanual manipulation. Results show that our method achieves state-of-the-art performance across all simulation benchmarks. Moreover, Xiaomi-Robotics-0 can roll out fast and smoothly on real robots using a consumer-grade GPU, achieving high success rates and throughput on both real-robot tasks. To facilitate future research, code and model checkpoints are open-sourced at https://xiaomi-robotics-0.github.io",
        "authors_display": "Quanyun Zhou Team",
        "pdf_url": "http://arxiv.org/abs/2602.12684",
        "code_url": "https://xiaomi-robotics-0.github.io",
        "date": "2026-02-13",
        "primary_category": "cs.RO",
        "chinese_summary": "本文介绍了小米机器人0号（Xiaomi-Robotics-0），一个针对高性能、快速且平滑实时执行优化的先进视觉-语言-动作（VLA）模型。该方法的核心在于精心设计的训练配方和部署策略。模型首先在大规模跨实体机器人轨迹和视觉-语言数据上进行预训练，赋予其广泛且泛化的动作生成能力，同时避免灾难性遗忘底层VLM的视觉语义知识。在后期训练中，作者提出了多种技术用于异步执行训练，以解决真实机器人执行时的推理延迟问题。部署时，仔细对齐连续预测动作块的时间步，确保实时执行的连续性和无缝性。广泛的模拟基准测试和两个需要精确灵巧双臂操作的真实机器人任务评估表明，该方法在所有模拟基准上均达到了最先进的性能，并在真实机器人任务上使用消费级GPU实现了高成功率和吞吐量，代码和模型检查点已开源。"
      },
      {
        "paper_id": "2602.12532",
        "title": "CRAFT: Adapting VLA Models to Contact-rich Manipulation via Force-aware Curriculum Fine-tuning",
        "abstract": "Vision-Language-Action (VLA) models have shown a strong capability in enabling robots to execute general instructions, yet they struggle with contact-rich manipulation tasks, where success requires precise alignment, stable contact maintenance, and effective handling of deformable objects. A fundamental challenge arises from the imbalance between high-entropy vision and language inputs and low-entropy but critical force signals, which often leads to over-reliance on perception and unstable control. To address this, we introduce CRAFT, a force-aware curriculum fine-tuning framework that integrates a variational information bottleneck module to regulate vision and language embeddings during early training. This curriculum strategy encourages the model to prioritize force signals initially, before progressively restoring access to the full multimodal information. To enable force-aware learning, we further design a homologous leader-follower teleoperation system that collects synchronized vision, language, and force data across diverse contact-rich tasks. Real-world experiments demonstrate that CRAFT consistently improves task success, generalizes to unseen objects and novel task variations, and adapts effectively across diverse VLA architectures, enabling robust and generalizable contact-rich manipulation.",
        "authors_display": "Jingtao Sun Team",
        "pdf_url": "http://arxiv.org/abs/2602.12532",
        "code_url": null,
        "date": "2026-02-13",
        "primary_category": "cs.RO",
        "chinese_summary": "视觉-语言-动作（VLA）模型在处理接触密集型操作任务时面临挑战，因为成功需要精确对齐、稳定接触和处理变形物体，而高熵视觉语言输入与低熵但关键的力信号之间存在不平衡，导致模型过度依赖感知并产生不稳定控制。针对此问题，本文引入CRAFT，一个力感知课程微调框架，该框架集成了一个变分信息瓶颈模块，以在早期训练中调节视觉和语言嵌入，鼓励模型优先处理力信号，然后逐步恢复完整的多模态信息。为实现力感知学习，作者设计了一个同源主从遥操作系统，用于收集各种接触密集型任务中同步的视觉、语言和力数据。真实世界实验表明，CRAFT持续提高了任务成功率，泛化到未见物体和新任务变体，并能有效适应不同VLA架构，从而实现鲁棒且可泛化的接触密集型操作。"
      },
      {
        "paper_id": "2602.13476",
        "title": "AsyncVLA: An Asynchronous VLA for Fast and Robust Navigation on the Edge",
        "abstract": "Robotic foundation models achieve strong generalization by leveraging internet-scale vision-language representations, but their massive computational cost creates a fundamental bottleneck: high inference latency. In dynamic environments, this latency breaks the control loop, rendering powerful models unsafe for real-time deployment. We propose AsyncVLA, an asynchronous control framework that decouples semantic reasoning from reactive execution. Inspired by hierarchical control, AsyncVLA runs a large foundation model on a remote workstation to provide high-level guidance, while a lightweight, onboard Edge Adapter continuously refines actions at high frequency. To bridge the domain gap between these asynchronous streams, we introduce an end-to-end finetuning protocol and a trajectory re-weighting strategy that prioritizes dynamic interactions. We evaluate our approach on real-world vision-based navigation tasks with communication delays up to 6 seconds. AsyncVLA achieves a 40% higher success rate than state-of-the-art baselines, effectively bridging the gap between the semantic intelligence of large models and the reactivity required for edge robotics.",
        "authors_display": "Sergey Levine Team",
        "pdf_url": "http://arxiv.org/abs/2602.13476",
        "code_url": null,
        "date": "2026-02-13",
        "primary_category": "cs.RO",
        "chinese_summary": "当前机器人基础模型虽泛化能力强，但推理延迟高，导致在动态环境中不安全。针对此问题，本文提出了AsyncVLA异步控制框架，将语义推理与反应性执行解耦。该框架通过远程工作站上的大型基础模型提供高层指导，同时由轻量级板载Edge Adapter高频精细化动作，并通过端到端微调协议和轨迹重加权策略弥合异步流间的域间隙。在面对高达6秒通信延迟的真实视觉导航任务中，AsyncVLA的成功率比现有最佳基线高出40%，成功连接了大型模型的语义智能与边缘机器人所需的实时反应能力。"
      },
      {
        "paper_id": "2602.13444",
        "title": "FlowHOI: Flow-based Semantics-Grounded Generation of Hand-Object Interactions for Dexterous Robot Manipulation",
        "abstract": "Recent vision-language-action (VLA) models can generate plausible end-effector motions, yet they often fail in long-horizon, contact-rich tasks because the underlying hand-object interaction (HOI) structure is not explicitly represented. An embodiment-agnostic interaction representation that captures this structure would make manipulation behaviors easier to validate and transfer across robots. We propose FlowHOI, a two-stage flow-matching framework that generates semantically grounded, temporally coherent HOI sequences, comprising hand poses, object poses, and hand-object contact states, conditioned on an egocentric observation, a language instruction, and a 3D Gaussian splatting (3DGS) scene reconstruction. We decouple geometry-centric grasping from semantics-centric manipulation, conditioning the latter on compact 3D scene tokens and employing a motion-text alignment loss to semantically ground the generated interactions in both the physical scene layout and the language instruction. To address the scarcity of high-fidelity HOI supervision, we introduce a reconstruction pipeline that recovers aligned hand-object trajectories and meshes from large-scale egocentric videos, yielding an HOI prior for robust generation. Across the GRAB and HOT3D benchmarks, FlowHOI achieves the highest action recognition accuracy and a 1.7$\\times$ higher physics simulation success rate than the strongest diffusion-based baseline, while delivering a 40$\\times$ inference speedup. We further demonstrate real-robot execution on four dexterous manipulation tasks, illustrating the feasibility of retargeting generated HOI representations to real-robot execution pipelines.",
        "authors_display": "Xingxing Zuo Team",
        "pdf_url": "http://arxiv.org/abs/2602.13444",
        "code_url": "https://huajian-zeng.github.io/projects/flowhoi/",
        "date": "2026-02-13",
        "primary_category": "cs.RO",
        "chinese_summary": "现有视觉-语言-动作（VLA）模型在长周期、接触密集型任务中表现不佳，原因在于缺乏对手-物体交互（HOI）结构的明确表示。为解决此问题，本文提出FlowHOI，一个两阶段流匹配框架，可根据自我中心观察、语言指令和3D高斯泼溅场景重建生成语义明确、时间连贯的HOI序列。该方法将以几何为中心的抓取与以语义为中心的操控解耦，并利用3D场景令牌和运动-文本对齐损失进行语义接地，同时通过从大规模自我中心视频重建HOI轨迹的方法弥补高保真HOI监督的稀缺性。实验结果显示，FlowHOI在GRAB和HOT3D基准测试中实现了最高的动作识别准确率，物理模拟成功率比扩散基线高1.7倍，推理速度提升40倍，并成功在真实机器人上执行了灵巧操作任务。"
      },
      {
        "paper_id": "2602.12281",
        "title": "Scaling Verification Can Be More Effective than Scaling Policy Learning for Vision-Language-Action Alignment",
        "abstract": "The long-standing vision of general-purpose robots hinges on their ability to understand and act upon natural language instructions. Vision-Language-Action (VLA) models have made remarkable progress toward this goal, yet their generated actions can still misalign with the given instructions. In this paper, we investigate test-time verification as a means to shrink the \"intention-action gap.'' We first characterize the test-time scaling law for embodied instruction following and demonstrate that jointly scaling the number of rephrased instructions and generated actions greatly increases test-time sample diversity, often recovering correct actions more efficiently than scaling each dimension independently. To capitalize on these scaling laws, we present CoVer, a contrastive verifier for vision-language-action alignment, and show that our architecture scales gracefully with additional computational resources and data. We then introduce \"boot-time compute\" and a hierarchical verification inference pipeline for VLAs. At deployment, our framework precomputes a diverse set of rephrased instructions from a Vision-Language-Model (VLM), repeatedly generates action candidates for each instruction, and then uses a verifier to select the optimal high-level prompt and low-level action chunks. Compared to scaling policy pre-training on the same data, our verification approach yields 22% gains in-distribution and 13% out-of-distribution on the SIMPLER benchmark, with a further 45% improvement in real-world experiments. On the PolaRiS benchmark, CoVer achieves 14% gains in task progress and 9% in success rate.",
        "authors_display": "Marco Pavone Team",
        "pdf_url": "http://arxiv.org/abs/2602.12281",
        "code_url": null,
        "date": "2026-02-12",
        "primary_category": "cs.RO",
        "chinese_summary": "通用机器人理解并执行自然语言指令是长期愿景，尽管视觉-语言-动作（VLA）模型已取得显著进展，但其生成动作仍可能与指令不符。为缩小“意图-动作差距”，本文研究了测试时验证方法。通过分析具身指令遵循的测试时缩放定律，发现联合缩放复述指令和生成动作的数量能更有效地增加测试时样本多样性。在此基础上，提出了CoVer，一种用于VLA对齐的对比验证器，该架构能随计算资源和数据的增加而良好扩展。进一步引入“启动时计算”和分层验证推理流程：在部署时预计算多样化的复述指令，为每条指令重复生成动作候选，然后使用验证器选择最优高层提示和低层动作块。实验表明，相比扩展策略预训练，CoVer在SIMPLER基准上实现了22%的分布内和13%的分布外增益，并在真实世界实验中进一步提高了45%，在PolaRiS基准上任务进度和成功率分别提升了14%和9%。"
      },
      {
        "paper_id": "2602.12099",
        "title": "GigaBrain-0.5M*: a VLA That Learns From World Model-Based Reinforcement Learning",
        "abstract": "Vision-language-action (VLA) models that directly predict multi-step action chunks from current observations face inherent limitations due to constrained scene understanding and weak future anticipation capabilities. In contrast, video world models pre-trained on web-scale video corpora exhibit robust spatiotemporal reasoning and accurate future prediction, making them a natural foundation for enhancing VLA learning. Therefore, we propose \\textit{GigaBrain-0.5M*}, a VLA model trained via world model-based reinforcement learning. Built upon \\textit{GigaBrain-0.5}, which is pre-trained on over 10,000 hours of robotic manipulation data, whose intermediate version currently ranks first on the international RoboChallenge benchmark. \\textit{GigaBrain-0.5M*} further integrates world model-based reinforcement learning via \\textit{RAMP} (Reinforcement leArning via world Model-conditioned Policy) to enable robust cross-task adaptation. Empirical results demonstrate that \\textit{RAMP} achieves substantial performance gains over the RECAP baseline, yielding improvements of approximately 30\\% on challenging tasks including \\texttt{Laundry Folding}, \\texttt{Box Packing}, and \\texttt{Espresso Preparation}. Critically, \\textit{GigaBrain-0.5M$^*$} exhibits reliable long-horizon execution, consistently accomplishing complex manipulation tasks without failure as validated by real-world deployment videos on our \\href{https://gigabrain05m.github.io}{project page}.",
        "authors_display": "Zheng Zhu Team",
        "pdf_url": "http://arxiv.org/abs/2602.12099",
        "code_url": "https://gigabrain05m.github.io/",
        "date": "2026-02-12",
        "primary_category": "cs.CV",
        "chinese_summary": "针对现有视觉-语言-动作（VLA）模型在场景理解和未来预测方面的局限性，本研究提出GigaBrain-0.5M*，一个通过世界模型强化学习训练的VLA模型。该模型基于已在大量机器人操作数据上预训练的GigaBrain-0.5，并整合了RAMP（Reinforcement learning via world Model-conditioned Policy）以实现鲁棒的跨任务适应。实验结果表明，RAMP在RECAP基线之上取得了显著的性能提升，在洗衣折叠、箱子包装和咖啡制作等挑战性任务上提升约30%，并在真实部署中展示了可靠的长期执行能力，能够无故障完成复杂的操纵任务。"
      },
      {
        "paper_id": "2602.12063",
        "title": "VLAW: Iterative Co-Improvement of Vision-Language-Action Policy and World Model",
        "abstract": "The goal of this paper is to improve the performance and reliability of vision-language-action (VLA) models through iterative online interaction. Since collecting policy rollouts in the real world is expensive, we investigate whether a learned simulator-specifically, an action-conditioned video generation model-can be used to generate additional rollout data. Unfortunately, existing world models lack the physical fidelity necessary for policy improvement: they are predominantly trained on demonstration datasets that lack coverage of many different physical interactions (particularly failure cases) and struggle to accurately model small yet critical physical details in contact-rich object manipulation. We propose a simple iterative improvement algorithm that uses real-world roll-out data to improve the fidelity of the world model, which can then, in turn, be used to generate supplemental synthetic data for improving the VLA model. In our experiments on a real robot, we use this approach to improve the performance of a state-of-the-art VLA model on multiple downstream tasks. We achieve a 39.2% absolute success rate improvement over the base policy and 11.6% improvement from training with the generated synthetic rollouts. Videos can be found at this anonymous website: https://sites.google.com/view/vla-w",
        "authors_display": "Chelsea Finn Team",
        "pdf_url": "http://arxiv.org/abs/2602.12063",
        "code_url": null,
        "date": "2026-02-12",
        "primary_category": "cs.RO",
        "chinese_summary": "为提升视觉-语言-动作（VLA）模型性能和可靠性，并解决真实世界数据收集成本高及现有世界模型物理保真度不足的问题，本研究提出一个迭代改进算法。该算法利用少量真实世界试错数据提高世界模型的保真度，然后世界模型生成补充合成数据以改进VLA模型。在真实机器人上的实验表明，该方法使先进VLA模型的成功率相较于基础策略绝对提升39.2%，并且通过生成的合成试错数据训练，又额外提升了11.6%。"
      },
      {
        "paper_id": "2602.12062",
        "title": "HoloBrain-0 Technical Report",
        "abstract": "In this work, we introduce HoloBrain-0, a comprehensive Vision-Language-Action (VLA) framework that bridges the gap between foundation model research and reliable real-world robot deployment. The core of our system is a novel VLA architecture that explicitly incorporates robot embodiment priors, including multi-view camera parameters and kinematic descriptions (URDF), to enhance 3D spatial reasoning and support diverse embodiments. We validate this design through a scalable ``pre-train then post-train\" paradigm, achieving state-of-the-art results on simulation benchmarks such as RoboTwin 2.0, LIBERO, and GenieSim, as well as strong results on challenging long-horizon real-world manipulation tasks. Notably, our efficient 0.2B-parameter variant rivals significantly larger baselines, enabling low-latency on-device deployment. To further accelerate research and practical adoption, we fully open-source the entire HoloBrain ecosystem, which includes: (1) powerful pre-trained VLA foundations; (2) post-trained checkpoints for multiple simulation suites and real-world tasks; and (3) RoboOrchard, a full-stack VLA infrastructure for data curation, model training and deployment. Together with standardized data collection protocols, this release provides the community with a complete, reproducible path toward high-performance robotic manipulation.",
        "authors_display": "Zhizhong Su Team",
        "pdf_url": "http://arxiv.org/abs/2602.12062",
        "code_url": null,
        "date": "2026-02-12",
        "primary_category": "cs.RO",
        "chinese_summary": "针对基础模型研究与可靠机器人真实部署之间的差距，本研究提出了HoloBrain-0，一个全面的视觉-语言-动作（VLA）框架。其核心是一个新颖的VLA架构，通过明确整合机器人具身先验（如多视角相机参数和运动学描述）来增强3D空间推理并支持多样化具身形态。该设计通过“预训练后微调”范式验证，在多个仿真基准和长时程真实世界操作任务中取得领先结果，且一个高效的0.2B参数变体能支持低延迟部署。研究还全面开源了HoloBrain生态系统，旨在加速研究和实际应用。"
      },
      {
        "paper_id": "2602.12032",
        "title": "When would Vision-Proprioception Policies Fail in Robotic Manipulation?",
        "abstract": "Proprioceptive information is critical for precise servo control by providing real-time robotic states. Its collaboration with vision is highly expected to enhance performances of the manipulation policy in complex tasks. However, recent studies have reported inconsistent observations on the generalization of vision-proprioception policies. In this work, we investigate this by conducting temporally controlled experiments. We found that during task sub-phases that robot's motion transitions, which require target localization, the vision modality of the vision-proprioception policy plays a limited role. Further analysis reveals that the policy naturally gravitates toward concise proprioceptive signals that offer faster loss reduction when training, thereby dominating the optimization and suppressing the learning of the visual modality during motion-transition phases. To alleviate this, we propose the Gradient Adjustment with Phase-guidance (GAP) algorithm that adaptively modulates the optimization of proprioception, enabling dynamic collaboration within the vision-proprioception policy. Specifically, we leverage proprioception to capture robotic states and estimate the probability of each timestep in the trajectory belonging to motion-transition phases. During policy learning, we apply fine-grained adjustment that reduces the magnitude of proprioception's gradient based on estimated probabilities, leading to robust and generalizable vision-proprioception policies. The comprehensive experiments demonstrate GAP is applicable in both simulated and real-world environments, across one-arm and dual-arm setups, and compatible with both conventional and Vision-Language-Action models. We believe this work can offer valuable insights into the development of vision-proprioception policies in robotic manipulation.",
        "authors_display": "Di Hu Team",
        "pdf_url": "http://arxiv.org/abs/2602.12032",
        "code_url": null,
        "date": "2026-02-12",
        "primary_category": "cs.RO",
        "chinese_summary": "针对视觉-本体感受策略在机器人运动过渡阶段视觉模态作用受限、策略倾向于利用本体感受信号导致视觉学习受抑制的问题，本研究提出了梯度调整与阶段引导（GAP）算法。该算法通过利用本体感受信息估计运动过渡阶段的概率，并自适应地调整本体感受梯度的幅值，以实现视觉和本体感受模态的动态协同。综合实验表明，GAP算法能够提升视觉-本体感受策略的鲁棒性和泛化性，适用于模拟和真实环境、单臂和双臂设置，并兼容多种VLA模型。"
      },
      {
        "paper_id": "2602.11832",
        "title": "JEPA-VLA: Video Predictive Embedding is Needed for VLA Models",
        "abstract": "Recent vision-language-action (VLA) models built upon pretrained vision-language models (VLMs) have achieved significant improvements in robotic manipulation. However, current VLAs still suffer from low sample efficiency and limited generalization. This paper argues that these limitations are closely tied to an overlooked component, pretrained visual representation, which offers insufficient knowledge on both aspects of environment understanding and policy prior. Through an in-depth analysis, we find that commonly used visual representations in VLAs, whether pretrained via language-image contrastive learning or image-based self-supervised learning, remain inadequate at capturing crucial, task-relevant environment information and at inducing effective policy priors, i.e., anticipatory knowledge of how the environment evolves under successful task execution. In contrast, we discover that predictive embeddings pretrained on videos, in particular V-JEPA 2, are adept at flexibly discarding unpredictable environment factors and encoding task-relevant temporal dynamics, thereby effectively compensating for key shortcomings of existing visual representations in VLAs. Building on these observations, we introduce JEPA-VLA, a simple yet effective approach that adaptively integrates predictive embeddings into existing VLAs. Our experiments demonstrate that JEPA-VLA yields substantial performance gains across a range of benchmarks, including LIBERO, LIBERO-plus, RoboTwin2.0, and real-robot tasks.",
        "authors_display": "Mingsheng Long Team",
        "pdf_url": "http://arxiv.org/abs/2602.11832",
        "code_url": null,
        "date": "2026-02-12",
        "primary_category": "cs.CV",
        "chinese_summary": "针对现有视觉-语言-动作（VLA）模型存在的样本效率低和泛化能力有限问题，本研究发现其根源在于预训练视觉表示在环境理解和策略先验方面的知识不足。通过深入分析，研究指出在视频上预训练的预测嵌入，特别是V-JEPA 2，能更有效地捕捉任务相关时态动态并忽略不可预测因素，从而弥补了现有视觉表示的缺陷。在此基础上，提出了JEPA-VLA，一种将预测嵌入自适应整合到现有VLA中的方法。实验证明，JEPA-VLA在LIBERO、LIBERO-plus、RoboTwin2.0和真实机器人任务等一系列基准测试中均取得了显著性能提升。"
      },
      {
        "paper_id": "2602.11598",
        "title": "ABot-N0: Technical Report on the VLA Foundation Model for Versatile Embodied Navigation",
        "abstract": "Embodied navigation has long been fragmented by task-specific architectures. We introduce ABot-N0, a unified Vision-Language-Action (VLA) foundation model that achieves a ``Grand Unification'' across 5 core tasks: Point-Goal, Object-Goal, Instruction-Following, POI-Goal, and Person-Following. ABot-N0 utilizes a hierarchical ``Brain-Action'' architecture, pairing an LLM-based Cognitive Brain for semantic reasoning with a Flow Matching-based Action Expert for precise, continuous trajectory generation.   To support large-scale learning, we developed the ABot-N0 Data Engine, curating 16.9M expert trajectories and 5.0M reasoning samples across 7,802 high-fidelity 3D scenes (10.7 $\\text{km}^2$). ABot-N0 achieves new SOTA performance across 7 benchmarks, significantly outperforming specialized models. Furthermore, our Agentic Navigation System integrates a planner with hierarchical topological memory, enabling robust, long-horizon missions in dynamic real-world environments.",
        "authors_display": "Mu Xu Team",
        "pdf_url": "http://arxiv.org/abs/2602.11598",
        "code_url": "https://amap-cvlab.github.io/ABot-Navigation/ABot-N0/",
        "date": "2026-02-12",
        "primary_category": "cs.RO",
        "chinese_summary": "针对具身导航任务碎片化的问题，本研究引入了ABot-N0，一个统一的视觉-语言-动作（VLA）基础模型，旨在实现点目标、物体目标、指令遵循、兴趣点目标和人员跟踪五大核心任务的“大一统”。该模型采用分层“大脑-动作”架构，利用大型语言模型进行语义推理，并结合流匹配专家生成精确轨迹。为支持大规模学习，研究构建了ABot-N0数据引擎，整理了海量专家轨迹和推理样本。实验结果表明，ABot-N0在7个基准测试中取得了最先进的性能，并能通过集成的Agentic导航系统实现动态真实世界环境中的鲁棒、长时程任务执行。"
      },
      {
        "paper_id": "2602.10983",
        "title": "Scaling World Model for Hierarchical Manipulation Policies",
        "abstract": "Vision-Language-Action (VLA) models are promising for generalist robot manipulation but remain brittle in out-of-distribution (OOD) settings, especially with limited real-robot data. To resolve the generalization bottleneck, we introduce a hierarchical Vision-Language-Action framework \\our{} that leverages the generalization of large-scale pre-trained world model for robust and generalizable VIsual Subgoal TAsk decomposition VISTA. Our hierarchical framework \\our{} consists of a world model as the high-level planner and a VLA as the low-level executor. The high-level world model first divides manipulation tasks into subtask sequences with goal images, and the low-level policy follows the textual and visual guidance to generate action sequences. Compared to raw textual goal specification, these synthesized goal images provide visually and physically grounded details for low-level policies, making it feasible to generalize across unseen objects and novel scenarios. We validate both visual goal synthesis and our hierarchical VLA policies in massive out-of-distribution scenarios, and the performance of the same-structured VLA in novel scenarios could boost from 14% to 69% with the guidance generated by the world model. Results demonstrate that our method outperforms previous baselines with a clear margin, particularly in out-of-distribution scenarios. Project page: \\href{https://vista-wm.github.io/}{https://vista-wm.github.io}",
        "authors_display": "Xinghang Li Team",
        "pdf_url": "http://arxiv.org/abs/2602.10983",
        "code_url": null,
        "date": "2026-02-12",
        "primary_category": "cs.RO",
        "chinese_summary": "针对视觉-语言-动作（VLA）模型在域外（OOD）设置下泛化能力不足的问题，本研究引入了一个分层VLA框架VISTA。VISTA利用大规模预训练世界模型的泛化能力实现鲁棒且可泛化的视觉子目标任务分解，其中高层世界模型规划任务分解为带有目标图像的子任务序列，低层VLA策略遵循文本和视觉指导生成动作。这些合成的目标图像为低层策略提供了视觉和物理上的详细指导，使其能够泛化到未见过的物体和新场景。实验结果表明，在世界模型生成的指导下，VISTA在大量OOD场景中显著提升了VLA性能，在novel场景中性能从14%提高到69%。"
      },
      {
        "paper_id": "2602.12322",
        "title": "ForeAct: Steering Your VLA with Efficient Visual Foresight Planning",
        "abstract": "Vision-Language-Action (VLA) models convert high-level language instructions into concrete, executable actions, a task that is especially challenging in open-world environments. We present Visual Foresight Planning (ForeAct), a general and efficient planner that guides a VLA step-by-step using imagined future observations and subtask descriptions. With an imagined future observation, the VLA can focus on visuo-motor inference rather than high-level semantic reasoning, leading to improved accuracy and generalization. Our planner comprises a highly efficient foresight image generation module that predicts a high-quality 640$\\times$480 future observation from the current visual input and language instruction within only 0.33s on an H100 GPU, together with a vision-language model that reasons over the task and produces subtask descriptions for both the generator and the VLA. Importantly, state-of-the-art VLAs can integrate our planner seamlessly by simply augmenting their visual inputs, without any architectural modification. The foresight generator is pretrained on over 1 million multi-task, cross-embodiment episodes, enabling it to learn robust embodied dynamics. We evaluate our framework on a benchmark that consists of 11 diverse, multi-step real-world tasks. It achieves an average success rate of 87.4%, demonstrating a +40.9% absolute improvement over the $π_0$ baseline (46.5%) and a +30.3% absolute improvement over $π_0$ augmented with textual subtask guidance (57.1%).",
        "authors_display": "Song Han Team",
        "pdf_url": "http://arxiv.org/abs/2602.12322",
        "code_url": null,
        "date": "2026-02-12",
        "primary_category": "cs.RO",
        "chinese_summary": "视觉-语言-动作（VLA）模型将高级语言指令转换为具体可执行动作，在开放世界环境中尤具挑战性。本文提出了Visual Foresight Planning (ForeAct)，这是一种通用高效的规划器，通过想象的未来观察和子任务描述逐步指导VLA。该规划器包含一个高效的预测图像生成模块（在0.33秒内预测高质量未来观察）和一个视觉-语言模型，后者负责推理任务并生成子任务描述。重要的是，ForeAct能够无缝集成到现有VLA中，只需扩充视觉输入而无需修改架构。经过百万级跨具身任务的预训练，预测生成器学习了鲁棒的具身动力学。在包含11项多样化、多步骤真实世界任务的基准测试中，ForeAct实现了87.4%的平均成功率，比基线$π_0$提高了40.9%，比带有文本子任务指导的$π_0$提高了30.3%。"
      },
      {
        "paper_id": "2602.11291",
        "title": "H-WM: Robotic Task and Motion Planning Guided by Hierarchical World Model",
        "abstract": "World models are becoming central to robotic planning and control, as they enable prediction of future state transitions. Existing approaches often emphasize video generation or natural language prediction, which are difficult to directly ground in robot actions and suffer from compounding errors over long horizons. Traditional task and motion planning relies on symbolic logic world models, such as planning domains, that are robot-executable and robust for long-horizon reasoning. However, these methods typically operate independently of visual perception, preventing synchronized symbolic and perceptual state prediction. We propose a Hierarchical World Model (H-WM) that jointly predicts logical and visual state transitions within a unified bilevel framework. H-WM combines a high-level logical world model with a low-level visual world model, integrating the robot-executable, long-horizon robustness of symbolic reasoning with perceptual grounding from visual observations. The hierarchical outputs provide stable and consistent intermediate guidance for long-horizon tasks, mitigating error accumulation and enabling robust execution across extended task sequences. To train H-WM, we introduce a robotic dataset that aligns robot motion with symbolic states, actions, and visual observations. Experiments across vision-language-action (VLA) control policies demonstrate the effectiveness and generality of the approach.",
        "authors_display": "Yingxue Zhang Team",
        "pdf_url": "http://arxiv.org/abs/2602.11291",
        "code_url": null,
        "date": "2026-02-11",
        "primary_category": "cs.RO",
        "chinese_summary": "针对现有世界模型在长时程机器人规划中累积误差的问题，以及传统符号逻辑世界模型缺乏视觉感知接地的局限性，本研究提出分层世界模型（H-WM）。H-WM在一个统一的双层框架内联合预测逻辑和视觉状态转换，将符号推理的鲁棒性与视觉观察的感知基础相结合。为训练H-WM，研究引入了一个对齐机器人运动与符号状态、动作和视觉观察的机器人数据集。实验证明，分层输出为长时程任务提供了稳定一致的中间指导，有效减轻了误差积累，并在VLA控制策略上展示了该方法的有效性和通用性。"
      },
      {
        "paper_id": "2602.11075",
        "title": "RISE: Self-Improving Robot Policy with Compositional World Model",
        "abstract": "Despite the sustained scaling on model capacity and data acquisition, Vision-Language-Action (VLA) models remain brittle in contact-rich and dynamic manipulation tasks, where minor execution deviations can compound into failures. While reinforcement learning (RL) offers a principled path to robustness, on-policy RL in the physical world is constrained by safety risk, hardware cost, and environment reset. To bridge this gap, we present RISE, a scalable framework of robotic reinforcement learning via imagination. At its core is a Compositional World Model that (i) predicts multi-view future via a controllable dynamics model, and (ii) evaluates imagined outcomes with a progress value model, producing informative advantages for the policy improvement. Such compositional design allows state and value to be tailored by best-suited yet distinct architectures and objectives. These components are integrated into a closed-loop self-improving pipeline that continuously generates imaginary rollouts, estimates advantages, and updates the policy in imaginary space without costly physical interaction. Across three challenging real-world tasks, RISE yields significant improvement over prior art, with more than +35% absolute performance increase in dynamic brick sorting, +45% for backpack packing, and +35% for box closing, respectively.",
        "authors_display": "Hongyang Li Team",
        "pdf_url": "http://arxiv.org/abs/2602.11075",
        "code_url": "https://opendrivelab.com/kai0-rl/",
        "date": "2026-02-11",
        "primary_category": "cs.RO",
        "chinese_summary": "针对视觉-语言-动作（VLA）模型在接触密集和动态操作任务中易受执行偏差影响的脆弱性，以及物理世界中在线强化学习（RL）的限制，本研究提出了RISE，一个通过想象进行机器人强化学习的可扩展框架。其核心是一个组合世界模型，该模型能预测多视角未来并通过进度价值模型评估想象结果，从而为策略改进提供信息丰富的优势。这些组件被整合到一个闭环自改进流水线中，在想象空间中持续生成试错并更新策略。在三个真实世界任务中，RISE相对于现有技术取得了显著性能提升，如在动态砖块分类、背包包装和盒子关闭任务中，绝对性能分别提升超过35%、45%和35%。"
      },
      {
        "paper_id": "2602.10980",
        "title": "RADAR: Benchmarking Vision-Language-Action Generalization via Real-World Dynamics, Spatial-Physical Intelligence, and Autonomous Evaluation",
        "abstract": "VLA models have achieved remarkable progress in embodied intelligence; however, their evaluation remains largely confined to simulations or highly constrained real-world settings. This mismatch creates a substantial reality gap, where strong benchmark performance often masks poor generalization in diverse physical environments. We identify three systemic shortcomings in current benchmarking practices that hinder fair and reliable model comparison. (1) Existing benchmarks fail to model real-world dynamics, overlooking critical factors such as dynamic object configurations, robot initial states, lighting changes, and sensor noise. (2) Current protocols neglect spatial--physical intelligence, reducing evaluation to rote manipulation tasks that do not probe geometric reasoning. (3) The field lacks scalable fully autonomous evaluation, instead relying on simplistic 2D metrics that miss 3D spatial structure or on human-in-the-loop systems that are costly, biased, and unscalable. To address these limitations, we introduce RADAR (Real-world Autonomous Dynamics And Reasoning), a benchmark designed to systematically evaluate VLA generalization under realistic conditions. RADAR integrates three core components: (1) a principled suite of physical dynamics; (2) dedicated tasks that explicitly test spatial reasoning and physical understanding; and (3) a fully autonomous evaluation pipeline based on 3D metrics, eliminating the need for human supervision. We apply RADAR to audit multiple state-of-the-art VLA models and uncover severe fragility beneath their apparent competence. Performance drops precipitously under modest physical dynamics, with the expectation of 3D IoU declining from 0.261 to 0.068 under sensor noise. Moreover, models exhibit limited spatial reasoning capability. These findings position RADAR as a necessary bench toward reliable and generalizable real-world evaluation of VLA models.",
        "authors_display": "Guangrun Wang Team",
        "pdf_url": "http://arxiv.org/abs/2602.10980",
        "code_url": null,
        "date": "2026-02-11",
        "primary_category": "cs.RO",
        "chinese_summary": "针对现有视觉-语言-动作（VLA）模型评估主要局限于仿真或高度受限的真实世界，导致现实差距大、泛化能力差的问题，本研究提出RADAR（Real-world Autonomous Dynamics And Reasoning）基准。RADAR旨在系统评估VLA在真实条件下的泛化能力，集成了物理动力学套件、专门测试空间推理和物理理解的任务，以及基于3D指标的全自主评估流程。通过RADAR对多个先进VLA模型进行审计，发现模型在适度物理动态下性能急剧下降，例如在传感器噪声下3D IoU从0.261下降到0.068，且空间推理能力有限，揭示了模型在真实世界条件下的严重脆弱性，强调了RADAR作为可靠泛化评估基准的必要性。"
      },
      {
        "paper_id": "2602.10719",
        "title": "From Representational Complementarity to Dual Systems: Synergizing VLM and Vision-Only Backbones for End-to-End Driving",
        "abstract": "Vision-Language-Action (VLA) driving augments end-to-end (E2E) planning with language-enabled backbones, yet it remains unclear what changes beyond the usual accuracy--cost trade-off. We revisit this question with 3--RQ analysis in RecogDrive by instantiating the system with a full VLM and vision-only backbones, all under an identical diffusion Transformer planner. RQ1: At the backbone level, the VLM can introduce additional subspaces upon the vision-only backbones. RQ2: This unique subspace leads to a different behavioral in some long-tail scenario: the VLM tends to be more aggressive whereas ViT is more conservative, and each decisively wins on about 2--3% of test scenarios; With an oracle that selects, per scenario, the better trajectory between the VLM and ViT branches, we obtain an upper bound of 93.58 PDMS. RQ3: To fully harness this observation, we propose HybridDriveVLA, which runs both ViT and VLM branches and selects between their endpoint trajectories using a learned scorer, improving PDMS to 92.10. Finally, DualDriveVLA implements a practical fast--slow policy: it runs ViT by default and invokes the VLM only when the scorer's confidence falls below a threshold; calling the VLM on 15% of scenarios achieves 91.00 PDMS while improving throughput by 3.2x. Code will be released.",
        "authors_display": "Yan Wang Team",
        "pdf_url": "http://arxiv.org/abs/2602.10719",
        "code_url": null,
        "date": "2026-02-11",
        "primary_category": "cs.RO",
        "chinese_summary": "VLA驾驶将语言功能引入端到端规划，但其核心变化除准确性-成本权衡外尚不明确。本文通过RecogDrive进行3-RQ分析，比较VLM和纯视觉骨干网络在相同规划器下的表现。研究发现VLM能引入独特子空间，导致在长尾场景下行为差异，VLM更激进而ViT更保守。基于此，本文提出HybridDriveVLA，通过学习评分器融合两者的轨迹，将PDMS提升至92.10。进一步，DualDriveVLA实现了快慢策略，仅在低置信度时调用VLM，在15%场景中调用VLM即可达到91.00 PDMS，并使吞吐量提高3.2倍。"
      },
      {
        "paper_id": "2602.10717",
        "title": "Say, Dream, and Act: Learning Video World Models for Instruction-Driven Robot Manipulation",
        "abstract": "Robotic manipulation requires anticipating how the environment evolves in response to actions, yet most existing systems lack this predictive capability, often resulting in errors and inefficiency. While Vision-Language Models (VLMs) provide high-level guidance, they cannot explicitly forecast future states, and existing world models either predict only short horizons or produce spatially inconsistent frames. To address these challenges, we propose a framework for fast and predictive video-conditioned action. Our approach first selects and adapts a robust video generation model to ensure reliable future predictions, then applies adversarial distillation for fast, few-step video generation, and finally trains an action model that leverages both generated videos and real observations to correct spatial errors. Extensive experiments show that our method produces temporally coherent, spatially accurate video predictions that directly support precise manipulation, achieving significant improvements in embodiment consistency, spatial referring ability, and task completion over existing baselines. Codes & Models will be released.",
        "authors_display": "Yanwei Fu Team",
        "pdf_url": "http://arxiv.org/abs/2602.10717",
        "code_url": null,
        "date": "2026-02-11",
        "primary_category": "cs.RO",
        "chinese_summary": "机器人操作需要预测环境对动作的响应，但现有系统常缺乏此能力，导致效率低下。鉴于VLMs无法明确预测未来状态且现有世界模型存在预测短期或空间不一致帧的问题，本文提出了一种快速且预测性的视频条件动作框架。该方法首先适配鲁棒的视频生成模型以确保未来预测的可靠性，接着通过对抗蒸馏实现快速视频生成，最后训练一个动作模型利用生成视频和真实观测纠正空间错误。实验结果表明，该方法生成的视频预测在时间连贯性和空间准确性方面表现优异，显著提升了具身一致性、空间指代能力和任务完成度。"
      },
      {
        "paper_id": "2602.10698",
        "title": "AugVLA-3D: Depth-Driven Feature Augmentation for Vision-Language-Action Models",
        "abstract": "Vision-Language-Action (VLA) models have recently achieved remarkable progress in robotic perception and control, yet most existing approaches primarily rely on VLM trained using 2D images, which limits their spatial understanding and action grounding in complex 3D environments. To address this limitation, we propose a novel framework that integrates depth estimation into VLA models to enrich 3D feature representations. Specifically, we employ a depth estimation baseline called VGGT to extract geometry-aware 3D cues from standard RGB inputs, enabling efficient utilization of existing large-scale 2D datasets while implicitly recovering 3D structural information. To further enhance the reliability of these depth-derived features, we introduce a new module called action assistant, which constrains the learned 3D representations with action priors and ensures their consistency with downstream control tasks. By fusing the enhanced 3D features with conventional 2D visual tokens, our approach significantly improves the generalization ability and robustness of VLA models. Experimental results demonstrate that the proposed method not only strengthens perception in geometrically ambiguous scenarios but also leads to superior action prediction accuracy. This work highlights the potential of depth-driven data augmentation and auxiliary expert supervision for bridging the gap between 2D observations and 3D-aware decision-making in robotic systems.",
        "authors_display": "F. Richard Yu Team",
        "pdf_url": "http://arxiv.org/abs/2602.10698",
        "code_url": null,
        "date": "2026-02-11",
        "primary_category": "cs.CV",
        "chinese_summary": "当前VLA模型主要依赖2D图像训练，限制了其在复杂3D环境中的空间理解和动作落地。为解决此问题，本文提出了一个将深度估计集成到VLA模型中的新框架，以丰富3D特征表示。该方法利用VGGT从RGB输入中提取3D几何线索，并引入“动作助手”模块，通过动作先验约束3D表示以确保与下游控制任务的一致性。实验结果表明，所提出的方法不仅增强了几何模糊场景中的感知，还显著提高了动作预测准确性，强调了深度驱动数据增强对弥合2D观测与3D决策差距的潜力。"
      },
      {
        "paper_id": "2602.10619",
        "title": "Improving Medical Visual Reinforcement Fine-Tuning via Perception and Reasoning Augmentation",
        "abstract": "While recent advances in Reinforcement Fine-Tuning (RFT) have shown that rule-based reward schemes can enable effective post-training for large language models, their extension to cross-modal, vision-centric domains remains largely underexplored. This limitation is especially pronounced in the medical imaging domain, where effective performance requires both robust visual perception and structured reasoning. In this work, we address this gap by proposing VRFT-Aug, a visual reinforcement fine-tuning framework tailored for the medical domain. VRFT-Aug introduces a series of training strategies designed to augment both perception and reasoning, including prior knowledge injection, perception-driven policy refinement, medically informed reward shaping, and behavioral imitation. Together, these methods aim to stabilize and improve the RFT process.   Through extensive experiments across multiple medical datasets, we show that our approaches consistently outperform both standard supervised fine-tuning and RFT baselines. Moreover, we provide empirically grounded insights and practical training heuristics that can be generalized to other medical image tasks. We hope this work contributes actionable guidance and fresh inspiration for the ongoing effort to develop reliable, reasoning-capable models for high-stakes medical applications.",
        "authors_display": "Qicheng Lao Team",
        "pdf_url": "http://arxiv.org/abs/2602.10619",
        "code_url": null,
        "date": "2026-02-11",
        "primary_category": "cs.CV",
        "chinese_summary": "强化微调（RFT）在语言模型后训练中展现潜力，但在跨模态、以视觉为中心的医学影像领域应用不足，而该领域需要强大的视觉感知和结构化推理。为此，本文提出了VRFT-Aug，一个专为医学领域设计的视觉强化微调框架。VRFT-Aug通过引入先验知识注入、感知驱动策略优化、医学知情奖励塑造和行为模仿等训练策略来增强感知和推理。实验结果表明，该方法在多个医学数据集上持续优于标准监督微调和RFT基线，并提供了可推广到其他医学图像任务的实用训练启发。"
      },
      {
        "paper_id": "2602.10556",
        "title": "LAP: Language-Action Pre-Training Enables Zero-shot Cross-Embodiment Transfer",
        "abstract": "A long-standing goal in robotics is a generalist policy that can be deployed zero-shot on new robot embodiments without per-embodiment adaptation. Despite large-scale multi-embodiment pre-training, existing Vision-Language-Action models (VLAs) remain tightly coupled to their training embodiments and typically require costly fine-tuning. We introduce Language-Action Pre-training (LAP), a simple recipe that represents low-level robot actions directly in natural language, aligning action supervision with the pre-trained vision-language model's input-output distribution. LAP requires no learned tokenizer, no costly annotation, and no embodiment-specific architectural design. Based on LAP, we present LAP-3B, which to the best of our knowledge is the first VLA to achieve substantial zero-shot transfer to previously unseen robot embodiments without any embodiment-specific fine-tuning. Across multiple novel robots and manipulation tasks, LAP-3B attains over 50% average zero-shot success, delivering roughly a 2x improvement over the strongest prior VLAs. We further show that LAP enables efficient adaptation and favorable scaling, while unifying action prediction and VQA in a shared language-action format that yields additional gains through co-training.",
        "authors_display": "Anirudha Majumdar Team",
        "pdf_url": "http://arxiv.org/abs/2602.10556",
        "code_url": "https://lap-vla.github.io",
        "date": "2026-02-11",
        "primary_category": "cs.RO",
        "chinese_summary": "机器人学长期目标是实现零样本部署的通用策略，但现有VLA模型与训练实体紧密耦合。针对此问题，本文提出了语言-动作预训练（LAP），一种将低级机器人动作直接用自然语言表示的方法，使动作监督与预训练VLM的输入-输出分布对齐，无需定制化设计或昂贵标注。基于LAP，本文提出了LAP-3B，实现了在未见机器人上的显著零样本迁移，平均成功率超过50%，比现有VLA模型提升约2倍。此外，LAP还支持高效适应、有利扩展，并通过统一的语言-动作格式在协同训练中获得额外收益。"
      },
      {
        "paper_id": "2602.10458",
        "title": "Found-RL: foundation model-enhanced reinforcement learning for autonomous driving",
        "abstract": "Reinforcement Learning (RL) has emerged as a dominant paradigm for end-to-end autonomous driving (AD). However, RL suffers from sample inefficiency and a lack of semantic interpretability in complex scenarios. Foundation Models, particularly Vision-Language Models (VLMs), can mitigate this by offering rich, context-aware knowledge, yet their high inference latency hinders deployment in high-frequency RL training loops. To bridge this gap, we present Found-RL, a platform tailored to efficiently enhance RL for AD using foundation models. A core innovation is the asynchronous batch inference framework, which decouples heavy VLM reasoning from the simulation loop, effectively resolving latency bottlenecks to support real-time learning. We introduce diverse supervision mechanisms: Value-Margin Regularization (VMR) and Advantage-Weighted Action Guidance (AWAG) to effectively distill expert-like VLM action suggestions into the RL policy. Additionally, we adopt high-throughput CLIP for dense reward shaping. We address CLIP's dynamic blindness via Conditional Contrastive Action Alignment, which conditions prompts on discretized speed/command and yields a normalized, margin-based bonus from context-specific action-anchor scoring. Found-RL provides an end-to-end pipeline for fine-tuned VLM integration and shows that a lightweight RL model can achieve near-VLM performance compared with billion-parameter VLMs while sustaining real-time inference (approx. 500 FPS). Code, data, and models will be publicly available at https://github.com/ys-qu/found-rl.",
        "authors_display": "Sikai Chen Team",
        "pdf_url": "http://arxiv.org/abs/2602.10458",
        "code_url": null,
        "date": "2026-02-11",
        "primary_category": "cs.AI",
        "chinese_summary": "强化学习在自动驾驶中面临样本效率低和语义可解释性不足问题，而基础模型（特别是VLM）虽能提供丰富知识，但高推理延迟阻碍其在RL训练中的实时部署。为此，本文提出了Found-RL平台，通过异步批量推理框架将VLM推理与仿真循环解耦，解决了延迟瓶颈。该平台引入了价值边际正则化和优势加权动作指导等监督机制，将VLM建议有效提炼到RL策略中。此外，通过条件对比动作对齐解决CLIP的动态盲区问题，实现密集奖励塑造。Found-RL使得轻量级RL模型在保持实时推理（500 FPS）的同时，达到接近十亿参数VLM的性能。"
      },
      {
        "paper_id": "2602.10377",
        "title": "Hardware Co-Design Scaling Laws via Roofline Modelling for On-Device LLMs",
        "abstract": "Vision-Language-Action Models (VLAs) have emerged as a key paradigm of Physical AI and are increasingly deployed in autonomous vehicles, robots, and smart spaces. In these resource-constrained on-device settings, selecting an appropriate large language model (LLM) backbone is a critical challenge: models must balance accuracy with strict inference latency and hardware efficiency constraints. This makes hardware-software co-design a game-changing requirement for on-device LLM deployment, where each hardware platform demands a tailored architectural solution. We propose a hardware co-design law that jointly captures model accuracy and inference performance. Specifically, we model training loss as an explicit function of architectural hyperparameters and characterise inference latency via roofline modelling. We empirically evaluate 1,942 candidate architectures on NVIDIA Jetson Orin, training 170 selected models for 10B tokens each to fit a scaling law relating architecture to training loss. By coupling this scaling law with latency modelling, we establish a direct accuracy-latency correspondence and identify the Pareto frontier for hardware co-designed LLMs. We further formulate architecture search as a joint optimisation over precision and performance, deriving feasible design regions under industrial hardware and application budgets. Our approach reduces architecture selection from months to days. At the same latency as Qwen2.5-0.5B on the target hardware, our co-designed architecture achieves 19.42% lower perplexity on WikiText-2. To our knowledge, this is the first principled and operational framework for hardware co-design scaling laws in on-device LLM deployment. We will make the code and related checkpoints publicly available.",
        "authors_display": "Cheng Deng Team",
        "pdf_url": "http://arxiv.org/abs/2602.10377",
        "code_url": null,
        "date": "2026-02-10",
        "primary_category": "cs.LG",
        "chinese_summary": "VLA模型在资源受限设备部署中面临LLM骨干网络选择挑战，需平衡准确性与推理延迟及硬件效率。本文提出了硬件协同设计法则，将模型训练损失建模为架构超参数的函数，并通过屋脊线模型表征推理延迟，从而建立了准确性-延迟的直接对应关系。通过对1,942个候选架构进行经验评估和训练，拟合出架构与训练损失的缩放定律。将该定律与延迟建模结合，本文确定了硬件协同设计LLM的帕累托前沿，并将架构搜索公式化为精度与性能的联合优化。结果显示，该方法将架构选择时间从数月缩短到数天，并在相同延迟下，其协同设计的架构在WikiText-2上的困惑度降低了19.42%。"
      },
      {
        "paper_id": "2602.10109",
        "title": "ST4VLA: Spatially Guided Training for Vision-Language-Action Models",
        "abstract": "Large vision-language models (VLMs) excel at multimodal understanding but fall short when extended to embodied tasks, where instructions must be transformed into low-level motor actions. We introduce ST4VLA, a dual-system Vision-Language-Action framework that leverages Spatial Guided Training to align action learning with spatial priors in VLMs. ST4VLA includes two stages: (i) spatial grounding pre-training, which equips the VLM with transferable priors via scalable point, box, and trajectory prediction from both web-scale and robot-specific data, and (ii) spatially guided action post-training, which encourages the model to produce richer spatial priors to guide action generation via spatial prompting. This design preserves spatial grounding during policy learning and promotes consistent optimization across spatial and action objectives. Empirically, ST4VLA achieves substantial improvements over vanilla VLA, with performance increasing from 66.1 -> 84.6 on Google Robot and from 54.7 -> 73.2 on WidowX Robot, establishing new state-of-the-art results on SimplerEnv. It also demonstrates stronger generalization to unseen objects and paraphrased instructions, as well as robustness to long-horizon perturbations in real-world settings. These results highlight scalable spatially guided training as a promising direction for robust, generalizable robot learning. Source code, data and models are released at https://internrobotics.github.io/internvla-m1.github.io/",
        "authors_display": "Jiangmiao Pang Team",
        "pdf_url": "http://arxiv.org/abs/2602.10109",
        "code_url": null,
        "date": "2026-02-10",
        "primary_category": "cs.RO",
        "chinese_summary": "大型VLM虽擅长多模态理解，但在具身任务中将指令转化为低级运动动作时表现不足。为此，本文引入了ST4VLA，一个双系统VLA框架，通过空间引导训练使动作学习与VLM中的空间先验对齐。ST4VLA包含两阶段：首先是空间基础预训练，利用大规模和机器人数据通过点、框、轨迹预测为VLM注入可迁移先验；其次是空间引导动作后训练，通过空间提示促使模型生成更丰富的空间先验以指导动作生成。实验表明，ST4VLA显著提升了Google Robot和WidowX Robot的性能，在SimplerEnv上创造了SOTA，并展现出对未见物体、转述指令的更强泛化能力及在真实世界中对扰动的鲁棒性。"
      },
      {
        "paper_id": "2602.10106",
        "title": "EgoHumanoid: Unlocking In-the-Wild Loco-Manipulation with Robot-Free Egocentric Demonstration",
        "abstract": "Human demonstrations offer rich environmental diversity and scale naturally, making them an appealing alternative to robot teleoperation. While this paradigm has advanced robot-arm manipulation, its potential for the more challenging, data-hungry problem of humanoid loco-manipulation remains largely unexplored. We present EgoHumanoid, the first framework to co-train a vision-language-action policy using abundant egocentric human demonstrations together with a limited amount of robot data, enabling humanoids to perform loco-manipulation across diverse real-world environments. To bridge the embodiment gap between humans and robots, including discrepancies in physical morphology and viewpoint, we introduce a systematic alignment pipeline spanning from hardware design to data processing. A portable system for scalable human data collection is developed, and we establish practical collection protocols to improve transferability. At the core of our human-to-humanoid alignment pipeline lies two key components. The view alignment reduces visual domain discrepancies caused by camera height and perspective variation. The action alignment maps human motions into a unified, kinematically feasible action space for humanoid control. Extensive real-world experiments demonstrate that incorporating robot-free egocentric data significantly outperforms robot-only baselines by 51\\%, particularly in unseen environments. Our analysis further reveals which behaviors transfer effectively and the potential for scaling human data.",
        "authors_display": "Li Chen Team",
        "pdf_url": "http://arxiv.org/abs/2602.10106",
        "code_url": "https://opendrivelab.com/EgoHumanoid",
        "date": "2026-02-10",
        "primary_category": "cs.RO",
        "chinese_summary": "人形机器人运动-操作任务面临数据需求高和具身差距挑战。针对此，本文提出了EgoHumanoid框架，首次利用大量以自我为中心的人类演示数据与有限机器人数据共同训练视觉-语言-动作策略，使人形机器人在多样真实环境中执行运动-操作。为弥合人类与机器人间的形态和视点差异，本文引入了包含硬件设计、数据处理的系统对齐流程，并开发了便携式人类数据收集系统。其核心是视点对齐和动作对齐，分别减少视觉域差异和将人类动作映射到机器人动作空间。真实世界实验表明，整合人类演示数据显著优于仅用机器人数据的基线（51%），尤其是在未见环境中。"
      }
    ],
    "Humanoid": [
      {
        "paper_id": "2602.23205",
        "title": "EmbodMocap: In-the-Wild 4D Human-Scene Reconstruction for Embodied Agents",
        "abstract": "Human behaviors in the real world naturally encode rich, long-term contextual information that can be leveraged to train embodied agents for perception, understanding, and acting. However, existing capture systems typically rely on costly studio setups and wearable devices, limiting the large-scale collection of scene-conditioned human motion data in the wild. To address this, we propose EmbodMocap, a portable and affordable data collection pipeline using two moving iPhones. Our key idea is to jointly calibrate dual RGB-D sequences to reconstruct both humans and scenes within a unified metric world coordinate frame. The proposed method allows metric-scale and scene-consistent capture in everyday environments without static cameras or markers, bridging human motion and scene geometry seamlessly. Compared with optical capture ground truth, we demonstrate that the dual-view setting exhibits a remarkable ability to mitigate depth ambiguity, achieving superior alignment and reconstruction performance over single iphone or monocular models. Based on the collected data, we empower three embodied AI tasks: monocular human-scene-reconstruction, where we fine-tune on feedforward models that output metric-scale, world-space aligned humans and scenes; physics-based character animation, where we prove our data could be used to scale human-object interaction skills and scene-aware motion tracking; and robot motion control, where we train a humanoid robot via sim-to-real RL to replicate human motions depicted in videos. Experimental results validate the effectiveness of our pipeline and its contributions towards advancing embodied AI research.",
        "authors_display": "Taku Komura Team",
        "pdf_url": "http://arxiv.org/abs/2602.23205",
        "code_url": null,
        "date": "2026-02-26",
        "primary_category": "cs.CV",
        "chinese_summary": "针对现有动作捕捉系统成本高昂且限制了野外场景化人体运动数据大规模采集的问题，本研究提出EmbodiMocap，一个基于两部移动iPhone的便携经济型数据采集系统。该方法通过联合校准双RGB-D序列，在统一度量世界坐标系中重建人体和场景，实现了在日常环境中度量尺度和场景一致的捕捉。实验结果表明，双视角设置有效缓解了深度模糊，重建性能优于单iPhone或单目模型，并成功支持了单目人-场景重建、基于物理的角色动画和机器人运动控制等具身AI任务。"
      },
      {
        "paper_id": "2602.21983",
        "title": "Humanizing Robot Gaze Shifts: A Framework for Natural Gaze Shifts in Humanoid Robots",
        "abstract": "Leveraging auditory and visual feedback for attention reorientation is essential for natural gaze shifts in social interaction. However, enabling humanoid robots to perform natural and context-appropriate gaze shifts in unconstrained human--robot interaction (HRI) remains challenging, as it requires the coupling of cognitive attention mechanisms and biomimetic motion generation. In this work, we propose the Robot Gaze-Shift (RGS) framework, which integrates these two components into a unified pipeline. First, RGS employs a vision--language model (VLM)-based gaze reasoning pipeline to infer context-appropriate gaze targets from multimodal interaction cues, ensuring consistency with human gaze-orienting regularities. Second, RGS introduces a conditional Vector Quantized-Variational Autoencoder (VQ-VAE) model for eye--head coordinated gaze-shift motion generation, producing diverse and human-like gaze-shift behaviors. Experiments validate that RGS effectively replicates human-like target selection and generates realistic, diverse gaze-shift motions.",
        "authors_display": "Zhouping Yin Team",
        "pdf_url": "http://arxiv.org/abs/2602.21983",
        "code_url": null,
        "date": "2026-02-25",
        "primary_category": "cs.RO",
        "chinese_summary": "鉴于仿人机器人在不受约束的人机交互中实现自然、符合语境的凝视转移仍面临挑战，本研究提出了Robot Gaze-Shift (RGS) 框架，将认知注意力机制与仿生运动生成统一整合。RGS首先利用基于视觉-语言模型(VLM)的凝视推理管线，从多模态交互线索推断凝视目标；随后引入条件变分矢量量化自编码器(VQ-VAE)模型生成多样化且类人化的眼-头协调凝视转移运动。实验验证RGS能有效复制类人目标选择，并生成逼真、多样化的凝视转移动作。"
      },
      {
        "paper_id": "2602.21723",
        "title": "LessMimic: Long-Horizon Humanoid Interaction with Unified Distance Field Representations",
        "abstract": "Humanoid robots that autonomously interact with physical environments over extended horizons represent a central goal of embodied intelligence. Existing approaches rely on reference motions or task-specific rewards, tightly coupling policies to particular object geometries and precluding multi-skill generalization within a single framework. A unified interaction representation enabling reference-free inference, geometric generalization, and long-horizon skill composition within one policy remains an open challenge. Here we show that Distance Field (DF) provides such a representation: LessMimic conditions a single whole-body policy on DF-derived geometric cues--surface distances, gradients, and velocity decompositions--removing the need for motion references, with interaction latents encoded via a Variational Auto-Encoder (VAE) and post-trained using Adversarial Interaction Priors (AIP) under Reinforcement Learning (RL). Through DAgger-style distillation that aligns DF latents with egocentric depth features, LessMimic further transfers seamlessly to vision-only deployment without motion capture (MoCap) infrastructure. A single LessMimic policy achieves 80--100% success across object scales from 0.4x to 1.6x on PickUp and SitStand where baselines degrade sharply, attains 62.1% success on 5 task instances trajectories, and remains viable up to 40 sequentially composed tasks. By grounding interaction in local geometry rather than demonstrations, LessMimic offers a scalable path toward humanoid robots that generalize, compose skills, and recover from failures in unstructured environments.",
        "authors_display": "Siyuan Huang Team",
        "pdf_url": "http://arxiv.org/abs/2602.21723",
        "code_url": null,
        "date": "2026-02-25",
        "primary_category": "cs.RO",
        "chinese_summary": "为解决仿人机器人在长期自主交互中多技能泛化能力差、策略与特定物体几何形状紧密耦合的挑战，本研究提出了LessMimic方法。该方法以距离场（DF）作为统一交互表示，使单一全身策略能够基于DF派生的几何线索进行无参考推理，并通过VAE编码交互潜在变量，在强化学习下利用对抗性交互先验进行后训练。LessMimic通过DAgger式蒸馏实现了纯视觉部署。实验证明，LessMimic策略在不同物体尺度下成功率高达80-100%，并在多任务组合和长时程任务中表现出色，为仿人机器人实现通用化、可组合和故障恢复提供了新途径。"
      },
      {
        "paper_id": "2602.21666",
        "title": "Biomechanical Comparisons Reveal Divergence of Human and Humanoid Gaits",
        "abstract": "It remains challenging to achieve human-like locomotion in legged robots due to fundamental discrepancies between biological and mechanical structures. Although imitation learning has emerged as a promising approach for generating natural robotic movements, simply replicating joint angle trajectories fails to capture the underlying principles of human motion. This study proposes a Gait Divergence Analysis Framework (GDAF), a unified biomechanical evaluation framework that systematically quantifies kinematic and kinetic discrepancies between humans and bipedal robots. We apply GDAF to systematically compare human and humanoid locomotion across 28 walking speeds. To enable reproducible analysis, we collect and release a speed-continuous humanoid locomotion dataset from a state-of-the-art humanoid controller. We further provide an open-source implementation of GDAF, including analysis, visualization, and MuJoCo-based tools, enabling quantitative, interpretable, and reproducible biomechanical analysis of humanoid locomotion. Results demonstrate that despite visually human-like motion generated by modern humanoid controllers, significant biomechanical divergence persists across speeds. Robots exhibit systematic deviations in gait symmetry, energy distribution, and joint coordination, indicating that substantial room remains for improving the biomechanical fidelity and energetic efficiency of humanoid locomotion. This work provides a quantitative benchmark for evaluating humanoid locomotion and offers data and versatile tools to support the development of more human-like and energetically efficient locomotion controllers. The data and code will be made publicly available upon acceptance of the paper.",
        "authors_display": "Wei Chen Team",
        "pdf_url": "http://arxiv.org/abs/2602.21666",
        "code_url": null,
        "date": "2026-02-25",
        "primary_category": "cs.RO",
        "chinese_summary": "鉴于腿式机器人难以实现类人行走，且模仿学习未能完全捕捉人类运动的生物力学原理，本研究提出了步态差异分析框架（GDAF）。GDAF是一个统一的生物力学评估框架，用于系统量化人类和双足机器人在运动学和动力学上的差异。研究团队将GDAF应用于比较人类与仿人机器人在28种行走速度下的步态，并发布了速度连续的仿人行走数据集与开源GDAF实现。结果显示，尽管现代仿人控制器视觉上呈现类人运动，但在步态对称性、能量分布和关节协调性方面仍存在显著差异，表明仿人步态的生物力学保真度和能效仍有较大提升空间。"
      },
      {
        "paper_id": "2602.21599",
        "title": "Iterative Closed-Loop Motion Synthesis for Scaling the Capabilities of Humanoid Control",
        "abstract": "Physics-based humanoid control relies on training with motion datasets that have diverse data distributions. However, the fixed difficulty distribution of datasets limits the performance ceiling of the trained control policies. Additionally, the method of acquiring high-quality data through professional motion capture systems is constrained by costs, making it difficult to achieve large-scale scalability. To address these issues, we propose a closed-loop automated motion data generation and iterative framework. It can generate high-quality motion data with rich action semantics, including martial arts, dance, combat, sports, gymnastics, and more. Furthermore, our framework enables difficulty iteration of policies and data through physical metrics and objective evaluations, allowing the trained tracker to break through its original difficulty limits. On the PHC single-primitive tracker, using only approximately 1/10 of the AMASS dataset size, the average failure rate on the test set (2201 clips) is reduced by 45\\% compared to the baseline. Finally, we conduct comprehensive ablation and comparative experiments to highlight the rationality and advantages of our framework.",
        "authors_display": "Renjing Xu Team",
        "pdf_url": "http://arxiv.org/abs/2602.21599",
        "code_url": null,
        "date": "2026-02-25",
        "primary_category": "cs.RO",
        "chinese_summary": "针对基于物理的仿人控制中运动数据集难度分布固定且高质量数据采集成本高昂的问题，本研究提出一个闭环自动化运动数据生成与迭代框架。该框架能够生成包括武术、舞蹈、战斗等丰富动作语义的高质量运动数据，并通过物理指标和客观评估实现策略和数据的难度迭代，从而突破训练追踪器的原有难度限制。在PHC单原语追踪器上的实验结果表明，仅使用约AMASS数据集1/10的数据量，测试集失败率比基线降低了45%，验证了该框架的合理性和优势。"
      },
      {
        "paper_id": "2602.16705",
        "title": "Learning Humanoid End-Effector Control for Open-Vocabulary Visual Loco-Manipulation",
        "abstract": "Visual loco-manipulation of arbitrary objects in the wild with humanoid robots requires accurate end-effector (EE) control and a generalizable understanding of the scene via visual inputs (e.g., RGB-D images). Existing approaches are based on real-world imitation learning and exhibit limited generalization due to the difficulty in collecting large-scale training datasets. This paper presents a new paradigm, HERO, for object loco-manipulation with humanoid robots that combines the strong generalization and open-vocabulary understanding of large vision models with strong control performance from simulated training. We achieve this by designing an accurate residual-aware EE tracking policy. This EE tracking policy combines classical robotics with machine learning. It uses a) inverse kinematics to convert residual end-effector targets into reference trajectories, b) a learned neural forward model for accurate forward kinematics, c) goal adjustment, and d) replanning. Together, these innovations help us cut down the end-effector tracking error by 3.2x. We use this accurate end-effector tracker to build a modular system for loco-manipulation, where we use open-vocabulary large vision models for strong visual generalization. Our system is able to operate in diverse real-world environments, from offices to coffee shops, where the robot is able to reliably manipulate various everyday objects (e.g., mugs, apples, toys) on surfaces ranging from 43cm to 92cm in height. Systematic modular and end-to-end tests in simulation and the real world demonstrate the effectiveness of our proposed design. We believe the advances in this paper can open up new ways of training humanoid robots to interact with daily objects.",
        "authors_display": "Saurabh Gupta Team",
        "pdf_url": "http://arxiv.org/abs/2602.16705",
        "code_url": "https://hero-humanoid.github.io/",
        "date": "2026-02-24",
        "primary_category": "cs.RO",
        "chinese_summary": "人形机器人在复杂真实环境中对任意物体进行视觉局部操作面临末端执行器（EE）控制精度和场景泛化理解的挑战，现有模仿学习方法因数据限制而泛化能力有限。本文提出HERO范式，通过结合大型视觉模型（LVMs）的开放词汇理解与模拟训练的控制性能。核心是设计了一个残差感知的EE跟踪策略，整合了逆运动学、神经正向模型、目标调整和重新规划。实验结果显示，该策略将EE跟踪误差降低了3.2倍，使系统能在多样真实场景中可靠操作多种日常物体，证明了其在视觉泛化和控制性能上的有效性。"
      },
      {
        "paper_id": "2602.12656",
        "title": "PMG: Parameterized Motion Generator for Human-like Locomotion Control",
        "abstract": "Recent advances in data-driven reinforcement learning and motion tracking have substantially improved humanoid locomotion, yet critical practical challenges remain. In particular, while low-level motion tracking and trajectory-following controllers are mature, whole-body reference-guided methods are difficult to adapt to higher-level command interfaces and diverse task contexts: they require large, high-quality datasets, are brittle across speed and pose regimes, and are sensitive to robot-specific calibration. To address these limitations, we propose the Parameterized Motion Generator (PMG), a real-time motion generator grounded in an analysis of human motion structure that synthesizes reference trajectories using only a compact set of parameterized motion data together with high-dimensional control commands. Combined with an imitation-learning pipeline and an optimization-based sim-to-real motor parameter identification module, we validate the complete approach on our humanoid prototype ZERITH Z1 and show that, within a single integrated system, PMG produces natural, human-like locomotion, responds precisely to high-dimensional control inputs-including VR-based teleoperation-and enables efficient, verifiable sim-to-real transfer. Together, these results establish a practical, experimentally validated pathway toward natural and deployable humanoid control. Website: https://pmg-icra26.github.io/",
        "authors_display": "Houde Liu Team",
        "pdf_url": "http://arxiv.org/abs/2602.12656",
        "code_url": "https://pmg-icra26.github.io/",
        "date": "2026-02-24",
        "primary_category": "cs.RO",
        "chinese_summary": "现有参考引导的人形机器人控制方法难以适应高级指令接口，且对数据集和校准敏感。本文提出参数化运动生成器（PMG），它基于人类运动结构分析，仅使用紧凑的参数化运动数据和高维控制指令合成参考轨迹。该方法结合模仿学习和基于优化的sim-to-real电机参数识别模块，在人形机器人ZERITH Z1上得到验证。结果表明，PMG能够生成自然、类人的运动，精确响应高维控制输入（包括VR遥操作），并实现了高效、可验证的sim-to-real迁移，为自然可部署的人形机器人控制提供了实用路径。"
      },
      {
        "paper_id": "2602.20915",
        "title": "Task-oriented grasping for dexterous robots using postural synergies and reinforcement learning",
        "abstract": "In this paper, we address the problem of task-oriented grasping for humanoid robots, emphasizing the need to align with human social norms and task-specific objectives. Existing methods, employ a variety of open-loop and closed-loop approaches but lack an end-to-end solution that can grasp several objects while taking into account the downstream task's constraints. Our proposed approach employs reinforcement learning to enhance task-oriented grasping, prioritizing the post-grasp intention of the agent. We extract human grasp preferences from the ContactPose dataset, and train a hand synergy model based on the Variational Autoencoder (VAE) to imitate the participant's grasping actions. Based on this data, we train an agent able to grasp multiple objects while taking into account distinct post-grasp intentions that are task-specific. By combining data-driven insights from human grasping behavior with learning by exploration provided by reinforcement learning, we can develop humanoid robots capable of context-aware manipulation actions, facilitating collaboration in human-centered environments.",
        "authors_display": "Plinio Moreno Team",
        "pdf_url": "http://arxiv.org/abs/2602.20915",
        "code_url": null,
        "date": "2026-02-24",
        "primary_category": "cs.RO",
        "chinese_summary": "针对仿人机器人面向任务抓取时难以兼顾人类社会规范和下游任务约束的问题，本研究提出了一种基于强化学习的端到端方法。该方法从ContactPose数据集中提取人类抓取偏好，并训练一个基于变分自编码器（VAE）的手部协同模型来模仿人类的抓取动作。在此基础上，训练一个智能体以实现多物体抓取，同时考虑任务特定的抓取后意图。通过结合人类抓取行为的数据洞察和强化学习的探索能力，该方法旨在开发具备上下文感知操作能力的仿人机器人，以促进在以人为中心环境中的协作。"
      },
      {
        "paper_id": "2602.13850",
        "title": "Humanoid Hanoi: Investigating Shared Whole-Body Control for Skill-Based Box Rearrangement",
        "abstract": "We investigate a skill-based framework for humanoid box rearrangement that enables long-horizon execution by sequencing reusable skills at the task level. In our architecture, all skills execute through a shared, task-agnostic whole-body controller (WBC), providing a consistent closed-loop interface for skill composition, in contrast to non-shared designs that use separate low-level controllers per skill. We find that naively reusing the same pretrained WBC can reduce robustness over long horizons, as new skills and their compositions induce shifted state and command distributions. We address this with a simple data aggregation procedure that augments shared-WBC training with rollouts from closed-loop skill execution under domain randomization. To evaluate the approach, we introduce Humanoid Hanoi, a long-horizon Tower-of-Hanoi box rearrangement benchmark, and report results in simulation and on the Digit V3 humanoid robot, demonstrating fully autonomous rearrangement over extended horizons and quantifying the benefits of the shared-WBC approach over non-shared baselines. Project page: https://osudrl.github.io/Humanoid_Hanoi/",
        "authors_display": "Alan Fern Team",
        "pdf_url": "http://arxiv.org/abs/2602.13850",
        "code_url": "https://osudrl.github.io/Humanoid_Hanoi/",
        "date": "2026-02-23",
        "primary_category": "cs.RO",
        "chinese_summary": "人形机器人执行长时程箱子重排任务时，面临着简单重用预训练全身控制器（WBC）可能导致鲁棒性下降的问题，因为新技能及其组合会改变状态和命令分布。本文提出一种基于技能的框架，通过在任务层面序列化可重用技能实现长时程执行，并所有技能通过共享的、任务无关的WBC执行。为解决鲁棒性问题，引入了简单的数据聚合过程，通过领域随机化下的闭环技能执行rollout来增强共享WBC的训练。在Humanoid Hanoi箱子重排基准上进行的模拟和Digit V3机器人实验结果显示，该方法实现了扩展时程内的全自主重排，并证明了共享WBC方法相对于非共享基线的优势。"
      },
      {
        "paper_id": "2602.20375",
        "title": "Generalizing from References using a Multi-Task Reference and Goal-Driven RL Framework",
        "abstract": "Learning agile humanoid behaviors from human motion offers a powerful route to natural, coordinated control, but existing approaches face a persistent trade-off: reference-tracking policies are often brittle outside the demonstration dataset, while purely task-driven Reinforcement Learning (RL) can achieve adaptability at the cost of motion quality. We introduce a unified multi-task RL framework that bridges this gap by treating reference motion as a prior for behavioral shaping rather than a deployment-time constraint. A single goal-conditioned policy is trained jointly on two tasks that share the same observation and action spaces, but differ in their initialization schemes, command spaces, and reward structures: (i) a reference-guided imitation task in which reference trajectories define dense imitation rewards but are not provided as policy inputs, and (ii) a goal-conditioned generalization task in which goals are sampled independently of any reference and where rewards reflect only task success. By co-optimizing these objectives within a shared formulation, the policy acquires structured, human-like motor skills from dense reference supervision while learning to adapt these skills to novel goals and initial conditions. This is achieved without adversarial objectives, explicit trajectory tracking, phase variables, or reference-dependent inference. We evaluate the method on a challenging box-based parkour playground that demands diverse athletic behaviors (e.g., jumping and climbing), and show that the learned controller transfers beyond the reference distribution while preserving motion naturalness. Finally, we demonstrate long-horizon behavior generation by composing multiple learned skills, illustrating the flexibility of the learned polices in complex scenarios.",
        "authors_display": "Farbod Farshidian Team",
        "pdf_url": "http://arxiv.org/abs/2602.20375",
        "code_url": null,
        "date": "2026-02-23",
        "primary_category": "cs.RO",
        "chinese_summary": "为解决从人类运动学习灵巧仿人行为时，参考跟踪策略脆弱与纯任务驱动强化学习牺牲运动质量的矛盾，本研究提出了一个统一的多任务强化学习框架。该框架将参考运动作为行为塑造的先验而非约束，使单一目标条件策略在参考引导模仿任务和目标条件泛化任务上联合训练。实验证明，该方法在复杂箱式跑酷场景中，无需对抗目标或显式轨迹跟踪，便能使学习到的控制器泛化超出参考分布，同时保持运动自然性，并通过技能组合展示了长时程行为生成能力。"
      },
      {
        "paper_id": "2602.20362",
        "title": "Energy-Based Injury Protection Database: Including Shearing Contact Thresholds for Hand and Finger Using Porcine Surrogates",
        "abstract": "While robotics research continues to propose strategies for collision avoidance in human-robot interaction, the reality of constrained environments and future humanoid systems makes contact inevitable. To mitigate injury risks, energy-constraining control approaches are commonly used, often relying on safety thresholds derived from blunt impact data in EN ISO 10218-2:2025. However, this dataset does not extend to edged or pointed collisions. Without scalable, clinically grounded datasets covering diverse contact scenarios, safety validation remains limited. Previous studies have laid the groundwork by assessing surrogate-based velocity and mass limits across various geometries, focusing on perpendicular impacts. This study expands those datasets by including shearing contact scenarios in unconstrained collisions, revealing that collision angle significantly affects injury outcomes. Notably, unconstrained shearing contacts result in fewer injuries than perpendicular ones. By reevaluating all prior porcine surrogate data, we establish energy thresholds across geometries and contact types, forming the first energy-based Injury Protection Database. This enables the development of meaningful energy-limiting controllers that ensure safety across a wide range of realistic collision events.",
        "authors_display": "Sami Haddadin Team",
        "pdf_url": "http://arxiv.org/abs/2602.20362",
        "code_url": null,
        "date": "2026-02-23",
        "primary_category": "cs.RO",
        "chinese_summary": "考虑到未来仿人系统在受限环境中接触的必然性，以及现有安全阈值数据集不适用于边缘或尖锐碰撞的局限性，本研究扩展了人机碰撞安全数据集。研究人员通过纳入非约束碰撞中的剪切接触场景，重新评估了猪肉替代物数据。实验结果发现碰撞角度显著影响伤害结果，非约束剪切接触导致的伤害少于垂直接触。最终，该研究建立了跨几何形状和接触类型的能量阈值，形成了首个基于能量的伤害保护数据库，为开发能在各种现实碰撞事件中确保安全的能量限制控制器奠定了基础。"
      },
      {
        "paper_id": "2602.13762",
        "title": "Impact-Robust Posture Optimization for Aerial Manipulation",
        "abstract": "We present a novel method for optimizing the posture of kinematically redundant torque-controlled robots to improve robustness during impacts. A rigid impact model is used as the basis for a configuration-dependent metric that quantifies the variation between pre- and post-impact velocities. By finding configurations (postures) that minimize the aforementioned metric, spikes in the robot's state and input commands can be significantly reduced during impacts, improving safety and robustness. The problem of identifying impact-robust postures is posed as a min-max optimization of the aforementioned metric. To overcome the real-time intractability of the problem, we reformulate it as a gradient-based motion task that iteratively guides the robot towards configurations that minimize the proposed metric. This task is embedded within a task-space inverse dynamics (TSID) whole-body controller, enabling seamless integration with other control objectives. The method is applied to a kinematically redundant aerial manipulator performing repeated point contact tasks. We test our method inside a realistic physics simulator and compare it with the nominal TSID. Our method leads to a reduction (up to 51% w.r.t. standard TSID) of post-impact spikes in the robot's configuration and successfully avoids actuator saturation. Moreover, we demonstrate the importance of kinematic redundancy for impact robustness using additional numerical simulations on a quadruped and a humanoid robot, resulting in up to 45% reduction of post-impact spikes in the robot's state w.r.t. nominal TSID.",
        "authors_display": "Antonio Franchi Team",
        "pdf_url": "http://arxiv.org/abs/2602.13762",
        "code_url": null,
        "date": "2026-02-22",
        "primary_category": "cs.RO",
        "chinese_summary": "优化运动冗余扭矩控制机器人的姿态以提高冲击鲁棒性是一个挑战，传统方法难以有效降低冲击带来的状态和输入命令尖峰。本文提出一种新方法，利用刚性冲击模型构建一个构型依赖的度量来量化冲击前后速度变化，并通过最小化该度量来识别冲击鲁棒姿态。将此问题重新表述为基于梯度的运动任务，并将其嵌入到任务空间逆动力学（TSID）全身控制器中。在物理模拟器中，该方法应用于空中机械手，使冲击后机器人构型尖峰减少高达51%，并成功避免执行器饱和。此外，对四足和人形机器人的数值模拟也证明了运动冗余对冲击鲁棒性的重要性，使冲击后机器人状态尖峰减少高达45%。"
      },
      {
        "paper_id": "2602.18813",
        "title": "Habilis-$β$: A Fast-Motion and Long-Lasting On-Device Vision-Language-Action Model",
        "abstract": "We introduce Habilis-$β$, a fast-motion and long-lasting on-device vision-language-action (VLA) model designed for real-world deployment. Current VLA evaluation remains largely confined to single-trial success rates under curated resets, which fails to capture the fast-motion and long-lasting capabilities essential for practical operation. To address this, we introduce the Productivity-Reliability Plane (PRP), which evaluates performance through Tasks per Hour (TPH) and Mean Time Between Intervention (MTBI) under a continuous-run protocol that demands both high-speed execution and sustained robustness. Habilis-$β$ achieves high performance by integrating language-free pre-training on large-scale play data for robust interaction priors with post-training on cyclic task demonstrations that capture state drift across consecutive task iterations. The system further employs ESPADA for phase-adaptive motion shaping to accelerate free-space transit, utilizes rectified-flow distillation to enable high-frequency control on edge devices, and incorporates classifier-free guidance (CFG) as a deployment-time knob to dynamically balance instruction adherence and learned interaction priors. In 1-hour continuous-run evaluations, Habilis-$β$ achieves strong performance under the PRP metrics, compared to $π_{0.5}$ in both simulation and real-world environments. In simulation, Habilis-$β$ achieves 572.6 TPH and 39.2 s MTBI (vs. 120.5 TPH and 30.5 s for $π_{0.5}$), while in a real-world humanoid logistics workflow it achieves 124 TPH and 137.4 s MTBI (vs. 19 TPH and 46.1 s for $π_{0.5}$). Finally, Habilis-$β$ achieves the highest reported performance on the standard RoboTwin 2.0 leaderboard across representative tasks, validating its effectiveness in complex manipulation scenarios.",
        "authors_display": "Theo Taeyeong Kim Team",
        "pdf_url": "http://arxiv.org/abs/2602.18813",
        "code_url": null,
        "date": "2026-02-21",
        "primary_category": "cs.RO",
        "chinese_summary": "针对现有VLA模型评估未能捕捉高速运动和持久能力的局限性，本研究提出了Habilis-β模型和生产力-可靠性平面（PRP）评估指标。Habilis-β通过大规模“玩耍”数据的无语言预训练、循环任务演示的后训练、ESPADA相位自适应运动塑形、整流流蒸馏以及无分类器指导等技术实现高性能和持续鲁棒性。在1小时连续运行评估中，Habilis-β在模拟和真实世界物流工作流程中，其每小时任务数和平均干预时间均显著优于基线，并在RoboTwin 2.0排行榜上取得了最高性能，验证了其在复杂操作场景中的高效性和可靠性。"
      },
      {
        "paper_id": "2602.18742",
        "title": "RoboCurate: Harnessing Diversity with Action-Verified Neural Trajectory for Robot Learning",
        "abstract": "Synthetic data generated by video generative models has shown promise for robot learning as a scalable pipeline, but it often suffers from inconsistent action quality due to imperfectly generated videos. Recently, vision-language models (VLMs) have been leveraged to validate video quality, but they have limitations in distinguishing physically accurate videos and, even then, cannot directly evaluate the generated actions themselves. To tackle this issue, we introduce RoboCurate, a novel synthetic robot data generation framework that evaluates and filters the quality of annotated actions by comparing them with simulation replay. Specifically, RoboCurate replays the predicted actions in a simulator and assesses action quality by measuring the consistency of motion between the simulator rollout and the generated video. In addition, we unlock observation diversity beyond the available dataset via image-to-image editing and apply action-preserving video-to-video transfer to further augment appearance. We observe RoboCurate's generated data yield substantial relative improvements in success rates compared to using real data only, achieving +70.1% on GR-1 Tabletop (300 demos), +16.1% on DexMimicGen in the pre-training setup, and +179.9% in the challenging real-world ALLEX humanoid dexterous manipulation setting.",
        "authors_display": "Jinwoo Shin Team",
        "pdf_url": "http://arxiv.org/abs/2602.18742",
        "code_url": "https://seungkukim.github.io/robocurate/",
        "date": "2026-02-21",
        "primary_category": "cs.RO",
        "chinese_summary": "为解决视频生成模型合成数据在机器人学习中动作质量不一致的问题，本研究提出了RoboCurate框架。该框架通过将预测动作在模拟器中回放，并测量模拟器输出与生成视频之间的运动一致性，来评估和过滤标注动作的质量。此外，通过图像到图像编辑和动作保留视频到视频传输来增强观察多样性。实验结果表明，RoboCurate生成的数据在GR-1 Tabletop、DexMimicGen和ALLEX仿人灵巧操作等任务中，相较于仅使用真实数据，成功率有显著提升，验证了其在改进合成数据质量方面的有效性。"
      },
      {
        "paper_id": "2602.15060",
        "title": "CLOT: Closed-Loop Global Motion Tracking for Whole-Body Humanoid Teleoperation",
        "abstract": "Long-horizon whole-body humanoid teleoperation remains challenging due to accumulated global pose drift, particularly on full-sized humanoids. Although recent learning-based tracking methods enable agile and coordinated motions, they typically operate in the robot's local frame and neglect global pose feedback, leading to drift and instability during extended execution. In this work, we present CLOT, a real-time whole-body humanoid teleoperation system that achieves closed-loop global motion tracking via high-frequency localization feedback. CLOT synchronizes operator and robot poses in a closed loop, enabling drift-free human-to-humanoid mimicry over long timehorizons. However, directly imposing global tracking rewards in reinforcement learning, often results in aggressive and brittle corrections. To address this, we propose a data-driven randomization strategy that decouples observation trajectories from reward evaluation, enabling smooth and stable global corrections. We further regularize the policy with an adversarial motion prior to suppress unnatural behaviors. To support CLOT, we collect 20 hours of carefully curated human motion data for training the humanoid teleoperation policy. We design a transformer-based policy and train it for over 1300 GPU hours. The policy is deployed on a full-sized humanoid with 31 DoF (excluding hands). Both simulation and real-world experiments verify high-dynamic motion, high-precision tracking, and strong robustness in sim-to-real humanoid teleoperation. Motion data, demos and code can be found in our website.",
        "authors_display": "Yichao Yan Team",
        "pdf_url": "http://arxiv.org/abs/2602.15060",
        "code_url": null,
        "date": "2026-02-20",
        "primary_category": "cs.RO",
        "chinese_summary": "长周期全身人形机器人遥操作面临全局姿态漂移累积问题，现有方法常忽略全局反馈。本文提出CLOT系统，通过高频定位反馈实现闭环全局运动跟踪，在长周期内同步操作员和机器人姿态，实现无漂移模仿。为避免直接奖励导致的激进修正，引入数据驱动的随机化策略，解耦观测轨迹和奖励评估，实现平滑稳定的全局修正，并通过对抗性运动先验抑制不自然行为。仿真和真实实验验证了该策略在高动态运动、高精度跟踪和sim-to-real鲁棒性方面的卓越表现。"
      },
      {
        "paper_id": "2602.16511",
        "title": "VIGOR: Visual Goal-In-Context Inference for Unified Humanoid Fall Safety",
        "abstract": "Reliable fall recovery is critical for humanoids operating in cluttered environments. Unlike quadrupeds or wheeled robots, humanoids experience high-energy impacts, complex whole-body contact, and large viewpoint changes during a fall, making recovery essential for continued operation. Existing methods fragment fall safety into separate problems such as fall avoidance, impact mitigation, and stand-up recovery, or rely on end-to-end policies trained without vision through reinforcement learning or imitation learning, often on flat terrain. At a deeper level, fall safety is treated as monolithic data complexity, coupling pose, dynamics, and terrain and requiring exhaustive coverage, limiting scalability and generalization. We present a unified fall safety approach that spans all phases of fall recovery. It builds on two insights: 1) Natural human fall and recovery poses are highly constrained and transferable from flat to complex terrain through alignment, and 2) Fast whole-body reactions require integrated perceptual-motor representations. We train a privileged teacher using sparse human demonstrations on flat terrain and simulated complex terrains, and distill it into a deployable student that relies only on egocentric depth and proprioception. The student learns how to react by matching the teacher's goal-in-context latent representation, which combines the next target pose with the local terrain, rather than separately encoding what it must perceive and how it must act. Results in simulation and on a real Unitree G1 humanoid demonstrate robust, zero-shot fall safety across diverse non-flat environments without real-world fine-tuning. The project page is available at https://vigor2026.github.io/",
        "authors_display": "Stella X. Yu Team",
        "pdf_url": "http://arxiv.org/abs/2602.16511",
        "code_url": null,
        "date": "2026-02-18",
        "primary_category": "cs.RO",
        "chinese_summary": "人形机器人在杂乱环境中摔倒恢复至关重要，但现有方法常将摔倒安全问题碎片化或依赖缺乏视觉的端到端策略，且泛化性差。本文提出一种统一的摔倒安全方法，基于人类摔倒姿势的可迁移性以及感知-运动一体化表征的洞察。研究通过在平坦和模拟复杂地形上使用稀疏人类示范训练特权教师模型，并蒸馏成仅依赖自我中心深度和本体感受的学生模型，学生模型通过匹配教师的目标-语境潜在表征来学习。模拟和真实Unitree G1机器人上的结果表明，该方法在多样非平坦环境中实现了鲁棒的零样本摔倒安全，无需真实世界微调。"
      },
      {
        "paper_id": "2602.15827",
        "title": "Perceptive Humanoid Parkour: Chaining Dynamic Human Skills via Motion Matching",
        "abstract": "While recent advances in humanoid locomotion have achieved stable walking on varied terrains, capturing the agility and adaptivity of highly dynamic human motions remains an open challenge. In particular, agile parkour in complex environments demands not only low-level robustness, but also human-like motion expressiveness, long-horizon skill composition, and perception-driven decision-making. In this paper, we present Perceptive Humanoid Parkour (PHP), a modular framework that enables humanoid robots to autonomously perform long-horizon, vision-based parkour across challenging obstacle courses. Our approach first leverages motion matching, formulated as nearest-neighbor search in a feature space, to compose retargeted atomic human skills into long-horizon kinematic trajectories. This framework enables the flexible composition and smooth transition of complex skill chains while preserving the elegance and fluidity of dynamic human motions. Next, we train motion-tracking reinforcement learning (RL) expert policies for these composed motions, and distill them into a single depth-based, multi-skill student policy, using a combination of DAgger and RL. Crucially, the combination of perception and skill composition enables autonomous, context-aware decision-making: using only onboard depth sensing and a discrete 2D velocity command, the robot selects and executes whether to step over, climb onto, vault or roll off obstacles of varying geometries and heights. We validate our framework with extensive real-world experiments on a Unitree G1 humanoid robot, demonstrating highly dynamic parkour skills such as climbing tall obstacles up to 1.25m (96% robot height), as well as long-horizon multi-obstacle traversal with closed-loop adaptation to real-time obstacle perturbations.",
        "authors_display": "C. Karen Liu Team",
        "pdf_url": "http://arxiv.org/abs/2602.15827",
        "code_url": null,
        "date": "2026-02-17",
        "primary_category": "cs.RO",
        "chinese_summary": "人形机器人实现高动态人类跑酷运动面临敏捷性、运动表现力、长时程技能组合及感知驱动决策的挑战。本文提出Perceptive Humanoid Parkour（PHP）模块化框架，通过运动匹配将人类原子技能组合成长时程运动学轨迹，实现复杂技能链的灵活过渡。随后，为这些组合运动训练RL专家策略，并蒸馏为基于深度感知的多技能学生策略。关键在于结合感知与技能组合，使机器人能基于机载深度传感自主决策跨越、攀爬或滚越障碍。真实世界实验在Unitree G1机器人上验证了其高度动态的跑酷能力，包括攀爬高达1.25m障碍物和多障碍物穿越的闭环适应性。"
      },
      {
        "paper_id": "2602.15733",
        "title": "MeshMimic: Geometry-Aware Humanoid Motion Learning through 3D Scene Reconstruction",
        "abstract": "Humanoid motion control has witnessed significant breakthroughs in recent years, with deep reinforcement learning (RL) emerging as a primary catalyst for achieving complex, human-like behaviors. However, the high dimensionality and intricate dynamics of humanoid robots make manual motion design impractical, leading to a heavy reliance on expensive motion capture (MoCap) data. These datasets are not only costly to acquire but also frequently lack the necessary geometric context of the surrounding physical environment. Consequently, existing motion synthesis frameworks often suffer from a decoupling of motion and scene, resulting in physical inconsistencies such as contact slippage or mesh penetration during terrain-aware tasks. In this work, we present MeshMimic, an innovative framework that bridges 3D scene reconstruction and embodied intelligence to enable humanoid robots to learn coupled \"motion-terrain\" interactions directly from video. By leveraging state-of-the-art 3D vision models, our framework precisely segments and reconstructs both human trajectories and the underlying 3D geometry of terrains and objects. We introduce an optimization algorithm based on kinematic consistency to extract high-quality motion data from noisy visual reconstructions, alongside a contact-invariant retargeting method that transfers human-environment interaction features to the humanoid agent. Experimental results demonstrate that MeshMimic achieves robust, highly dynamic performance across diverse and challenging terrains. Our approach proves that a low-cost pipeline utilizing only consumer-grade monocular sensors can facilitate the training of complex physical interactions, offering a scalable path toward the autonomous evolution of humanoid robots in unstructured environments.",
        "authors_display": "Yijie Guo Team",
        "pdf_url": "http://arxiv.org/abs/2602.15733",
        "code_url": null,
        "date": "2026-02-17",
        "primary_category": "cs.RO",
        "chinese_summary": "人形机器人运动控制虽有突破，但高度依赖昂贵且缺乏环境几何信息的动作捕捉数据，导致运动与场景解耦，在地形感知任务中常出现物理不一致。本文提出MeshMimic框架，利用先进3D视觉模型从视频中精确重建人类轨迹和3D地形物体，从而直接学习耦合的“运动-地形”交互。通过基于运动学一致性的优化算法提取高质量运动数据，并用接触不变重定向方法将人与环境交互特征转移至机器人。实验证明，该框架在多样地形上实现了鲁棒高动态性能，表明仅利用消费级单目传感器即可实现复杂物理交互训练，为人形机器人自主演化提供可扩展途径。"
      },
      {
        "paper_id": "2602.14551",
        "title": "Replanning Human-Robot Collaborative Tasks with Vision-Language Models via Semantic and Physical Dual-Correction",
        "abstract": "Human-Robot Collaboration (HRC) plays an important role in assembly tasks by enabling robots to plan and adjust their motions based on interactive, real-time human instructions. However, such instructions are often linguistically ambiguous and underspecified, making it difficult to generate physically feasible and cooperative robot behaviors. To address this challenge, many studies have applied Vision-Language Models (VLMs) to interpret high-level instructions and generate corresponding actions. Nevertheless, VLM-based approaches still suffer from hallucinated reasoning and an inability to anticipate physical execution failures. To address these challenges, we propose an HRC framework that augments a VLM-based reasoning with a dual-correction mechanism: an internal correction model that verifies logical consistency and task feasibility prior to action execution, and an external correction model that detects and rectifies physical failures through post-execution feedback. Simulation ablation studies demonstrate that the proposed method improves the success rate compared to baselines without correction models. Our real-world experiments in collaborative assembly tasks supported by object fixation or tool preparation by an upper body humanoid robot further confirm the framewor's effectiveness in enabling interactive replanning across different collaborative tasks in response to human instructions, validating its practical feasibility.",
        "authors_display": "Kensuke Harada Team",
        "pdf_url": "http://arxiv.org/abs/2602.14551",
        "code_url": null,
        "date": "2026-02-16",
        "primary_category": "cs.RO",
        "chinese_summary": "人机协作（HRC）在装配任务中面临人类指令模糊不清、机器人难以生成可行协作行为的挑战，现有基于视觉-语言模型（VLMs）的方法常出现幻觉推理和物理执行失败。本文提出一个HRC框架，通过双重校正机制增强VLM的推理能力。该机制包含一个在动作执行前验证逻辑和可行性的内部校正模型，以及一个通过执行后反馈纠正物理失败的外部校正模型。仿真研究显示该方法提高了成功率，真实世界的人形机器人协作装配实验进一步证实了其在响应人类指令时实现交互式重规划的有效性和实际可行性。"
      },
      {
        "paper_id": "2602.14363",
        "title": "AdaptManip: Learning Adaptive Whole-Body Object Lifting and Delivery with Online Recurrent State Estimation",
        "abstract": "This paper presents Adaptive Whole-body Loco-Manipulation, AdaptManip, a fully autonomous framework for humanoid robots to perform integrated navigation, object lifting, and delivery. Unlike prior imitation learning-based approaches that rely on human demonstrations and are often brittle to disturbances, AdaptManip aims to train a robust loco-manipulation policy via reinforcement learning without human demonstrations or teleoperation data. The proposed framework consists of three coupled components: (1) a recurrent object state estimator that tracks the manipulated object in real time under limited field-of-view and occlusions; (2) a whole-body base policy for robust locomotion with residual manipulation control for stable object lifting and delivery; and (3) a LiDAR-based robot global position estimator that provides drift-robust localization. All components are trained in simulation using reinforcement learning and deployed on real hardware in a zero-shot manner. Experimental results show that AdaptManip significantly outperforms baseline methods, including imitation learning-based approaches, in adaptability and overall success rate, while accurate object state estimation improves manipulation performance even under occlusion. We further demonstrate fully autonomous real-world navigation, object lifting, and delivery on a humanoid robot.",
        "authors_display": "Sehoon Ha Team",
        "pdf_url": "http://arxiv.org/abs/2602.14363",
        "code_url": "https://morganbyrd03.github.io/adaptmanip/",
        "date": "2026-02-16",
        "primary_category": "cs.RO",
        "chinese_summary": "现有基于模仿学习的人形机器人局部操作方法通常需要人类示范且对扰动敏感。本文提出自适应全身局部操作框架AdaptManip，旨在通过强化学习训练出鲁棒的局部操作策略，无需人类示范或遥操作数据。该框架包含实时对象状态估计器、结合残差操作控制的全身基础策略和基于LiDAR的鲁棒全局定位器。所有组件均在仿真中训练并零样本部署到真实硬件。实验结果表明，AdaptManip在适应性和整体成功率方面显著优于基线方法，精确的对象状态估计即使在遮挡下也能提高操作性能，并成功演示了人形机器人的全自主真实世界导航、物体抓取和递送。"
      },
      {
        "paper_id": "2602.14351",
        "title": "WIMLE: Uncertainty-Aware World Models with IMLE for Sample-Efficient Continuous Control",
        "abstract": "Model-based reinforcement learning promises strong sample efficiency but often underperforms in practice due to compounding model error, unimodal world models that average over multi-modal dynamics, and overconfident predictions that bias learning. We introduce WIMLE, a model-based method that extends Implicit Maximum Likelihood Estimation (IMLE) to the model-based RL framework to learn stochastic, multi-modal world models without iterative sampling and to estimate predictive uncertainty via ensembles and latent sampling. During training, WIMLE weights each synthetic transition by its predicted confidence, preserving useful model rollouts while attenuating bias from uncertain predictions and enabling stable learning. Across $40$ continuous-control tasks spanning DeepMind Control, MyoSuite, and HumanoidBench, WIMLE achieves superior sample efficiency and competitive or better asymptotic performance than strong model-free and model-based baselines. Notably, on the challenging Humanoid-run task, WIMLE improves sample efficiency by over $50$\\% relative to the strongest competitor, and on HumanoidBench it solves $8$ of $14$ tasks (versus $4$ for BRO and $5$ for SimbaV2). These results highlight the value of IMLE-based multi-modality and uncertainty-aware weighting for stable model-based RL.",
        "authors_display": "Ke Li Team",
        "pdf_url": "http://arxiv.org/abs/2602.14351",
        "code_url": "https://openreview.net/forum?id=mzLOnTb3WH",
        "date": "2026-02-15",
        "primary_category": "cs.LG",
        "chinese_summary": "基于模型的强化学习（MBRL）在样本效率上具有潜力，但常因模型误差累积、单模态世界模型以及过度自信预测导致的学习偏差而表现不佳。本文引入WIMLE方法，将隐式最大似然估计（IMLE）扩展到MBRL框架，以学习随机、多模态的世界模型，并利用集成和潜在采样估计预测不确定性。训练期间，WIMLE根据预测置信度加权合成转换，保留有用模型rollout并减少不确定预测的偏差。实验结果表明，在40项连续控制任务中，WIMLE实现了卓越的样本效率和具竞争力的渐近性能，尤其在Humanoid-run任务上样本效率提升超过50%，并在HumanoidBench中解决了更多任务，证明了IMLE驱动的多模态和不确定性感知加权对稳定MBRL的价值。"
      },
      {
        "paper_id": "2602.14048",
        "title": "ProAct: A Dual-System Framework for Proactive Embodied Social Agents",
        "abstract": "Embodied social agents have recently advanced in generating synchronized speech and gestures. However, most interactive systems remain fundamentally reactive, responding only to current sensory inputs within a short temporal window. Proactive social behavior, in contrast, requires deliberation over accumulated context and intent inference, which conflicts with the strict latency budget of real-time interaction. We present \\emph{ProAct}, a dual-system framework that reconciles this time-scale conflict by decoupling a low-latency \\emph{Behavioral System} for streaming multimodal interaction from a slower \\emph{Cognitive System} which performs long-horizon social reasoning and produces high-level proactive intentions. To translate deliberative intentions into continuous non-verbal behaviors without disrupting fluency, we introduce a streaming flow-matching model conditioned on intentions via ControlNet. This mechanism supports asynchronous intention injection, enabling seamless transitions between reactive and proactive gestures within a single motion stream. We deploy ProAct on a physical humanoid robot and evaluate both motion quality and interactive effectiveness. In real-world interaction user studies, participants and observers consistently prefer ProAct over reactive variants in perceived proactivity, social presence, and overall engagement, demonstrating the benefits of dual-system proactive control for embodied social interaction.",
        "authors_display": "Libin Liu Team",
        "pdf_url": "http://arxiv.org/abs/2602.14048",
        "code_url": "https://proactrobot.github.io/",
        "date": "2026-02-15",
        "primary_category": "cs.RO",
        "chinese_summary": "具身社交智能体在生成同步语音和手势方面有所进展，但多数系统仍是被动响应，无法进行长时程的主动社交推理。本文提出ProAct双系统框架，通过解耦低延迟的“行为系统”和执行长时程社交推理的“认知系统”来解决时间尺度冲突。为将深思熟虑的意图流畅转化为连续非语言行为，引入了通过ControlNet以意图为条件的流匹配模型，支持异步意图注入，实现反应性与主动性手势的无缝转换。在物理人形机器人上的用户研究表明，ProAct在感知到的主动性、社交存在感和整体参与度方面优于被动变体，验证了双系统主动控制在具身社交交互中的益处。"
      },
      {
        "paper_id": "2602.13656",
        "title": "A Kung Fu Athlete Bot That Can Do It All Day: Highly Dynamic, Balance-Challenging Motion Dataset and Autonomous Fall-Resilient Tracking",
        "abstract": "Current humanoid motion tracking systems can execute routine and moderately dynamic behaviors, yet significant gaps remain near hardware performance limits and algorithmic robustness boundaries. Martial arts represent an extreme case of highly dynamic human motion, characterized by rapid center-of-mass shifts, complex coordination, and abrupt posture transitions. However, datasets tailored to such high-intensity scenarios remain scarce. To address this gap, we construct KungFuAthlete, a high-dynamic martial arts motion dataset derived from professional athletes' daily training videos. The dataset includes ground and jump subsets covering representative complex motion patterns. The jump subset exhibits substantially higher joint, linear, and angular velocities compared to commonly used datasets such as LAFAN1, PHUMA, and AMASS, indicating significantly increased motion intensity and complexity. Importantly, even professional athletes may fail during highly dynamic movements. Similarly, humanoid robots are prone to instability and falls under external disturbances or execution errors. Most prior work assumes motion execution remains within safe states and lacks a unified strategy for modeling unsafe states and enabling reliable autonomous recovery. We propose a novel training paradigm that enables a single policy to jointly learn high-dynamic motion tracking and fall recovery, unifying agile execution and stabilization within one framework. This framework expands robotic capability from pure motion tracking to recovery-enabled execution, promoting more robust and autonomous humanoid performance in real-world high-dynamic scenarios.",
        "authors_display": "Xuesong Li Team",
        "pdf_url": "http://arxiv.org/abs/2602.13656",
        "code_url": null,
        "date": "2026-02-14",
        "primary_category": "cs.RO",
        "chinese_summary": "针对人形机器人在高动态运动追踪方面存在性能瓶颈，且缺乏对非安全状态建模和自主恢复策略的问题，本文首先构建了KungFuAthlete高动态武术运动数据集，其运动强度和复杂性远超现有数据集。在此基础上，提出了一种新颖的训练范式，使单一策略能够同时学习高动态运动追踪和跌倒恢复。该框架将人形机器人的能力从单纯的运动追踪扩展至具有恢复功能的执行，显著提升了其在真实世界高动态场景中的鲁棒性和自主性。"
      },
      {
        "paper_id": "2602.12918",
        "title": "Adding internal audio sensing to internal vision enables human-like in-hand fabric recognition with soft robotic fingertips",
        "abstract": "Distinguishing the feel of smooth silk from coarse cotton is a trivial everyday task for humans. When exploring such fabrics, fingertip skin senses both spatio-temporal force patterns and texture-induced vibrations that are integrated to form a haptic representation of the explored material. It is challenging to reproduce this rich, dynamic perceptual capability in robots because tactile sensors typically cannot achieve both high spatial resolution and high temporal sampling rate. In this work, we present a system that can sense both types of haptic information, and we investigate how each type influences robotic tactile perception of fabrics. Our robotic hand's middle finger and thumb each feature a soft tactile sensor: one is the open-source Minsight sensor that uses an internal camera to measure fingertip deformation and force at 50 Hz, and the other is our new sensor Minsound that captures vibrations through an internal MEMS microphone with a bandwidth from 50 Hz to 15 kHz. Inspired by the movements humans make to evaluate fabrics, our robot actively encloses and rubs folded fabric samples between its two sensitive fingers. Our results test the influence of each sensing modality on overall classification performance, showing high utility for the audio-based sensor. Our transformer-based method achieves a maximum fabric classification accuracy of 97 % on a dataset of 20 common fabrics. Incorporating an external microphone away from Minsound increases our method's robustness in loud ambient noise conditions. To show that this audio-visual tactile sensing approach generalizes beyond the training data, we learn general representations of fabric stretchiness, thickness, and roughness.",
        "authors_display": "Katherine J. Kuchenbecker Team",
        "pdf_url": "http://arxiv.org/abs/2602.12918",
        "code_url": null,
        "date": "2026-02-13",
        "primary_category": "cs.RO",
        "chinese_summary": "人类区分织物触感涉及空间-时间力模式和纹理诱导振动的整合，但机器人触觉传感器通常难以同时实现高空间和时间分辨率。本文提出一个系统，通过集成Minsight视觉触觉传感器（测量形变和力）和Minsound振动传感器（捕捉振动）来感知这两种信息。机器人模仿人类评估织物的方式进行主动探索，并通过基于Transformer的方法对20种常见织物进行分类。结果显示，音频传感器对分类性能具有高效用，分类精度最高达97%，并能泛化学习织物弹性、厚度和粗糙度的通用表示。"
      },
      {
        "paper_id": "2602.11929",
        "title": "General Humanoid Whole-Body Control via Pretraining and Fast Adaptation",
        "abstract": "Learning a general whole-body controller for humanoid robots remains challenging due to the diversity of motion distributions, the difficulty of fast adaptation, and the need for robust balance in high-dynamic scenarios. Existing approaches often require task-specific training or suffer from performance degradation when adapting to new motions. In this paper, we present FAST, a general humanoid whole-body control framework that enables Fast Adaptation and Stable Motion Tracking. FAST introduces Parseval-Guided Residual Policy Adaptation, which learns a lightweight delta action policy under orthogonality and KL constraints, enabling efficient adaptation to out-of-distribution motions while mitigating catastrophic forgetting. To further improve physical robustness, we propose Center-of-Mass-Aware Control, which incorporates CoM-related observations and objectives to enhance balance when tracking challenging reference motions. Extensive experiments in simulation and real-world deployment demonstrate that FAST consistently outperforms state-of-the-art baselines in robustness, adaptation efficiency, and generalization.",
        "authors_display": "Zongqing Lu Team",
        "pdf_url": "http://arxiv.org/abs/2602.11929",
        "code_url": null,
        "date": "2026-02-12",
        "primary_category": "cs.RO",
        "chinese_summary": "通用人形机器人全身控制器面临运动多样性、快速适应和高动态平衡等挑战。本文提出FAST框架，旨在实现快速适应和稳定运动跟踪。FAST引入Parseval-Guided残差策略适应，通过正交性和KL约束学习轻量级delta动作策略，实现对分布外运动的高效适应并减轻灾难性遗忘。同时，提出质心感知控制，整合质心相关观测和目标以增强平衡。在仿真和真实世界的广泛实验表明，FAST在鲁棒性、适应效率和泛化性方面持续优于现有先进基线方法。"
      },
      {
        "paper_id": "2602.11758",
        "title": "HAIC: Humanoid Agile Object Interaction Control via Dynamics-Aware World Model",
        "abstract": "Humanoid robots show promise for complex whole-body tasks in unstructured environments. Although Human-Object Interaction (HOI) has advanced, most methods focus on fully actuated objects rigidly coupled to the robot, ignoring underactuated objects with independent dynamics and non-holonomic constraints. These introduce control challenges from coupling forces and occlusions. We present HAIC, a unified framework for robust interaction across diverse object dynamics without external state estimation. Our key contribution is a dynamics predictor that estimates high-order object states (velocity, acceleration) solely from proprioceptive history. These predictions are projected onto static geometric priors to form a spatially grounded dynamic occupancy map, enabling the policy to infer collision boundaries and contact affordances in blind spots. We use asymmetric fine-tuning, where a world model continuously adapts to the student policy's exploration, ensuring robust state estimation under distribution shifts. Experiments on a humanoid robot show HAIC achieves high success rates in agile tasks (skateboarding, cart pushing/pulling under various loads) by proactively compensating for inertial perturbations, and also masters multi-object long-horizon tasks like carrying a box across varied terrain by predicting the dynamics of multiple objects.",
        "authors_display": "Renjing Xu Team",
        "pdf_url": "http://arxiv.org/abs/2602.11758",
        "code_url": "https://haic-humanoid.github.io/",
        "date": "2026-02-12",
        "primary_category": "cs.RO",
        "chinese_summary": "为解决人形机器人在非结构化环境中与非完全受控物体进行交互的挑战，尤其是在存在独立动力学、非完整约束、耦合力和遮挡的情况下，本研究提出了HAIC统一框架。其核心贡献在于一个仅通过本体感知历史估计高阶物体状态的动力学预测器，这些预测被投射到静态几何先验上形成动态占用图，使策略能推断盲点中的碰撞边界。HAIC还利用不对称微调，确保在分布变化下鲁棒的状态估计。在人形机器人上的实验表明，HAIC通过主动补偿惯性扰动，在敏捷任务（如滑板和推拉推车）中实现了高成功率，并能预测多个物体动力学，成功执行多物体长时程任务。"
      },
      {
        "paper_id": "2602.11472",
        "title": "Future Mining: Learning for Safety and Security",
        "abstract": "Mining is rapidly evolving into an AI driven cyber physical ecosystem where safety and operational reliability depend on robust perception, trustworthy distributed intelligence, and continuous monitoring of miners and equipment. However, real world mining environments impose severe constraints, including poor illumination, GPS denied conditions, irregular underground topologies and intermittent connectivity. These factors degrade perception accuracy, disrupt situational awareness and weaken distributed learning systems. At the same time, emerging cyber physical threats such as backdoor triggers, sensor spoofing, label flipping attacks, and poisoned model updates further jeopardize operational safety as mines adopt autonomous vehicles, humanoid assistance, and federated learning for collaborative intelligence. Energy constrained sensors also experience uneven battery depletion, creating blind spots in safety coverage and disrupting hazard detection pipelines. This paper presents a vision for a Unified Smart Safety and Security Architecture that integrates multimodal perception, secure federated learning, reinforcement learning, DTN enabled communication, and energy aware sensing into a cohesive safety framework. We introduce five core modules: Miner Finder, Multimodal Situational Awareness, Backdoor Attack Monitor, TrustFed LFD, and IoT driven Equipment Health Monitoring. These modules collectively address miner localization, hazard understanding, federated robustness, and predictive maintenance. Together, they form an end to end framework capable of guiding miners through obstructed pathways, identifying compromised models or sensors, and ensuring mission critical equipment reliability. This work outlines a comprehensive research vision for building a resilient and trustworthy intelligent mining system capable of maintaining operational continuity under adversarial conditions.",
        "authors_display": "Sanjay Madria Team",
        "pdf_url": "http://arxiv.org/abs/2602.11472",
        "code_url": null,
        "date": "2026-02-12",
        "primary_category": "cs.CR",
        "chinese_summary": "针对采矿业向AI驱动的物理信息系统转型中，面临恶劣环境、网络物理威胁及传感器能量限制导致的感知与安全挑战，本研究提出了一个统一的智能安全与安保架构愿景。该架构整合了多模态感知、安全联邦学习、强化学习、DTN通信和能源感知传感。文章介绍了矿工定位、多模态态势感知、后门攻击监控、TrustFed LFD和IoT设备健康监控等五个核心模块，旨在共同解决矿工定位、危险理解、联邦鲁棒性和预测性维护等问题，以构建一个在对抗条件下能保持操作连续性的弹性、可信智能采矿系统。"
      },
      {
        "paper_id": "2602.06643",
        "title": "Humanoid Manipulation Interface: Humanoid Whole-Body Manipulation from Robot-Free Demonstrations",
        "abstract": "Current approaches for humanoid whole-body manipulation, primarily relying on teleoperation or visual sim-to-real reinforcement learning, are hindered by hardware logistics and complex reward engineering. Consequently, demonstrated autonomous skills remain limited and are typically restricted to controlled environments. In this paper, we present the Humanoid Manipulation Interface (HuMI), a portable and efficient framework for learning diverse whole-body manipulation tasks across various environments. HuMI enables robot-free data collection by capturing rich whole-body motion using portable hardware. This data drives a hierarchical learning pipeline that translates human motions into dexterous and feasible humanoid skills. Extensive experiments across five whole-body tasks--including kneeling, squatting, tossing, walking, and bimanual manipulation--demonstrate that HuMI achieves a 3x increase in data collection efficiency compared to teleoperation and attains a 70% success rate in unseen environments.",
        "authors_display": "Yang Gao Team",
        "pdf_url": "http://arxiv.org/abs/2602.06643",
        "code_url": "https://humanoid-manipulation-interface.github.io",
        "date": "2026-02-12",
        "primary_category": "cs.RO",
        "chinese_summary": "当前人形机器人全身操作方法受限于硬件物流和复杂奖励工程，导致自主技能有限且通常仅限于受控环境。为解决这些问题，本文提出了Humanoid Manipulation Interface (HuMI)，一个便携高效的框架，用于在各种环境中学习多样化的全身操作任务。HuMI通过便携硬件捕捉丰富的全身运动，实现无机器人数据收集，并利用分层学习流程将人类运动转化为灵巧且可行的人形技能。广泛实验表明，HuMI的数据收集效率比遥操作提高3倍，并在未知环境中取得了70%的成功率，有效提升了人形机器人的泛化操作能力。"
      },
      {
        "paper_id": "2602.11321",
        "title": "ExtremControl: Low-Latency Humanoid Teleoperation with Direct Extremity Control",
        "abstract": "Building a low-latency humanoid teleoperation system is essential for collecting diverse reactive and dynamic demonstrations. However, existing approaches rely on heavily pre-processed human-to-humanoid motion retargeting and position-only PD control, resulting in substantial latency that severely limits responsiveness and prevents tasks requiring rapid feedback and fast reactions. To address this problem, we propose ExtremControl, a low latency whole-body control framework that: (1) operates directly on SE(3) poses of selected rigid links, primarily humanoid extremities, to avoid full-body retargeting; (2) utilizes a Cartesian-space mapping to directly convert human motion to humanoid link targets; and (3) incorporates velocity feedforward control at low level to support highly responsive behavior under rapidly changing control interfaces. We further provide a unified theoretical formulation of ExtremControl and systematically validate its effectiveness through experiments in both simulation and real-world environments. Building on ExtremControl, we implement a low-latency humanoid teleoperation system that supports both optical motion capture and VR-based motion tracking, achieving end-to-end latency as low as 50ms and enabling highly responsive behaviors such as ping-pong ball balancing, juggling, and real-time return, thereby substantially surpassing the 200ms latency limit observed in prior work.",
        "authors_display": "Chuang Gan Team",
        "pdf_url": "http://arxiv.org/abs/2602.11321",
        "code_url": "https://owenowl.github.io/extremcontrol",
        "date": "2026-02-11",
        "primary_category": "cs.RO",
        "chinese_summary": "针对现有低延迟人形机器人遥操作系统因复杂运动重定向和仅位置PD控制导致的显著延迟问题，本研究提出了ExtremControl框架。该框架通过直接操作人形机器人末端执行器的SE(3)姿态来避免全身重定向，利用笛卡尔空间映射直接将人类运动转换为机器人链节目标，并在底层引入速度前馈控制以支持高响应行为。仿真和真实世界实验验证了其有效性。基于ExtremControl实现的遥操作系统可支持光学动作捕捉和VR运动跟踪，实现了低至50ms的端到端延迟，从而能够进行乒乓球平衡、杂耍和实时回击等高响应行为，显著超越了现有工作中200ms的延迟限制。"
      },
      {
        "paper_id": "2602.11143",
        "title": "APEX: Learning Adaptive High-Platform Traversal for Humanoid Robots",
        "abstract": "Humanoid locomotion has advanced rapidly with deep reinforcement learning (DRL), enabling robust feet-based traversal over uneven terrain. Yet platforms beyond leg length remain largely out of reach because current RL training paradigms often converge to jumping-like solutions that are high-impact, torque-limited, and unsafe for real-world deployment. To address this gap, we propose APEX, a system for perceptive, climbing-based high-platform traversal that composes terrain-conditioned behaviors: climb-up and climb-down at vertical edges, walking or crawling on the platform, and stand-up and lie-down for posture reconfiguration. Central to our approach is a generalized ratchet progress reward for learning contact-rich, goal-reaching maneuvers. It tracks the best-so-far task progress and penalizes non-improving steps, providing dense yet velocity-free supervision that enables efficient exploration under strong safety regularization. Based on this formulation, we train LiDAR-based full-body maneuver policies and reduce the sim-to-real perception gap through a dual strategy: modeling mapping artifacts during training and applying filtering and inpainting to elevation maps during deployment. Finally, we distill all six skills into a single policy that autonomously selects behaviors and transitions based on local geometry and commands. Experiments on a 29-DoF Unitree G1 humanoid demonstrate zero-shot sim-to-real traversal of 0.8 meter platforms (approximately 114% of leg length), with robust adaptation to platform height and initial pose, as well as smooth and stable multi-skill transitions.",
        "authors_display": "Ding Zhao Team",
        "pdf_url": "http://arxiv.org/abs/2602.11143",
        "code_url": "https://apex-humanoid.github.io/",
        "date": "2026-02-11",
        "primary_category": "cs.RO",
        "chinese_summary": "针对深度强化学习在类人机器人高平台跨越中易收敛于高冲击、不安全的跳跃式方案的不足，本研究提出了APEX系统，实现了感知型、基于攀爬的高平台穿越。APEX结合多种地形条件行为，并引入了广义棘轮进度奖励机制来学习接触密集型、目标导向的操纵。通过对LiDAR生成的高程图进行数据增强和过滤，减少了sim-to-real感知差距。最终将六种技能提炼成单一策略。实验结果表明，在Unitree G1机器人上实现了对0.8米平台（腿长约114%）的零样本sim-to-real穿越，并展现出鲁棒的适应性和平滑稳定的多技能转换。"
      },
      {
        "paper_id": "2602.10943",
        "title": "Towards Learning a Generalizable 3D Scene Representation from 2D Observations",
        "abstract": "We introduce a Generalizable Neural Radiance Field approach for predicting 3D workspace occupancy from egocentric robot observations. Unlike prior methods operating in camera-centric coordinates, our model constructs occupancy representations in a global workspace frame, making it directly applicable to robotic manipulation. The model integrates flexible source views and generalizes to unseen object arrangements without scene-specific finetuning. We demonstrate the approach on a humanoid robot and evaluate predicted geometry against 3D sensor ground truth. Trained on 40 real scenes, our model achieves 26mm reconstruction error, including occluded regions, validating its ability to infer complete 3D occupancy beyond traditional stereo vision methods.",
        "authors_display": "Stefan Wermter Team",
        "pdf_url": "http://arxiv.org/abs/2602.10943",
        "code_url": null,
        "date": "2026-02-11",
        "primary_category": "cs.CV",
        "chinese_summary": "为解决现有神经辐射场方法难以直接应用于机器人操作且需要场景特定微调的问题，本研究提出了一种可泛化的神经辐射场方法。该模型在全局工作空间坐标系中构建占用表示，使其能直接应用于机器人操作，并集成了灵活的源视图，能泛化到未见过的物体排列，无需场景特定微调。在类人机器人上进行的验证实验表明，该模型在40个真实场景上训练后实现了26毫米的重建误差，包括被遮挡区域，证明了其推断完整3D占用信息的能力超越了传统立体视觉方法。"
      },
      {
        "paper_id": "2602.08594",
        "title": "MOSAIC: Bridging the Sim-to-Real Gap in Generalist Humanoid Motion Tracking and Teleoperation with Rapid Residual Adaptation",
        "abstract": "Generalist humanoid motion trackers have recently achieved strong simulation metrics by scaling data and training, yet often remain brittle on hardware during sustained teleoperation due to interface- and dynamics-induced errors. We present MOSAIC, an open-source, full-stack system for humanoid motion tracking and whole-body teleoperation across multiple interfaces. MOSAIC first learns a teleoperation-oriented general motion tracker via RL on a multi-source motion bank with adaptive resampling and rewards that emphasize world-frame motion consistency, which is critical for mobile teleoperation. To bridge the sim-to-real interface gap without sacrificing generality, MOSAIC then performs rapid residual adaptation: an interface-specific policy is trained using minimal interface-specific data, and then distilled into the general tracker through an additive residual module, outperforming naive fine-tuning or continual learning. We validate MOSAIC with systematic ablations, out-of-distribution benchmarking, and real-robot experiments demonstrating robust offline motion replay and online long-horizon teleoperation under realistic latency and noise. Project page: baai-humanoid.github.io/MOSAIC.",
        "authors_display": "Alois Knoll Team",
        "pdf_url": "http://arxiv.org/abs/2602.08594",
        "code_url": null,
        "date": "2026-02-11",
        "primary_category": "cs.RO",
        "chinese_summary": "鉴于通用人形运动追踪器在模拟中表现优异，但在实际硬件持续遥操作时易受接口和动力学误差影响，本文提出了开源全栈系统MOSAIC。该系统首先通过强化学习在多源运动库上训练面向遥操作的通用运动追踪器，采用自适应重采样和强调世界坐标系运动一致性的奖励。为弥合模拟到真实世界的接口差距，MOSAIC通过快速残差适应，使用少量接口特定数据训练一个接口特定策略，并通过加性残差模块将其蒸馏到通用追踪器中，优于传统微调方法。实验结果（包括系统消融、分布外基准测试和真实机器人实验）证明，MOSAIC在实际延迟和噪声下能实现稳健的离线运动回放和在线长周期遥操作。"
      },
      {
        "paper_id": "2602.13326",
        "title": "MotionWeaver: Holistic 4D-Anchored Framework for Multi-Humanoid Image Animation",
        "abstract": "Character image animation, which synthesizes videos of reference characters driven by pose sequences, has advanced rapidly but remains largely limited to single-human settings. Existing methods struggle to generalize to multi-humanoid scenarios, which involve diverse humanoid forms, complex interactions, and frequent occlusions. We address this gap with two key innovations. First, we introduce unified motion representations that extract identity-agnostic motions and explicitly bind them to corresponding characters, enabling generalization across diverse humanoid forms and seamless extension to multi-humanoid scenarios. Second, we propose a holistic 4D-anchored paradigm that constructs a shared 4D space to fuse motion representations with video latents, and further reinforces this process with hierarchical 4D-level supervision to better handle interactions and occlusions. We instantiate these ideas in MotionWeaver, an end-to-end framework for multi-humanoid image animation. To support this setting, we curate a 46-hour dataset of multi-human videos with rich interactions, and construct a 300-video benchmark featuring paired humanoid characters. Quantitative and qualitative experiments demonstrate that MotionWeaver not only achieves state-of-the-art results on our benchmark but also generalizes effectively across diverse humanoid forms, complex interactions, and challenging multi-humanoid scenarios.",
        "authors_display": "Weizhan Zhang Team",
        "pdf_url": "http://arxiv.org/abs/2602.13326",
        "code_url": null,
        "date": "2026-02-11",
        "primary_category": "cs.CV",
        "chinese_summary": "鉴于现有角色图像动画方法难以泛化到涉及多样人形形式、复杂交互和频繁遮挡的多人场景，本文提出了MotionWeaver框架。该框架引入统一的运动表示，提取与身份无关的运动并明确绑定到角色，以泛化到多样人形并扩展到多人场景。同时，提出了整体4D锚定范式，构建共享4D空间融合运动与视频潜空间，并通过分层4D级别监督强化交互和遮挡处理。为支持此研究，构建了46小时多人视频数据集和300视频基准。定量和定性实验结果表明，MotionWeaver在自建基准上达到SOTA，并能有效泛化至复杂多人场景。"
      },
      {
        "paper_id": "2602.10106",
        "title": "EgoHumanoid: Unlocking In-the-Wild Loco-Manipulation with Robot-Free Egocentric Demonstration",
        "abstract": "Human demonstrations offer rich environmental diversity and scale naturally, making them an appealing alternative to robot teleoperation. While this paradigm has advanced robot-arm manipulation, its potential for the more challenging, data-hungry problem of humanoid loco-manipulation remains largely unexplored. We present EgoHumanoid, the first framework to co-train a vision-language-action policy using abundant egocentric human demonstrations together with a limited amount of robot data, enabling humanoids to perform loco-manipulation across diverse real-world environments. To bridge the embodiment gap between humans and robots, including discrepancies in physical morphology and viewpoint, we introduce a systematic alignment pipeline spanning from hardware design to data processing. A portable system for scalable human data collection is developed, and we establish practical collection protocols to improve transferability. At the core of our human-to-humanoid alignment pipeline lies two key components. The view alignment reduces visual domain discrepancies caused by camera height and perspective variation. The action alignment maps human motions into a unified, kinematically feasible action space for humanoid control. Extensive real-world experiments demonstrate that incorporating robot-free egocentric data significantly outperforms robot-only baselines by 51\\%, particularly in unseen environments. Our analysis further reveals which behaviors transfer effectively and the potential for scaling human data.",
        "authors_display": "Li Chen Team",
        "pdf_url": "http://arxiv.org/abs/2602.10106",
        "code_url": "https://opendrivelab.com/EgoHumanoid",
        "date": "2026-02-10",
        "primary_category": "cs.RO",
        "chinese_summary": "针对人形机器人动作操作对数据需求高，而现有方法未能充分利用人类演示数据，且存在人机体现差异的问题，本文提出了EgoHumanoid框架。该框架首次利用大量自我中心人类演示和少量机器人数据共同训练视觉-语言-动作策略，使人形机器人能在多样真实世界环境中执行动作操作。通过硬件设计到数据处理的系统对齐流水线，包括视图对齐和动作对齐，成功弥合了人机之间的形态和视角差异。广泛的真实世界实验表明，整合无机器人自我中心数据相比仅机器人基线性能显著提升51%，尤其是在未见过的环境中，且分析揭示了行为有效迁移及扩展人类数据的潜力。"
      },
      {
        "paper_id": "2602.10069",
        "title": "Humanoid Factors: Design Principles for AI Humanoids in Human Worlds",
        "abstract": "Human factors research has long focused on optimizing environments, tools, and systems to account for human performance. Yet, as humanoid robots begin to share our workplaces, homes, and public spaces, the design challenge expands. We must now consider not only factors for humans but also factors for humanoids, since both will coexist and interact within the same environments. Unlike conventional machines, humanoids introduce expectations of human-like behavior, communication, and social presence, which reshape usability, trust, and safety considerations. In this article, we introduce the concept of humanoid factors as a framework structured around four pillars - physical, cognitive, social, and ethical - that shape the development of humanoids to help them effectively coexist and collaborate with humans. This framework characterizes the overlap and divergence between human capabilities and those of general-purpose humanoids powered by AI foundation models. To demonstrate our framework's practical utility, we then apply the framework to evaluate a real-world humanoid control algorithm, illustrating how conventional task completion metrics in robotics overlook key human cognitive and interaction principles. We thus position humanoid factors as a foundational framework for designing, evaluating, and governing sustained human-humanoid coexistence.",
        "authors_display": "Lixiao Huang Team",
        "pdf_url": "http://arxiv.org/abs/2602.10069",
        "code_url": null,
        "date": "2026-02-10",
        "primary_category": "cs.RO",
        "chinese_summary": "随着人形机器人开始与人类共享空间，传统人因工程需要扩展，不仅考虑人类因素，也要考虑人形机器人因素。当前人形机器人带来了人类行为、沟通和社会存在的期望，重塑了可用性、信任和安全。本文引入“人形机器人因素”框架，围绕物理、认知、社会和伦理四大支柱，指导人形机器人开发，使其能有效与人类共存和协作，并表征了人机能力间的重叠与差异。通过评估真实人形机器人控制算法，该框架揭示了传统机器人任务指标如何忽视关键人类认知和交互原则，为设计、评估和管理持续人机共存提供了基础性框架。"
      },
      {
        "paper_id": "2602.09628",
        "title": "TeleGate: Whole-Body Humanoid Teleoperation via Gated Expert Selection with Motion Prior",
        "abstract": "Real-time whole-body teleoperation is a critical method for humanoid robots to perform complex tasks in unstructured environments. However, developing a unified controller that robustly supports diverse human motions remains a significant challenge. Existing methods typically distill multiple expert policies into a single general policy, which often inevitably leads to performance degradation, particularly on highly dynamic motions. This paper presents TeleGate, a unified whole-body teleoperation framework for humanoid robots that achieves high-precision tracking across various motions while avoiding the performance loss inherent in knowledge distillation. Our key idea is to preserve the full capability of domain-specific expert policies by training a lightweight gating network, which dynamically activates experts in real-time based on proprioceptive states and reference trajectories. Furthermore, to compensate for the absence of future reference trajectories in real-time teleoperation, we introduce a VAE-based motion prior module that extracts implicit future motion intent from historical observations, enabling anticipatory control for motions requiring prediction such as jumping and standing up. We conducted empirical evaluations in simulation and also deployed our technique on the Unitree G1 humanoid robot. Using only 2.5 hours of motion capture data for training, our TeleGate achieves high-precision real-time teleoperation across diverse dynamic motions (e.g., running, fall recovery, and jumping), significantly outperforming the baseline methods in both tracking accuracy and success rate.",
        "authors_display": "Rongyun Cao Team",
        "pdf_url": "http://arxiv.org/abs/2602.09628",
        "code_url": null,
        "date": "2026-02-10",
        "primary_category": "cs.RO",
        "chinese_summary": "针对人形机器人实时全身遥操作中，现有方法通过知识蒸馏将多专家策略整合，常导致高动态运动性能下降的挑战，本文提出了TeleGate统一遥操作框架。其核心思想是训练一个轻量级门控网络，根据本体感知状态和参考轨迹实时动态激活领域特定专家策略，从而保留其完整能力，避免知识蒸馏的性能损失。此外，引入基于VAE的运动先验模块，从历史观测中提取未来运动意图，实现预期控制。在模拟和Unitree G1机器人上的实验表明，TeleGate仅需2.5小时训练数据，即在跑步、跌倒恢复和跳跃等多样动态运动中实现了高精度实时遥操作，显著优于基线方法。"
      },
      {
        "paper_id": "2602.08518",
        "title": "Characteristics, Management, and Utilization of Muscles in Musculoskeletal Humanoids: Empirical Study on Kengoro and Musashi",
        "abstract": "Various musculoskeletal humanoids have been developed so far, and numerous studies on control mechanisms have been conducted to leverage the advantages of their biomimetic bodies. However, there has not been sufficient and unified discussion on the diverse properties inherent in these musculoskeletal structures, nor on how to manage and utilize them. Therefore, this study categorizes and analyzes the characteristics of muscles, as well as their management and utilization methods, based on the various research conducted on the musculoskeletal humanoids we have developed, Kengoro and Musashi. We classify the features of the musculoskeletal structure into five properties: Redundancy, Independency, Anisotropy, Variable Moment Arm, and Nonlinear Elasticity. We then organize the diverse advantages and disadvantages of musculoskeletal humanoids that arise from the combination of these properties. In particular, we discuss body schema learning and reflex control, along with muscle grouping and body schema adaptation. Also, we describe the implementation of movements through an integrated system and discuss future challenges and prospects.",
        "authors_display": "Masayuki Inaba Team",
        "pdf_url": "http://arxiv.org/abs/2602.08518",
        "code_url": null,
        "date": "2026-02-09",
        "primary_category": "cs.RO",
        "chinese_summary": "当前肌肉骨骼人形机器人研究中，对其生物仿生结构固有的多样属性及其管理利用方式缺乏统一讨论。本研究基于作者团队开发的Kengoro和Musashi机器人，将肌肉骨骼结构特征归纳为冗余性、独立性、各向异性、可变力臂和非线性弹性五大属性。文章进一步探讨了这些属性组合所带来的优势与劣势，并重点讨论了身体图式学习、反射控制、肌肉分组及身体图式适应等机制。最后，研究阐述了通过集成系统实现运动的实践，并展望了未来的研究挑战。"
      },
      {
        "paper_id": "2602.08370",
        "title": "Learning Human-Like Badminton Skills for Humanoid Robots",
        "abstract": "Realizing versatile and human-like performance in high-demand sports like badminton remains a formidable challenge for humanoid robotics. Unlike standard locomotion or static manipulation, this task demands a seamless integration of explosive whole-body coordination and precise, timing-critical interception. While recent advances have achieved lifelike motion mimicry, bridging the gap between kinematic imitation and functional, physics-aware striking without compromising stylistic naturalness is non-trivial. To address this, we propose Imitation-to-Interaction, a progressive reinforcement learning framework designed to evolve a robot from a \"mimic\" to a capable \"striker.\" Our approach establishes a robust motor prior from human data, distills it into a compact, model-based state representation, and stabilizes dynamics via adversarial priors. Crucially, to overcome the sparsity of expert demonstrations, we introduce a manifold expansion strategy that generalizes discrete strike points into a dense interaction volume. We validate our framework through the mastery of diverse skills, including lifts and drop shots, in simulation. Furthermore, we demonstrate the first zero-shot sim-to-real transfer of anthropomorphic badminton skills to a humanoid robot, successfully replicating the kinetic elegance and functional precision of human athletes in the physical world.",
        "authors_display": "Peng Lu Team",
        "pdf_url": "http://arxiv.org/abs/2602.08370",
        "code_url": null,
        "date": "2026-02-09",
        "primary_category": "cs.RO",
        "chinese_summary": "人形机器人实现羽毛球等高强度运动的类人表现面临巨大挑战，尤其是在运动学模仿与功能性、物理感知击打之间难以兼顾自然风格。为解决此问题，本文提出了Imitation-to-Interaction渐进式强化学习框架，旨在使机器人从“模仿者”进化为“击球手”。该方法通过人类数据建立运动先验，蒸馏到模型化状态表示中，并利用对抗性先验稳定动力学，同时引入流形扩展策略以应对稀疏的专家演示。实验结果显示，该框架在仿真中掌握了多样羽毛球技能，并首次实现了类人羽毛球技能从仿真到真实机器人的零样本迁移，展示了物理世界中的优雅和精准打击。"
      },
      {
        "paper_id": "2602.07506",
        "title": "VividFace: Real-Time and Realistic Facial Expression Shadowing for Humanoid Robots",
        "abstract": "Humanoid facial expression shadowing enables robots to realistically imitate human facial expressions in real time, which is critical for lifelike, facially expressive humanoid robots and affective human-robot interaction. Existing progress in humanoid facial expression imitation remains limited, often failing to achieve either real-time performance or realistic expressiveness due to offline video-based inference designs and insufficient ability to capture and transfer subtle expression details. To address these limitations, we present VividFace, a real-time and realistic facial expression shadowing system for humanoid robots. An optimized imitation framework X2CNet++ enhances expressiveness by fine-tuning the human-to-humanoid facial motion transfer module and introducing a feature-adaptation training strategy for better alignment across different image sources. Real-time shadowing is further enabled by a video-stream-compatible inference pipeline and a streamlined workflow based on asynchronous I/O for efficient communication across devices. VividFace produces vivid humanoid faces by mimicking human facial expressions within 0.05 seconds, while generalizing across diverse facial configurations. Extensive real-world demonstrations validate its practical utility. Videos are available at: https://lipzh5.github.io/VividFace/.",
        "authors_display": "Yang Zhang Team",
        "pdf_url": "http://arxiv.org/abs/2602.07506",
        "code_url": null,
        "date": "2026-02-07",
        "primary_category": "cs.RO",
        "chinese_summary": "人形机器人面部表情实时模仿对于实现逼真、情感丰富的人机交互至关重要，但现有方法常因离线推理和细节捕捉不足而难以同时达到实时性和逼真性。为解决这些局限，本文提出了VividFace，一个实时且逼真的人形机器人面部表情阴影系统。该系统通过优化模仿框架X2CNet++，并引入特征适应训练策略，显著增强了表情表现力；同时，通过视频流兼容推理管线和基于异步I/O的工作流，实现了高效的实时模仿。广泛的真实世界演示验证了VividFace在0.05秒内模仿人类表情并生成生动人形面部的实用能力，且能泛化至多种面部配置。"
      },
      {
        "paper_id": "2602.07439",
        "title": "TextOp: Real-time Interactive Text-Driven Humanoid Robot Motion Generation and Control",
        "abstract": "Recent advances in humanoid whole-body motion tracking have enabled the execution of diverse and highly coordinated motions on real hardware. However, existing controllers are commonly driven either by predefined motion trajectories, which offer limited flexibility when user intent changes, or by continuous human teleoperation, which requires constant human involvement and limits autonomy. This work addresses the problem of how to drive a universal humanoid controller in a real-time and interactive manner. We present TextOp, a real-time text-driven humanoid motion generation and control framework that supports streaming language commands and on-the-fly instruction modification during execution. TextOp adopts a two-level architecture in which a high-level autoregressive motion diffusion model continuously generates short-horizon kinematic trajectories conditioned on the current text input, while a low-level motion tracking policy executes these trajectories on a physical humanoid robot. By bridging interactive motion generation with robust whole-body control, TextOp unlocks free-form intent expression and enables smooth transitions across multiple challenging behaviors such as dancing and jumping, within a single continuous motion execution. Extensive real-robot experiments and offline evaluations demonstrate instant responsiveness, smooth whole-body motion, and precise control. The project page and the open-source code are available at https://text-op.github.io/",
        "authors_display": "Xuelong Li Team",
        "pdf_url": "http://arxiv.org/abs/2602.07439",
        "code_url": "https://text-op.github.io/",
        "date": "2026-02-07",
        "primary_category": "cs.RO",
        "chinese_summary": "现有的人形机器人全身控制器在灵活性和自主性方面存在局限，难以实现实时和交互式驱动。为解决这一问题，本文提出了TextOp，一个实时文本驱动的人形运动生成与控制框架，支持流式语言指令和即时修改。TextOp采用两级架构：高级运动扩散模型根据文本生成短时域轨迹，低级运动跟踪策略则在机器人上执行这些轨迹。广泛的真实机器人实验和离线评估表明，TextOp实现了即时响应、平滑全身运动和精确控制，在舞蹈、跳跃等复杂行为中展现出自由形式的意图表达和流畅过渡。"
      },
      {
        "paper_id": "2602.07434",
        "title": "Bridging Speech, Emotion, and Motion: a VLM-based Multimodal Edge-deployable Framework for Humanoid Robots",
        "abstract": "Effective human-robot interaction requires emotionally rich multimodal expressions, yet most humanoid robots lack coordinated speech, facial expressions, and gestures. Meanwhile, real-world deployment demands on-device solutions that can operate autonomously without continuous cloud connectivity. To bridging \\underline{\\textit{S}}peech, \\underline{\\textit{E}}motion, and \\underline{\\textit{M}}otion, we present \\textit{SeM$^2$}, a Vision Language Model-based framework that orchestrates emotionally coherent multimodal interactions through three key components: a multimodal perception module capturing user contextual cues, a Chain-of-Thought reasoning for response planning, and a novel Semantic-Sequence Aligning Mechanism (SSAM) that ensures precise temporal coordination between verbal content and physical expressions. We implement both cloud-based and \\underline{\\textit{e}}dge-deployed versions (\\textit{SeM$^2_e$}), with the latter knowledge distilled to operate efficiently on edge hardware while maintaining 95\\% of the relative performance. Comprehensive evaluations demonstrate that our approach significantly outperforms unimodal baselines in naturalness, emotional clarity, and modal coherence, advancing socially expressive humanoid robotics for diverse real-world environments.",
        "authors_display": "Miao Li Team",
        "pdf_url": "http://arxiv.org/abs/2602.07434",
        "code_url": null,
        "date": "2026-02-07",
        "primary_category": "cs.RO",
        "chinese_summary": "为解决多数人形机器人缺乏协调的语音、面部表情和手势，以及在设备上自主运行的需求，本文提出了SeM²，一个基于视觉语言模型的框架。SeM²通过多模态感知模块捕捉用户上下文，结合思维链推理规划响应，并利用语义序列对齐机制确保言语内容与物理表达的精确时间协调，从而实现情感一致的多模态交互。研究实现了云端及边缘部署版本，其中边缘版本通过知识蒸馏高效运行。综合评估显示，SeM²在自然度、情感清晰度和模态一致性方面显著优于单模态基线，推动了社交表达性人形机器人在多样现实环境中的应用。"
      },
      {
        "paper_id": "2602.07227",
        "title": "Cerebellar-Inspired Residual Control for Fault Recovery: From Inference-Time Adaptation to Structural Consolidation",
        "abstract": "Robotic policies deployed in real-world environments often encounter post-training faults, where retraining, exploration, or system identification are impractical. We introduce an inference-time, cerebellar-inspired residual control framework that augments a frozen reinforcement learning policy with online corrective actions, enabling fault recovery without modifying base policy parameters. The framework instantiates core cerebellar principles, including high-dimensional pattern separation via fixed feature expansion, parallel microzone-style residual pathways, and local error-driven plasticity with excitatory and inhibitory eligibility traces operating at distinct time scales. These mechanisms enable fast, localized correction under post-training disturbances while avoiding destabilizing global policy updates. A conservative, performance-driven meta-adaptation regulates residual authority and plasticity, preserving nominal behavior and suppressing unnecessary intervention. Experiments on MuJoCo benchmarks under actuator, dynamic, and environmental perturbations show improvements of up to $+66\\%$ on \\texttt{HalfCheetah-v5} and $+53\\%$ on \\texttt{Humanoid-v5} under moderate faults, with graceful degradation under severe shifts and complementary robustness from consolidating persistent residual corrections into policy parameters.",
        "authors_display": "Amit Ranjan Trivedi Team",
        "pdf_url": "http://arxiv.org/abs/2602.07227",
        "code_url": null,
        "date": "2026-02-06",
        "primary_category": "cs.LG",
        "chinese_summary": "针对机器人策略在真实世界部署中常遇到的训练后故障，且不便重新训练的问题，本文提出了一种推理时、受小脑启发的残差控制框架。该框架通过在线校正动作增强冻结的强化学习策略，无需修改基础策略参数即可实现故障恢复。它实例化了小脑核心原理，如高维模式分离、并行残差路径和局部误差驱动可塑性，并通过保守的元适应调节残差权限。实验结果表明，在MuJoCo基准测试中，该框架在执行器、动力学和环境扰动下，对HalfCheetah-v5和Humanoid-v5在适度故障下性能显著提升，并在严重故障下表现出优雅的性能下降。"
      },
      {
        "paper_id": "2602.06827",
        "title": "DynaRetarget: Dynamically-Feasible Retargeting using Sampling-Based Trajectory Optimization",
        "abstract": "In this paper, we introduce DynaRetarget, a complete pipeline for retargeting human motions to humanoid control policies. The core component of DynaRetarget is a novel Sampling-Based Trajectory Optimization (SBTO) framework that refines imperfect kinematic trajectories into dynamically feasible motions. SBTO incrementally advances the optimization horizon, enabling optimization over the entire trajectory for long-horizon tasks. We validate DynaRetarget by successfully retargeting hundreds of humanoid-object demonstrations and achieving higher success rates than the state of the art. The framework also generalizes across varying object properties, such as mass, size, and geometry, using the same tracking objective. This ability to robustly retarget diverse demonstrations opens the door to generating large-scale synthetic datasets of humanoid loco-manipulation trajectories, addressing a major bottleneck in real-world data collection.",
        "authors_display": "Majid Khadiv Team",
        "pdf_url": "http://arxiv.org/abs/2602.06827",
        "code_url": null,
        "date": "2026-02-06",
        "primary_category": "cs.RO",
        "chinese_summary": "将人类运动重定向到人形机器人控制策略并确保其动态可行性是一项挑战。本文介绍了DynaRetarget，一个将人类运动重定向到人形控制策略的完整流程。其核心是新颖的基于采样的轨迹优化（SBTO）框架，该框架能将不完善的运动学轨迹细化为动态可行的运动，并通过逐步推进优化范围来处理长时域任务。DynaRetarget在重定向数百个人形-物体演示中取得了比现有技术更高的成功率，并能泛化到不同物体属性的场景，为生成大规模人形局部操作轨迹合成数据集提供了可能。"
      },
      {
        "paper_id": "2602.06445",
        "title": "ECO: Energy-Constrained Optimization with Reinforcement Learning for Humanoid Walking",
        "abstract": "Achieving stable and energy-efficient locomotion is essential for humanoid robots to operate continuously in real-world applications. Existing MPC and RL approaches often rely on energy-related metrics embedded within a multi-objective optimization framework, which require extensive hyperparameter tuning and often result in suboptimal policies. To address these challenges, we propose ECO (Energy-Constrained Optimization), a constrained RL framework that separates energy-related metrics from rewards, reformulating them as explicit inequality constraints. This method provides a clear and interpretable physical representation of energy costs, enabling more efficient and intuitive hyperparameter tuning for improved energy efficiency. ECO introduces dedicated constraints for energy consumption and reference motion, enforced by the Lagrangian method, to achieve stable, symmetric, and energy-efficient walking for humanoid robots. We evaluated ECO against MPC, standard RL with reward shaping, and four state-of-the-art constrained RL methods. Experiments, including sim-to-sim and sim-to-real transfers on the kid-sized humanoid robot BRUCE, demonstrate that ECO significantly reduces energy consumption compared to baselines while maintaining robust walking performance. These results highlight a substantial advancement in energy-efficient humanoid locomotion. All experimental demonstrations can be found on the project website: https://sites.google.com/view/eco-humanoid.",
        "authors_display": "Yao Su Team",
        "pdf_url": "http://arxiv.org/abs/2602.06445",
        "code_url": null,
        "date": "2026-02-06",
        "primary_category": "cs.RO",
        "chinese_summary": "为解决仿人机器人节能稳定运动中现有方法需大量超参数调优且易导致次优策略的问题，本研究提出了ECO（Energy-Constrained Optimization）约束强化学习框架。该框架将能量指标明确定义为不等式约束，通过拉格朗日法强制执行能量消耗和参考运动约束，从而实现稳定、对称且节能的行走。ECO在仿真和真实机器人（BRUCE）上的实验结果表明，在保持鲁棒行走性能的同时，其能耗显著低于MPC、标准RL及其他SOTA约束RL方法，实现了仿人机器人节能运动的实质性进步。"
      },
      {
        "paper_id": "2602.06382",
        "title": "Now You See That: Learning End-to-End Humanoid Locomotion from Raw Pixels",
        "abstract": "Achieving robust vision-based humanoid locomotion remains challenging due to two fundamental issues: the sim-to-real gap introduces significant perception noise that degrades performance on fine-grained tasks, and training a unified policy across diverse terrains is hindered by conflicting learning objectives. To address these challenges, we present an end-to-end framework for vision-driven humanoid locomotion. For robust sim-to-real transfer, we develop a high-fidelity depth sensor simulation that captures stereo matching artifacts and calibration uncertainties inherent in real-world sensing. We further propose a vision-aware behavior distillation approach that combines latent space alignment with noise-invariant auxiliary tasks, enabling effective knowledge transfer from privileged height maps to noisy depth observations. For versatile terrain adaptation, we introduce terrain-specific reward shaping integrated with multi-critic and multi-discriminator learning, where dedicated networks capture the distinct dynamics and motion priors of each terrain type. We validate our approach on two humanoid platforms equipped with different stereo depth cameras. The resulting policy demonstrates robust performance across diverse environments, seamlessly handling extreme challenges such as high platforms and wide gaps, as well as fine-grained tasks including bidirectional long-term staircase traversal.",
        "authors_display": "Zongwu Xie Team",
        "pdf_url": "http://arxiv.org/abs/2602.06382",
        "code_url": null,
        "date": "2026-02-06",
        "primary_category": "cs.RO",
        "chinese_summary": "为解决基于视觉的仿人机器人鲁棒运动中模拟到真实差距产生的感知噪声和多样地形下学习目标冲突的挑战，本研究提出了一个端到端的视觉驱动运动框架。为实现鲁棒的sim-to-real迁移，该框架开发了高保真深度传感器模拟，并提出了视觉感知行为蒸馏方法。为适应多样地形，引入了地形特定奖励塑形，并结合多评论家和多判别器学习。在配备不同立体深度摄像头的两个仿人机器人平台上，该策略展现出在多样环境中的鲁棒性能，能处理极端挑战和精细任务，包括双向长期楼梯遍历。"
      }
    ]
  }
}